[{"path":"index.html","id":"análisis-de-datos-espaciales-con-r","chapter":"Análisis de datos espaciales con R","heading":"Análisis de datos espaciales con R","text":"\n\n\nEste obra está bajo una\n\nLicencia Creative Commons Atribución-CompartirIgual 4.0 Internacional\n.","code":""},{"path":"prólogo.html","id":"prólogo","chapter":"Prólogo","heading":"Prólogo","text":"Las ciencias regionales, un conjunto amplio de especialidades que estudian los fenómenos sociales, económicos y políticos que tienen una dimensión espacial1, son ya una disciplina menor, pasaron de ser una suerte de espacio para la experimentación de teorías y métodos que tenían cabida en las corrientes principales de los cuerpos científicos plenamente reconocidos y se han ido abriendo paso y consolidando como una disciplina totalmente reconocida. Las ciencias regionales pueden ser entendidas como un conjunto de campos disciplinares en los que el punto en común es el interés dado al espacio (ya sea este entendido como regiones, ciudades, infraestructuras, medio ambiente) en los fenómenos sociales tales como las disparidades socioeconómicas, el crecimiento urbano y regional o el transporte y la logística (Nijkamp Ratajczak 2015). Las ciencias regionales han ido adquiriendo “una amplia orientación multidisciplinaria sobre temas regionales y urbanos, combinando y siendo un complemento la economía regional, geografía social y económica, economía urbana, ciencia del transporte, ciencia ambiental, ciencia política y teoría de la planificación” (Fischer Nijkamp 2014). La realidad es que las ciencias regionales han evolucionado como un campo de investigación en pleno derecho. Tras un largo recorrido que incluso se podría remontar los inicios de la economía como ciencia, comenzando con los trabajos de Adam Smith que trataban algunos elementos relacionados con la localización de actividades comerciales, pasando por los estudios clásicos de Von Thünen, Weber, Lösch (todos ellos relacionados con la denominada teoría de la localización), hasta llegar la consolidación de las ciencias regionales como disciplina gracias al impulso de Isaard que trata de brindar una perspectiva teórica y metodológica coherente con un fuerte soporte empírico desde una perspectiva multidisciplinaria. Las contribuciones de W. Isaard abarcan una multitud de campos tales como la ecología, el estudio de los transportes e incluso el manejo de conflictos sociales.Si bien las ciencias regionales abarcan múltiples líneas temáticas, hay dos de ellas que se constituyen en tópicos clásicos y que podría denominárseles corriente principal de las ciencias regionales: el estudio de las fuerzas de aglomeración y los determinantes de la localización de la actividad. Buena parte de las investigaciones de estos ejes buscan responder preguntas como ¿por qué las actividades económicas se distribuyen de forma homogénea en el espacio?, o bien, ¿qué hace que determinadas actividades se localicen en unos sitios y en otros?En su “Manual de Ciencias regionales” (Handbook Regional Science, Fischer y Nijkamp señalan que quizá sean dos los elementos clave que constituyen la ciencia regional: un enfoque multidisciplinario y un fuerte análisis cuantitativo2. Respecto al primer elemento, si bien es cierto que la economía espacial, urbana y regional, han representado pasos alentadores hacia la aproximación del economista con otros científicos sociales, aquellos suelen mirar hacia otras disciplinas con la frecuencia que exigen las problemáticas sociales. De este modo, aún sigue siendo necesario construir alternativas de formación en la fase final de los estudios de licenciatura que busquen romper la endogamia de la profesión.En tanto, sobre el enfoque cuantitativo que caracteriza las ciencias regionales, es cierto que en todos los planes de estudio de economía encontramos un sólido repertorio de instrumentos de carácter cuantitativo: matemáticas, estadística y, por supuesto, econometría. Además, resulta indudablemente que el soporte empírico que caracteriza las ciencias regionales requiere un tipo particular de datos, los datos espaciales, es decir datos que hacen explícita la ubicación de los fenómenos estudiados en la superficie terrestre, y por tanto, de herramientas específicas para el tratamiento de dichos datos. obstante, las herramientas requeridas para el análisis de la realidad desde una perspectiva espacial siguen siendo escasas dentro de la formación nivel de licenciatura. Estas notas buscan ser una contribución, por mínima que esta sea, para subsanar dicha situación.Este trabajo está, por tanto, fundamentalmente dirigido las estudiantes de licenciatura en economía y campos afines interesados en el desarrollo de habilidades técnicas para el análisis cuantitativo que exigen las ciencias regionales y el enfoque espacial de la economía. Así pues, el objetivo de este libro es guiar la estudiante, desde un enfoque fundamentalmente práctico, en el conocimiento y manejo de técnicas para la exploración, análisis y modelado de información espacial mediante el uso de R, un programa informático de software libre y lenguaje de programación enfocado en el análisis estadístico y visualización de información y RStudio un entorno de desarrollo integrado (IDE) desde donde se puede interactuar con R más eficientemente.Este libro se estructura en este momento, enero de 2023, en 5 capítulos, aunque se busca integrar gradualmente material extra. En el Capítulo 1 se presentan los elementos básicos de R y RStudio través de la utilización del enfoque del análisis exploratorio de datos, es decir, introducimos de lleno la estudiante en el uso del software para plantear y resolver preguntas relativas la estructura de la información utilizada través de diversas herramientas de visualización y manipulación de la información. En el Capítulo 2 se muestra cómo elaborar diversos tipos de mapas coropléticos y la enorme flexibilidad de personalización de estilos que tiene R para tal efecto. En el Capítulo 3 se presentan las herramientas para llevar cabo un análisis exploratorio de datos espaciales donde la estudiante encontrará la manera de definir las interrelaciones que se dan en el espacio través de la construcción de matrices de pesos espaciales y aprenderá sobre la autocorrelación espacial y sus implicaciones en el análisis de la información. En tanto, en el Capítulo 4, se presenta un repaso muy sintético de los modelos de regresión simple enfatizando el problema de la autocorrelación que se puede presentar cuando se estima un modelo lineal con datos espaciales. Finalmente, en el Capítulo 5, mostramos algunas de las diferentes alternativas de modelación econométrica espacial disponibles en R.Todos los ejercicios que se desarrollan en esta versión del libro corresponden la realidad económica y social de la Zona Metropolitana del Valle de México, en la región centro de este país latonoamericano, y están disponibles para que las estudiantes puedan replicar todos los resultados aquí ilustrados. Así, se guia la estudiante través de la exploración de la información epidemiológica relacionada con la primera ola de la pandemia por COVID19 en relación con factores sociodemográficos y económicos de la zona metropolitana más grande de México, en la que residen más de 20 millones de personas.El libro se concibe como un proyecto que puede ser difundido y ampliado, por lo que está licenciado bajo una Licencia Internacional Creative Commons Attribution 4.0. Este proyecto es un proyecto vivo, en permanente construcción y modificación, por lo que todos los comentarios y observaciones serán bienvenidas. Si tienes alguna duda, comentario o encuentras algún error, házmelo saber. Una guía de cómo contribuir este proyecto puede ser encontrada aquí.Finalmente, es necesario mencionar que este libro es más que un esfuerzo de recopilación, sistematización y aplicación que se ha hecho partir de infinidad de materiales que la activa comunidad de R, interesada en el análisis espacial en México y el mundo, comparte desinteresadamente través de internet. Creemos que así debería ser siempre el conocimiento: libre y abierto, como el software que es usado aquí. Así pues, si algún mérito tiene este material es justamente su sentido didáctico que confiamos permita contribuir la formación de profesionales interesandas en las ciencias regionales y en la mejora de las condiciones de vida de las mayorías.","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"r-una-introducción-desde-la-exploración-de-información","chapter":"1 R: Una introducción desde la exploración de información","heading":"1 R: Una introducción desde la exploración de información","text":"En este capítulo buscamos cubrir dos objetivos. El primero corresponde que la estudiante se familiarice con los diversos elementos que componen la intefaz de trabajo en RStudio y conozca la definición de objeto y sus tipos, en el contexto de los lenguajes de programación. El segundo es que conozca cómo desarrollar un análisis exploratorio de datos, través de un tipo de objetos específicos (los dataframe).","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"r-lenguaje-de-programación-y-plataforma-de-análisis","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.1 R: Lenguaje de programación y plataforma de análisis","text":"R es un lenguaje de programación orientado objetos, libre y de código abierto y además un ambiente enfocado al análisis estadístico y gráfico. Que sea libre significa, diferencia de otros programas, que cualquier persona puede usarlo, redistribuirlo o modificarlo, sin necesidad de contar con una licencia pago.R es un entorno de software libre para computación estadística y gráficos. Compila y se ejecuta en una amplia variedad de plataformas UNIX, Windows y MacOS. Fuente:  http://cran.r-project.orgUsar R en lugar de otras plataformas que requieren licencia pago es recomendable al menos por tres razones:Es libre: en contraste con otros populares programas informáticos como lo es STATA, del que una licencia sencilla ronda los 100 dólares por año.R es sólo un software, también es un lenguaje de programación; así, al aprenderlo se obtiene un resultado doble. De modo que cuando llegues manejarlo con profundidad, puedes crear tus propias librerías. sólo sirve para hacer análisis gráfico y estadístico, con el puedes hacer páginas web, ser usado como Sistema de Información Geográfica e incluso para escribir libros, como éste que ahora estas leyendo.Es un lenguaje con un amplio soporte, es decir, con multitud de guías y materiales para el autoaprendizaje.lo largo de este libro usaremos R través de RStudio, un entorno de desarrollo integrado o IDE por sus siglas en inglés. Así que, lo primero será descargar e instalar R y luego descargar e instalar RStudio.R puede ser descargado de aquí. Al seleccionar “download R” hay que elegir cualquier CRAN Mirror (del inglés Comprehensive R Archive Network que son repositorios que contienen una copia del código base de R y sus paquetes). México tiene dos, puedes elegir cualquiera.En tanto, RStudio puede ser descargado de aquí. Ve la sección “Download” en la parte superior y elije la versión gratuita. El navegador debería seleccionar la versión adecuada para tu equipo, sino es así, selecciona del listado que aparece en la parte inferior. También puedes consultar este video para seguir paso paso la instalación de ambos programas.Al abrir RStudio se mostrará una pantalla como la que aparece en la Figura 1.1. La pantalla principal de RStudio está dividida en cuatro partes. En caso de que el recuadro superior izquierdo aparezca, da clic en el ícono de la hoja en blanco con el signo de más (New file, parte superior izquierda de la pantalla) para que se despliegue.\nFigura 1.1: Panel principal\nLas citadas cuatro secciones son:Fuente u origen (source): esta sección, zona superior izquierda, aparecerá una vez que des clic en New script. Aquí podrás escribir las líneas de código de tu proyecto y guardarlas para consultarlas cuando lo desees, visualizarás tus bases de datos organizadas en pestañas y varios otros elementos.Consola: localizada en la parte inferior izquierda, es donde aparece el puntero o prompt (>); este es el espacio en el que se insertarán o escribirán las líneas de código para ser ejecutadas por R y en donde se mostrarán parte de los resultados. Cuando el prompt aparezca, R está listo para recibir tus instrucciones.Archivo: en el espacio inferior derecho aparecen una serie de pestañas que muestran las carpetas y archivos de tu equipo, las gráficas realizadas y los paquetes disponibles, así como la ayuda de R y de sus paquetes.Ambiente: por último el área superior derecha, mostrará las variables creadas, bases de datos cargadas y todos los objetos activos en la sesión de R. Esta sección contiene otras dos pestañas: historial y conexiones. La primera ofrece una memoria de todas las instrucciones escritas en la consola, en tanto, la segunda le será útil cuando haga trabajo colaborativo través de, por ejemplo, GitHub.","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"códigos-objetos-paquetes","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2 Códigos, objetos, paquetes","text":"Los programas informáticos están construidos partir de código, éste puede ser entendido como un lenguaje partir del cual se dan las instrucciones la computadora para que realice alguna acción. Como en todo lenguaje, un aspecto sustantivo es la sintaxis y la ortografía, es decir, el orden y la corrección en la forma en que uno se comunica con el sistema de cómputo, de modo que si se escribe alguna instrucción de forma incorrecta o incompleta (faltas de ortografía o errores de sintaxis) la instrucción se ejecutará.En este sentido, se debe señalar que R es sensible al uso de mayúsculas y minúsculas, esto quiere decir para nuestro programa es lo mismo Árbol y árbol. Además, en este programa constantemente estará usando paréntesis y comas en las instrucciones, por lo que hay que poner especial atención para olvidar colocar alguno de ellos en la sentencia escrita. Otro elemento importante son las llamadas “palabras clave,” Keywords. Éstas son palabras reservadas, es decir, palabras con un significado especial para R y que hacen referencia instrucciones que pertenecen la estructura base del programa R por lo que está recomendado su uso. Algunos ejemplos de palabras reservadas son son: , else, break. Además, el nombre de las variables debe comenzar con caracteres especiales (&,#,$,%) o números.Si bien R es tremendamente potente, puede incluso utilizarse como simple calculadora. Los operadores matemáticos que pueden emplearse en R se muestran en el cuadro 1.1:Cuadro 1.1De esta forma si colocas en la consola el número 5 seguido del signo aritmético correspondiente, por ejemplo un signo de más, +, y luego otro número, R devolverá el resultado de la operación:En R, todos los elementos con los que se interactúa son denominados objetos (por eso se dice que es un lenguaje de programación orientado objetos), los hay de distintas clases y cada clase se puede identificar por un nombre. Los objetos más comunes en R son: constantes, variables, vectores, listas, matrices y arreglos de datos (data frames). Estos últimos los hay en diferentes versiones dependiendo de algunas de sus características.Los objetos pueden contener diferentes tipos de información como: números enteros (integer), valores numéricos (numeric), números complejos (complex) o valores lógicos (logic) y de cadena/texto (character). Distinguir entre los tipos de información que contiene cada objeto es importante para conocer las operaciones que se pueden hacer entre ellos. Por ejemplo, es posible hacer operaciones matemáticas con variables tipo character ya que tiene sentido sumar aritméticamente “casa” y “azul,” pero sí es posible concatenarlos.Para crear objetos, en R se utiliza el signo “menor que” seguido de un guion para formar una suerte de flecha que apunta la izquierda, <-, éste singo es llamado operador de asignación y precisamente asigna contenido objetos, es decir, con el operador <- creamos un objeto que tendrá determinado contenido. Ve la sección consola de tu entorno de trabajo e ingresa las siguientes líneas de instrucciones (o simplemente da click en el ícono de copiar que aparece en la esquina superior derecha del cuadro que contiene el código y pégalo en la consola):Tras haber escrito estas líneas de código notarás que en la sección superior derecha de RStudio aparecen, en la pestaña de ambiente, los tres objetos creados, cada uno con un nombre y contenido diferente entre ellos: x es un objeto numérico, y es una cadena de texto o carácter y z contiene un valor lógico. Basta con escribir el nombre del objeto en la consola para ver su contenido.Ahora, vuelve crear un objeto de nombre x que contenga el resultado de la suma de 5+10:Como es posible identificar, si en una sesión de R se utiliza dos veces el mismo nombre para una variable, se sobrescribe la información asignada esa variable, es decir, sólo permanece el último valor asignado: x ya contiene la primera asignación, 3, solamente la última, 15.Hay dos funciones muy útiles para conocer el tipo de objeto que tenemos en nuestro entorno de trabajo: la función class() y la función str(). La primera indica el tipo de objeto, en tanto, la segunda nos dice el tipo de objeto y su valor. En el siguiente ejemplo puedes ver que el objeto y es de tipo carácter y su contenido es la expresión “Hello World.”Los vectores son una forma de almacenar más de un elemento en un objeto y dichos elementos tienen que ser necesariamente del mismo tipo, aunque es importante tener en cuenta que es recomendable combinar diferentes tipos de objetos porque se pueden alterar las clases de cada uno de los elementos originales. Cuando se quiere crear un vector que contiene cadenas de texto, cada cadena (palabra u oración) se debe poner entre comillas, como se ilustra continuación:El uso de vectores es útil para modificar las etiquetas de una gráfica o bien los encabezados de una tabla, ya que R tomará los valores de texto almacenados en el vector para utilizarlos en la forma deseada. Aquí se muestran otros tipos de objetos en un vector:Imagina que deseas crear un vector numérico con una secuencia numérica del 1 al 10. Para hacerlo, deberías proceder como:Es decir, fue necesario insertar en el vector cada uno de los 10 números, sino que la secuencia continua (del 1 al 10) se señaló con los dos puntos.Las matrices son un tipo de objeto que se distingue porque entre sus propiedades está el de tener dimensión (filas y columnas). Se puede generar una matriz en R con la función matrix() y se procede como:Las funciones, tanto las previamente citadas como esta, requieren que el usuario indique argumentos que son los elementos que aparecen entre paréntesis. La función matrix() requirió indicar varios: los datos que forman el contenido que tendrá la matriz,data=, el número de filas , nrow= y el número de columnas, ncol=. Recuerda: toda función requiere especificar determinados argumentos para que pueda funcionar.Alternativamente se pueden construir matrices partir de vectores. Para ello, se pueden usar las funciones cbind() y rbind(). Primero creamos dos vectores, x y y:La función cbind() permite “unir por columna” (column bind) y rbind() que “une por fila” (row bind). Así, con los vectores previamente construidos podemos tener dos matrices diferentes, esto dependerá si las unimos por fila:o columna:Son un tipo especial de vectores y son utilizados para representar información categórica, lo que permite organizarla en niveles para analizarla mejor. Para mejor entender esto, basta recordar la definición de variable. Las “variables” o “atributos” son medidas o características de los elementos que conforman la población. Las medidas son cuantitativas y las características son cuantitativas. Justamente los factores se refieren variables que recogen cualidades. Por ejemplo, si la variable responde la pregunta “¿el paciente recibió la vacuna contra COVID-19?” y hay dos posibles respuestas, es recomendable que en lugar de designar cada alernativa como 1 o 2 se usen variables categóricas como “Vacuna aplicada” y “Vacuna aplicada,” es decir, es recomendable almacenar la información como factor:Esta estructura de datos es la más usada para realizar análisis en R. Son estructuras de datos de dos dimensiones, es decir, están compuestas por filas y columnas. Los renglones de un data frame admiten datos de distintos tipos, pero sus columnas tienen la restricción de contener datos sólo de un tipo. Para comprender mejor esto piensa en un data frame como si de una hoja de cálculo se tratara: los renglones representan casos, individuos u observaciones, mientras que las columnas representan atributos, rasgos o variables. Una columna con la variable “ingreso” deberá ser del mismo tipo para todos los casos, por ejemplo un valor numérico, en tanto, para el caso o individuo 1 (fila 1) tendremos información relativa sólo al ingreso, sino al sexo, estatura y edad, es decir, variables categóricas, numéricas y enteros, respectivamente.Cuando arribemos un poco más adelante al proceso de importación de información al entorno de trabajo se presentará cómo luce un arreglo de datos de estas características.Se dijo que R es un programa especializado en el análisis estadístico y la representación gráfica, pero R sólo se limita lo que ofrece cuando lo descarga por primera vez, es decir su paquete base o paquete base. Por ejemplo, dentro del paquete base se cuenta con la función para calcular la media de una muestra, mean(), entre otras tantas.Uno de los elementos que hace de R una potente herramienta es la posibilidad de ampliar su potencial través de la instalación de paquetes que expanden sus funciones básicas. Existen paquetes de R para múltiples campos disciplinares y especialidades, por ejemplo estas notas se sirven de los paquetes especializados en el análisis espacial, como se observará en los capítulos subsecuentes. De momento mencionemos sólo un par:tmap: ofrece un enfoque flexible, basado en capas y fácil de usar para crear mapas temáticos.spatialreg: paquete para la elaboración de regresiones con componentes espaciales.Los paquetes, que expanden las capaciades de la versión base de R, son elaborados por múltiples colaboradores. Para conocer quines están detrás de ello, podemos usar la función citation(), que nos muestra cómo dar crédito los autores de los paquetes cuando los usamos.Para poder hacer uso de los paquetes que amplían el potencial de R es necesario descargarlos, instalarlos y, en cada sesión de trabajo, llamarlos. Para la descarga e instalación podemos ir Archivo (File) en la cinta de menú que se localiza en la parte superior de tu entorno de trabajo y seleccionar tools/install package. continuación, deberás colocar el nombre del paquete deseado y dar click en instalar. O bien, alternativamente, puedes ir la sección de Archivo, en la zona inferior derecha, seleccionar la pestaña Packages y continuación el ícono de Install. Una tercera manera de instalar paquetes es desde la consola (sección inferior izquierda), para instalar un paquete la vez:O bien, varios paquetes la vez:Tras el proceso de instalación, en tu consola, R informará sobre el resultado de la instalación y te ofrecerá algunos datos sobre la ubicación del paquete en tu equipo. Para poder hacer uso de los paquetes basta con descargarlos e instalarlos, es necesario “llamarlos” en cada sesión de trabajo de R para ser utilizados. Para ello deberás usar la función library() y como argumento el nombre del paquete:Cada uno de los paquetes y funciones de R está acompañado por materiales de referencia que explican con detalle su uso, así como los diferentes argumentos en el caso de las funciones. Para solicitar ayuda en R puedes recurrir la función help() e indicar como argumento el nombre de la función:Alternativamente, para solicitar ayuda puedes escribir un signo de interrogación y el nombre del paquete o dos signos para una función:También puedes buscar ayuda de una función específica, por ejemplo, de las funciones para crear una matriz (matrix())o para calcular una media (mean()):En la cinta de menú, en la sección ayuda, Help, encontrarás una serie de materiales muy útiles para familiarizarse con los paquetes instalados. Estos materiales reciben el nombre de “hojas de trucos,” Cheatsheet. Se recomienda ampliamente revisar cada una de ellas, incluidas las versiones en epañol. Además, R es un programa con un sin número de entusiastas usuarios y con un amplio soporte técnico por lo que cuando te encuentres con una dificultad para usar algún paquete o función puedes remitite al sitio de Ayuda del R, o bien, alguno de los repositorios especializados para presentar y resolver dudas como Stackoverflow. Para ilustrar esto, ingresa al sitio de Stackoverflow y coloca en la búsqueda “histograma en R.”\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2023-02-06.book built bookdown R package.","code":"\n5+10## [1] 15\n#Nota: el símbolo de # representa un comentario y no se ejecuta con el código\nx <- 3 #Numérico\ny <- \"Hola\" #Carácter \nz <- FALSE # Lógico\nx <- 5+10\nclass(y)## [1] \"character\"\nstr(y)##  chr \"Hola\"\nx <- c(\"a\",\"v\")\nx## [1] \"a\" \"v\"\nx <- c(\"a\",\"v\")\nx## [1] \"a\" \"v\"\nx <- c(TRUE,FALSE)\nx## [1]  TRUE FALSE\nx<- c(4+5i,3+2i)\nx## [1] 4+5i 3+2i\nx<-c(1:10)\nx##  [1]  1  2  3  4  5  6  7  8  9 10\nm <- matrix(data=1:6,nrow = 2,ncol = 3)\nx<-1:3\nx## [1] 1 2 3\ny<-10:12\ny## [1] 10 11 12\ncbind(x,y)##      x  y\n## [1,] 1 10\n## [2,] 2 11\n## [3,] 3 12\nrbind(x,y)##   [,1] [,2] [,3]\n## x    1    2    3\n## y   10   11   12\nx <- factor(c(\"Vacuna no aplicada\",\"Vacuna no aplicada\",\"Vacuna no aplicada\",\"Vacuna no aplicada\",\"Vacuna no aplicada\"))\nx## [1] Vacuna no aplicada Vacuna no aplicada Vacuna no aplicada Vacuna no aplicada\n## [5] Vacuna no aplicada\n## Levels: Vacuna no aplicada\ncitation(\"tmap\")## \n## To cite tmap/tmaptools in publications use:\n## \n##   Tennekes M (2018). \"tmap: Thematic Maps in R.\" _Journal of\n##   Statistical Software_, *84*(6), 1-39. doi:10.18637/jss.v084.i06\n##   <https://doi.org/10.18637/jss.v084.i06>.\n## \n## A BibTeX entry for LaTeX users is\n## \n##   @Article{,\n##     title = {{tmap}: Thematic Maps in {R}},\n##     author = {Martijn Tennekes},\n##     journal = {Journal of Statistical Software},\n##     year = {2018},\n##     volume = {84},\n##     number = {6},\n##     pages = {1--39},\n##     doi = {10.18637/jss.v084.i06},\n##   }\ninstall.packages(\"tmap\")\ninstall.packages(\"spatialreg\")\ninstall.packages(c(\"tmap\", \"spatialreg\"))\nlibrary(tmap)\nhelp(tmap)\n?ggplot\nhelp(matrix)\nhelp(mean)"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"código","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.1 Código","text":"Los programas informáticos están construidos partir de código, éste puede ser entendido como un lenguaje partir del cual se dan las instrucciones la computadora para que realice alguna acción. Como en todo lenguaje, un aspecto sustantivo es la sintaxis y la ortografía, es decir, el orden y la corrección en la forma en que uno se comunica con el sistema de cómputo, de modo que si se escribe alguna instrucción de forma incorrecta o incompleta (faltas de ortografía o errores de sintaxis) la instrucción se ejecutará.En este sentido, se debe señalar que R es sensible al uso de mayúsculas y minúsculas, esto quiere decir para nuestro programa es lo mismo Árbol y árbol. Además, en este programa constantemente estará usando paréntesis y comas en las instrucciones, por lo que hay que poner especial atención para olvidar colocar alguno de ellos en la sentencia escrita. Otro elemento importante son las llamadas “palabras clave,” Keywords. Éstas son palabras reservadas, es decir, palabras con un significado especial para R y que hacen referencia instrucciones que pertenecen la estructura base del programa R por lo que está recomendado su uso. Algunos ejemplos de palabras reservadas son son: , else, break. Además, el nombre de las variables debe comenzar con caracteres especiales (&,#,$,%) o números.","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"objetos-operadores-y-tipos-de-variables","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.2 Objetos, operadores y tipos de variables","text":"Si bien R es tremendamente potente, puede incluso utilizarse como simple calculadora. Los operadores matemáticos que pueden emplearse en R se muestran en el cuadro 1.1:Cuadro 1.1De esta forma si colocas en la consola el número 5 seguido del signo aritmético correspondiente, por ejemplo un signo de más, +, y luego otro número, R devolverá el resultado de la operación:En R, todos los elementos con los que se interactúa son denominados objetos (por eso se dice que es un lenguaje de programación orientado objetos), los hay de distintas clases y cada clase se puede identificar por un nombre. Los objetos más comunes en R son: constantes, variables, vectores, listas, matrices y arreglos de datos (data frames). Estos últimos los hay en diferentes versiones dependiendo de algunas de sus características.Los objetos pueden contener diferentes tipos de información como: números enteros (integer), valores numéricos (numeric), números complejos (complex) o valores lógicos (logic) y de cadena/texto (character). Distinguir entre los tipos de información que contiene cada objeto es importante para conocer las operaciones que se pueden hacer entre ellos. Por ejemplo, es posible hacer operaciones matemáticas con variables tipo character ya que tiene sentido sumar aritméticamente “casa” y “azul,” pero sí es posible concatenarlos.Para crear objetos, en R se utiliza el signo “menor que” seguido de un guion para formar una suerte de flecha que apunta la izquierda, <-, éste singo es llamado operador de asignación y precisamente asigna contenido objetos, es decir, con el operador <- creamos un objeto que tendrá determinado contenido. Ve la sección consola de tu entorno de trabajo e ingresa las siguientes líneas de instrucciones (o simplemente da click en el ícono de copiar que aparece en la esquina superior derecha del cuadro que contiene el código y pégalo en la consola):Tras haber escrito estas líneas de código notarás que en la sección superior derecha de RStudio aparecen, en la pestaña de ambiente, los tres objetos creados, cada uno con un nombre y contenido diferente entre ellos: x es un objeto numérico, y es una cadena de texto o carácter y z contiene un valor lógico. Basta con escribir el nombre del objeto en la consola para ver su contenido.Ahora, vuelve crear un objeto de nombre x que contenga el resultado de la suma de 5+10:Como es posible identificar, si en una sesión de R se utiliza dos veces el mismo nombre para una variable, se sobrescribe la información asignada esa variable, es decir, sólo permanece el último valor asignado: x ya contiene la primera asignación, 3, solamente la última, 15.Hay dos funciones muy útiles para conocer el tipo de objeto que tenemos en nuestro entorno de trabajo: la función class() y la función str(). La primera indica el tipo de objeto, en tanto, la segunda nos dice el tipo de objeto y su valor. En el siguiente ejemplo puedes ver que el objeto y es de tipo carácter y su contenido es la expresión “Hello World.”","code":"\n5+10## [1] 15\n#Nota: el símbolo de # representa un comentario y no se ejecuta con el código\nx <- 3 #Numérico\ny <- \"Hola\" #Carácter \nz <- FALSE # Lógico\nx <- 5+10\nclass(y)## [1] \"character\"\nstr(y)##  chr \"Hola\""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"vectores","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.2.1 Vectores","text":"Los vectores son una forma de almacenar más de un elemento en un objeto y dichos elementos tienen que ser necesariamente del mismo tipo, aunque es importante tener en cuenta que es recomendable combinar diferentes tipos de objetos porque se pueden alterar las clases de cada uno de los elementos originales. Cuando se quiere crear un vector que contiene cadenas de texto, cada cadena (palabra u oración) se debe poner entre comillas, como se ilustra continuación:El uso de vectores es útil para modificar las etiquetas de una gráfica o bien los encabezados de una tabla, ya que R tomará los valores de texto almacenados en el vector para utilizarlos en la forma deseada. Aquí se muestran otros tipos de objetos en un vector:Imagina que deseas crear un vector numérico con una secuencia numérica del 1 al 10. Para hacerlo, deberías proceder como:Es decir, fue necesario insertar en el vector cada uno de los 10 números, sino que la secuencia continua (del 1 al 10) se señaló con los dos puntos.","code":"\nx <- c(\"a\",\"v\")\nx## [1] \"a\" \"v\"\nx <- c(\"a\",\"v\")\nx## [1] \"a\" \"v\"\nx <- c(TRUE,FALSE)\nx## [1]  TRUE FALSE\nx<- c(4+5i,3+2i)\nx## [1] 4+5i 3+2i\nx<-c(1:10)\nx##  [1]  1  2  3  4  5  6  7  8  9 10"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"matrices","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.2.2 Matrices","text":"Las matrices son un tipo de objeto que se distingue porque entre sus propiedades está el de tener dimensión (filas y columnas). Se puede generar una matriz en R con la función matrix() y se procede como:Las funciones, tanto las previamente citadas como esta, requieren que el usuario indique argumentos que son los elementos que aparecen entre paréntesis. La función matrix() requirió indicar varios: los datos que forman el contenido que tendrá la matriz,data=, el número de filas , nrow= y el número de columnas, ncol=. Recuerda: toda función requiere especificar determinados argumentos para que pueda funcionar.Alternativamente se pueden construir matrices partir de vectores. Para ello, se pueden usar las funciones cbind() y rbind(). Primero creamos dos vectores, x y y:La función cbind() permite “unir por columna” (column bind) y rbind() que “une por fila” (row bind). Así, con los vectores previamente construidos podemos tener dos matrices diferentes, esto dependerá si las unimos por fila:o columna:","code":"\nm <- matrix(data=1:6,nrow = 2,ncol = 3)\nx<-1:3\nx## [1] 1 2 3\ny<-10:12\ny## [1] 10 11 12\ncbind(x,y)##      x  y\n## [1,] 1 10\n## [2,] 2 11\n## [3,] 3 12\nrbind(x,y)##   [,1] [,2] [,3]\n## x    1    2    3\n## y   10   11   12"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"factores","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.2.3 Factores","text":"Son un tipo especial de vectores y son utilizados para representar información categórica, lo que permite organizarla en niveles para analizarla mejor. Para mejor entender esto, basta recordar la definición de variable. Las “variables” o “atributos” son medidas o características de los elementos que conforman la población. Las medidas son cuantitativas y las características son cuantitativas. Justamente los factores se refieren variables que recogen cualidades. Por ejemplo, si la variable responde la pregunta “¿el paciente recibió la vacuna contra COVID-19?” y hay dos posibles respuestas, es recomendable que en lugar de designar cada alernativa como 1 o 2 se usen variables categóricas como “Vacuna aplicada” y “Vacuna aplicada,” es decir, es recomendable almacenar la información como factor:","code":"\nx <- factor(c(\"Vacuna no aplicada\",\"Vacuna no aplicada\",\"Vacuna no aplicada\",\"Vacuna no aplicada\",\"Vacuna no aplicada\"))\nx## [1] Vacuna no aplicada Vacuna no aplicada Vacuna no aplicada Vacuna no aplicada\n## [5] Vacuna no aplicada\n## Levels: Vacuna no aplicada"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"data-frame-o-arreglo-de-datos","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.2.4 Data frame o arreglo de datos","text":"Esta estructura de datos es la más usada para realizar análisis en R. Son estructuras de datos de dos dimensiones, es decir, están compuestas por filas y columnas. Los renglones de un data frame admiten datos de distintos tipos, pero sus columnas tienen la restricción de contener datos sólo de un tipo. Para comprender mejor esto piensa en un data frame como si de una hoja de cálculo se tratara: los renglones representan casos, individuos u observaciones, mientras que las columnas representan atributos, rasgos o variables. Una columna con la variable “ingreso” deberá ser del mismo tipo para todos los casos, por ejemplo un valor numérico, en tanto, para el caso o individuo 1 (fila 1) tendremos información relativa sólo al ingreso, sino al sexo, estatura y edad, es decir, variables categóricas, numéricas y enteros, respectivamente.Cuando arribemos un poco más adelante al proceso de importación de información al entorno de trabajo se presentará cómo luce un arreglo de datos de estas características.","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"paquetes","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.3 Paquetes","text":"Se dijo que R es un programa especializado en el análisis estadístico y la representación gráfica, pero R sólo se limita lo que ofrece cuando lo descarga por primera vez, es decir su paquete base o paquete base. Por ejemplo, dentro del paquete base se cuenta con la función para calcular la media de una muestra, mean(), entre otras tantas.Uno de los elementos que hace de R una potente herramienta es la posibilidad de ampliar su potencial través de la instalación de paquetes que expanden sus funciones básicas. Existen paquetes de R para múltiples campos disciplinares y especialidades, por ejemplo estas notas se sirven de los paquetes especializados en el análisis espacial, como se observará en los capítulos subsecuentes. De momento mencionemos sólo un par:tmap: ofrece un enfoque flexible, basado en capas y fácil de usar para crear mapas temáticos.spatialreg: paquete para la elaboración de regresiones con componentes espaciales.Los paquetes, que expanden las capaciades de la versión base de R, son elaborados por múltiples colaboradores. Para conocer quines están detrás de ello, podemos usar la función citation(), que nos muestra cómo dar crédito los autores de los paquetes cuando los usamos.Para poder hacer uso de los paquetes que amplían el potencial de R es necesario descargarlos, instalarlos y, en cada sesión de trabajo, llamarlos. Para la descarga e instalación podemos ir Archivo (File) en la cinta de menú que se localiza en la parte superior de tu entorno de trabajo y seleccionar tools/install package. continuación, deberás colocar el nombre del paquete deseado y dar click en instalar. O bien, alternativamente, puedes ir la sección de Archivo, en la zona inferior derecha, seleccionar la pestaña Packages y continuación el ícono de Install. Una tercera manera de instalar paquetes es desde la consola (sección inferior izquierda), para instalar un paquete la vez:O bien, varios paquetes la vez:Tras el proceso de instalación, en tu consola, R informará sobre el resultado de la instalación y te ofrecerá algunos datos sobre la ubicación del paquete en tu equipo. Para poder hacer uso de los paquetes basta con descargarlos e instalarlos, es necesario “llamarlos” en cada sesión de trabajo de R para ser utilizados. Para ello deberás usar la función library() y como argumento el nombre del paquete:","code":"\ncitation(\"tmap\")## \n## To cite tmap/tmaptools in publications use:\n## \n##   Tennekes M (2018). \"tmap: Thematic Maps in R.\" _Journal of\n##   Statistical Software_, *84*(6), 1-39. doi:10.18637/jss.v084.i06\n##   <https://doi.org/10.18637/jss.v084.i06>.\n## \n## A BibTeX entry for LaTeX users is\n## \n##   @Article{,\n##     title = {{tmap}: Thematic Maps in {R}},\n##     author = {Martijn Tennekes},\n##     journal = {Journal of Statistical Software},\n##     year = {2018},\n##     volume = {84},\n##     number = {6},\n##     pages = {1--39},\n##     doi = {10.18637/jss.v084.i06},\n##   }\ninstall.packages(\"tmap\")\ninstall.packages(\"spatialreg\")\ninstall.packages(c(\"tmap\", \"spatialreg\"))\nlibrary(tmap)"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"solicitar-ayuda-de-un-paquete-o-función-de-r","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.4 Solicitar ayuda de un paquete o función de R","text":"Cada uno de los paquetes y funciones de R está acompañado por materiales de referencia que explican con detalle su uso, así como los diferentes argumentos en el caso de las funciones. Para solicitar ayuda en R puedes recurrir la función help() e indicar como argumento el nombre de la función:Alternativamente, para solicitar ayuda puedes escribir un signo de interrogación y el nombre del paquete o dos signos para una función:También puedes buscar ayuda de una función específica, por ejemplo, de las funciones para crear una matriz (matrix())o para calcular una media (mean()):En la cinta de menú, en la sección ayuda, Help, encontrarás una serie de materiales muy útiles para familiarizarse con los paquetes instalados. Estos materiales reciben el nombre de “hojas de trucos,” Cheatsheet. Se recomienda ampliamente revisar cada una de ellas, incluidas las versiones en epañol. Además, R es un programa con un sin número de entusiastas usuarios y con un amplio soporte técnico por lo que cuando te encuentres con una dificultad para usar algún paquete o función puedes remitite al sitio de Ayuda del R, o bien, alguno de los repositorios especializados para presentar y resolver dudas como Stackoverflow. Para ilustrar esto, ingresa al sitio de Stackoverflow y coloca en la búsqueda “histograma en R.”","code":"\nhelp(tmap)\n?ggplot\nhelp(matrix)\nhelp(mean)"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"cargar-bases-de-datos","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.3 Cargar bases de datos","text":"En R hay algunas bases de datos que acompañan los paquetes que han sido instalados y sirven para ilustrar su funcionamiento. Las bases precargadas se pueden observar con la siguiente función:Verás una nueva pestaña en la sección de Fuente con el nombre de las bases y una breve descripción. Para cargar alguna de ellas basta introducir su nombre como argumento de la función data(), por ejemplo:tidyverse es un conjunto de paquetes diseñados especialmente para la Ciencia de Datos. Algunos de los paquetes de la familia tidyverse que usaremos aquí y un poco más adelante son:\n* ggplot2: es un sistema para crear gráficos basado en la llamada gramática de las gráficas.\n* dplyr: proporciona una serie de funciones para manipulación de datos. * readr: permite leer datos rectangualres provenientes de múltiples formatos.Como más adelante usaremos otros de los paquetes de la familia tidyverse instalaremos todos en este momento:Si deseas aprender cómo usar con detalle los paquetes de la familia tidyverse y elementos básicos de Ciencia de Datos, el libro de Hadley Wickham y Garrett Grolemund, R Data Science es una magnífica opción de la que existe una versión en castellano.Utilizando el paquete readxl de la familia tidyverse es posible cargar una base de datos en el formato de la popular hoja de cálculo de Microsoft Office. partir de aquí introduciremos una notación particular al usar una función en R, lo que te permitirá recordar qué paquete pertenece dicha función. La notación es: “paquete :: función(),” es decir, primero se colocará el nombre del paquete y, separado por dos pares de dos puntos, en el nombre de la función que pertenece dicho paquete (resulta claro que los paquetes están entonces integrados por múltiples funciones).Sigamos los siguientes pasos para cargar la base de datos covid_zmvm.xlsx que contiene información de los casos positivos y defunciones por COVID19 en los municipios de la Zona Metropolitana del Valle de México durante la primera ola de la pandemia, entre marzo septiembre de 2020, así como múltiples variables sociodemográficas y económicas.Llamemos específicamente al paquete que nos interesa:La función para cargar un libro de Excel es read_excel() y el argumento indispensable es la ruta o directorio donde está almacenado nuestro libro, el nombre y extensión del mismo, path. Creemos pues el objeto covid_zmvm:Para que puedas cargar satisfactoriamente la base, deberás sustituir la ruta por el directorio en el que está almacenado el archivo en tu equipo de cómputo. Una función útil para generar la cadena de texto de la ruta es file.choose(), del paquete base de R. Lleva la consola el código:y presiona enter, verás que aparece un cuadro de diálogo en el que deberás seleccionar el archivo deseado y luego pulsar “abrir.” El resultado aparecerá en tu consola como una cadena de texto entre comillas. Usa esa información para leer la base covid_zmvm.Revisa la ayuda de la función read_excel() para comprender todos los argumentos con los que puede operar la función y que te permitirán personalizar la carga de la base de datos en caso de que tengas múltiples hojas en el libro de Excel o desees rangos personalizados para importar.Ejercicio¿Quién es el autor o autora del paquete readxl?¿Quién es el autor o autora del paquete readxl?Descarga la hoja de trucos del paquete desde el sitio de tidyverse y responde, ¿el paquete sirve sólo para cargar información o es posible escribir y almacenar bases de datos con él?Descarga la hoja de trucos del paquete desde el sitio de tidyverse y responde, ¿el paquete sirve sólo para cargar información o es posible escribir y almacenar bases de datos con él?Alternativamente, es posible que la base que deseemos cargar se encuentre en un formato diferente, por ejemplo, una base de datos con valores separados por comas (coma separate values, CSV). Para cargar una base en dicho formato, usaremos el paquete readr, que forma parte del paquete utilsLlama el paquete tu entorno de trabajo:Con la función anterior, file.choose() obtendremos la cadena de texto que indica la ruta del archivo cargar:Y, finalmente, cargamos la información con la función read.csv()Otra manera de cargar archivos de Excel o de texto (formato CSV) es ubicarnos en la sección de ambiente, ventana superior derecha, y seleccionar el ícono Import Dataset. Se desplegará un menú donde habremos de elegir el tipo de archivo que se desee importar. R sólo permite importar archivos de texto o Excel, también bases de SPSS o STATA. Siguiendo con la ilustración relativa libros de Excel, en la ventana que se despliega, habremos de señalar la ruta exacta o directorio donde está nuestro archivo, incluyendo el nombre y extensión de éste. Al pulsar en el botón actulizar (Update) se desplegara una vista previa de la base. Si aparece de forma correcta, seleccionaremos importar (Import), en caso contrario, se deberá modificar las opciones de importación. Después de seleccionar Import Dataset/Excel y elegir la ubicación del archivo que deseas importar, deberías ver en tu pantalla una imagen como la de la Figura 1.3:\nFigura 1.2: Importar datos de excel\n","code":"\ndata()\ndata(CO2)\ninstall.packages(\"tidyverse\")\nlibrary(readxl)\n#Al hacer uso de la notación readxl::read_excel, no es necesario usar library(readxl). Sin embargo, por cuestiones pedagógicas si lo haremos\ncovid_zmvm <- readxl::read_excel(path=\"base de datos\\\\covid_zmvm.xlsx\")\nbase::file.choose()\nlibrary(readr)\nruta <- base::file.choose()\ncovid_zmvm <- utils::read.csv(ruta)"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"cargar-un-archivo-de-excel-xls-xslx","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.3.1 Cargar un archivo de Excel (xls, xslx)","text":"tidyverse es un conjunto de paquetes diseñados especialmente para la Ciencia de Datos. Algunos de los paquetes de la familia tidyverse que usaremos aquí y un poco más adelante son:\n* ggplot2: es un sistema para crear gráficos basado en la llamada gramática de las gráficas.\n* dplyr: proporciona una serie de funciones para manipulación de datos. * readr: permite leer datos rectangualres provenientes de múltiples formatos.Como más adelante usaremos otros de los paquetes de la familia tidyverse instalaremos todos en este momento:Si deseas aprender cómo usar con detalle los paquetes de la familia tidyverse y elementos básicos de Ciencia de Datos, el libro de Hadley Wickham y Garrett Grolemund, R Data Science es una magnífica opción de la que existe una versión en castellano.Utilizando el paquete readxl de la familia tidyverse es posible cargar una base de datos en el formato de la popular hoja de cálculo de Microsoft Office. partir de aquí introduciremos una notación particular al usar una función en R, lo que te permitirá recordar qué paquete pertenece dicha función. La notación es: “paquete :: función(),” es decir, primero se colocará el nombre del paquete y, separado por dos pares de dos puntos, en el nombre de la función que pertenece dicho paquete (resulta claro que los paquetes están entonces integrados por múltiples funciones).Sigamos los siguientes pasos para cargar la base de datos covid_zmvm.xlsx que contiene información de los casos positivos y defunciones por COVID19 en los municipios de la Zona Metropolitana del Valle de México durante la primera ola de la pandemia, entre marzo septiembre de 2020, así como múltiples variables sociodemográficas y económicas.Llamemos específicamente al paquete que nos interesa:La función para cargar un libro de Excel es read_excel() y el argumento indispensable es la ruta o directorio donde está almacenado nuestro libro, el nombre y extensión del mismo, path. Creemos pues el objeto covid_zmvm:Para que puedas cargar satisfactoriamente la base, deberás sustituir la ruta por el directorio en el que está almacenado el archivo en tu equipo de cómputo. Una función útil para generar la cadena de texto de la ruta es file.choose(), del paquete base de R. Lleva la consola el código:y presiona enter, verás que aparece un cuadro de diálogo en el que deberás seleccionar el archivo deseado y luego pulsar “abrir.” El resultado aparecerá en tu consola como una cadena de texto entre comillas. Usa esa información para leer la base covid_zmvm.Revisa la ayuda de la función read_excel() para comprender todos los argumentos con los que puede operar la función y que te permitirán personalizar la carga de la base de datos en caso de que tengas múltiples hojas en el libro de Excel o desees rangos personalizados para importar.Ejercicio¿Quién es el autor o autora del paquete readxl?¿Quién es el autor o autora del paquete readxl?Descarga la hoja de trucos del paquete desde el sitio de tidyverse y responde, ¿el paquete sirve sólo para cargar información o es posible escribir y almacenar bases de datos con él?Descarga la hoja de trucos del paquete desde el sitio de tidyverse y responde, ¿el paquete sirve sólo para cargar información o es posible escribir y almacenar bases de datos con él?","code":"\ninstall.packages(\"tidyverse\")\nlibrary(readxl)\n#Al hacer uso de la notación readxl::read_excel, no es necesario usar library(readxl). Sin embargo, por cuestiones pedagógicas si lo haremos\ncovid_zmvm <- readxl::read_excel(path=\"base de datos\\\\covid_zmvm.xlsx\")\nbase::file.choose()"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"cargar-archivos-separados-por-comas-csv","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.3.2 Cargar archivos separados por comas (csv)","text":"Alternativamente, es posible que la base que deseemos cargar se encuentre en un formato diferente, por ejemplo, una base de datos con valores separados por comas (coma separate values, CSV). Para cargar una base en dicho formato, usaremos el paquete readr, que forma parte del paquete utilsLlama el paquete tu entorno de trabajo:Con la función anterior, file.choose() obtendremos la cadena de texto que indica la ruta del archivo cargar:Y, finalmente, cargamos la información con la función read.csv()","code":"\nlibrary(readr)\nruta <- base::file.choose()\ncovid_zmvm <- utils::read.csv(ruta)"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"cargar-bases-con-la-funcionalidad-importar","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.3.3 Cargar bases con la funcionalidad importar","text":"Otra manera de cargar archivos de Excel o de texto (formato CSV) es ubicarnos en la sección de ambiente, ventana superior derecha, y seleccionar el ícono Import Dataset. Se desplegará un menú donde habremos de elegir el tipo de archivo que se desee importar. R sólo permite importar archivos de texto o Excel, también bases de SPSS o STATA. Siguiendo con la ilustración relativa libros de Excel, en la ventana que se despliega, habremos de señalar la ruta exacta o directorio donde está nuestro archivo, incluyendo el nombre y extensión de éste. Al pulsar en el botón actulizar (Update) se desplegara una vista previa de la base. Si aparece de forma correcta, seleccionaremos importar (Import), en caso contrario, se deberá modificar las opciones de importación. Después de seleccionar Import Dataset/Excel y elegir la ubicación del archivo que deseas importar, deberías ver en tu pantalla una imagen como la de la Figura 1.3:\nFigura 1.2: Importar datos de excel\n","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"exploración-inicial-a-través-de-gráficos","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.4 Exploración inicial a través de gráficos","text":"Ahora que revisado algunas de las diversas maneras de cargar tu información al entorno de trabajo en R, es momento de comenzar analizarla. Si importaste la base de datos través de la función readxl::read_excel() notarás que el nuestra base covid_zmvm un objeto de tipo tbl_df que es un subtipo de data.frame o arreglo de datos como el que se mencionó antes.Las siguientes instrucciones nos ayudarán conocer los nombres de las columnas, la estructura de los datos y la dimensión de la base:Recuerda la notación que mencionamos antes, “paquete::función”; además, si el paquete está cargado, es necesario especificar su nombre al usar la función; al arrancar cada sesión de R, un conjunto de paquetes se cargan de forma automática, dichos paquetes aparecen marcados con una palomita, \\(\\checkmark\\), en la pestaña Packages en la sección de archivo.Ahora bien, si deseas ver la base completa puedes llamarla una nueva pestaña que se visualizará en la sección de fuente. Usa la siguiente instrucción:Alternativamente, puedes llamar la base para que aparezca en la consola, lo que siempre es recomendable si la base es muy grande.En su lugar, podrías preferir ver en la consola sólo los últimos y primeros casos:¿Qué tan grande es la base de datos? Si bien ya la función str() nos brindó información sobre la estructura de la base, puedes conocer el número de filas y columnas que componen tu arreglo de datos con las siguientes funciones:Conviene que se revise el archivo en formato txt que acompaña nuestra base y que opera modo de diccionario para que conocer el significado de cada una de las variables que componen nuestro archivo.Plantemos pues algunas preguntas y busquemos responderlas través de un análisis gráfico:¿Habrá alguna relación entre el número de casos positivos por cada mil habitantes, pos_hab, o muertes por cada mil habitantes, def_hab, con el nivel de estudios promedio de la población por municipio?Si esta relación existe, ¿es más intensa entre las muertes o los casos positivos?Para responder esta pregunta podemos recurrir la construcción de diagramas de dispersión. La elaboración de gráficas dentro de tidyverse corre cargo del paquete ggplot2, una potente librería para la representación visual de información. Este paquete se basa en lo que se denomina “gramática de gráficas” que es otra cosa que un conjunto de reglas coherentes con base en las cuales se elaboran los gráficos (Wickham Grolemund 2016).La construcción de gráficas con ggplot2 se hace partir de una suerte de “capas” o “enunciados.” Cada enunciado constituye una parte de la gráfica. Según la hoja de trucos del paquete (que puedes consultar desde la barra de menú en la opción Help/CheatSheet):ggplot2 se basa en la gramática de los gráficos, la idea de que puedes construir cada gráfico partir de los mismos componentes: un conjunto de datos, un sistema de coordenadas y “geoms,” es decir, marcas visuales que representan puntos de datos.Asi pues, se requieren tres elementos para la construcción de una gráfica con ggplot2: datos, geometría y sistema de coordenadas. Dentro de la geometría se especifican sólo la o las variables representar, también algunos elementos estéticos como colores. En nuestro caso, la información está contenida en la base covid_zmvm pero habrá que especificar qué tipo de gráfica deseamos y sus elementos estéticos. Veamos esto en acción:Tenemos pues dos enunciados o “capas.” La primera corresponde la función ggplot() donde colocamos (casi siempre) el primer elemento requerido: la base de datos de donde tomaremos la o las variables graficar, en tanto, el segundo enunciado corresponde la geometría, geom_ cuya parte después del guion bajo se modificará en función del tipo de gráfica utilizar. Algunos (sólo algunos) tipos de gráficas y su función de geometría asociada en ggplot2 aparecen en el cuadro 1.2:Cuadro 1.2Por favor, revisa la hoja de trucos del paquete para que te familiarices con las otras tantas geometrías y su uso, pues cada geometría requiere diferentes argumentos y admite varias posibles configuraciones. En general, dentro de la función de geometría, geom_, será necesario especificar los elementos estéticos, aes(), entre los que se cuenta la o las variables graficar y cómo serán representadas.Añadamos una “capa” adicional nuestra gráfica, combinando otra geometría como una línea de ajuste (pongamos atención en el uso del signo de más):Puedes ver cómo, al añadir una nueva geometría, fue necesario especificar los elementos estéticos del caso, pero los tres enunciados nos permiten tener una sola gráfica. Observa cómo R en tu consola R indica que estás usando determinado método de suavizamiento, el llamado “loess” que significa locally estimated scatterplot smoothing, por tanto, es fácil deducir que debe haber otros métodos de suavizamiento. Observa las siguientes líneas de código:Aquí, hemos añadido un argumento adicional, method=, para indicar un suavizamiento dado por un modelo lineal, lm, de linear model. Tenemos pues que entre el número de casos positivos y el número promedio de grados cursados hay una aparente relación positiva. ¿Qué hay para el caso de las defunciones:Si bien hay un poco más de dispersión, la aparente relación positiva se mantiene. ¿Qué crees que explique esta asociación? ¿Las personas con más años estudiados son susceptibles de contagiarse más fácil de COVID19? ¿O quizá se relacionará más con las diferencias que nivel territorial se dan entre los niveles de estudio de la población en el Valle de México?EjercicioElabora algunas gráficas de dispersión para responder las siguientes preguntas:¿Qué tipo de asociación lineal hay entre los casos positivos y defunciones por COVID19 con las características de la población según el número de dormitorios de las casas, el número de personas que habitan en cada casa y la densidad de población?¿Qué tipo de asociación lineal hay entre los casos positivos y defunciones por COVID19 con las características de la población según el número de dormitorios de las casas, el número de personas que habitan en cada casa y la densidad de población?¿Encuentras algún tipo de asociación entre las variables COVID19 y las características económicas de los municipios de la Zona Metropolitana del Valle de México?¿Encuentras algún tipo de asociación entre las variables COVID19 y las características económicas de los municipios de la Zona Metropolitana del Valle de México?Quizá lo que te interese sólo sea la asociación entre pares de variables, sino el comportamiento individual de cada variable para conocer la forma de su distribución e identificar algunos valores extremos o atípicos (los llamados ourliers). Para ello, convendrá un tipo de gráfica que permita representar sólo una variable continúa (casos positivos o defunciones por COVID19), tal como un histograma, una gráfica de densidad o de caja.Los tres gráficos dan cuenta que la forma de la distribución muestra un marcado sesgo positivo (la derecha), lo que significa que hay algunos municipios o alcaldías con valores muy altos para esta variable, además de algunos valores extremadamente altos según el diagrama de caja, ¿cuáles podrán ser y qué explicaría este comportamiento atípicamente alto?EjercicioConsulta la ayuda de cada una de las funciones de geometría e intenta:Elaborar un histograma con diferentes categorías (bins). ¿Cambia esto la forma de la distribución?Elaborar un histograma con diferentes categorías (bins). ¿Cambia esto la forma de la distribución?Señalar de un color diferente las observaciones atípicas en el diagrama de caja ¿Hay manera de identificar qué alcaldías o municipios pertenecen dichos valores atípicos?Señalar de un color diferente las observaciones atípicas en el diagrama de caja ¿Hay manera de identificar qué alcaldías o municipios pertenecen dichos valores atípicos?Además, también es posible modificar las etiquetas de los ejes:Nuestra base de datos contiene una variable de tipo categórico, grado de marginación (gm_2020), asociada al índice de marginación (im). Para este tipo de variable puede ser conveniente construir un gráfico de barras especialmente diseñado para variables categóricas echando mano de la geometría geom_bar():Este gráfico revela que, de acuerdo con la clasificación del Consejo Nacional de Población y Vivienda (CONAPO), la gran mayoría de los municipios y alcaldías que componen el Valle de México están catalogados como de muy bajo grado de marginación.EjercicioObtén una gráfica de barras donde se muestre el número de unidades administrativas (municipios o alcaldías) por de cada una de las entidades que integran la Zona Metropolitana del Valle de México.Ahora bien, ¿será posible incorporar en un plano bidimensional una tercera variable? ¿Cómo añadirías un diagrama de dispersión, que relaciona dos variables, una tercera? Este instrumento es veces denominado gráfico de burbujas y si lo piensas con cuidado es una solución muy ingeniosa para añadir información extra sin renunciar la simpleza. Para hacerlo, basta con añadir el argumento size= dentro de los elementos estéticos de la función geom_point():EjercicioResponde lo siguiente:Prueba cambiar la variable que define el tamaño del círculo por nuestra variable categórica grado de marginación, ¿es recomendable usar una variable de este tipo en esta representación?Prueba cambiar la variable que define el tamaño del círculo por nuestra variable categórica grado de marginación, ¿es recomendable usar una variable de este tipo en esta representación?¿Qué papel juega el argumento alpha= en la segunda gráfica?¿Qué papel juega el argumento alpha= en la segunda gráfica?¿Cuál es la diferencia en colocar el argumento color dentro o fuera del grupo de elementos estéticos (aes())? ¿Notaste la diferencia en el par de gráficas anteriores? Prueba cambiar el argumento color dentro de aes() por otra variable y el color de los círculos.¿Cuál es la diferencia en colocar el argumento color dentro o fuera del grupo de elementos estéticos (aes())? ¿Notaste la diferencia en el par de gráficas anteriores? Prueba cambiar el argumento color dentro de aes() por otra variable y el color de los círculos.Finalmente, elaboremos una gráfica que nos ayude sintetizar pares de asociaciones entre variables (diagramas de dispersión) y formas de distribución través de un gráfico muy elegante construido con una extensión del paquete ggplot2: GGally. Para instalarlo, como es usual:Luego, para poder usarlo, lo llamamos al entorno de trabajo:La gráfica que buscamos es una matriz de diagramas de dispersión y la construiremos con la función ggpairs() y sólo dos argumentos: la fuente de datos y las variables que deseamos incluir. Veamos:La matriz de diagramas de dispersión construida con ggpairs es, desde mi personal punto de vista, sólo muy elegante, sino que ofrece elementos informativos en el gráfico que antes teníamos, tales como el coeficiente de correlación lineal y su nivel de significancia.Antes de pasar tratar otro elemento, hay que decir que RStudio permite exportar las gráficas hechas, tanto en diversos formatos de imagen (jpg, png, tiff) como en PDF.EjercicioElabora tu propia matriz de diagramas de dispersión con las variables que, tu juicio, resulten más relevantes para explicar la dinámica tanto de las muertes como de los casos positivos de COVID19.","code":"\nbase::names(covid_zmvm) #Nombres de las variables##  [1] \"cvemun\"     \"cve_mun\"    \"nom_mun\"    \"cve_ent\"    \"nom_ent\"   \n##  [6] \"nom_abr\"    \"nom_zm\"     \"cvm\"        \"ext\"        \"pob20\"     \n## [11] \"pob20_h\"    \"pob20_m\"    \"positivos\"  \"defuncione\" \"pos_mil\"   \n## [16] \"pos_hab\"    \"def_hab\"    \"ss\"         \"ppob_sines\" \"ppob_basi\" \n## [21] \"ppob_media\" \"ppob_sup\"   \"ocviv\"      \"occu\"       \"pintegra4_\"\n## [26] \"pintegra6_\" \"pintegra8_\" \"ppob_5_o_m\" \"ppob_3_o_m\" \"ppob_1\"    \n## [31] \"ppob_1dorm\" \"ppob_2dorm\" \"ppob_3dorm\" \"pviv_ocu5_\" \"pviv_ocu7_\"\n## [36] \"pviv_ocu9_\" \"analf\"      \"sbasc\"      \"vhac\"       \"po2sm\"     \n## [41] \"idh2015\"    \"im\"         \"gm_2020\"    \"grad\"       \"grad_h\"    \n## [46] \"grad_m\"     \"poind\"      \"pocom\"      \"poss\"       \"tmind\"     \n## [51] \"tmcom\"      \"tmss\"       \"rmind\"      \"rmcom\"      \"rmss\"      \n## [56] \"den\"\nutils::str(covid_zmvm) #Estructura de la base de datos## tibble [76 × 56] (S3: tbl_df/tbl/data.frame)\n##  $ cvemun    : chr [1:76] \"09010\" \"09012\" \"09015\" \"09017\" ...\n##  $ cve_mun   : chr [1:76] \"010\" \"012\" \"015\" \"017\" ...\n##  $ nom_mun   : chr [1:76] \"Álvaro Obregón\" \"Tlalpan\" \"Cuauhtémoc\" \"Venustiano Carranza\" ...\n##  $ cve_ent   : chr [1:76] \"09\" \"09\" \"09\" \"09\" ...\n##  $ nom_ent   : chr [1:76] \"Ciudad de México\" \"Ciudad de México\" \"Ciudad de México\" \"Ciudad de México\" ...\n##  $ nom_abr   : chr [1:76] \"CDMX\" \"CDMX\" \"CDMX\" \"CDMX\" ...\n##  $ nom_zm    : chr [1:76] \"Valle de México\" \"Valle de México\" \"Valle de México\" \"Valle de México\" ...\n##  $ cvm       : num [1:76] 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 ...\n##  $ ext       : num [1:76] 96.2 310.4 32.5 33.9 85.8 ...\n##  $ pob20     : num [1:76] 759137 699928 545884 443704 392313 ...\n##  $ pob20_h   : num [1:76] 361007 334877 260951 210118 190190 ...\n##  $ pob20_m   : num [1:76] 398130 365051 284933 233586 202123 ...\n##  $ positivos : num [1:76] 10905 11887 7289 7172 6812 ...\n##  $ defuncione: num [1:76] 868 542 754 585 289 713 648 397 190 391 ...\n##  $ pos_mil   : num [1:76] 14.4 17 13.4 16.2 17.4 ...\n##  $ pos_hab   : num [1:76] 14.4 17 13.4 16.2 17.4 ...\n##  $ def_hab   : num [1:76] 1.143 0.774 1.381 1.318 0.737 ...\n##  $ ss        : num [1:76] 75 71.1 71.6 71.6 72.7 ...\n##  $ ppob_sines: num [1:76] 0.0204 0.0195 0.0119 0.0121 0.0193 ...\n##  $ ppob_basi : num [1:76] 0.41 0.391 0.307 0.379 0.469 ...\n##  $ ppob_media: num [1:76] 0.252 0.253 0.251 0.291 0.299 ...\n##  $ ppob_sup  : num [1:76] 0.314 0.335 0.427 0.316 0.211 ...\n##  $ ocviv     : num [1:76] 3.45 3.44 2.75 3.26 3.67 3.21 3.2 3.74 3.6 2.81 ...\n##  $ occu      : num [1:76] 0.81 0.81 0.72 0.82 0.93 0.77 0.7 0.91 0.81 0.66 ...\n##  $ pintegra4_: num [1:76] 0.648 0.644 0.48 0.609 0.699 ...\n##  $ pintegra6_: num [1:76] 0.252 0.226 0.142 0.219 0.262 ...\n##  $ pintegra8_: num [1:76] 0.1071 0.0864 0.0422 0.0798 0.0982 ...\n##  $ ppob_5_o_m: num [1:76] 0.726 0.73 0.888 0.821 0.792 ...\n##  $ ppob_3_o_m: num [1:76] 0.32 0.315 0.363 0.354 0.354 ...\n##  $ ppob_1    : num [1:76] 0.0369 0.0499 0.0249 0.0247 0.0513 ...\n##  $ ppob_1dorm: num [1:76] 0.211 0.212 0.199 0.202 0.209 ...\n##  $ ppob_2dorm: num [1:76] 0.566 0.547 0.8 0.696 0.614 ...\n##  $ ppob_3dorm: num [1:76] 0.832 0.848 0.947 0.89 0.864 ...\n##  $ pviv_ocu5_: num [1:76] 0.226 0.22 0.127 0.198 0.267 ...\n##  $ pviv_ocu7_: num [1:76] 0.0641 0.0568 0.026 0.0513 0.0701 ...\n##  $ pviv_ocu9_: num [1:76] 0.02334 0.01815 0.00645 0.01593 0.02167 ...\n##  $ analf     : num [1:76] 1.574 1.606 0.953 1.085 1.673 ...\n##  $ sbasc     : num [1:76] 19 18.2 13.5 16.9 20.3 ...\n##  $ vhac      : num [1:76] 15.25 15.19 9.42 14.43 18.3 ...\n##  $ po2sm     : num [1:76] 49.3 61.2 41.4 57.2 65.4 ...\n##  $ idh2015   : num [1:76] 0.815 0.832 0.854 0.826 0.784 0.842 0.868 0.812 0.812 0.888 ...\n##  $ im        : num [1:76] 60.4 59.5 61.3 60.3 59.3 ...\n##  $ gm_2020   : chr [1:76] \"Muy Bajo\" \"Muy Bajo\" \"Muy Bajo\" \"Muy Bajo\" ...\n##  $ grad      : num [1:76] 11.3 11.5 12.4 11.5 10.5 ...\n##  $ grad_h    : num [1:76] 11.5 11.7 12.8 11.7 10.7 ...\n##  $ grad_m    : num [1:76] 11.1 11.4 12.1 11.3 10.4 ...\n##  $ poind     : num [1:76] 0.0912 0.0874 0.1334 0.0714 0.2442 ...\n##  $ pocom     : num [1:76] 0.16 0.185 0.166 0.289 0.394 ...\n##  $ poss      : num [1:76] 0.749 0.728 0.7 0.64 0.361 ...\n##  $ tmind     : num [1:76] 20.32 8.9 27.57 6.57 6.42 ...\n##  $ tmcom     : num [1:76] 5.95 3.48 4.36 2.66 2.12 ...\n##  $ tmss      : num [1:76] 27.69 13.91 24.87 9.94 2.59 ...\n##  $ rmind     : num [1:76] 68.2 116.7 55.9 64.9 78.3 ...\n##  $ rmcom     : num [1:76] 50.6 31.3 45.5 31.2 23.5 ...\n##  $ rmss      : num [1:76] 178.3 36.3 255.6 101.4 23.4 ...\n##  $ den       : num [1:76] 7895 2255 16786 13104 4575 ...\nbase::dim(covid_zmvm) #Dimensiones de la base## [1] 76 56\nutils::View(covid_zmvm)\n#Que es equivalente a:\nView(covid_zmvm)\nbase::print(covid_zmvm)## # A tibble: 76 × 56\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Obre… 09      Ciudad… CDMX    Valle…  9.01  96.2 759137\n##  2 09012  012     Tlalpan      09      Ciudad… CDMX    Valle…  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc   09      Ciudad… CDMX    Valle…  9.01  32.5 545884\n##  4 09017  017     Venustiano … 09      Ciudad… CDMX    Valle…  9.01  33.9 443704\n##  5 09011  011     Tláhuac      09      Ciudad… CDMX    Valle…  9.01  85.8 392313\n##  6 09002  002     Azcapotzalco 09      Ciudad… CDMX    Valle…  9.01  33.5 432205\n##  7 09003  003     Coyoacán     09      Ciudad… CDMX    Valle…  9.01  53.9 614447\n##  8 09013  013     Xochimilco   09      Ciudad… CDMX    Valle…  9.01 118.  442178\n##  9 09004  004     Cuajimalpa … 09      Ciudad… CDMX    Valle…  9.01  71.2 217686\n## 10 09016  016     Miguel Hida… 09      Ciudad… CDMX    Valle…  9.01  46.4 414470\n## # … with 66 more rows, and 46 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, …\nutils::head(covid_zmvm) #Para los primeros casos## # A tibble: 6 × 56\n##   cvemun cve_mun nom_mun       cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##   <chr>  <chr>   <chr>         <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n## 1 09010  010     Álvaro Obreg… 09      Ciudad… CDMX    Valle…  9.01  96.2 759137\n## 2 09012  012     Tlalpan       09      Ciudad… CDMX    Valle…  9.01 310.  699928\n## 3 09015  015     Cuauhtémoc    09      Ciudad… CDMX    Valle…  9.01  32.5 545884\n## 4 09017  017     Venustiano C… 09      Ciudad… CDMX    Valle…  9.01  33.9 443704\n## 5 09011  011     Tláhuac       09      Ciudad… CDMX    Valle…  9.01  85.8 392313\n## 6 09002  002     Azcapotzalco  09      Ciudad… CDMX    Valle…  9.01  33.5 432205\n## # … with 46 more variables: pob20_h <dbl>, pob20_m <dbl>, positivos <dbl>,\n## #   defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>, def_hab <dbl>, ss <dbl>,\n## #   ppob_sines <dbl>, ppob_basi <dbl>, ppob_media <dbl>, ppob_sup <dbl>,\n## #   ocviv <dbl>, occu <dbl>, pintegra4_ <dbl>, pintegra6_ <dbl>,\n## #   pintegra8_ <dbl>, ppob_5_o_m <dbl>, ppob_3_o_m <dbl>, ppob_1 <dbl>,\n## #   ppob_1dorm <dbl>, ppob_2dorm <dbl>, ppob_3dorm <dbl>, pviv_ocu5_ <dbl>,\n## #   pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, analf <dbl>, sbasc <dbl>, vhac <dbl>, …\nutils::tail(covid_zmvm) #Para los últimos casos## # A tibble: 6 × 56\n##   cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm    ext  pob20\n##   <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl>  <dbl>  <dbl>\n## 1 15120  120     Zumpango     15      México  Mex.    Valle…  9.01 224.   2.80e5\n## 2 15121  121     Cuautitlán … 15      México  Mex.    Valle…  9.01 110.   5.55e5\n## 3 15122  122     Valle de Ch… 15      México  Mex.    Valle…  9.01  46.6  3.92e5\n## 4 15125  125     Tonanitla    15      México  Mex.    Valle…  9.01   9.04 1.49e4\n## 5 15058  058     Nezahualcóy… 15      México  Mex.    Valle…  9.01  63.3  1.08e6\n## 6 09005  005     Gustavo A. … 09      Ciudad… CDMX    Valle…  9.01  87.9  1.17e6\n## # … with 46 more variables: pob20_h <dbl>, pob20_m <dbl>, positivos <dbl>,\n## #   defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>, def_hab <dbl>, ss <dbl>,\n## #   ppob_sines <dbl>, ppob_basi <dbl>, ppob_media <dbl>, ppob_sup <dbl>,\n## #   ocviv <dbl>, occu <dbl>, pintegra4_ <dbl>, pintegra6_ <dbl>,\n## #   pintegra8_ <dbl>, ppob_5_o_m <dbl>, ppob_3_o_m <dbl>, ppob_1 <dbl>,\n## #   ppob_1dorm <dbl>, ppob_2dorm <dbl>, ppob_3dorm <dbl>, pviv_ocu5_ <dbl>,\n## #   pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, analf <dbl>, sbasc <dbl>, vhac <dbl>, …\nbase::ncol(covid_zmvm) #Para conocer el número de columnas## [1] 56\nbase::nrow(covid_zmvm) #Para conocer el número de filas## [1] 76\nlibrary(tidyverse)\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab))\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab))+\n  ggplot2::geom_smooth(aes(x=grad,y=pos_hab))\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab))+\n  ggplot2::geom_smooth(aes(x=grad,y=pos_hab), method = \"lm\")\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=def_hab))+\n  ggplot2::geom_smooth(aes(x=grad,y=def_hab), method = \"lm\")\n#Histograma para los casos positivos por cada mil habitantes\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_histogram(aes(x=pos_hab))\n#Gráfico de densidad para los casos positivos por cada mil habitantes\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_density(aes(x=pos_hab))\n#Gráfico de caja para los casos positivos por cada mil habitantes\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_boxplot(aes(x=pos_hab))\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_histogram(aes(x=pos_hab))+\n  ggplot2::labs(x=\"Casos positivos por cada mil habitantes\", y=\"Frecuencia\")## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nggplot2::ggplot(covid_zmvm)+\n ggplot2:: geom_bar(aes(x=gm_2020))+\n  ggplot2::labs(x=\"Grado de marginación 2020\", y=\"Frecuencia\")\n#Si tienes la librería ggplot2 cargada en el enorno de trabajo, alternativamente, puedes escribir:\nggplot(covid_zmvm)+\n geom_bar(aes(x=gm_2020))+\n  labs(x=\"Grado de marginación 2020\", y=\"Frecuencia\")\n#Tamaño del circulo dado por los años estudiados\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab,size=def_hab))\n#Transparencia del círculo dado por los años estudiados\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab,alpha=def_hab, size=def_hab), color=\"navyblue\")\n#Transparencia del círculo dado por los años estudiados\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab,color=def_hab))\ninstall.packages(\"GGally\")\nlibrary(GGally)\n#Definiendo los nombres de las variables\nGGally::ggpairs(data=covid_zmvm, columns = c(\"pos_hab\",\"def_hab\",\"im\",\"grad\"))\n#Indicando el número de columna según el lugar que ocupa en la base\nGGally::ggpairs(data=covid_zmvm, columns = c(16,17,41,43))## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"manipulación-de-la-información","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.5 Manipulación de la información","text":"Tareas relacionadas con la preparación y manipulación de datos, que en una hoja de cálculo como Excel de Office o Calc de LibreOffice, son rutinarias y muy sencillas, veces en R pueden significar un dolor de cabeza. obstante, dentro de los paquetes de la familia tidyverse encontramos dplyr. Este paquete brinda herramientas de manipulación de datos basadas en “gramática,” que es otra cosa que funciones (“verbos”) que permiten seleccionar o filtrar elementos en una base de datos, o bien, agrupar y crear nuevas variables.Si deseas profundizar en el conocimiento de este potente paquete consulta la Introducción al paquete dplyr, o bien, su Viñeta de ayuda.La base de datos que cargamos contiene 55 columnas, es decir, 55 variables. Quizá para su análisis te sean de interés todas, por lo que querrías generar una base de datos más compacta mediante la selección de algunas que son de utilidad. Para llevar cabo dicha tarea echaremos mano de la función select(), pero antes de hacerlo, es necesario introducir el operador “tubería,” pipe (%>%).Este operador sirve para indicar cada fase del proceso por el que un objeto es tratado través de la aplicación de diferentes funciones u operaciones. Por ejemplo, queremos que la base covid_zmvm “pase” por un proceso que consiste en la selección de sólo algunas variables. Comencemos usando el “verbo” seleccionar, select(), para una sola variable, por ejemplo pos_hab:La instrucción anterior es equivalente escribir lo siguiente:Así, usar el operador tubería permite sólo ahorrar la escritura de algunos argumentos, sino simplificar la forma en que las instrucciones dadas R son leídas por un ser humano. Veamos cómo seleccionar ahora, por ejemplo, tres variables: la clave de municipio (cvemun), los casos positivos (pos_hab) y el grado de marginación (gm_2020):Ahora, imagina que queremos todas las variables excepto el nombre de la Zona Metropolitana, nom_zm. Sería ocioso colocar todos los nombres de las variables menos el citado. Para hacer esto más eficiente es necesario introducir solamente un signo de menos (-) antes del nombre de la variable, para de esta sencilla forma seleccionar todas las variables menos nom_zm.En lugar de usar los nombres de las variables, puedes recurrir al número de columna en la que se hallan, contando partir del 1. Por ejemplo, nom_mun es la columna 3 y pos_hab es la columna 16, entonces podríamos escribir:Y si es de nuestro interés seleccionar un número consecutivo de columnas, podemos servirnos de:EjercicioPara finalizar lo relacionado con la selección, genera una nueva base de datos llamada covid que contenga las siguientes variables: cvemun, nom_mun, cve_ent, nom_ent, ext, pos_hab, def_hab, ocviv, occu, ppob_1, ppob_1dorm, im, gm_2020, grad, poind, pocom, poss, tmind, tmcom, tmss, rmind, rmcom, rmss, den, pob20.Si ahora lo que te interesa es seleccionar filas o casos en lugar de columnas, hay que echar mano de otro “verbo,” es decir, de la función filtrar, filter(). Por ejemplo, imagina que deseamos seleccionar sólo los municipios del Estado de México. Estos municipios cumplen con la condición de que todos ellos tienen el valor “México” en la variable nom_ent (nombre de entidad), o bien, tener el valor “15” en el de cve_ent (clave de entidad). Para poder hacer un uso eficiente de la función de filtrado es necesario introducir también otro tipo de operadores, los llamados operadores lógicos, que aparecen en el cuadro 1.3:Cuadro 1.3Entonces, para usar la función filter() y tomando como base la información previa:Para hacer una selección de dos entidades, incluiremos dos expresiones en una misma consulta echando mano del operador lógico “o,” identificado con el signo |:EjercicioSelecciona los municipios con una extensión territorial mayor 84 \\(km^2\\).Selecciona los municipios con una extensión territorial mayor 84 \\(km^2\\).Selecciona los municipios con entre 10 y 20 casos positivos por cada mil habitantes.Selecciona los municipios con entre 10 y 20 casos positivos por cada mil habitantes.Selecciona los municipios del Estado de México que tengan más de 10 años promedio de estudio y menos de 10 casos positivos por cada mil habitantes.Selecciona los municipios del Estado de México que tengan más de 10 años promedio de estudio y menos de 10 casos positivos por cada mil habitantes.¿Qué otros filtros relevantes podrías pensar y elaborar?Una operación útil, cuando se analiza información, tiene que ver con ordenar la base de datos con arreglo al valor de una variable. En R con dplyr de tidyverse esto se hace con la función arrange(), por ejemplo:El resultado de la función anterior ordena nuestra base de menor mayor, con base en la clave municipal. Además, es posible incluir varios criterios de ordenación, por ejemplo, si tuvieras los campos año, mes y día podría ordenarse conforme al calendario. En el caso del ejemplo siguiente, primero se ordena con arreglo la clave de entidad y luego con respecto la clave municipal y luego por la extensión territorial por cada mil habitantes:Si deseas ordenar de forma descendente, es decir, de mayor menor hay que introducir una función adicional, desc(), por ejemplo:La función anterior primero ordena los municipios con base en la clave de municipio de forma ascendente (de menor mayor) y simultáneamente coloca de mayor menor los municipios de acuerdo con su extensión territorial.EjerciciosGenera una nueva base de datos (un nuevo objeto) que contenga sólo las alcaldías de la Ciudad de México y las variables casos por cada mil habitantes, luego ordena esa base de datos con arreglo al número de casos positivos de forma ascendente.Genera una nueva base de datos (un nuevo objeto) que contenga sólo las alcaldías de la Ciudad de México y las variables casos por cada mil habitantes, luego ordena esa base de datos con arreglo al número de casos positivos de forma ascendente.lo mismo que en el inciso anterior, pero para los municipios del Estado de México y las defunciones.lo mismo que en el inciso anterior, pero para los municipios del Estado de México y las defunciones.Para añadir una nueva variable partir de las existentes recurrimos la función mutate(). Por ejemplo, calculemos la variable densidad de casos positivos, es decir, número de casos positivos dividido entre la extensión territorial, dicha variable la llamaremos pos_den. Pero antes, generemos una nueva base de datos que sólo contenga algunas de las variables de la base original:Notarás como la nueva variable aparece al final de la base. Vamos intentar unir varios de los elementos que hemos aprendido hasta este punto para que veas el potencial de uso de la tubería. Vamos crear una nueva variable, la misma que hace un momento, luego, tomaremos la base que contiene la nueva variable y construiremos con ella un diagrama de dispersión con la nueva variable y el número de defunciones por cada mil habitantes:¿Cómo explicarías la relación entre la variable creada, densidad de casos positivos, y las defunciones por cada mil habitables?Ejercicio¿Cómo añadirías más de una nueva variable tu base original?¿Cómo añadirías más de una nueva variable tu base original?Construye la variable “densidad de defunciones” y grafícala en un diagrama de dispersión con la densidad de población? Usa las cañerías.Construye la variable “densidad de defunciones” y grafícala en un diagrama de dispersión con la densidad de población? Usa las cañerías.Quizá lo que te interese sea sólo quedarte con algunas de las variables originales y las nuevas variables creadas partir de la información de la variable original. Para ello es útil la función transmute():El segmento de código anterior genera una nueva base en la que sólo conservamos algunas de las variables originales y añadimos dos nuevas partir de la información original.Quizá una de las funciones más potentes y sencillas que tiene dplyr es la función de resumen (summarise()). Veamos cómo opera. Imagina que deseas un promedio del número de casos que terminaron en muerte:Esto es particularmente útil si se le compara con la función del paquete base de R, summary(), pero si la combinamos con la función de agrupamiento, group_by() la situación cambia. Imagina que queremos el promedio de casos positivos por cada mil habitantes para cada conjunto de municipios de las tres entidades que componen el Valle de México:El en segmento de código anterior primero se agrupa la información con arreglo al criterio de clave de entidad, cve_ent, y luego, para cada grupo, calcula el resultado especificado: un promedio. Dentro de la función de resumen, summarise(), se puede llevar cabo sólo múltiples operaciones sino que estas pueden ser de diferente naturaleza, incluso por ejemplo una suma o una cuenta:En el segmento de código anterior creamos tres variables de resumen: la población total, pob_tot, que es la suma de la población para cada entidad, el promedio del índice de marginación, im_medio y el número de municipios de cada entidad través de una cuenta con la función n().EjercicioConstruye una gráfica de barras donde aparezca el total de población para cada entidad, partir del segmento código anterior.Construye una gráfica de barras donde aparezca el total de población para cada entidad, partir del segmento código anterior.¿Es posible agrupar la información con arreglo otra variable? Construye algunas medidas de resumen con esos grupos y gráficalos.¿Es posible agrupar la información con arreglo otra variable? Construye algunas medidas de resumen con esos grupos y gráficalos.Estas y otras tantas funciones son desarrolladas con todo detalle y de forma muy amena en el citado libro de Wickham y Grolemund, R data science, por lo que su estudio se recomienda ampliamente.En el siguiente capítulo continuamos con la exploración de la información, pero de un tipo particular de información: la información espacial.","code":"\ncovid_zmvm %>% dplyr::select(pos_hab)## # A tibble: 76 × 1\n##    pos_hab\n##      <dbl>\n##  1    14.4\n##  2    17.0\n##  3    13.4\n##  4    16.2\n##  5    17.4\n##  6    17.3\n##  7    14.9\n##  8    17.4\n##  9    18.7\n## 10    13.7\n## # … with 66 more rows\ndplyr::select(covid_zmvm,pos_hab)## # A tibble: 76 × 1\n##    pos_hab\n##      <dbl>\n##  1    14.4\n##  2    17.0\n##  3    13.4\n##  4    16.2\n##  5    17.4\n##  6    17.3\n##  7    14.9\n##  8    17.4\n##  9    18.7\n## 10    13.7\n## # … with 66 more rows\ncovid_zmvm %>% dplyr::select(cvemun,pos_hab,gm_2020)## # A tibble: 76 × 3\n##    cvemun pos_hab gm_2020 \n##    <chr>    <dbl> <chr>   \n##  1 09010     14.4 Muy Bajo\n##  2 09012     17.0 Muy Bajo\n##  3 09015     13.4 Muy Bajo\n##  4 09017     16.2 Muy Bajo\n##  5 09011     17.4 Muy Bajo\n##  6 09002     17.3 Muy Bajo\n##  7 09003     14.9 Muy Bajo\n##  8 09013     17.4 Muy Bajo\n##  9 09004     18.7 Muy Bajo\n## 10 09016     13.7 Muy Bajo\n## # … with 66 more rows\ncovid_zmvm %>% dplyr::select(-nom_zm)## # A tibble: 76 × 55\n##    cvemun cve_mun nom_mun     cve_ent nom_ent nom_abr   cvm   ext  pob20 pob20_h\n##    <chr>  <chr>   <chr>       <chr>   <chr>   <chr>   <dbl> <dbl>  <dbl>   <dbl>\n##  1 09010  010     Álvaro Obr… 09      Ciudad… CDMX     9.01  96.2 759137  361007\n##  2 09012  012     Tlalpan     09      Ciudad… CDMX     9.01 310.  699928  334877\n##  3 09015  015     Cuauhtémoc  09      Ciudad… CDMX     9.01  32.5 545884  260951\n##  4 09017  017     Venustiano… 09      Ciudad… CDMX     9.01  33.9 443704  210118\n##  5 09011  011     Tláhuac     09      Ciudad… CDMX     9.01  85.8 392313  190190\n##  6 09002  002     Azcapotzal… 09      Ciudad… CDMX     9.01  33.5 432205  204950\n##  7 09003  003     Coyoacán    09      Ciudad… CDMX     9.01  53.9 614447  289110\n##  8 09013  013     Xochimilco  09      Ciudad… CDMX     9.01 118.  442178  215452\n##  9 09004  004     Cuajimalpa… 09      Ciudad… CDMX     9.01  71.2 217686  104149\n## 10 09016  016     Miguel Hid… 09      Ciudad… CDMX     9.01  46.4 414470  195467\n## # … with 66 more rows, and 45 more variables: pob20_m <dbl>, positivos <dbl>,\n## #   defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>, def_hab <dbl>, ss <dbl>,\n## #   ppob_sines <dbl>, ppob_basi <dbl>, ppob_media <dbl>, ppob_sup <dbl>,\n## #   ocviv <dbl>, occu <dbl>, pintegra4_ <dbl>, pintegra6_ <dbl>,\n## #   pintegra8_ <dbl>, ppob_5_o_m <dbl>, ppob_3_o_m <dbl>, ppob_1 <dbl>,\n## #   ppob_1dorm <dbl>, ppob_2dorm <dbl>, ppob_3dorm <dbl>, pviv_ocu5_ <dbl>,\n## #   pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, analf <dbl>, sbasc <dbl>, vhac <dbl>, …\ncovid_zmvm %>% dplyr::select(3,16)## # A tibble: 76 × 2\n##    nom_mun               pos_hab\n##    <chr>                   <dbl>\n##  1 Álvaro Obregón           14.4\n##  2 Tlalpan                  17.0\n##  3 Cuauhtémoc               13.4\n##  4 Venustiano Carranza      16.2\n##  5 Tláhuac                  17.4\n##  6 Azcapotzalco             17.3\n##  7 Coyoacán                 14.9\n##  8 Xochimilco               17.4\n##  9 Cuajimalpa de Morelos    18.7\n## 10 Miguel Hidalgo           13.7\n## # … with 66 more rows\ncovid_zmvm %>% dplyr::select(1:10)## # A tibble: 76 × 10\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Obre… 09      Ciudad… CDMX    Valle…  9.01  96.2 759137\n##  2 09012  012     Tlalpan      09      Ciudad… CDMX    Valle…  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc   09      Ciudad… CDMX    Valle…  9.01  32.5 545884\n##  4 09017  017     Venustiano … 09      Ciudad… CDMX    Valle…  9.01  33.9 443704\n##  5 09011  011     Tláhuac      09      Ciudad… CDMX    Valle…  9.01  85.8 392313\n##  6 09002  002     Azcapotzalco 09      Ciudad… CDMX    Valle…  9.01  33.5 432205\n##  7 09003  003     Coyoacán     09      Ciudad… CDMX    Valle…  9.01  53.9 614447\n##  8 09013  013     Xochimilco   09      Ciudad… CDMX    Valle…  9.01 118.  442178\n##  9 09004  004     Cuajimalpa … 09      Ciudad… CDMX    Valle…  9.01  71.2 217686\n## 10 09016  016     Miguel Hida… 09      Ciudad… CDMX    Valle…  9.01  46.4 414470\n## # … with 66 more rows\ncovid_zmvm %>% dplyr::filter(cve_ent==\"15\")## # A tibble: 59 × 56\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 15002  002     Acolman      15      México  Mex.    Valle…  9.01  83.9 171507\n##  2 15009  009     Amecameca    15      México  Mex.    Valle…  9.01 189.   53441\n##  3 15010  010     Apaxco       15      México  Mex.    Valle…  9.01  75.7  31898\n##  4 15011  011     Atenco       15      México  Mex.    Valle…  9.01  84.6  75489\n##  5 15013  013     Atizapán de… 15      México  Mex.    Valle…  9.01  91.1 523674\n##  6 15015  015     Atlautla     15      México  Mex.    Valle…  9.01 162.   31900\n##  7 15016  016     Axapusco     15      México  Mex.    Valle…  9.01 231.   29128\n##  8 15017  017     Ayapango     15      México  Mex.    Valle…  9.01  36.4  10053\n##  9 15020  020     Coacalco de… 15      México  Mex.    Valle…  9.01  35.1 293444\n## 10 15022  022     Cocotitlán   15      México  Mex.    Valle…  9.01  15.0  15107\n## # … with 49 more rows, and 46 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, …\n#O bien\ncovid_zmvm %>% dplyr::filter(nom_ent==\"México\")## # A tibble: 59 × 56\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 15002  002     Acolman      15      México  Mex.    Valle…  9.01  83.9 171507\n##  2 15009  009     Amecameca    15      México  Mex.    Valle…  9.01 189.   53441\n##  3 15010  010     Apaxco       15      México  Mex.    Valle…  9.01  75.7  31898\n##  4 15011  011     Atenco       15      México  Mex.    Valle…  9.01  84.6  75489\n##  5 15013  013     Atizapán de… 15      México  Mex.    Valle…  9.01  91.1 523674\n##  6 15015  015     Atlautla     15      México  Mex.    Valle…  9.01 162.   31900\n##  7 15016  016     Axapusco     15      México  Mex.    Valle…  9.01 231.   29128\n##  8 15017  017     Ayapango     15      México  Mex.    Valle…  9.01  36.4  10053\n##  9 15020  020     Coacalco de… 15      México  Mex.    Valle…  9.01  35.1 293444\n## 10 15022  022     Cocotitlán   15      México  Mex.    Valle…  9.01  15.0  15107\n## # … with 49 more rows, and 46 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, …\ncovid_zmvm %>% dplyr::filter(cve_ent==\"09\"|cve_ent==\"15\")## # A tibble: 75 × 56\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Obre… 09      Ciudad… CDMX    Valle…  9.01  96.2 759137\n##  2 09012  012     Tlalpan      09      Ciudad… CDMX    Valle…  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc   09      Ciudad… CDMX    Valle…  9.01  32.5 545884\n##  4 09017  017     Venustiano … 09      Ciudad… CDMX    Valle…  9.01  33.9 443704\n##  5 09011  011     Tláhuac      09      Ciudad… CDMX    Valle…  9.01  85.8 392313\n##  6 09002  002     Azcapotzalco 09      Ciudad… CDMX    Valle…  9.01  33.5 432205\n##  7 09003  003     Coyoacán     09      Ciudad… CDMX    Valle…  9.01  53.9 614447\n##  8 09013  013     Xochimilco   09      Ciudad… CDMX    Valle…  9.01 118.  442178\n##  9 09004  004     Cuajimalpa … 09      Ciudad… CDMX    Valle…  9.01  71.2 217686\n## 10 09016  016     Miguel Hida… 09      Ciudad… CDMX    Valle…  9.01  46.4 414470\n## # … with 65 more rows, and 46 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, …\ncovid_zmvm %>% dplyr::arrange(cvemun)## # A tibble: 76 × 56\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotzalco 09      Ciudad… CDMX    Valle…  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán     09      Ciudad… CDMX    Valle…  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalpa … 09      Ciudad… CDMX    Valle…  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A. … 09      Ciudad… CDMX    Valle…  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco    09      Ciudad… CDMX    Valle…  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa   09      Ciudad… CDMX    Valle…  9.01 113.  1.84e6\n##  7 09008  008     La Magdalen… 09      Ciudad… CDMX    Valle…  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta   09      Ciudad… CDMX    Valle…  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Obre… 09      Ciudad… CDMX    Valle…  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac      09      Ciudad… CDMX    Valle…  9.01  85.8 3.92e5\n## # … with 66 more rows, and 46 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, …\ncovid_zmvm %>% dplyr::arrange(cvemun,ext)## # A tibble: 76 × 56\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotzalco 09      Ciudad… CDMX    Valle…  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán     09      Ciudad… CDMX    Valle…  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalpa … 09      Ciudad… CDMX    Valle…  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A. … 09      Ciudad… CDMX    Valle…  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco    09      Ciudad… CDMX    Valle…  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa   09      Ciudad… CDMX    Valle…  9.01 113.  1.84e6\n##  7 09008  008     La Magdalen… 09      Ciudad… CDMX    Valle…  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta   09      Ciudad… CDMX    Valle…  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Obre… 09      Ciudad… CDMX    Valle…  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac      09      Ciudad… CDMX    Valle…  9.01  85.8 3.92e5\n## # … with 66 more rows, and 46 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, …\ncovid_zmvm %>% dplyr::arrange(cvemun,desc(ext))## # A tibble: 76 × 56\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotzalco 09      Ciudad… CDMX    Valle…  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán     09      Ciudad… CDMX    Valle…  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalpa … 09      Ciudad… CDMX    Valle…  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A. … 09      Ciudad… CDMX    Valle…  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco    09      Ciudad… CDMX    Valle…  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa   09      Ciudad… CDMX    Valle…  9.01 113.  1.84e6\n##  7 09008  008     La Magdalen… 09      Ciudad… CDMX    Valle…  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta   09      Ciudad… CDMX    Valle…  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Obre… 09      Ciudad… CDMX    Valle…  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac      09      Ciudad… CDMX    Valle…  9.01  85.8 3.92e5\n## # … with 66 more rows, and 46 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, …\ncovid_zmvm %>% dplyr::select(cvemun,nom_mun,positivos,pos_hab,ext) %>%\n  dplyr::mutate(pos_den=positivos/ext)## # A tibble: 76 × 6\n##    cvemun nom_mun               positivos pos_hab   ext pos_den\n##    <chr>  <chr>                     <dbl>   <dbl> <dbl>   <dbl>\n##  1 09010  Álvaro Obregón            10905    14.4  96.2   113. \n##  2 09012  Tlalpan                   11887    17.0 310.     38.3\n##  3 09015  Cuauhtémoc                 7289    13.4  32.5   224. \n##  4 09017  Venustiano Carranza        7172    16.2  33.9   212. \n##  5 09011  Tláhuac                    6812    17.4  85.8    79.4\n##  6 09002  Azcapotzalco               7483    17.3  33.5   223. \n##  7 09003  Coyoacán                   9182    14.9  53.9   170. \n##  8 09013  Xochimilco                 7696    17.4 118.     65.1\n##  9 09004  Cuajimalpa de Morelos      4071    18.7  71.2    57.2\n## 10 09016  Miguel Hidalgo             5669    13.7  46.4   122. \n## # … with 66 more rows\ncovid_zmvm %>% \n  dplyr::mutate(pos_den=positivos/ext) %>% \n  ggplot2::ggplot()+\n    ggplot2::geom_point(aes(x=pos_den,y=def_hab))+\n    ggplot2::geom_smooth(aes(x=pos_den,y=def_hab))+\n    ggplot2::labs(x=\"Densidad de casos positivos\",y=\"Defunciones por cada mil habitantes\")\ncovid_zmvm %>% \n  dplyr::transmute(cvemun,nom_mun,pos_hab,def_hab,pos_den=positivos/ext,def_den=defuncione/ext)## # A tibble: 76 × 6\n##    cvemun nom_mun               pos_hab def_hab pos_den def_den\n##    <chr>  <chr>                   <dbl>   <dbl>   <dbl>   <dbl>\n##  1 09010  Álvaro Obregón           14.4   1.14    113.     9.03\n##  2 09012  Tlalpan                  17.0   0.774    38.3    1.75\n##  3 09015  Cuauhtémoc               13.4   1.38    224.    23.2 \n##  4 09017  Venustiano Carranza      16.2   1.32    212.    17.3 \n##  5 09011  Tláhuac                  17.4   0.737    79.4    3.37\n##  6 09002  Azcapotzalco             17.3   1.65    223.    21.3 \n##  7 09003  Coyoacán                 14.9   1.05    170.    12.0 \n##  8 09013  Xochimilco               17.4   0.898    65.1    3.36\n##  9 09004  Cuajimalpa de Morelos    18.7   0.873    57.2    2.67\n## 10 09016  Miguel Hidalgo           13.7   0.943   122.     8.43\n## # … with 66 more rows\ncovid_zmvm %>%\n  dplyr::summarise(promedio_def=mean(def_hab, na.rm=TRUE))## # A tibble: 1 × 1\n##   promedio_def\n##          <dbl>\n## 1        0.683\ncovid_zmvm %>%\n  dplyr::group_by(cve_ent) %>% \n  summarise(promedio_def=mean(def_hab, na.rm=TRUE), promedio_pos=mean(pos_hab))## # A tibble: 3 × 3\n##   cve_ent promedio_def promedio_pos\n##   <chr>          <dbl>        <dbl>\n## 1 09             1.08         15.8 \n## 2 13             0.790         5.88\n## 3 15             0.572         4.57\ncovid_zmvm %>% \n  dplyr::group_by(cve_ent) %>%\n  dplyr::summarise(pob_tot=sum(pob20),im_medio=mean(im),tot_mun=n())## # A tibble: 3 × 4\n##   cve_ent  pob_tot im_medio tot_mun\n##   <chr>      <dbl>    <dbl>   <int>\n## 1 09       9209944     60.1      16\n## 2 13        168302     59.4       1\n## 3 15      12426269     58.0      59"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"seleccionar-variables","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.5.1 Seleccionar variables","text":"Tareas relacionadas con la preparación y manipulación de datos, que en una hoja de cálculo como Excel de Office o Calc de LibreOffice, son rutinarias y muy sencillas, veces en R pueden significar un dolor de cabeza. obstante, dentro de los paquetes de la familia tidyverse encontramos dplyr. Este paquete brinda herramientas de manipulación de datos basadas en “gramática,” que es otra cosa que funciones (“verbos”) que permiten seleccionar o filtrar elementos en una base de datos, o bien, agrupar y crear nuevas variables.Si deseas profundizar en el conocimiento de este potente paquete consulta la Introducción al paquete dplyr, o bien, su Viñeta de ayuda.La base de datos que cargamos contiene 55 columnas, es decir, 55 variables. Quizá para su análisis te sean de interés todas, por lo que querrías generar una base de datos más compacta mediante la selección de algunas que son de utilidad. Para llevar cabo dicha tarea echaremos mano de la función select(), pero antes de hacerlo, es necesario introducir el operador “tubería,” pipe (%>%).Este operador sirve para indicar cada fase del proceso por el que un objeto es tratado través de la aplicación de diferentes funciones u operaciones. Por ejemplo, queremos que la base covid_zmvm “pase” por un proceso que consiste en la selección de sólo algunas variables. Comencemos usando el “verbo” seleccionar, select(), para una sola variable, por ejemplo pos_hab:La instrucción anterior es equivalente escribir lo siguiente:Así, usar el operador tubería permite sólo ahorrar la escritura de algunos argumentos, sino simplificar la forma en que las instrucciones dadas R son leídas por un ser humano. Veamos cómo seleccionar ahora, por ejemplo, tres variables: la clave de municipio (cvemun), los casos positivos (pos_hab) y el grado de marginación (gm_2020):Ahora, imagina que queremos todas las variables excepto el nombre de la Zona Metropolitana, nom_zm. Sería ocioso colocar todos los nombres de las variables menos el citado. Para hacer esto más eficiente es necesario introducir solamente un signo de menos (-) antes del nombre de la variable, para de esta sencilla forma seleccionar todas las variables menos nom_zm.En lugar de usar los nombres de las variables, puedes recurrir al número de columna en la que se hallan, contando partir del 1. Por ejemplo, nom_mun es la columna 3 y pos_hab es la columna 16, entonces podríamos escribir:Y si es de nuestro interés seleccionar un número consecutivo de columnas, podemos servirnos de:EjercicioPara finalizar lo relacionado con la selección, genera una nueva base de datos llamada covid que contenga las siguientes variables: cvemun, nom_mun, cve_ent, nom_ent, ext, pos_hab, def_hab, ocviv, occu, ppob_1, ppob_1dorm, im, gm_2020, grad, poind, pocom, poss, tmind, tmcom, tmss, rmind, rmcom, rmss, den, pob20.","code":"\ncovid_zmvm %>% dplyr::select(pos_hab)## # A tibble: 76 × 1\n##    pos_hab\n##      <dbl>\n##  1    14.4\n##  2    17.0\n##  3    13.4\n##  4    16.2\n##  5    17.4\n##  6    17.3\n##  7    14.9\n##  8    17.4\n##  9    18.7\n## 10    13.7\n## # … with 66 more rows\ndplyr::select(covid_zmvm,pos_hab)## # A tibble: 76 × 1\n##    pos_hab\n##      <dbl>\n##  1    14.4\n##  2    17.0\n##  3    13.4\n##  4    16.2\n##  5    17.4\n##  6    17.3\n##  7    14.9\n##  8    17.4\n##  9    18.7\n## 10    13.7\n## # … with 66 more rows\ncovid_zmvm %>% dplyr::select(cvemun,pos_hab,gm_2020)## # A tibble: 76 × 3\n##    cvemun pos_hab gm_2020 \n##    <chr>    <dbl> <chr>   \n##  1 09010     14.4 Muy Bajo\n##  2 09012     17.0 Muy Bajo\n##  3 09015     13.4 Muy Bajo\n##  4 09017     16.2 Muy Bajo\n##  5 09011     17.4 Muy Bajo\n##  6 09002     17.3 Muy Bajo\n##  7 09003     14.9 Muy Bajo\n##  8 09013     17.4 Muy Bajo\n##  9 09004     18.7 Muy Bajo\n## 10 09016     13.7 Muy Bajo\n## # … with 66 more rows\ncovid_zmvm %>% dplyr::select(-nom_zm)## # A tibble: 76 × 55\n##    cvemun cve_mun nom_mun     cve_ent nom_ent nom_abr   cvm   ext  pob20 pob20_h\n##    <chr>  <chr>   <chr>       <chr>   <chr>   <chr>   <dbl> <dbl>  <dbl>   <dbl>\n##  1 09010  010     Álvaro Obr… 09      Ciudad… CDMX     9.01  96.2 759137  361007\n##  2 09012  012     Tlalpan     09      Ciudad… CDMX     9.01 310.  699928  334877\n##  3 09015  015     Cuauhtémoc  09      Ciudad… CDMX     9.01  32.5 545884  260951\n##  4 09017  017     Venustiano… 09      Ciudad… CDMX     9.01  33.9 443704  210118\n##  5 09011  011     Tláhuac     09      Ciudad… CDMX     9.01  85.8 392313  190190\n##  6 09002  002     Azcapotzal… 09      Ciudad… CDMX     9.01  33.5 432205  204950\n##  7 09003  003     Coyoacán    09      Ciudad… CDMX     9.01  53.9 614447  289110\n##  8 09013  013     Xochimilco  09      Ciudad… CDMX     9.01 118.  442178  215452\n##  9 09004  004     Cuajimalpa… 09      Ciudad… CDMX     9.01  71.2 217686  104149\n## 10 09016  016     Miguel Hid… 09      Ciudad… CDMX     9.01  46.4 414470  195467\n## # … with 66 more rows, and 45 more variables: pob20_m <dbl>, positivos <dbl>,\n## #   defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>, def_hab <dbl>, ss <dbl>,\n## #   ppob_sines <dbl>, ppob_basi <dbl>, ppob_media <dbl>, ppob_sup <dbl>,\n## #   ocviv <dbl>, occu <dbl>, pintegra4_ <dbl>, pintegra6_ <dbl>,\n## #   pintegra8_ <dbl>, ppob_5_o_m <dbl>, ppob_3_o_m <dbl>, ppob_1 <dbl>,\n## #   ppob_1dorm <dbl>, ppob_2dorm <dbl>, ppob_3dorm <dbl>, pviv_ocu5_ <dbl>,\n## #   pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, analf <dbl>, sbasc <dbl>, vhac <dbl>, …\ncovid_zmvm %>% dplyr::select(3,16)## # A tibble: 76 × 2\n##    nom_mun               pos_hab\n##    <chr>                   <dbl>\n##  1 Álvaro Obregón           14.4\n##  2 Tlalpan                  17.0\n##  3 Cuauhtémoc               13.4\n##  4 Venustiano Carranza      16.2\n##  5 Tláhuac                  17.4\n##  6 Azcapotzalco             17.3\n##  7 Coyoacán                 14.9\n##  8 Xochimilco               17.4\n##  9 Cuajimalpa de Morelos    18.7\n## 10 Miguel Hidalgo           13.7\n## # … with 66 more rows\ncovid_zmvm %>% dplyr::select(1:10)## # A tibble: 76 × 10\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Obre… 09      Ciudad… CDMX    Valle…  9.01  96.2 759137\n##  2 09012  012     Tlalpan      09      Ciudad… CDMX    Valle…  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc   09      Ciudad… CDMX    Valle…  9.01  32.5 545884\n##  4 09017  017     Venustiano … 09      Ciudad… CDMX    Valle…  9.01  33.9 443704\n##  5 09011  011     Tláhuac      09      Ciudad… CDMX    Valle…  9.01  85.8 392313\n##  6 09002  002     Azcapotzalco 09      Ciudad… CDMX    Valle…  9.01  33.5 432205\n##  7 09003  003     Coyoacán     09      Ciudad… CDMX    Valle…  9.01  53.9 614447\n##  8 09013  013     Xochimilco   09      Ciudad… CDMX    Valle…  9.01 118.  442178\n##  9 09004  004     Cuajimalpa … 09      Ciudad… CDMX    Valle…  9.01  71.2 217686\n## 10 09016  016     Miguel Hida… 09      Ciudad… CDMX    Valle…  9.01  46.4 414470\n## # … with 66 more rows"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"filtrar-seleccionar-casos","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.5.2 Filtrar (seleccionar casos)","text":"Si ahora lo que te interesa es seleccionar filas o casos en lugar de columnas, hay que echar mano de otro “verbo,” es decir, de la función filtrar, filter(). Por ejemplo, imagina que deseamos seleccionar sólo los municipios del Estado de México. Estos municipios cumplen con la condición de que todos ellos tienen el valor “México” en la variable nom_ent (nombre de entidad), o bien, tener el valor “15” en el de cve_ent (clave de entidad). Para poder hacer un uso eficiente de la función de filtrado es necesario introducir también otro tipo de operadores, los llamados operadores lógicos, que aparecen en el cuadro 1.3:Cuadro 1.3Entonces, para usar la función filter() y tomando como base la información previa:Para hacer una selección de dos entidades, incluiremos dos expresiones en una misma consulta echando mano del operador lógico “o,” identificado con el signo |:EjercicioSelecciona los municipios con una extensión territorial mayor 84 \\(km^2\\).Selecciona los municipios con una extensión territorial mayor 84 \\(km^2\\).Selecciona los municipios con entre 10 y 20 casos positivos por cada mil habitantes.Selecciona los municipios con entre 10 y 20 casos positivos por cada mil habitantes.Selecciona los municipios del Estado de México que tengan más de 10 años promedio de estudio y menos de 10 casos positivos por cada mil habitantes.Selecciona los municipios del Estado de México que tengan más de 10 años promedio de estudio y menos de 10 casos positivos por cada mil habitantes.¿Qué otros filtros relevantes podrías pensar y elaborar?","code":"\ncovid_zmvm %>% dplyr::filter(cve_ent==\"15\")## # A tibble: 59 × 56\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 15002  002     Acolman      15      México  Mex.    Valle…  9.01  83.9 171507\n##  2 15009  009     Amecameca    15      México  Mex.    Valle…  9.01 189.   53441\n##  3 15010  010     Apaxco       15      México  Mex.    Valle…  9.01  75.7  31898\n##  4 15011  011     Atenco       15      México  Mex.    Valle…  9.01  84.6  75489\n##  5 15013  013     Atizapán de… 15      México  Mex.    Valle…  9.01  91.1 523674\n##  6 15015  015     Atlautla     15      México  Mex.    Valle…  9.01 162.   31900\n##  7 15016  016     Axapusco     15      México  Mex.    Valle…  9.01 231.   29128\n##  8 15017  017     Ayapango     15      México  Mex.    Valle…  9.01  36.4  10053\n##  9 15020  020     Coacalco de… 15      México  Mex.    Valle…  9.01  35.1 293444\n## 10 15022  022     Cocotitlán   15      México  Mex.    Valle…  9.01  15.0  15107\n## # … with 49 more rows, and 46 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, …\n#O bien\ncovid_zmvm %>% dplyr::filter(nom_ent==\"México\")## # A tibble: 59 × 56\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 15002  002     Acolman      15      México  Mex.    Valle…  9.01  83.9 171507\n##  2 15009  009     Amecameca    15      México  Mex.    Valle…  9.01 189.   53441\n##  3 15010  010     Apaxco       15      México  Mex.    Valle…  9.01  75.7  31898\n##  4 15011  011     Atenco       15      México  Mex.    Valle…  9.01  84.6  75489\n##  5 15013  013     Atizapán de… 15      México  Mex.    Valle…  9.01  91.1 523674\n##  6 15015  015     Atlautla     15      México  Mex.    Valle…  9.01 162.   31900\n##  7 15016  016     Axapusco     15      México  Mex.    Valle…  9.01 231.   29128\n##  8 15017  017     Ayapango     15      México  Mex.    Valle…  9.01  36.4  10053\n##  9 15020  020     Coacalco de… 15      México  Mex.    Valle…  9.01  35.1 293444\n## 10 15022  022     Cocotitlán   15      México  Mex.    Valle…  9.01  15.0  15107\n## # … with 49 more rows, and 46 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, …\ncovid_zmvm %>% dplyr::filter(cve_ent==\"09\"|cve_ent==\"15\")## # A tibble: 75 × 56\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Obre… 09      Ciudad… CDMX    Valle…  9.01  96.2 759137\n##  2 09012  012     Tlalpan      09      Ciudad… CDMX    Valle…  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc   09      Ciudad… CDMX    Valle…  9.01  32.5 545884\n##  4 09017  017     Venustiano … 09      Ciudad… CDMX    Valle…  9.01  33.9 443704\n##  5 09011  011     Tláhuac      09      Ciudad… CDMX    Valle…  9.01  85.8 392313\n##  6 09002  002     Azcapotzalco 09      Ciudad… CDMX    Valle…  9.01  33.5 432205\n##  7 09003  003     Coyoacán     09      Ciudad… CDMX    Valle…  9.01  53.9 614447\n##  8 09013  013     Xochimilco   09      Ciudad… CDMX    Valle…  9.01 118.  442178\n##  9 09004  004     Cuajimalpa … 09      Ciudad… CDMX    Valle…  9.01  71.2 217686\n## 10 09016  016     Miguel Hida… 09      Ciudad… CDMX    Valle…  9.01  46.4 414470\n## # … with 65 more rows, and 46 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, …"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"ordenar","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.5.3 Ordenar","text":"Una operación útil, cuando se analiza información, tiene que ver con ordenar la base de datos con arreglo al valor de una variable. En R con dplyr de tidyverse esto se hace con la función arrange(), por ejemplo:El resultado de la función anterior ordena nuestra base de menor mayor, con base en la clave municipal. Además, es posible incluir varios criterios de ordenación, por ejemplo, si tuvieras los campos año, mes y día podría ordenarse conforme al calendario. En el caso del ejemplo siguiente, primero se ordena con arreglo la clave de entidad y luego con respecto la clave municipal y luego por la extensión territorial por cada mil habitantes:Si deseas ordenar de forma descendente, es decir, de mayor menor hay que introducir una función adicional, desc(), por ejemplo:La función anterior primero ordena los municipios con base en la clave de municipio de forma ascendente (de menor mayor) y simultáneamente coloca de mayor menor los municipios de acuerdo con su extensión territorial.EjerciciosGenera una nueva base de datos (un nuevo objeto) que contenga sólo las alcaldías de la Ciudad de México y las variables casos por cada mil habitantes, luego ordena esa base de datos con arreglo al número de casos positivos de forma ascendente.Genera una nueva base de datos (un nuevo objeto) que contenga sólo las alcaldías de la Ciudad de México y las variables casos por cada mil habitantes, luego ordena esa base de datos con arreglo al número de casos positivos de forma ascendente.lo mismo que en el inciso anterior, pero para los municipios del Estado de México y las defunciones.lo mismo que en el inciso anterior, pero para los municipios del Estado de México y las defunciones.","code":"\ncovid_zmvm %>% dplyr::arrange(cvemun)## # A tibble: 76 × 56\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotzalco 09      Ciudad… CDMX    Valle…  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán     09      Ciudad… CDMX    Valle…  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalpa … 09      Ciudad… CDMX    Valle…  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A. … 09      Ciudad… CDMX    Valle…  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco    09      Ciudad… CDMX    Valle…  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa   09      Ciudad… CDMX    Valle…  9.01 113.  1.84e6\n##  7 09008  008     La Magdalen… 09      Ciudad… CDMX    Valle…  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta   09      Ciudad… CDMX    Valle…  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Obre… 09      Ciudad… CDMX    Valle…  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac      09      Ciudad… CDMX    Valle…  9.01  85.8 3.92e5\n## # … with 66 more rows, and 46 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, …\ncovid_zmvm %>% dplyr::arrange(cvemun,ext)## # A tibble: 76 × 56\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotzalco 09      Ciudad… CDMX    Valle…  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán     09      Ciudad… CDMX    Valle…  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalpa … 09      Ciudad… CDMX    Valle…  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A. … 09      Ciudad… CDMX    Valle…  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco    09      Ciudad… CDMX    Valle…  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa   09      Ciudad… CDMX    Valle…  9.01 113.  1.84e6\n##  7 09008  008     La Magdalen… 09      Ciudad… CDMX    Valle…  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta   09      Ciudad… CDMX    Valle…  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Obre… 09      Ciudad… CDMX    Valle…  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac      09      Ciudad… CDMX    Valle…  9.01  85.8 3.92e5\n## # … with 66 more rows, and 46 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, …\ncovid_zmvm %>% dplyr::arrange(cvemun,desc(ext))## # A tibble: 76 × 56\n##    cvemun cve_mun nom_mun      cve_ent nom_ent nom_abr nom_zm   cvm   ext  pob20\n##    <chr>  <chr>   <chr>        <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotzalco 09      Ciudad… CDMX    Valle…  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán     09      Ciudad… CDMX    Valle…  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalpa … 09      Ciudad… CDMX    Valle…  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A. … 09      Ciudad… CDMX    Valle…  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco    09      Ciudad… CDMX    Valle…  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa   09      Ciudad… CDMX    Valle…  9.01 113.  1.84e6\n##  7 09008  008     La Magdalen… 09      Ciudad… CDMX    Valle…  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta   09      Ciudad… CDMX    Valle…  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Obre… 09      Ciudad… CDMX    Valle…  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac      09      Ciudad… CDMX    Valle…  9.01  85.8 3.92e5\n## # … with 66 more rows, and 46 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, …"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"crear-nuevas-variables","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.5.4 Crear nuevas variables","text":"Para añadir una nueva variable partir de las existentes recurrimos la función mutate(). Por ejemplo, calculemos la variable densidad de casos positivos, es decir, número de casos positivos dividido entre la extensión territorial, dicha variable la llamaremos pos_den. Pero antes, generemos una nueva base de datos que sólo contenga algunas de las variables de la base original:Notarás como la nueva variable aparece al final de la base. Vamos intentar unir varios de los elementos que hemos aprendido hasta este punto para que veas el potencial de uso de la tubería. Vamos crear una nueva variable, la misma que hace un momento, luego, tomaremos la base que contiene la nueva variable y construiremos con ella un diagrama de dispersión con la nueva variable y el número de defunciones por cada mil habitantes:¿Cómo explicarías la relación entre la variable creada, densidad de casos positivos, y las defunciones por cada mil habitables?Ejercicio¿Cómo añadirías más de una nueva variable tu base original?¿Cómo añadirías más de una nueva variable tu base original?Construye la variable “densidad de defunciones” y grafícala en un diagrama de dispersión con la densidad de población? Usa las cañerías.Construye la variable “densidad de defunciones” y grafícala en un diagrama de dispersión con la densidad de población? Usa las cañerías.Quizá lo que te interese sea sólo quedarte con algunas de las variables originales y las nuevas variables creadas partir de la información de la variable original. Para ello es útil la función transmute():El segmento de código anterior genera una nueva base en la que sólo conservamos algunas de las variables originales y añadimos dos nuevas partir de la información original.","code":"\ncovid_zmvm %>% dplyr::select(cvemun,nom_mun,positivos,pos_hab,ext) %>%\n  dplyr::mutate(pos_den=positivos/ext)## # A tibble: 76 × 6\n##    cvemun nom_mun               positivos pos_hab   ext pos_den\n##    <chr>  <chr>                     <dbl>   <dbl> <dbl>   <dbl>\n##  1 09010  Álvaro Obregón            10905    14.4  96.2   113. \n##  2 09012  Tlalpan                   11887    17.0 310.     38.3\n##  3 09015  Cuauhtémoc                 7289    13.4  32.5   224. \n##  4 09017  Venustiano Carranza        7172    16.2  33.9   212. \n##  5 09011  Tláhuac                    6812    17.4  85.8    79.4\n##  6 09002  Azcapotzalco               7483    17.3  33.5   223. \n##  7 09003  Coyoacán                   9182    14.9  53.9   170. \n##  8 09013  Xochimilco                 7696    17.4 118.     65.1\n##  9 09004  Cuajimalpa de Morelos      4071    18.7  71.2    57.2\n## 10 09016  Miguel Hidalgo             5669    13.7  46.4   122. \n## # … with 66 more rows\ncovid_zmvm %>% \n  dplyr::mutate(pos_den=positivos/ext) %>% \n  ggplot2::ggplot()+\n    ggplot2::geom_point(aes(x=pos_den,y=def_hab))+\n    ggplot2::geom_smooth(aes(x=pos_den,y=def_hab))+\n    ggplot2::labs(x=\"Densidad de casos positivos\",y=\"Defunciones por cada mil habitantes\")\ncovid_zmvm %>% \n  dplyr::transmute(cvemun,nom_mun,pos_hab,def_hab,pos_den=positivos/ext,def_den=defuncione/ext)## # A tibble: 76 × 6\n##    cvemun nom_mun               pos_hab def_hab pos_den def_den\n##    <chr>  <chr>                   <dbl>   <dbl>   <dbl>   <dbl>\n##  1 09010  Álvaro Obregón           14.4   1.14    113.     9.03\n##  2 09012  Tlalpan                  17.0   0.774    38.3    1.75\n##  3 09015  Cuauhtémoc               13.4   1.38    224.    23.2 \n##  4 09017  Venustiano Carranza      16.2   1.32    212.    17.3 \n##  5 09011  Tláhuac                  17.4   0.737    79.4    3.37\n##  6 09002  Azcapotzalco             17.3   1.65    223.    21.3 \n##  7 09003  Coyoacán                 14.9   1.05    170.    12.0 \n##  8 09013  Xochimilco               17.4   0.898    65.1    3.36\n##  9 09004  Cuajimalpa de Morelos    18.7   0.873    57.2    2.67\n## 10 09016  Miguel Hidalgo           13.7   0.943   122.     8.43\n## # … with 66 more rows"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"resúmenes-de-información-y-grupos","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.5.5 Resúmenes de información y grupos","text":"Quizá una de las funciones más potentes y sencillas que tiene dplyr es la función de resumen (summarise()). Veamos cómo opera. Imagina que deseas un promedio del número de casos que terminaron en muerte:Esto es particularmente útil si se le compara con la función del paquete base de R, summary(), pero si la combinamos con la función de agrupamiento, group_by() la situación cambia. Imagina que queremos el promedio de casos positivos por cada mil habitantes para cada conjunto de municipios de las tres entidades que componen el Valle de México:El en segmento de código anterior primero se agrupa la información con arreglo al criterio de clave de entidad, cve_ent, y luego, para cada grupo, calcula el resultado especificado: un promedio. Dentro de la función de resumen, summarise(), se puede llevar cabo sólo múltiples operaciones sino que estas pueden ser de diferente naturaleza, incluso por ejemplo una suma o una cuenta:En el segmento de código anterior creamos tres variables de resumen: la población total, pob_tot, que es la suma de la población para cada entidad, el promedio del índice de marginación, im_medio y el número de municipios de cada entidad través de una cuenta con la función n().EjercicioConstruye una gráfica de barras donde aparezca el total de población para cada entidad, partir del segmento código anterior.Construye una gráfica de barras donde aparezca el total de población para cada entidad, partir del segmento código anterior.¿Es posible agrupar la información con arreglo otra variable? Construye algunas medidas de resumen con esos grupos y gráficalos.¿Es posible agrupar la información con arreglo otra variable? Construye algunas medidas de resumen con esos grupos y gráficalos.Estas y otras tantas funciones son desarrolladas con todo detalle y de forma muy amena en el citado libro de Wickham y Grolemund, R data science, por lo que su estudio se recomienda ampliamente.En el siguiente capítulo continuamos con la exploración de la información, pero de un tipo particular de información: la información espacial.","code":"\ncovid_zmvm %>%\n  dplyr::summarise(promedio_def=mean(def_hab, na.rm=TRUE))## # A tibble: 1 × 1\n##   promedio_def\n##          <dbl>\n## 1        0.683\ncovid_zmvm %>%\n  dplyr::group_by(cve_ent) %>% \n  summarise(promedio_def=mean(def_hab, na.rm=TRUE), promedio_pos=mean(pos_hab))## # A tibble: 3 × 3\n##   cve_ent promedio_def promedio_pos\n##   <chr>          <dbl>        <dbl>\n## 1 09             1.08         15.8 \n## 2 13             0.790         5.88\n## 3 15             0.572         4.57\ncovid_zmvm %>% \n  dplyr::group_by(cve_ent) %>%\n  dplyr::summarise(pob_tot=sum(pob20),im_medio=mean(im),tot_mun=n())## # A tibble: 3 × 4\n##   cve_ent  pob_tot im_medio tot_mun\n##   <chr>      <dbl>    <dbl>   <int>\n## 1 09       9209944     60.1      16\n## 2 13        168302     59.4       1\n## 3 15      12426269     58.0      59"},{"path":"mapas-coropléticos-en-r.html","id":"mapas-coropléticos-en-r","chapter":"2 Mapas coropléticos en R","heading":"2 Mapas coropléticos en R","text":"De forma reciente, la información georreferenciada y los Sistemas de Información Geográfica (SIG) se han vuelto instrumentos de primer orden para el análisis y la presentación de fenómenos asociados al territorO. Por ejemplo, los Tableros de la Universidad Johns Hopkins sobre la pandemia por COVID19, o los del Gobierno de México y CentroGeo han estado constantemente en los medios de comunicación.En este capítulo señalamos algunas de las características básicas de los datos y datos espaciales; además, buscamos brindar algunos elementos generales que te permitan familiarizarte con ese tipo de información y representarla gráficamente para usarla para en el analisis de fenómenos socioterritoriales.Los datos son valores, números o registros originados partir de un proceso de recolección y procesamiento. Sirven para diversos propósitos, tales como el análisis científico y la toma de decisiones. Los datos se originan partir de mediciones de conjuntos de objetos de la realidad. En estadística, ese conjunto de objetos de los que nos interesa saber ciertas cosas es denominado población y la información de sus rasgos o características es denominada variable o atributo. Así, las variables son medidas o características de los elementos que conforman la población. Si quieres explorar un poco y ampliar tus nociones de qué es un dato, puedes visitar esta entrada de Wikipedia o esta de Economipedia.Las variables pueden ser medidas o información sobre cualidades de los objetos (por ejemplo, de una persona si está vacunada o , si está enferma o sana) o bien, medidas o información sobre cantidades o elementos numéricos (por ejemplo, de una persona interesaría saber el número de dosis de una vacuna recibida, su edad) y las llamamos respectivamente variables cualitativas y variables cuantitativas.Un tipo particular de datos son lo que denominamos datos espaciales o datos georeferenciados. Cuando utilizamos el apelativo “espaciales” o “georeferenciados” estamos diciendo con ello que dichos datos refieren un punto específico sobre la superficie terrestre. El proceso de referenciar geográficamente un dato es complejo pues remite problemas sobre la forma de la Tierra y de su representación bidimencional través de proyecciones cartográficas. Recomendamos revisar este material de Antonio Vazquez Brust donde se abunda sobre este problema.Los datos espaciales se caracterizan por poseer tres componentes: localización, atributos y tiempo, aunque éste último siempre aparece en todos los tipos de datos espaciales. Hay otro aspecto relacionado con la información espacial denominado calidad del dato geoespacial, que se refiere que nuestro conjunto de información cumpla los requisitos necesarios para satisfacer la necesidad para la que se recabó, es decir, que sea útil para resolver la pregunta que motivó su recolección o uso.La realidad es continua y compleja, además, muchos fenómenos tienen un referente territorial, es decir, se desarrollan y ocurren en un determinado lugar. Los datos espaciales implican un esfuerzo de abstracción, es decir, de simplificación de la realidad, esto se le suele denominar veces modelo espacial (Olaya 2020). Este proceso de abstracción consiste en “reducir o dividir esta continuidad en entidades numéricas discretas, observables y susceptibles de medición matemática” (así, un dato espacial podría definirse como la) “observación de una variable asociada una localización del espacio geográfico” (Chasco 2003, 17). De forma general, es posible dividir los modelos de representación espacial de la realidad en dos tipos: ráster y vectorial.Datos vectoriales: es una forma de representación de la realidad que consiste en el uso ya sea de puntos, líneas o polígonos (llamados veces “primitivas”) para recoger ciertos aspectos de la realidad. ¿través de qué primitiva podrías representar un camino? ¿Cuál para representar los límites de la localidad donde resides? Quizá podrías representar una vialidad través de líneas, por un lado, y la representación de los límites administrativos de una región, través de polígonos, por otro.Datos ráster: es un modelo de representación espacial que “se basa en una división sistemática del espacio, la cual cubre todo este, caracterizándolo como un conjunto de unidades elementales” los que se llama pixeles (Olaya 2020: Modelos de representación) . Por ejemplo, pueden ser imágenes del territorio provenientes de instrumentos ópticos (cámaras, sensores, aunqe se limitan ellos) e incluso el propio usuario puede definir dicha representación.3Una definición mucho más cuidadosa de datos ráster y vectoriales puede encontrarse en el libro de Víctor Olaya sobre sistemas de información geográfica donde trata los tipos de datos espaciales.Los ejemplos y ejercicios aquí desarrollado utilizan información espacial de tipo vectorial. Diversos formatos sirven para almacenar información de tipo vectorial, tales como Shape, GeoJSON (Geográphical JavaScript Object Notation), GeoPackage o KML (Keyhole Markup Language). obstante, los shapefiles son aún los más comunes.Los archivos vectoriales de tipo SHP fueron originalmente desarrollados por ESRI, empresa dedicada la cosultoría de fenómenos territoriales, aunque hoy día se usan extensamente prácticamente en cualquier ámbito de interés científico donde se involucre el territorio. Almacenar información vectorial en un archivo SHP implica usar en realidad, al menos, tres archivos diferentes:Archivo con extensión .shp: es un archivo que almacena las entidades espaciales representadas ya sea través de puntos, líneas o polígonos.Archivo con extensión .dbf (data base file): contiene los atributos o variables de cada objeto espacial contenido en el archivo shp.Archivo con extensión .shx (index file): archivo que sirve de vínculo entre los dos previos.Estos tres archivos son los mínimos indispensables para poder trabajar con modelos vectoriales de información, sin embargo son los únicos que encontrarás cuando descargues archivos de este tipo.4 Los tres archivos deben llevar el mismo nombre y deben estar en el mismo directorio (carpeta) para poder funcionar correctamente, es decir, para que sean leídos por el sistema de cómputo.Para que comprendas lo anterior cabalidad, descarga el conjunto de información espacial que usaremos aquí. Tras descomprimir la carpeta verás que contiene, entre otros, los siguientes archivos:covid_zmvm.shpcovid_zmvm.shxcovid_zmvm.dbfEjercicioRevisa la carpeta descargada y descomprimida para responder lo siguiente:¿Cuáles son las extensiones de los otros archivos del mismo nombre?¿Cuáles son las extensiones de los otros archivos del mismo nombre?¿Para qué sirven esos archivos?¿Para qué sirven esos archivos?Como señalamos, la exploración y representación de la información través de mapas es sólo una herramienta muy útil y potente, sino que se ha popularizado en los últimos años gracias sólo la mayor disponibilidad de información georreferenciada, sino la facilidad con la que ahora se puede acceder software para la gestión y manipulación de este tipo de información.Existen diversas herramientas informáticas para el análisis de información espacial y su representación, entre ellos se cuentan las alternativas de ESRI: ArcGIS y ArcMap. obstante, el precio de una licencia individual es considerablemente alto, convirtiéndolos en inaccesibles las mayorías. Una alternativa cada vez más popular es el programa QGIS, una iniciativa de software libre de la Fundación OSGeo, o bien, otra posibilidad también libre es el intuitivo programa de Luc Anselin (Universidad de Chicago, Centro para las Ciencias de Datos Espaciales) y su equipo para iniciarse en el análisis espacial, GeoDa.Además, la popularidad del software R como plataforma de análisis ha hecho que múltiples entusiastas programadoras interesadas en el análisis espacial desarrollaran paquetes enfocados en el tratamiento y representación de datos espaciales para esta plataforma. Además de ser libre, entre las ventajas que tiene usar R como Sistema de Información Goegráfica y como espacio para el tratamiento de datos espaciales, es que se puede tener sólo pleno control de la edición de los materiales cartográficos, sino que permite un entorno de trabajo integrado. En el resto de este capítulo se presenta de forma introductoria la manera en que R, través de algunos paquetes, se puede convertir en un espacio de edición de mapas.Para la elaboración de mapas en R requeriremos de los siguientes paquetes:sf: un paquete útil para manejar datos espaciales de tipo vectorial.tmap: permite crear mapas para visualizar cómo una variable se distribuye en el espacio.RColorBrewer: proporciona esquemas de colores para los mapas, basados en el trabajo de la cartógrafa estadounidense Cynthia Brewer.cartogram: permite elaborar un tipo de mapa llamado cartograma.Instala estos paquetes como es usual:Para cargar los paquetes recién instalados procedemos como es habitual:La base de datos utilizar es la misma que la del capítulo anterior, es decir, información de la primera ola de la pandemia por COVID19 en la Zona Metropolitana del Valle de México, en el centro de México, y algunas variables sociodemográficas y económicas de sus unidades espaciales; obstante, ahora haremos uso del shapefile (recuerda, al menos tres archivos: .shp, .dbf y .shx). Debemos cargar la geometría asociada nuestra base de datos, el archivo con extensión .shp, para ello hay que usar la función st_read() del paquete sf:Nota cómo aparece en tu entorno de trabajo el objeto solicitado, zmvm_cov_sf, ¿logras ver qué tipo de objeto es? Tener esto en mente te ayudará comprender que dependiendo del tipo de objeto con el que interactuas en R habrá determinadas funciones que podrás usar. Así como hay algunas funciones que nos permiten familiarizarnos con los objetos de tipo dataframe, las hay para el caso de los objetos de tipo sf (simple features). La función st_crs() nos ofrece información sobre el sistema de coordenadas de referencia, coordinate reference system (crs), de la capa cargada:De la enorme cantidad de información que aparece en tu consola, nota cómo la proyección de nuestra base de datos geográfica es la Cónica Conforme de Labert, Lambert_Conformal_Conic. En su Introducción ligera los SIG el equipo de QGIS señala que “Con la ayuda de Sistemas de Referencia de Coordenadas (SRC) cualquier punto de la tierra puede ser definido por tres números denominados coordenadas. En general, los CRS se pueden dividir en sistemas de referencia de coordenadas proyectados (también denominados Cartesianos o sistemas de referencia de coordenadas rectangulares) y sistemas de referencia de coordenadas geográficos.”Los sistemas proyectados están basados en determinada representación de los sistemas geográficos. La selección de un adecuado CRS asegura que las características deseadas se representen en el mapa de forma precisa. Existen varios elementos que diferencian los Sistema de Coordenadas Proyectadas (PCS) y los Sistema de coordenadas geográficas (GCS). Mientras que un “sistema de coordenadas geográficas es un método para describir la posición de una ubicación geográfica en la superficie de la Tierra utilizando mediciones esféricas de latitud y longitud (además de que) se trata de mediciones de los ángulos (en grados) desde el centro de la Tierra hasta un punto en la superficie de la Tierra representada como una esfera,” tal y como se señala en la ayuda del sitio de internet de la aplicación de sistemas de información geográfica (SIG), ArcMap (ESRI 2023b).En tanto, “un sistema de coordenadas proyectadas se define sobre una superficie plana de dos dimensiones. diferencia de un sistema de coordenadas geográficas, un sistema de coordenadas proyectadas posee longitudes, ángulos y áreas constantes en las dos dimensiones. Un sistema de coordenadas proyectadas siempre está basado en un sistema de coordenadas geográficas basado en una esfera o un esferoide” (ESRI 2023a).Cuando se trabaja con datos espaciales es importante que todos ellos estén definidos en el mismo SRC, para que sean compatibles entre sí. Por ejemplo, si tengo datos de puntos que representan hospitales en un barrio, digamos el barrio Central, pero el polígono que representa dicho barrio está en un SRC diferente al de los puntos, la representación de los hospitales en el polígono se corresponderá con la realidad.través de diversos programas es posible cambiar el sistema de referencia de coordenadas, es decir, reproyectar un conjunto de datos vectoriales desde un CRS otro distinto. Es particularmente útil cambiar la proyección desde un sistema de coordenadas geográficas un sistema de coordenadas proyectadas cuando se pretende calcular distancias. Esta operación puede ejecutarse de forma muy sencilla en programas como QGIS, o bien en R, pero ello está fuera del interés de este capítulo.Usando la función plot() que pertenece al paquete graphics, es posible elaborar una primera representación de algunas de las variables de nuestra base. Esto es sólo para darnos cuenta de que ahora tratamos con información espacial, es decir, objetos espaciales (unidades adminsitrativas: alcaldías y municipios) los que se asocian determinados atributos o variables.Los mapas de coropletas o mapas coropléticos se construyen en R con el paquete tmap. Este tipo de mapas, que pertenece la categoría más general de los llamadso mapas temáticos, permiten representar la distribución espacial de una variable, es decir, cómo luce la variable representada el espacio geográfico considerado; por ejemplo, el número de casos positivos por COVID19 en alguna región de México. De nueva cuenta, te recomendamos revisar el libro de Víctor Olaya, en la sección El mapa y la comunicación cartográfica, para una exposición más amplia sobre la representación de información espacial través de mapas.La lógica de la construcción de mapas con tmap es semejante la de ggplot2: se usan “enunciados” para completar diferentes elementos del mapa. Así, para construir un mapa completo habrá que indicar, la mayor parte de las veces, tres elementos: ) la geometría de origen (función tm_shape()), ii) los límites internos de las formas utilizadas (función tm_borders()), y iii) la manera en que han de ser rellenados con colores los polígonos según los valores de una variable (función tm_fill()).Así, la elaboración de nuestro mapa implica un segmento de código con tres elementos:Resulta evidente que el segmento de código anterior arrojará un error si lo ejecutas en tu consola, pues hay que especificar los argumentos del caso, sustituyendo las expresiones entrecomilladas. Por ejemplo, para representar nuestra información usando los tres elementos para la variable pos_hab, número de casos positivos de COVID19 por cada mil habitantes, tenemos:Nota cómo dentro de la función de borde se encuentran operando argumentos por defecto, es decir, que sin indicar ningún argumento específico (como tipo de borde o ancho), arroja un resultado. Sobre dichos argumentos volveremos después.EjercicioPrueba eliminando alternativamente una de las funciones de la triada anterior. ¿Qué resultado obtienes con cada combinación?Alternativamente, una manera rápida de suplir las funciones de borde y relleno es sustituirlas con la función tm_polygons()La función tm_polygons() representará la variable indicada en el argumento col=. Además, tanto en esta función como en la de relleno, tm_fill(), el método de clasificación es el método o estilo estético o “bonito” en el que los cortes o categorías se dividen en números enteros siempre que sea posible y los espacia uniformemente (argumento style = \"pretty\").Con lo anterior, tienes ya las bases y la lógica para construir mapas coropléticos en R, lo demás son más que elementos de personalización para cada una de las funciones previas, además de la incorporación de funciones adicionales para elaborar mapas mucho más profesionales.Te recomendamos consultar, además del libro de Víctor Olaya ya comentado previamente, el espléndido material Geocomputation R de (Lovelace, Nowosad, Muenchow 2019), así como el material y todo lo relacionado con el paquete tmap en tmap: Thematic Maps R (Tennekes 2018).Nos concentraremos ahora en revisar cómo podemos hacer “mapas la medida,” es decir, personalizar prácticamente cada parte del mapa, desde la paleta de colores hasta títulos y leyendas. Comencemos revisando las posibilidades de personalización dentro de la función de relleno, tm_fill(). Veremos primero cómo cambiar la paleta de colores de relleno través del argumento palette=. Por defecto, los colores corresponden una gama cromática de rojos, pero podemos cambiarla una de color naranja:EjercicioPrueba cambiar el esquema de colores por aquellos que sean de tu gusto, ¿cuáles son los colores admitidos por R dentro del argumento de paleta?Una manera alternativa de cambiar la paleta de colores consiste en fijar un color para la clase inicial y uno para la clase final, como si se tratara de colores ancla. Para ello nos servimos de un vector como valor del argumento palette=:Para tener una paleta divergente insertamos otro color en el centro:obstante, la manera más adecuada para la selección de una paleta de colores en términos de comunicación visual es través las propuestas de especialistas, como las de la célebre cartógrafa estadounidense Cynthia Brewer.El uso de una determinada paleta de colores depende de la idea transmitir, del tipo de datos representar, de si el mapa será impreso o digital e incluso de quién va dirigido. El términos generales, se puede hacer uso de tres tipos de paletas: ) secuenciales, ii) divergentes y iii) cuantitativas. Las primeras son adecuadas para ordenar datos en forma ascendente, lo que permite observar la distribución espacial de la variable y facilita la identificación de patrones de asociación, en tanto, las paletas divergentes están diseñadas para hacer énfasis en valores extremos (muy altos o muy bajos) y facilitan su identificación en el espacio, finalmente, las paletas cuantitativas buscan clasificar variables categóricas, sin algún orden de magnitud definido. Una exposición más detallada de las paletas de colores y sus usos puede ser consultada aquí.La doctora Brewer diseñó toda una serie de paletas para la representación de información espacial en función del tipo de variable representar y del número de categorías deseadas para clasificar la información. En R, se cuenta con el paquete RColorBrewer que permite, siguiendo los principios de la doctora Brewer, elegir entre una amplia gama de posibilidades de paletas de colores, ya sea secuenciales, divergentes o cualitativas. Además, el paquete permite crear un esquema de colores personalizado indicando los “colores ancla” y el número de categorías.Ilustremos cómo nos servimos del paquete RColorBrewer para crear una paleta de seis colores, en una gama cromática del azul al verde. La función brewer.pal() es la indicada para ello:Los códigos hexagesimales resultado de la función anterior poco nos dicen sobre colores, entonces, hay que visualizarlos. Para ello hemos de usar la función display.brewer.pal()Ahora, ya que tenemos una paleta que luce más estética, podemos aplicarla nuestro mapa:EjercicioSolicita ayuda del paquete RColorBrewer para ver los diferentes tipos de paletas (con las que se cuenta (secuenciales, divergentes y cualitativas) e intenta hacer algunos mapas con dichas paletas.Solicita ayuda del paquete RColorBrewer para ver los diferentes tipos de paletas (con las que se cuenta (secuenciales, divergentes y cualitativas) e intenta hacer algunos mapas con dichas paletas.Ejecuta el siguiente segmento de código para explorar una aplicación con shiny, otro paquete más de R, que te permitirá ver todas las posibilidades de paletas creadas por la doctora Brewer.Ejecuta el siguiente segmento de código para explorar una aplicación con shiny, otro paquete más de R, que te permitirá ver todas las posibilidades de paletas creadas por la doctora Brewer.Para modificar el título de la leyenda, el argumento del caso es justamente title=, dentro de la función tm_fill(), través de una cadena de texto. Por ejemplo:Podemos también agregar elementos informativos adicionales nuestro mapa, como un histograma, través del argumento lógico legend.hist=:Como habrás notado, hay infinidad de elementos que es posible modificar. Particularmente para el caso de la leyenda del mapa, su formato está gobernado por el argumento legend.format= en el que es posible especificar desde las etiquetas, el formato de los números, sufijos o prefijos de las categorías, entre otros tantos elementos. Solicita ayuda sobre la función tm_fill() y observa los argumentos que la integran. Cerramos esta sección mencionando únicamente cómo cambiarla leyenda de nuestro mapa para que las etiquetas de ésta queden en castellano (para que diga “0 5” en vez de “0 5”). Esto lo hacemos añadiendo el argumento citado, legend.format=, en el que especificamos, mediante una lista, cómo debe lucir el texto que separa los valores de cada una de las categorías de nuestra leyenda legend.format = list(text.separator=\"\"):Quizá el elemento de personalización más importante en la edición de mapas tiene que ver con el método de clasificación de la variable que busca ser representada. Todas las alternativas de cómo deben construirse las categorías resultado de la clasificación deben ser especificadas dentro de la función tm_fill().Hay diferentes métodos de clasificación y, por tanto, diferentes tipos de mapas según el método de clasificación. Podemos organizar dichos tipos de mapas de coropletas en tres grandes familias, tal como se muestra en el cuadro 2.1:Cuadro 2.1. Formas de clasificación de familias de mapasTodos estos tipos de mapas pueden ser fácilmente invocados en un programa como GeoDa o QGIS, sin embargo, hasta donde sabemos, en R todos están disponibles y veces la construcción de alguno de ellos exige algunos fundamentos de programación. pesar de ello, R ofrece bastantes alternativas sencillas para construir mapas de clasificación común.EjercicioVe la ayuda de la función tm_fill(), identifica el argumento style=, sigue la documentación sugerida y responde:¿Cuántos métodos de clasificación ofrece R?¿Cuántos métodos de clasificación ofrece R?¿En qué consiste el método de clasificación de k-medias?¿En qué consiste el método de clasificación de k-medias?¿En el trabajo de quiénes está basado el método de clasificación fisher?.¿En el trabajo de quiénes está basado el método de clasificación fisher?.Los mapas que es posible construir en R sin mayores complicaciones se muestran en el cuadro 2.2, así como los valores que debes indicar en el argumento del caso:Cuadro 2.2. Tipos de mapasstyle=quantilestyle=equalstyle=jenksstyle=prettyComo se dijo, los mapas de clasificación común quedarán indicados en los argumentos de la función tm_fill(). Para construir un mapa de 5 cuantiles (quintiles), que es la opción por defecto para el número de categorías:Si deseas cambiar el número de categorías, por ejemplo cuatro, deberás indicar explícitamente su número en el argumento n=:EjerciciosConstruye:Un mapa de Jenks con 6 categorías.Un mapa de Jenks con 6 categorías.Un mapa de intervalos iguales con cuatro categorías.Un mapa de intervalos iguales con cuatro categorías.Un mapa partir de la clasificación por clusters jerárquicos.Un mapa partir de la clasificación por clusters jerárquicos.Se dijo antes que pesar de haber especificado argumento alguno en la función de borde, en ésta operan argumentos por defecto. Es momento de modificar dichos argumentos. Las opciones de borde se especifican dentro de tm_border() donde es posible modificar el color (col=), grosor (lwd=) y tipo de borde (lty=):EjercicioSolicita ayuda de la función tm_fill() y explora qué otras opciones de borde existen. Haz algunos mapas para la variable def_hab cambiando el tipo de borde.Con la combinación de las funciones previamente descritas se puede generar un mapa básico, además, dentro de ellas es posible personalizar múltiples elementos. obstante, un diseño más adecuado y profesional es logrado través de la función tm_layout(), que abarca, entre otras cosas, la posición de la leyenda, el título principal del mapa y tamaños de fuente. Veamos cómo opera.Para cambiar la posición de la leyenda, los argumentos deben colocarse dentro de la función, tm_layout(), través de un vector que indique tanto la orientación vertical como la horizontal de la leyenda. Para la orientación horizontal: \"left\",\"right\" o \"center\" y para la vertical: \"top\", \"center\" o \"bottom\", tal que:Para posicionar la leyenda fuera del mapa, dentro de la función tm_layout() usa el argumento lógico legend.outside y si deseas especificar la posición, el argumento será legend.outside.position que tomará los valores tipo texto de \"top\", \"bottom\", \"right\" o \"left\". Por ejemplo:Para personalizar el título dentro del mapa, en la función tm_layout() hay que agregar el argumento title= y para la posición: title.position=:O bien, si queremos el título afuera del mapa, con un tamaño de fuente diferente y centrado:En R es posible añadir nuestros mapas temáticos un mapa base para dar contexto nuestra representación. Para ello, es necesario activar una suerte de “modo interactivo,” para ser más exactos, lo que activamos es el modo de visualización para poder desplazarnos sobre los mapas. Para cambiar al modo de visualización:La función que permite agregar mapas base es tm_basemap().Ejercicio¿Cuántos tipos de mapas base es posible usar en R? Revisa la ayuda de la función y familiarízate con sus argumentos.De los argumentos para el mapa base, dos son los básicos: el servidor de donde tomaremos el mapa (consejo: ya dentro de los argumentos de la función, escribe providers$ y observarás todas las opciones disponibles) y transparencia (argumento alpha=) que toma valores de 0 1; el argumento alpha= altera tanto la transparencia del mapa base cuando es usado dentro de la función tm_basemap(), como la transparencia de los colores usados para representar la información si se usa dentro de la función tm_fill(). Ejecuta el siguiente segmento de código y navega sobre el mapa representado:Una vez que terminado de navegar y trabajar con mapas base es necesario desactivar el modo de visualización y regresar al modo estático:Un cartograma es un tipo de mapa que deforma la geometría de las áreas de interés en figuras cuyo tamaño depende de la magnitud de la variable representada. Para elaborar un cartograma con circulos, debemos primero generar la geometría deformada por la variable para luego rellenarla. Para tal efecto, usamos la función cartogram_dorling() que nos permitirá generar un nuevo objeto que contiene las geometrías deformadas. Luego, usaremos dicha geometría como argumento de tm_shape(). Primero creamos la nueva geometría circular:Hay que tener en cuenta que la función sólo admite variables negativas. Ahora bien, este objeto recién creado será usado tal como lo hemos hecho antes:Otro tipo de cartograma, más estético, es aquel que garantiza la contigüidad entre las unidades espaciales después de su deformación. Se procede de forma semejante lo hecho antes, sólo que ahora usamos la función cartogram_cont():Ahora, usamos esta información para construir nuestro cartograma:En el blog de mappingGIS encontramos la siguiente definición de punto medio:Toda geometría vectorial (punto, línea o polígono) contiene un punto central denominado centroide.Calcular el centroide de una geometría suele ser una tarea habitual cuando se trabaja con información espacial, por ejemplo, para contar con un punto de referencia partir del cual contar distancias lineales entre polígonos para definir vecindades, como se verá en el capítulo siguiente. Para crear una nueva capa que contenga los centroides de nuestra geometría usamos en R la función st_centroid(). La función generará una nueva serie de archivos SHP que contendrán el conjunto de variables de la base de datos y los centroides:El conjunto de datos recién creado y que contiene los centroides puede ser representado agregando otra “capa” con la función tm_shape(). En el ejemplo, sólo se superponen las dos capas: la de los polígonos originales y la de los centroides:Como habrás podido notar, hemos agregado otra función que define la forma en que ha de representarse la capa de los centroides: tm_dots(). Ahí, es posible especificar la manera en que deseamos que aparezcan los centroides, por ejemplo, en color rojo y más grandes:Para guardar la capa que contiene los centroides en un nuevo archivo SHP, usamos las siguientes líneas de código:Como siempre, se recomienda revisar la documentación de ayuda de la función st_write() para conocer todos los detalles de los argumentos.En esta sección se construyen dos tipos de mapas de clasificación especial, es decir, destinados hacer notar los valores atípicos. obstante, la construcción de dichos mapas en R supone cierto conocimiento sobre programación que está fuera del alcance de estas notas. Si deseas profundizar en el aprendizaje de programación en R, el libro de Roger D. Peng, R Programming Data Science es un excelente material, particularmente el capítulo 14 dedicado las funciones en R (Roger D. Peng 2015).Con esta advertencia, llevamos cabo la exposición de esta sección, esperando motivarte para que tú misma profundices en los tópicos de programación.Se señaló antes que, hasta donde tenemos conocimiento, en R es posible construir mapas de clasificación especial con tmap través de las opciones de estilo, obstante, con algunos elementos de programación es posible solventar esta tarea. En esta sección nos servimos del código proporcionado por Luc Anselin y su equipo quienes, en un esfuerzo de difusión del conocimiento sobre el uso de estas herramientas, pone nuestra disposición una basta cantidad de materiales en la página del Center Spatial Data Science de la Universidad de Chicago (Anselin Morrison 2018).Para construir un mapa de intervalos definidos por el usuario, hay que recurrir al argumento breaks= dentro de la función tm_fill(). Para definir los intervalos de forma adecuada, se recomienda mirar las características resumen de la variable de interés, en este caso el número de casos positivos por COVID19 por cada mil habitantes, pos_hab:Esto nos permitirá conocer los valores que toma la variable de interés y pensar la manera en que deseamos delimitar las categorías de nuestro mapa. Supongamos que deseamos 6 categorías y, dado el rango de nuestra variable (valores entre 1.720 y 22.7), podemos fijar los cortes de los intervalos en 2.0, 6.0, 12., 18.0 y 24.0; además, se requiere incluir también un mínimo y máximo, digamos 1.0 y 25.0, para las categorías.La información, tanto de los cortes como de los máximos y mínimos deber ser especifica en forma de vector: c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0). Debemos además especificar la paleta de colores que deseamos usar, atendiendo lo dicho antes, usaremos una paleta con tres anclas: del amarillo, al naranja y al café, Yellow-Orange-Brown,YlOrBr:Un mapa de percentiles es un tipo espacial de mapa de cuantiles en el que se especifican seis categorías: 0-1%,1-10%, 10-50%,50-90%,90-99% y 99-100%; de ellas las que interesan son las categorías que agrupan el 1% de los valores más bajos (0-1%) y el 1% de los valores más altos (99-100%), es decir, las observaciones extremas. Este tipo de mapas es útil, justamente, para identificar la localización de valores atíticos en el espacio. Para poder construirlo, en R debemos llevar cabo los siguientes pasos:Extraer la variable de nuestro arreglo de datos.Calcular los percentiles de nuestro interés, es decir, 0.0,0.01,0.1,0.5,0.9,0.99,1.0.Construir el mapa con base en los intervalos definidos través de la función tm_fill().Para el paso , lo primero será construir una función que llamaremos get.var, que tendrá dos argumentos, varnom y df, el primero indicará, entre comillas, el nombre de la variable utilizar y el segundo indicará la base de la que proviene. Esta función permite que, al extraer la variable de la base, se eliminen los “aspectos espaciales” asociadas ella. Esta función sólo se requiere construir una vez.Ahora, echando mano de la función creada extraeremos la variable de interés sin sus aspectos espaciales:Para el paso ii, hemos de crear un objeto en el que se guarden los percentiles de interés, un vector de 6 elementos:Ahora, se calculan y guardan valores de los percentiles con base en el par de objetos creados, percentiles y pos_hab, es decir, obtendremos los valores de la variable pos_hab en los percentiles de interés:Ahora, en el paso iii, la construcción del mapa de percentiles es posible uniendo todos los elementos:Con el camino que hemos seguido en la sección previa, es posible ahorrarnos muchos pasos y crear nuestra propia función (que opera en el entorno de trabajo activo de la sesión) para construir mapas de percentiles. Nuestra función se llamará percentmap() y tendrá los siguientes argumentos:varnom: nombre de la variable (especificada como texto entre comillas).df: base de datos que contiene el variable.legtitle: titulo de la leyenda del mapa.mtitle: título del mapa.Para crear la función:La función que hemos creado para hacer nuestros mapas tiene cuatro argumentos: varnom,df,legtitle y mtitle. Los dos últimos tienen valores por omisión, lo que significa que es necesario especificarlos al usar la función. Los dos primeros tienen valor por omisión, por lo que será forzoso especificar dichos argumentos.Ahora, invocando nuestra propia función e indicando los argumentos forzosos, varnom y df tenemos que:Podemos, como es obvio, cambiar los dos argumentos dados por omisión:Los capítulos 1 y 2 de este libro, constituyen lo que suele ser denominado Análisis Exploratorio de Datos, (Exploratory Data Analysis, EDA). El capítulo 1 del e-Handbook Statistical Methods expone con detalle esta concepción en el análisis de información (Croarkin Tobias 2014). Además, en el capítulo 7 del ya citado libro R Data Science también explica con detalle el enfoque EDA usando R.En el siguiente capítulo continuamos con la exploración de la información, pero incorporando una estructura de relaciones en el espacio, por lo que dicho enfoque se le conoce como Análisis Exploratorio de Datos Espaciales. En dicho capítulo será de nuestro interés particular un rasgo que suele estar presente en la información georreferenciada: la autocorrelación espacial.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2023-02-06.book built bookdown R package.","code":"\ninstall.packages(c(\"sf\",\"tmap\", \"RColorBrewer\", \"cartogram\" ))\nlibrary(sf)\nlibrary(tmap)\nlibrary(RColorBrewer)\nlibrary(cartogram)\nzmvm_cov_sf <-sf::st_read(\"base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp\")## Reading layer `covid_zmvm' from data source \n##   `C:\\Repositorios\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 76 features and 57 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5\n## Projected CRS: Lambert_Conformal_Conic\nsf::st_crs(zmvm_cov_sf)## Coordinate Reference System:\n##   User input: Lambert_Conformal_Conic \n##   wkt:\n## PROJCRS[\"Lambert_Conformal_Conic\",\n##     BASEGEOGCRS[\"GCS_GRS_1980_IUGG_1980\",\n##         DATUM[\"D_unknown\",\n##             ELLIPSOID[\"GRS80\",6378137,298.257222101,\n##                 LENGTHUNIT[\"metre\",1,\n##                     ID[\"EPSG\",9001]]]],\n##         PRIMEM[\"Greenwich\",0,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n##     CONVERSION[\"unnamed\",\n##         METHOD[\"Lambert Conic Conformal (2SP)\",\n##             ID[\"EPSG\",9802]],\n##         PARAMETER[\"Latitude of false origin\",12,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433],\n##             ID[\"EPSG\",8821]],\n##         PARAMETER[\"Longitude of false origin\",-102,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433],\n##             ID[\"EPSG\",8822]],\n##         PARAMETER[\"Latitude of 1st standard parallel\",17.5,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433],\n##             ID[\"EPSG\",8823]],\n##         PARAMETER[\"Latitude of 2nd standard parallel\",29.5,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433],\n##             ID[\"EPSG\",8824]],\n##         PARAMETER[\"Easting at false origin\",2500000,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8826]],\n##         PARAMETER[\"Northing at false origin\",0,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8827]]],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"metre\",1,\n##                 ID[\"EPSG\",9001]]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"metre\",1,\n##                 ID[\"EPSG\",9001]]]]\ngraphics::plot(zmvm_cov_sf)## Warning: plotting the first 9 out of 57 attributes; use max.plot = 57 to plot\n## all\ntmap::tm_shape(\"capa shp origen de la información\")+\n  tmap::tm_borders(\"elementos de edición del borde interno\")+\n  tmap::tm_fill(\"elementos de edición del relleno y representación de la variable\")\ntmap::tm_shape(shp=zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_polygons(col=\"pos_hab\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = \"Oranges\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = c(\"blue\",\"red\"))\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = c(\"blue\",\"white\", \"red\"))\nRColorBrewer::brewer.pal(6,\"BuGn\")## [1] \"#EDF8FB\" \"#CCECE6\" \"#99D8C9\" \"#66C2A4\" \"#2CA25F\" \"#006D2C\"\nRColorBrewer::display.brewer.pal(6,\"BuGn\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = \"BuGn\" )\ninstall.packages(\"shiny\",\"shinyjs\")#Instala Shiny y sus dependencias\ntmaptools::palette_explorer() #Lanza la aplicación de Shiny para mostrar las paletas Brewer\n#El modo estatico del libro no permite que se pueda correr este codigo, no obstante, con R en ejecución no existe ningún problema\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Casos positivos COVID19\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Casos positivos COVID19\",legend.hist = TRUE)\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\",\n                palette = \"BuGn\",\n                legend.format = list(text.separator=\"a\") )\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", style = \"quantile\", title = \"Casos positivos\")+\n  tmap::tm_layout(title = \"Mapa de cuantiles\", title.position = c(\"center\", \"bottom\"))#Más adelante se explica esta función\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap:: tm_borders()+\n  tmap::tm_fill(\"pos_hab\", n= 4, style = \"quantile\", title = \"Casos positivos\")+\n  tmap::tm_layout(title = \"Mapa de cuantiles\", title.position = c(\"center\", \"bottom\"))#Más adelante se explica esta función\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders(col=\"black\",lwd=2, lty = 3)\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders()+\n  tmap::tm_layout(legend.position = c(\"right\", \"bottom\"))\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders()+\n  tmap::tm_layout(legend.outside = TRUE,legend.outside.position = \"left\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Casos positivos COVID19\")+\n  tmap::tm_layout(title = \"Casos positivos COVID19 por cada mil habitales\", title.position = c(\"center\",\"top\"))\ntmap::tm_shape(zmvm_cov_sf)+\n tmap:: tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Ingreso 2010\")+\n  tmap::tm_layout(main.title = \"Casos positivos COVID19 por cada mil habitantes\", main.title.position = \"center\", title.size = 1.3)\ntmap::tmap_mode(\"view\")## tmap mode set to interactive viewing\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", alpha=0.7)+\n  tmap::tm_basemap(providers$OpenStreetMap,alpha = 0.5)\ntmap::tmap_mode(\"plot\")## tmap mode set to plotting\ncartograma_circulos <- cartogram::cartogram_dorling(zmvm_cov_sf,\"pos_hab\")\nclass(cartograma_circulos)## [1] \"sf\"         \"data.frame\"\ntmap::tm_shape(cartograma_circulos) +\n  tmap::tm_borders()+\n  tmap:: tm_fill(\"pos_hab\")\ncartograma_cont <- cartogram_cont(zmvm_cov_sf,\"pos_hab\")\ntmap::tm_shape(cartograma_cont) +\n  tmap::tm_fill(\"pos_hab\") +\n  tmap:: tm_borders()\nzmvm_cntrds <- st_centroid(zmvm_cov_sf)\nsummary(zmvm_cntrds)##     cvemun            cve_ent            cve_mun             nom_zm         \n##  Length:76          Length:76          Length:76          Length:76         \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n##      cve_zm       nom_mun            nom_ent            nom_abr         \n##  Min.   :9.01   Length:76          Length:76          Length:76         \n##  1st Qu.:9.01   Class :character   Class :character   Class :character  \n##  Median :9.01   Mode  :character   Mode  :character   Mode  :character  \n##  Mean   :9.01                                                           \n##  3rd Qu.:9.01                                                           \n##  Max.   :9.01                                                           \n##       ext             pob20            pob20_h          pob20_m      \n##  Min.   :  3.17   Min.   :   4862   Min.   :  2338   Min.   :  2524  \n##  1st Qu.: 37.52   1st Qu.:  31900   1st Qu.: 15621   1st Qu.: 16302  \n##  Median : 76.22   Median : 160445   Median : 78574   Median : 81871  \n##  Mean   :103.51   Mean   : 286902   Mean   :138458   Mean   :148443  \n##  3rd Qu.:157.01   3rd Qu.: 432692   3rd Qu.:206156   3rd Qu.:226858  \n##  Max.   :434.26   Max.   :1835486   Max.   :887651   Max.   :947835  \n##    positivos         defuncione        pos_mil         lag_poshab    \n##  Min.   :   18.0   Min.   :   0.0   Min.   : 1.720   Min.   : 2.382  \n##  1st Qu.:  131.5   1st Qu.:  13.5   1st Qu.: 3.337   1st Qu.: 3.977  \n##  Median :  767.0   Median :  86.0   Median : 5.117   Median : 5.275  \n##  Mean   : 2614.0   Mean   : 269.7   Mean   : 6.964   Mean   : 6.733  \n##  3rd Qu.: 3617.2   3rd Qu.: 387.0   3rd Qu.: 8.636   3rd Qu.: 8.229  \n##  Max.   :18767.0   Max.   :2078.0   Max.   :22.700   Max.   :16.443  \n##     pos_hab          def_hab             ss           ppob_sines      \n##  Min.   : 1.720   Min.   :0.0000   Min.   :0.5480   Min.   :0.004937  \n##  1st Qu.: 3.337   1st Qu.:0.4222   1st Qu.:0.6415   1st Qu.:0.019428  \n##  Median : 5.117   Median :0.6528   Median :0.6763   Median :0.023845  \n##  Mean   : 6.964   Mean   :0.6735   Mean   :0.6793   Mean   :0.026127  \n##  3rd Qu.: 8.636   3rd Qu.:0.8791   3rd Qu.:0.7266   3rd Qu.:0.031717  \n##  Max.   :22.700   Max.   :1.6497   Max.   :0.7980   Max.   :0.064699  \n##    ppob_basi        ppob_media        ppob_sup           ocviv      \n##  Min.   :0.1410   Min.   :0.1645   Min.   :0.07101   Min.   :2.460  \n##  1st Qu.:0.4481   1st Qu.:0.2466   1st Qu.:0.13614   1st Qu.:3.530  \n##  Median :0.5295   Median :0.2644   Median :0.17064   Median :3.745  \n##  Mean   :0.5041   Mean   :0.2624   Mean   :0.20549   Mean   :3.690  \n##  3rd Qu.:0.5718   3rd Qu.:0.2831   3rd Qu.:0.25936   3rd Qu.:3.900  \n##  Max.   :0.6842   Max.   :0.3235   Max.   :0.67480   Max.   :4.520  \n##       occu          pintegra4_       pintegra6_        pintegra8_     \n##  Min.   :0.5600   Min.   :0.3657   Min.   :0.06644   Min.   :0.01745  \n##  1st Qu.:0.8600   1st Qu.:0.6772   1st Qu.:0.22736   1st Qu.:0.07521  \n##  Median :0.9900   Median :0.7161   Median :0.26250   Median :0.08992  \n##  Mean   :0.9692   Mean   :0.7013   Mean   :0.26006   Mean   :0.09188  \n##  3rd Qu.:1.0625   3rd Qu.:0.7474   3rd Qu.:0.29090   3rd Qu.:0.10844  \n##  Max.   :1.2900   Max.   :0.8261   Max.   :0.42810   Max.   :0.20046  \n##    ppob_5_o_m       ppob_3_o_m         ppob_1          ppob_1dorm     \n##  Min.   :0.6825   Min.   :0.1680   Min.   :0.00611   Min.   :0.08567  \n##  1st Qu.:0.7781   1st Qu.:0.3425   1st Qu.:0.03234   1st Qu.:0.20592  \n##  Median :0.8183   Median :0.3925   Median :0.04198   Median :0.22763  \n##  Mean   :0.8133   Mean   :0.3934   Mean   :0.04413   Mean   :0.23367  \n##  3rd Qu.:0.8569   3rd Qu.:0.4406   3rd Qu.:0.05341   3rd Qu.:0.26855  \n##  Max.   :0.9150   Max.   :0.5947   Max.   :0.09461   Max.   :0.38234  \n##    ppob_2dorm       ppob_3dorm       pviv_ocu5_       pviv_ocu7_      \n##  Min.   :0.4875   Min.   :0.7781   Min.   :0.0654   Min.   :0.009724  \n##  1st Qu.:0.5883   1st Qu.:0.8581   1st Qu.:0.2434   1st Qu.:0.056245  \n##  Median :0.6237   Median :0.8848   Median :0.2835   Median :0.069583  \n##  Mean   :0.6307   Mean   :0.8808   Mean   :0.2799   Mean   :0.069614  \n##  3rd Qu.:0.6757   3rd Qu.:0.9042   3rd Qu.:0.3209   3rd Qu.:0.082869  \n##  Max.   :0.8006   Max.   :0.9484   Max.   :0.4420   Max.   :0.155107  \n##    pviv_ocu9_           analf            sbasc             vhac      \n##  Min.   :0.002354   Min.   :0.3534   Min.   : 5.535   Min.   : 3.95  \n##  1st Qu.:0.015350   1st Qu.:1.6505   1st Qu.:19.915   1st Qu.:16.40  \n##  Median :0.020738   Median :1.9798   Median :22.973   Median :21.19  \n##  Mean   :0.020961   Mean   :2.3555   Mean   :23.160   Mean   :21.01  \n##  3rd Qu.:0.025183   3rd Qu.:2.8267   3rd Qu.:27.193   3rd Qu.:25.00  \n##  Max.   :0.060174   Max.   :7.4096   Max.   :41.399   Max.   :38.81  \n##      po2sm          idh2015             im          gm_2020         \n##  Min.   :28.45   Min.   :0.6240   Min.   :53.57   Length:76         \n##  1st Qu.:61.27   1st Qu.:0.7288   1st Qu.:57.36   Class :character  \n##  Median :67.40   Median :0.7585   Median :58.46   Mode  :character  \n##  Mean   :66.38   Mean   :0.7629   Mean   :58.47                     \n##  3rd Qu.:72.76   3rd Qu.:0.7945   3rd Qu.:59.65                     \n##  Max.   :86.67   Max.   :0.9290   Max.   :62.36                     \n##       grad            grad_h           grad_m           poind        \n##  Min.   : 8.080   Min.   : 8.140   Min.   : 8.030   Min.   :0.06475  \n##  1st Qu.: 9.557   1st Qu.: 9.658   1st Qu.: 9.490   1st Qu.:0.11379  \n##  Median :10.025   Median :10.160   Median : 9.945   Median :0.17993  \n##  Mean   :10.252   Mean   :10.376   Mean   :10.139   Mean   :0.19532  \n##  3rd Qu.:10.840   3rd Qu.:10.967   3rd Qu.:10.730   3rd Qu.:0.26407  \n##  Max.   :14.550   Max.   :14.930   Max.   :14.220   Max.   :0.44291  \n##      pocom             poss            tmind            tmcom       \n##  Min.   :0.1269   Min.   :0.2113   Min.   : 1.636   Min.   : 1.380  \n##  1st Qu.:0.3363   1st Qu.:0.3033   1st Qu.: 3.166   1st Qu.: 1.876  \n##  Median :0.4358   Median :0.3610   Median : 5.086   Median : 2.139  \n##  Mean   :0.4085   Mean   :0.3961   Mean   : 9.086   Mean   : 2.759  \n##  3rd Qu.:0.4970   3rd Qu.:0.4176   3rd Qu.: 9.195   3rd Qu.: 3.009  \n##  Max.   :0.6867   Max.   :0.7686   Max.   :48.067   Max.   :10.310  \n##       tmss            rmind            rmcom             rmss        \n##  Min.   : 1.670   Min.   : 10.26   Min.   : 4.083   Min.   :  2.586  \n##  1st Qu.: 2.072   1st Qu.: 30.95   1st Qu.:14.085   1st Qu.: 16.312  \n##  Median : 2.775   Median : 63.61   Median :21.002   Median : 28.922  \n##  Mean   : 5.765   Mean   : 67.19   Mean   :24.502   Mean   : 39.904  \n##  3rd Qu.: 4.604   3rd Qu.: 88.88   3rd Qu.:31.630   3rd Qu.: 52.806  \n##  Max.   :41.086   Max.   :177.45   Max.   :67.438   Max.   :255.589  \n##       den                   geometry \n##  Min.   :  123.2   POINT        :76  \n##  1st Qu.:  463.5   epsg:NA      : 0  \n##  Median : 1830.4   +proj=lcc ...: 0  \n##  Mean   : 4227.5                     \n##  3rd Qu.: 6216.9                     \n##  Max.   :17519.3\ntm_shape(zmvm_cov_sf) +\n  tm_borders() +\n  tm_shape(zmvm_cntrds) +\n  tm_dots()\ntm_shape(zmvm_cov_sf) +\n  tm_borders() +\n  tm_shape(zmvm_cntrds) +\n  tm_dots(size=0.2,col=\"red\")\nsf::st_write(obj=zmvm_cntrds, \"zmvm_cntrds\", driver = \"ESRI Shapefile\")\nsummary(zmvm_cov_sf$pos_hab)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   1.720   3.337   5.117   6.964   8.636  22.700\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\",title=\"Casos positivos covid\",breaks=c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0),palette=\"YlOrBr\")+\n  tm_layout(title = \"Cortes personalizados\", title.position = c(\"center\",\"top\"))\nget.var <- function(varnom,df) {#Definición de la función\n  v <- df[varnom] %>% st_set_geometry(NULL) #Extracción de la variable de interés y remoción de sus características geográficas\n  v <- unname(v[,1]) #Selección de la columna del data frame extraído que contiene la variable de interés y elimina su nombre pues sólo queremos un vector.\n  return(v) #Resultado de la función: la variable como vector sin sus características espaciales\n}\npos_hab<-get.var(\"pos_hab\",zmvm_cov_sf)\npos_hab##  [1] 14.364996 16.983175 13.352654 16.163929 17.363687 17.313543 14.943518\n##  [8] 17.404756 18.701249 13.677709 20.559563 10.224540 14.717256 22.700331\n## [15] 11.088257  5.882283  4.938574  5.239423  2.570694  3.351482  4.267922\n## [22]  2.382445  7.896182  2.088929  5.500198  5.891309  5.601076  9.197806\n## [29]  8.113844  3.295057  3.676214  3.250036  3.583416  6.155522  2.124319\n## [36]  2.787239  3.293624  6.120050  1.760416  5.184329  2.941489  3.873824\n## [43]  2.507745  5.423064  7.354686  2.224869  3.992095  8.598203  6.385731\n## [50]  4.385253  3.702180  3.693010  5.894044  4.147922  9.766454  3.853830\n## [57]  5.546263  4.766342  8.751090  1.719690  3.024390  2.372319  2.380410\n## [64]  3.876611  4.272596  7.134726  6.920539  5.049320  4.917293  2.582625\n## [71]  3.918632  3.399002  2.473636  2.822012  6.913242 13.935302\npercentiles <- c(0.0,0.01,0.1,0.5,0.9,0.99,1.0)\nvarperc <- quantile(pos_hab,percentiles)\nvarperc##        0%        1%       10%       50%       90%       99%      100% \n##  1.719690  1.750234  2.428041  5.116824 15.553724 21.094755 22.700331\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\",title=\"Índice de marginación 2010\", breaks=varperc, palette=\"-RdBu\",labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))+\n  tmap::tm_borders() +\n  tmap::tm_layout(title = \"Mapa de percentiles\", title.position = c(\"center\",\"bottom\"))\n  percentmap <- function(varnom,df,titulo.leyenda=NA,titulo.principal=\"Mapa de percentiles\"){ #Definición de la función y sus argumentos\n  #Elementos preliminares\n  percent <- c(0,.01,.1,.5,.9,.99,1) #Vector que contiene los percentiles de interés para el mapa\n  var <- get.var(varnom,df) #Extracción de la variable de la base de datos con la función anterior\n  varperc <- quantile(var,percent) #Cálculo de los percentiles de la variable extraída\n  #Especificaciones del mapa\n  tm_shape(df) +\n     tm_fill(varnom,title=titulo.leyenda,breaks=varperc,palette=\"-RdBu\", labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(title = titulo.principal, title.position = c(\"center\",\"bottom\"))\n}\npercentmap(\"pos_hab\",zmvm_cov_sf)\npercentmap(\"pos_hab\",zmvm_cov_sf, titulo.leyenda = \"Categorías\", titulo.principal = \"El título que yo quiera\" )"},{"path":"mapas-coropléticos-en-r.html","id":"datos-y-datos-espaciales","chapter":"2 Mapas coropléticos en R","heading":"2.1 Datos y datos espaciales","text":"Los datos son valores, números o registros originados partir de un proceso de recolección y procesamiento. Sirven para diversos propósitos, tales como el análisis científico y la toma de decisiones. Los datos se originan partir de mediciones de conjuntos de objetos de la realidad. En estadística, ese conjunto de objetos de los que nos interesa saber ciertas cosas es denominado población y la información de sus rasgos o características es denominada variable o atributo. Así, las variables son medidas o características de los elementos que conforman la población. Si quieres explorar un poco y ampliar tus nociones de qué es un dato, puedes visitar esta entrada de Wikipedia o esta de Economipedia.Las variables pueden ser medidas o información sobre cualidades de los objetos (por ejemplo, de una persona si está vacunada o , si está enferma o sana) o bien, medidas o información sobre cantidades o elementos numéricos (por ejemplo, de una persona interesaría saber el número de dosis de una vacuna recibida, su edad) y las llamamos respectivamente variables cualitativas y variables cuantitativas.Un tipo particular de datos son lo que denominamos datos espaciales o datos georeferenciados. Cuando utilizamos el apelativo “espaciales” o “georeferenciados” estamos diciendo con ello que dichos datos refieren un punto específico sobre la superficie terrestre. El proceso de referenciar geográficamente un dato es complejo pues remite problemas sobre la forma de la Tierra y de su representación bidimencional través de proyecciones cartográficas. Recomendamos revisar este material de Antonio Vazquez Brust donde se abunda sobre este problema.Los datos espaciales se caracterizan por poseer tres componentes: localización, atributos y tiempo, aunque éste último siempre aparece en todos los tipos de datos espaciales. Hay otro aspecto relacionado con la información espacial denominado calidad del dato geoespacial, que se refiere que nuestro conjunto de información cumpla los requisitos necesarios para satisfacer la necesidad para la que se recabó, es decir, que sea útil para resolver la pregunta que motivó su recolección o uso.La realidad es continua y compleja, además, muchos fenómenos tienen un referente territorial, es decir, se desarrollan y ocurren en un determinado lugar. Los datos espaciales implican un esfuerzo de abstracción, es decir, de simplificación de la realidad, esto se le suele denominar veces modelo espacial (Olaya 2020). Este proceso de abstracción consiste en “reducir o dividir esta continuidad en entidades numéricas discretas, observables y susceptibles de medición matemática” (así, un dato espacial podría definirse como la) “observación de una variable asociada una localización del espacio geográfico” (Chasco 2003, 17). De forma general, es posible dividir los modelos de representación espacial de la realidad en dos tipos: ráster y vectorial.Datos vectoriales: es una forma de representación de la realidad que consiste en el uso ya sea de puntos, líneas o polígonos (llamados veces “primitivas”) para recoger ciertos aspectos de la realidad. ¿través de qué primitiva podrías representar un camino? ¿Cuál para representar los límites de la localidad donde resides? Quizá podrías representar una vialidad través de líneas, por un lado, y la representación de los límites administrativos de una región, través de polígonos, por otro.Datos ráster: es un modelo de representación espacial que “se basa en una división sistemática del espacio, la cual cubre todo este, caracterizándolo como un conjunto de unidades elementales” los que se llama pixeles (Olaya 2020: Modelos de representación) . Por ejemplo, pueden ser imágenes del territorio provenientes de instrumentos ópticos (cámaras, sensores, aunqe se limitan ellos) e incluso el propio usuario puede definir dicha representación.3Una definición mucho más cuidadosa de datos ráster y vectoriales puede encontrarse en el libro de Víctor Olaya sobre sistemas de información geográfica donde trata los tipos de datos espaciales.Los ejemplos y ejercicios aquí desarrollado utilizan información espacial de tipo vectorial. Diversos formatos sirven para almacenar información de tipo vectorial, tales como Shape, GeoJSON (Geográphical JavaScript Object Notation), GeoPackage o KML (Keyhole Markup Language). obstante, los shapefiles son aún los más comunes.Los archivos vectoriales de tipo SHP fueron originalmente desarrollados por ESRI, empresa dedicada la cosultoría de fenómenos territoriales, aunque hoy día se usan extensamente prácticamente en cualquier ámbito de interés científico donde se involucre el territorio. Almacenar información vectorial en un archivo SHP implica usar en realidad, al menos, tres archivos diferentes:Archivo con extensión .shp: es un archivo que almacena las entidades espaciales representadas ya sea través de puntos, líneas o polígonos.Archivo con extensión .dbf (data base file): contiene los atributos o variables de cada objeto espacial contenido en el archivo shp.Archivo con extensión .shx (index file): archivo que sirve de vínculo entre los dos previos.Estos tres archivos son los mínimos indispensables para poder trabajar con modelos vectoriales de información, sin embargo son los únicos que encontrarás cuando descargues archivos de este tipo.4 Los tres archivos deben llevar el mismo nombre y deben estar en el mismo directorio (carpeta) para poder funcionar correctamente, es decir, para que sean leídos por el sistema de cómputo.Para que comprendas lo anterior cabalidad, descarga el conjunto de información espacial que usaremos aquí. Tras descomprimir la carpeta verás que contiene, entre otros, los siguientes archivos:covid_zmvm.shpcovid_zmvm.shxcovid_zmvm.dbfEjercicioRevisa la carpeta descargada y descomprimida para responder lo siguiente:¿Cuáles son las extensiones de los otros archivos del mismo nombre?¿Cuáles son las extensiones de los otros archivos del mismo nombre?¿Para qué sirven esos archivos?¿Para qué sirven esos archivos?Como señalamos, la exploración y representación de la información través de mapas es sólo una herramienta muy útil y potente, sino que se ha popularizado en los últimos años gracias sólo la mayor disponibilidad de información georreferenciada, sino la facilidad con la que ahora se puede acceder software para la gestión y manipulación de este tipo de información.Existen diversas herramientas informáticas para el análisis de información espacial y su representación, entre ellos se cuentan las alternativas de ESRI: ArcGIS y ArcMap. obstante, el precio de una licencia individual es considerablemente alto, convirtiéndolos en inaccesibles las mayorías. Una alternativa cada vez más popular es el programa QGIS, una iniciativa de software libre de la Fundación OSGeo, o bien, otra posibilidad también libre es el intuitivo programa de Luc Anselin (Universidad de Chicago, Centro para las Ciencias de Datos Espaciales) y su equipo para iniciarse en el análisis espacial, GeoDa.Además, la popularidad del software R como plataforma de análisis ha hecho que múltiples entusiastas programadoras interesadas en el análisis espacial desarrollaran paquetes enfocados en el tratamiento y representación de datos espaciales para esta plataforma. Además de ser libre, entre las ventajas que tiene usar R como Sistema de Información Goegráfica y como espacio para el tratamiento de datos espaciales, es que se puede tener sólo pleno control de la edición de los materiales cartográficos, sino que permite un entorno de trabajo integrado. En el resto de este capítulo se presenta de forma introductoria la manera en que R, través de algunos paquetes, se puede convertir en un espacio de edición de mapas.","code":""},{"path":"mapas-coropléticos-en-r.html","id":"los-paquetes","chapter":"2 Mapas coropléticos en R","heading":"2.2 Los paquetes","text":"Para la elaboración de mapas en R requeriremos de los siguientes paquetes:sf: un paquete útil para manejar datos espaciales de tipo vectorial.tmap: permite crear mapas para visualizar cómo una variable se distribuye en el espacio.RColorBrewer: proporciona esquemas de colores para los mapas, basados en el trabajo de la cartógrafa estadounidense Cynthia Brewer.cartogram: permite elaborar un tipo de mapa llamado cartograma.Instala estos paquetes como es usual:Para cargar los paquetes recién instalados procedemos como es habitual:","code":"\ninstall.packages(c(\"sf\",\"tmap\", \"RColorBrewer\", \"cartogram\" ))\nlibrary(sf)\nlibrary(tmap)\nlibrary(RColorBrewer)\nlibrary(cartogram)"},{"path":"mapas-coropléticos-en-r.html","id":"carga-de-la-base-de-datos-geográfica","chapter":"2 Mapas coropléticos en R","heading":"2.3 Carga de la base de datos geográfica","text":"La base de datos utilizar es la misma que la del capítulo anterior, es decir, información de la primera ola de la pandemia por COVID19 en la Zona Metropolitana del Valle de México, en el centro de México, y algunas variables sociodemográficas y económicas de sus unidades espaciales; obstante, ahora haremos uso del shapefile (recuerda, al menos tres archivos: .shp, .dbf y .shx). Debemos cargar la geometría asociada nuestra base de datos, el archivo con extensión .shp, para ello hay que usar la función st_read() del paquete sf:Nota cómo aparece en tu entorno de trabajo el objeto solicitado, zmvm_cov_sf, ¿logras ver qué tipo de objeto es? Tener esto en mente te ayudará comprender que dependiendo del tipo de objeto con el que interactuas en R habrá determinadas funciones que podrás usar. Así como hay algunas funciones que nos permiten familiarizarnos con los objetos de tipo dataframe, las hay para el caso de los objetos de tipo sf (simple features). La función st_crs() nos ofrece información sobre el sistema de coordenadas de referencia, coordinate reference system (crs), de la capa cargada:De la enorme cantidad de información que aparece en tu consola, nota cómo la proyección de nuestra base de datos geográfica es la Cónica Conforme de Labert, Lambert_Conformal_Conic. En su Introducción ligera los SIG el equipo de QGIS señala que “Con la ayuda de Sistemas de Referencia de Coordenadas (SRC) cualquier punto de la tierra puede ser definido por tres números denominados coordenadas. En general, los CRS se pueden dividir en sistemas de referencia de coordenadas proyectados (también denominados Cartesianos o sistemas de referencia de coordenadas rectangulares) y sistemas de referencia de coordenadas geográficos.”Los sistemas proyectados están basados en determinada representación de los sistemas geográficos. La selección de un adecuado CRS asegura que las características deseadas se representen en el mapa de forma precisa. Existen varios elementos que diferencian los Sistema de Coordenadas Proyectadas (PCS) y los Sistema de coordenadas geográficas (GCS). Mientras que un “sistema de coordenadas geográficas es un método para describir la posición de una ubicación geográfica en la superficie de la Tierra utilizando mediciones esféricas de latitud y longitud (además de que) se trata de mediciones de los ángulos (en grados) desde el centro de la Tierra hasta un punto en la superficie de la Tierra representada como una esfera,” tal y como se señala en la ayuda del sitio de internet de la aplicación de sistemas de información geográfica (SIG), ArcMap (ESRI 2023b).En tanto, “un sistema de coordenadas proyectadas se define sobre una superficie plana de dos dimensiones. diferencia de un sistema de coordenadas geográficas, un sistema de coordenadas proyectadas posee longitudes, ángulos y áreas constantes en las dos dimensiones. Un sistema de coordenadas proyectadas siempre está basado en un sistema de coordenadas geográficas basado en una esfera o un esferoide” (ESRI 2023a).Cuando se trabaja con datos espaciales es importante que todos ellos estén definidos en el mismo SRC, para que sean compatibles entre sí. Por ejemplo, si tengo datos de puntos que representan hospitales en un barrio, digamos el barrio Central, pero el polígono que representa dicho barrio está en un SRC diferente al de los puntos, la representación de los hospitales en el polígono se corresponderá con la realidad.través de diversos programas es posible cambiar el sistema de referencia de coordenadas, es decir, reproyectar un conjunto de datos vectoriales desde un CRS otro distinto. Es particularmente útil cambiar la proyección desde un sistema de coordenadas geográficas un sistema de coordenadas proyectadas cuando se pretende calcular distancias. Esta operación puede ejecutarse de forma muy sencilla en programas como QGIS, o bien en R, pero ello está fuera del interés de este capítulo.Usando la función plot() que pertenece al paquete graphics, es posible elaborar una primera representación de algunas de las variables de nuestra base. Esto es sólo para darnos cuenta de que ahora tratamos con información espacial, es decir, objetos espaciales (unidades adminsitrativas: alcaldías y municipios) los que se asocian determinados atributos o variables.","code":"\nzmvm_cov_sf <-sf::st_read(\"base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp\")## Reading layer `covid_zmvm' from data source \n##   `C:\\Repositorios\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 76 features and 57 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5\n## Projected CRS: Lambert_Conformal_Conic\nsf::st_crs(zmvm_cov_sf)## Coordinate Reference System:\n##   User input: Lambert_Conformal_Conic \n##   wkt:\n## PROJCRS[\"Lambert_Conformal_Conic\",\n##     BASEGEOGCRS[\"GCS_GRS_1980_IUGG_1980\",\n##         DATUM[\"D_unknown\",\n##             ELLIPSOID[\"GRS80\",6378137,298.257222101,\n##                 LENGTHUNIT[\"metre\",1,\n##                     ID[\"EPSG\",9001]]]],\n##         PRIMEM[\"Greenwich\",0,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n##     CONVERSION[\"unnamed\",\n##         METHOD[\"Lambert Conic Conformal (2SP)\",\n##             ID[\"EPSG\",9802]],\n##         PARAMETER[\"Latitude of false origin\",12,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433],\n##             ID[\"EPSG\",8821]],\n##         PARAMETER[\"Longitude of false origin\",-102,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433],\n##             ID[\"EPSG\",8822]],\n##         PARAMETER[\"Latitude of 1st standard parallel\",17.5,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433],\n##             ID[\"EPSG\",8823]],\n##         PARAMETER[\"Latitude of 2nd standard parallel\",29.5,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433],\n##             ID[\"EPSG\",8824]],\n##         PARAMETER[\"Easting at false origin\",2500000,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8826]],\n##         PARAMETER[\"Northing at false origin\",0,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8827]]],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"metre\",1,\n##                 ID[\"EPSG\",9001]]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"metre\",1,\n##                 ID[\"EPSG\",9001]]]]\ngraphics::plot(zmvm_cov_sf)## Warning: plotting the first 9 out of 57 attributes; use max.plot = 57 to plot\n## all"},{"path":"mapas-coropléticos-en-r.html","id":"mapas-coropléticos-básicos","chapter":"2 Mapas coropléticos en R","heading":"2.4 Mapas coropléticos básicos","text":"Los mapas de coropletas o mapas coropléticos se construyen en R con el paquete tmap. Este tipo de mapas, que pertenece la categoría más general de los llamadso mapas temáticos, permiten representar la distribución espacial de una variable, es decir, cómo luce la variable representada el espacio geográfico considerado; por ejemplo, el número de casos positivos por COVID19 en alguna región de México. De nueva cuenta, te recomendamos revisar el libro de Víctor Olaya, en la sección El mapa y la comunicación cartográfica, para una exposición más amplia sobre la representación de información espacial través de mapas.La lógica de la construcción de mapas con tmap es semejante la de ggplot2: se usan “enunciados” para completar diferentes elementos del mapa. Así, para construir un mapa completo habrá que indicar, la mayor parte de las veces, tres elementos: ) la geometría de origen (función tm_shape()), ii) los límites internos de las formas utilizadas (función tm_borders()), y iii) la manera en que han de ser rellenados con colores los polígonos según los valores de una variable (función tm_fill()).Así, la elaboración de nuestro mapa implica un segmento de código con tres elementos:Resulta evidente que el segmento de código anterior arrojará un error si lo ejecutas en tu consola, pues hay que especificar los argumentos del caso, sustituyendo las expresiones entrecomilladas. Por ejemplo, para representar nuestra información usando los tres elementos para la variable pos_hab, número de casos positivos de COVID19 por cada mil habitantes, tenemos:Nota cómo dentro de la función de borde se encuentran operando argumentos por defecto, es decir, que sin indicar ningún argumento específico (como tipo de borde o ancho), arroja un resultado. Sobre dichos argumentos volveremos después.EjercicioPrueba eliminando alternativamente una de las funciones de la triada anterior. ¿Qué resultado obtienes con cada combinación?Alternativamente, una manera rápida de suplir las funciones de borde y relleno es sustituirlas con la función tm_polygons()La función tm_polygons() representará la variable indicada en el argumento col=. Además, tanto en esta función como en la de relleno, tm_fill(), el método de clasificación es el método o estilo estético o “bonito” en el que los cortes o categorías se dividen en números enteros siempre que sea posible y los espacia uniformemente (argumento style = \"pretty\").Con lo anterior, tienes ya las bases y la lógica para construir mapas coropléticos en R, lo demás son más que elementos de personalización para cada una de las funciones previas, además de la incorporación de funciones adicionales para elaborar mapas mucho más profesionales.Te recomendamos consultar, además del libro de Víctor Olaya ya comentado previamente, el espléndido material Geocomputation R de (Lovelace, Nowosad, Muenchow 2019), así como el material y todo lo relacionado con el paquete tmap en tmap: Thematic Maps R (Tennekes 2018).","code":"\ntmap::tm_shape(\"capa shp origen de la información\")+\n  tmap::tm_borders(\"elementos de edición del borde interno\")+\n  tmap::tm_fill(\"elementos de edición del relleno y representación de la variable\")\ntmap::tm_shape(shp=zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_polygons(col=\"pos_hab\")"},{"path":"mapas-coropléticos-en-r.html","id":"personalización","chapter":"2 Mapas coropléticos en R","heading":"2.5 Personalización","text":"Nos concentraremos ahora en revisar cómo podemos hacer “mapas la medida,” es decir, personalizar prácticamente cada parte del mapa, desde la paleta de colores hasta títulos y leyendas. Comencemos revisando las posibilidades de personalización dentro de la función de relleno, tm_fill(). Veremos primero cómo cambiar la paleta de colores de relleno través del argumento palette=. Por defecto, los colores corresponden una gama cromática de rojos, pero podemos cambiarla una de color naranja:EjercicioPrueba cambiar el esquema de colores por aquellos que sean de tu gusto, ¿cuáles son los colores admitidos por R dentro del argumento de paleta?Una manera alternativa de cambiar la paleta de colores consiste en fijar un color para la clase inicial y uno para la clase final, como si se tratara de colores ancla. Para ello nos servimos de un vector como valor del argumento palette=:Para tener una paleta divergente insertamos otro color en el centro:obstante, la manera más adecuada para la selección de una paleta de colores en términos de comunicación visual es través las propuestas de especialistas, como las de la célebre cartógrafa estadounidense Cynthia Brewer.El uso de una determinada paleta de colores depende de la idea transmitir, del tipo de datos representar, de si el mapa será impreso o digital e incluso de quién va dirigido. El términos generales, se puede hacer uso de tres tipos de paletas: ) secuenciales, ii) divergentes y iii) cuantitativas. Las primeras son adecuadas para ordenar datos en forma ascendente, lo que permite observar la distribución espacial de la variable y facilita la identificación de patrones de asociación, en tanto, las paletas divergentes están diseñadas para hacer énfasis en valores extremos (muy altos o muy bajos) y facilitan su identificación en el espacio, finalmente, las paletas cuantitativas buscan clasificar variables categóricas, sin algún orden de magnitud definido. Una exposición más detallada de las paletas de colores y sus usos puede ser consultada aquí.La doctora Brewer diseñó toda una serie de paletas para la representación de información espacial en función del tipo de variable representar y del número de categorías deseadas para clasificar la información. En R, se cuenta con el paquete RColorBrewer que permite, siguiendo los principios de la doctora Brewer, elegir entre una amplia gama de posibilidades de paletas de colores, ya sea secuenciales, divergentes o cualitativas. Además, el paquete permite crear un esquema de colores personalizado indicando los “colores ancla” y el número de categorías.Ilustremos cómo nos servimos del paquete RColorBrewer para crear una paleta de seis colores, en una gama cromática del azul al verde. La función brewer.pal() es la indicada para ello:Los códigos hexagesimales resultado de la función anterior poco nos dicen sobre colores, entonces, hay que visualizarlos. Para ello hemos de usar la función display.brewer.pal()Ahora, ya que tenemos una paleta que luce más estética, podemos aplicarla nuestro mapa:EjercicioSolicita ayuda del paquete RColorBrewer para ver los diferentes tipos de paletas (con las que se cuenta (secuenciales, divergentes y cualitativas) e intenta hacer algunos mapas con dichas paletas.Solicita ayuda del paquete RColorBrewer para ver los diferentes tipos de paletas (con las que se cuenta (secuenciales, divergentes y cualitativas) e intenta hacer algunos mapas con dichas paletas.Ejecuta el siguiente segmento de código para explorar una aplicación con shiny, otro paquete más de R, que te permitirá ver todas las posibilidades de paletas creadas por la doctora Brewer.Ejecuta el siguiente segmento de código para explorar una aplicación con shiny, otro paquete más de R, que te permitirá ver todas las posibilidades de paletas creadas por la doctora Brewer.Para modificar el título de la leyenda, el argumento del caso es justamente title=, dentro de la función tm_fill(), través de una cadena de texto. Por ejemplo:Podemos también agregar elementos informativos adicionales nuestro mapa, como un histograma, través del argumento lógico legend.hist=:Como habrás notado, hay infinidad de elementos que es posible modificar. Particularmente para el caso de la leyenda del mapa, su formato está gobernado por el argumento legend.format= en el que es posible especificar desde las etiquetas, el formato de los números, sufijos o prefijos de las categorías, entre otros tantos elementos. Solicita ayuda sobre la función tm_fill() y observa los argumentos que la integran. Cerramos esta sección mencionando únicamente cómo cambiarla leyenda de nuestro mapa para que las etiquetas de ésta queden en castellano (para que diga “0 5” en vez de “0 5”). Esto lo hacemos añadiendo el argumento citado, legend.format=, en el que especificamos, mediante una lista, cómo debe lucir el texto que separa los valores de cada una de las categorías de nuestra leyenda legend.format = list(text.separator=\"\"):Quizá el elemento de personalización más importante en la edición de mapas tiene que ver con el método de clasificación de la variable que busca ser representada. Todas las alternativas de cómo deben construirse las categorías resultado de la clasificación deben ser especificadas dentro de la función tm_fill().Hay diferentes métodos de clasificación y, por tanto, diferentes tipos de mapas según el método de clasificación. Podemos organizar dichos tipos de mapas de coropletas en tres grandes familias, tal como se muestra en el cuadro 2.1:Cuadro 2.1. Formas de clasificación de familias de mapasTodos estos tipos de mapas pueden ser fácilmente invocados en un programa como GeoDa o QGIS, sin embargo, hasta donde sabemos, en R todos están disponibles y veces la construcción de alguno de ellos exige algunos fundamentos de programación. pesar de ello, R ofrece bastantes alternativas sencillas para construir mapas de clasificación común.EjercicioVe la ayuda de la función tm_fill(), identifica el argumento style=, sigue la documentación sugerida y responde:¿Cuántos métodos de clasificación ofrece R?¿Cuántos métodos de clasificación ofrece R?¿En qué consiste el método de clasificación de k-medias?¿En qué consiste el método de clasificación de k-medias?¿En el trabajo de quiénes está basado el método de clasificación fisher?.¿En el trabajo de quiénes está basado el método de clasificación fisher?.Los mapas que es posible construir en R sin mayores complicaciones se muestran en el cuadro 2.2, así como los valores que debes indicar en el argumento del caso:Cuadro 2.2. Tipos de mapasstyle=quantilestyle=equalstyle=jenksstyle=prettyComo se dijo, los mapas de clasificación común quedarán indicados en los argumentos de la función tm_fill(). Para construir un mapa de 5 cuantiles (quintiles), que es la opción por defecto para el número de categorías:Si deseas cambiar el número de categorías, por ejemplo cuatro, deberás indicar explícitamente su número en el argumento n=:EjerciciosConstruye:Un mapa de Jenks con 6 categorías.Un mapa de Jenks con 6 categorías.Un mapa de intervalos iguales con cuatro categorías.Un mapa de intervalos iguales con cuatro categorías.Un mapa partir de la clasificación por clusters jerárquicos.Un mapa partir de la clasificación por clusters jerárquicos.Se dijo antes que pesar de haber especificado argumento alguno en la función de borde, en ésta operan argumentos por defecto. Es momento de modificar dichos argumentos. Las opciones de borde se especifican dentro de tm_border() donde es posible modificar el color (col=), grosor (lwd=) y tipo de borde (lty=):EjercicioSolicita ayuda de la función tm_fill() y explora qué otras opciones de borde existen. Haz algunos mapas para la variable def_hab cambiando el tipo de borde.Con la combinación de las funciones previamente descritas se puede generar un mapa básico, además, dentro de ellas es posible personalizar múltiples elementos. obstante, un diseño más adecuado y profesional es logrado través de la función tm_layout(), que abarca, entre otras cosas, la posición de la leyenda, el título principal del mapa y tamaños de fuente. Veamos cómo opera.Para cambiar la posición de la leyenda, los argumentos deben colocarse dentro de la función, tm_layout(), través de un vector que indique tanto la orientación vertical como la horizontal de la leyenda. Para la orientación horizontal: \"left\",\"right\" o \"center\" y para la vertical: \"top\", \"center\" o \"bottom\", tal que:Para posicionar la leyenda fuera del mapa, dentro de la función tm_layout() usa el argumento lógico legend.outside y si deseas especificar la posición, el argumento será legend.outside.position que tomará los valores tipo texto de \"top\", \"bottom\", \"right\" o \"left\". Por ejemplo:Para personalizar el título dentro del mapa, en la función tm_layout() hay que agregar el argumento title= y para la posición: title.position=:O bien, si queremos el título afuera del mapa, con un tamaño de fuente diferente y centrado:En R es posible añadir nuestros mapas temáticos un mapa base para dar contexto nuestra representación. Para ello, es necesario activar una suerte de “modo interactivo,” para ser más exactos, lo que activamos es el modo de visualización para poder desplazarnos sobre los mapas. Para cambiar al modo de visualización:La función que permite agregar mapas base es tm_basemap().Ejercicio¿Cuántos tipos de mapas base es posible usar en R? Revisa la ayuda de la función y familiarízate con sus argumentos.De los argumentos para el mapa base, dos son los básicos: el servidor de donde tomaremos el mapa (consejo: ya dentro de los argumentos de la función, escribe providers$ y observarás todas las opciones disponibles) y transparencia (argumento alpha=) que toma valores de 0 1; el argumento alpha= altera tanto la transparencia del mapa base cuando es usado dentro de la función tm_basemap(), como la transparencia de los colores usados para representar la información si se usa dentro de la función tm_fill(). Ejecuta el siguiente segmento de código y navega sobre el mapa representado:Una vez que terminado de navegar y trabajar con mapas base es necesario desactivar el modo de visualización y regresar al modo estático:Un cartograma es un tipo de mapa que deforma la geometría de las áreas de interés en figuras cuyo tamaño depende de la magnitud de la variable representada. Para elaborar un cartograma con circulos, debemos primero generar la geometría deformada por la variable para luego rellenarla. Para tal efecto, usamos la función cartogram_dorling() que nos permitirá generar un nuevo objeto que contiene las geometrías deformadas. Luego, usaremos dicha geometría como argumento de tm_shape(). Primero creamos la nueva geometría circular:Hay que tener en cuenta que la función sólo admite variables negativas. Ahora bien, este objeto recién creado será usado tal como lo hemos hecho antes:Otro tipo de cartograma, más estético, es aquel que garantiza la contigüidad entre las unidades espaciales después de su deformación. Se procede de forma semejante lo hecho antes, sólo que ahora usamos la función cartogram_cont():Ahora, usamos esta información para construir nuestro cartograma:En el blog de mappingGIS encontramos la siguiente definición de punto medio:Toda geometría vectorial (punto, línea o polígono) contiene un punto central denominado centroide.Calcular el centroide de una geometría suele ser una tarea habitual cuando se trabaja con información espacial, por ejemplo, para contar con un punto de referencia partir del cual contar distancias lineales entre polígonos para definir vecindades, como se verá en el capítulo siguiente. Para crear una nueva capa que contenga los centroides de nuestra geometría usamos en R la función st_centroid(). La función generará una nueva serie de archivos SHP que contendrán el conjunto de variables de la base de datos y los centroides:El conjunto de datos recién creado y que contiene los centroides puede ser representado agregando otra “capa” con la función tm_shape(). En el ejemplo, sólo se superponen las dos capas: la de los polígonos originales y la de los centroides:Como habrás podido notar, hemos agregado otra función que define la forma en que ha de representarse la capa de los centroides: tm_dots(). Ahí, es posible especificar la manera en que deseamos que aparezcan los centroides, por ejemplo, en color rojo y más grandes:Para guardar la capa que contiene los centroides en un nuevo archivo SHP, usamos las siguientes líneas de código:Como siempre, se recomienda revisar la documentación de ayuda de la función st_write() para conocer todos los detalles de los argumentos.En esta sección se construyen dos tipos de mapas de clasificación especial, es decir, destinados hacer notar los valores atípicos. obstante, la construcción de dichos mapas en R supone cierto conocimiento sobre programación que está fuera del alcance de estas notas. Si deseas profundizar en el aprendizaje de programación en R, el libro de Roger D. Peng, R Programming Data Science es un excelente material, particularmente el capítulo 14 dedicado las funciones en R (Roger D. Peng 2015).Con esta advertencia, llevamos cabo la exposición de esta sección, esperando motivarte para que tú misma profundices en los tópicos de programación.Se señaló antes que, hasta donde tenemos conocimiento, en R es posible construir mapas de clasificación especial con tmap través de las opciones de estilo, obstante, con algunos elementos de programación es posible solventar esta tarea. En esta sección nos servimos del código proporcionado por Luc Anselin y su equipo quienes, en un esfuerzo de difusión del conocimiento sobre el uso de estas herramientas, pone nuestra disposición una basta cantidad de materiales en la página del Center Spatial Data Science de la Universidad de Chicago (Anselin Morrison 2018).Para construir un mapa de intervalos definidos por el usuario, hay que recurrir al argumento breaks= dentro de la función tm_fill(). Para definir los intervalos de forma adecuada, se recomienda mirar las características resumen de la variable de interés, en este caso el número de casos positivos por COVID19 por cada mil habitantes, pos_hab:Esto nos permitirá conocer los valores que toma la variable de interés y pensar la manera en que deseamos delimitar las categorías de nuestro mapa. Supongamos que deseamos 6 categorías y, dado el rango de nuestra variable (valores entre 1.720 y 22.7), podemos fijar los cortes de los intervalos en 2.0, 6.0, 12., 18.0 y 24.0; además, se requiere incluir también un mínimo y máximo, digamos 1.0 y 25.0, para las categorías.La información, tanto de los cortes como de los máximos y mínimos deber ser especifica en forma de vector: c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0). Debemos además especificar la paleta de colores que deseamos usar, atendiendo lo dicho antes, usaremos una paleta con tres anclas: del amarillo, al naranja y al café, Yellow-Orange-Brown,YlOrBr:Un mapa de percentiles es un tipo espacial de mapa de cuantiles en el que se especifican seis categorías: 0-1%,1-10%, 10-50%,50-90%,90-99% y 99-100%; de ellas las que interesan son las categorías que agrupan el 1% de los valores más bajos (0-1%) y el 1% de los valores más altos (99-100%), es decir, las observaciones extremas. Este tipo de mapas es útil, justamente, para identificar la localización de valores atíticos en el espacio. Para poder construirlo, en R debemos llevar cabo los siguientes pasos:Extraer la variable de nuestro arreglo de datos.Calcular los percentiles de nuestro interés, es decir, 0.0,0.01,0.1,0.5,0.9,0.99,1.0.Construir el mapa con base en los intervalos definidos través de la función tm_fill().Para el paso , lo primero será construir una función que llamaremos get.var, que tendrá dos argumentos, varnom y df, el primero indicará, entre comillas, el nombre de la variable utilizar y el segundo indicará la base de la que proviene. Esta función permite que, al extraer la variable de la base, se eliminen los “aspectos espaciales” asociadas ella. Esta función sólo se requiere construir una vez.Ahora, echando mano de la función creada extraeremos la variable de interés sin sus aspectos espaciales:Para el paso ii, hemos de crear un objeto en el que se guarden los percentiles de interés, un vector de 6 elementos:Ahora, se calculan y guardan valores de los percentiles con base en el par de objetos creados, percentiles y pos_hab, es decir, obtendremos los valores de la variable pos_hab en los percentiles de interés:Ahora, en el paso iii, la construcción del mapa de percentiles es posible uniendo todos los elementos:Con el camino que hemos seguido en la sección previa, es posible ahorrarnos muchos pasos y crear nuestra propia función (que opera en el entorno de trabajo activo de la sesión) para construir mapas de percentiles. Nuestra función se llamará percentmap() y tendrá los siguientes argumentos:varnom: nombre de la variable (especificada como texto entre comillas).df: base de datos que contiene el variable.legtitle: titulo de la leyenda del mapa.mtitle: título del mapa.Para crear la función:La función que hemos creado para hacer nuestros mapas tiene cuatro argumentos: varnom,df,legtitle y mtitle. Los dos últimos tienen valores por omisión, lo que significa que es necesario especificarlos al usar la función. Los dos primeros tienen valor por omisión, por lo que será forzoso especificar dichos argumentos.Ahora, invocando nuestra propia función e indicando los argumentos forzosos, varnom y df tenemos que:Podemos, como es obvio, cambiar los dos argumentos dados por omisión:Los capítulos 1 y 2 de este libro, constituyen lo que suele ser denominado Análisis Exploratorio de Datos, (Exploratory Data Analysis, EDA). El capítulo 1 del e-Handbook Statistical Methods expone con detalle esta concepción en el análisis de información (Croarkin Tobias 2014). Además, en el capítulo 7 del ya citado libro R Data Science también explica con detalle el enfoque EDA usando R.En el siguiente capítulo continuamos con la exploración de la información, pero incorporando una estructura de relaciones en el espacio, por lo que dicho enfoque se le conoce como Análisis Exploratorio de Datos Espaciales. En dicho capítulo será de nuestro interés particular un rasgo que suele estar presente en la información georreferenciada: la autocorrelación espacial.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2023-02-06.book built bookdown R package.","code":"\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = \"Oranges\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = c(\"blue\",\"red\"))\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = c(\"blue\",\"white\", \"red\"))\nRColorBrewer::brewer.pal(6,\"BuGn\")## [1] \"#EDF8FB\" \"#CCECE6\" \"#99D8C9\" \"#66C2A4\" \"#2CA25F\" \"#006D2C\"\nRColorBrewer::display.brewer.pal(6,\"BuGn\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = \"BuGn\" )\ninstall.packages(\"shiny\",\"shinyjs\")#Instala Shiny y sus dependencias\ntmaptools::palette_explorer() #Lanza la aplicación de Shiny para mostrar las paletas Brewer\n#El modo estatico del libro no permite que se pueda correr este codigo, no obstante, con R en ejecución no existe ningún problema\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Casos positivos COVID19\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Casos positivos COVID19\",legend.hist = TRUE)\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\",\n                palette = \"BuGn\",\n                legend.format = list(text.separator=\"a\") )\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", style = \"quantile\", title = \"Casos positivos\")+\n  tmap::tm_layout(title = \"Mapa de cuantiles\", title.position = c(\"center\", \"bottom\"))#Más adelante se explica esta función\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap:: tm_borders()+\n  tmap::tm_fill(\"pos_hab\", n= 4, style = \"quantile\", title = \"Casos positivos\")+\n  tmap::tm_layout(title = \"Mapa de cuantiles\", title.position = c(\"center\", \"bottom\"))#Más adelante se explica esta función\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders(col=\"black\",lwd=2, lty = 3)\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders()+\n  tmap::tm_layout(legend.position = c(\"right\", \"bottom\"))\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders()+\n  tmap::tm_layout(legend.outside = TRUE,legend.outside.position = \"left\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Casos positivos COVID19\")+\n  tmap::tm_layout(title = \"Casos positivos COVID19 por cada mil habitales\", title.position = c(\"center\",\"top\"))\ntmap::tm_shape(zmvm_cov_sf)+\n tmap:: tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Ingreso 2010\")+\n  tmap::tm_layout(main.title = \"Casos positivos COVID19 por cada mil habitantes\", main.title.position = \"center\", title.size = 1.3)\ntmap::tmap_mode(\"view\")## tmap mode set to interactive viewing\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", alpha=0.7)+\n  tmap::tm_basemap(providers$OpenStreetMap,alpha = 0.5)\ntmap::tmap_mode(\"plot\")## tmap mode set to plotting\ncartograma_circulos <- cartogram::cartogram_dorling(zmvm_cov_sf,\"pos_hab\")\nclass(cartograma_circulos)## [1] \"sf\"         \"data.frame\"\ntmap::tm_shape(cartograma_circulos) +\n  tmap::tm_borders()+\n  tmap:: tm_fill(\"pos_hab\")\ncartograma_cont <- cartogram_cont(zmvm_cov_sf,\"pos_hab\")\ntmap::tm_shape(cartograma_cont) +\n  tmap::tm_fill(\"pos_hab\") +\n  tmap:: tm_borders()\nzmvm_cntrds <- st_centroid(zmvm_cov_sf)\nsummary(zmvm_cntrds)##     cvemun            cve_ent            cve_mun             nom_zm         \n##  Length:76          Length:76          Length:76          Length:76         \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n##      cve_zm       nom_mun            nom_ent            nom_abr         \n##  Min.   :9.01   Length:76          Length:76          Length:76         \n##  1st Qu.:9.01   Class :character   Class :character   Class :character  \n##  Median :9.01   Mode  :character   Mode  :character   Mode  :character  \n##  Mean   :9.01                                                           \n##  3rd Qu.:9.01                                                           \n##  Max.   :9.01                                                           \n##       ext             pob20            pob20_h          pob20_m      \n##  Min.   :  3.17   Min.   :   4862   Min.   :  2338   Min.   :  2524  \n##  1st Qu.: 37.52   1st Qu.:  31900   1st Qu.: 15621   1st Qu.: 16302  \n##  Median : 76.22   Median : 160445   Median : 78574   Median : 81871  \n##  Mean   :103.51   Mean   : 286902   Mean   :138458   Mean   :148443  \n##  3rd Qu.:157.01   3rd Qu.: 432692   3rd Qu.:206156   3rd Qu.:226858  \n##  Max.   :434.26   Max.   :1835486   Max.   :887651   Max.   :947835  \n##    positivos         defuncione        pos_mil         lag_poshab    \n##  Min.   :   18.0   Min.   :   0.0   Min.   : 1.720   Min.   : 2.382  \n##  1st Qu.:  131.5   1st Qu.:  13.5   1st Qu.: 3.337   1st Qu.: 3.977  \n##  Median :  767.0   Median :  86.0   Median : 5.117   Median : 5.275  \n##  Mean   : 2614.0   Mean   : 269.7   Mean   : 6.964   Mean   : 6.733  \n##  3rd Qu.: 3617.2   3rd Qu.: 387.0   3rd Qu.: 8.636   3rd Qu.: 8.229  \n##  Max.   :18767.0   Max.   :2078.0   Max.   :22.700   Max.   :16.443  \n##     pos_hab          def_hab             ss           ppob_sines      \n##  Min.   : 1.720   Min.   :0.0000   Min.   :0.5480   Min.   :0.004937  \n##  1st Qu.: 3.337   1st Qu.:0.4222   1st Qu.:0.6415   1st Qu.:0.019428  \n##  Median : 5.117   Median :0.6528   Median :0.6763   Median :0.023845  \n##  Mean   : 6.964   Mean   :0.6735   Mean   :0.6793   Mean   :0.026127  \n##  3rd Qu.: 8.636   3rd Qu.:0.8791   3rd Qu.:0.7266   3rd Qu.:0.031717  \n##  Max.   :22.700   Max.   :1.6497   Max.   :0.7980   Max.   :0.064699  \n##    ppob_basi        ppob_media        ppob_sup           ocviv      \n##  Min.   :0.1410   Min.   :0.1645   Min.   :0.07101   Min.   :2.460  \n##  1st Qu.:0.4481   1st Qu.:0.2466   1st Qu.:0.13614   1st Qu.:3.530  \n##  Median :0.5295   Median :0.2644   Median :0.17064   Median :3.745  \n##  Mean   :0.5041   Mean   :0.2624   Mean   :0.20549   Mean   :3.690  \n##  3rd Qu.:0.5718   3rd Qu.:0.2831   3rd Qu.:0.25936   3rd Qu.:3.900  \n##  Max.   :0.6842   Max.   :0.3235   Max.   :0.67480   Max.   :4.520  \n##       occu          pintegra4_       pintegra6_        pintegra8_     \n##  Min.   :0.5600   Min.   :0.3657   Min.   :0.06644   Min.   :0.01745  \n##  1st Qu.:0.8600   1st Qu.:0.6772   1st Qu.:0.22736   1st Qu.:0.07521  \n##  Median :0.9900   Median :0.7161   Median :0.26250   Median :0.08992  \n##  Mean   :0.9692   Mean   :0.7013   Mean   :0.26006   Mean   :0.09188  \n##  3rd Qu.:1.0625   3rd Qu.:0.7474   3rd Qu.:0.29090   3rd Qu.:0.10844  \n##  Max.   :1.2900   Max.   :0.8261   Max.   :0.42810   Max.   :0.20046  \n##    ppob_5_o_m       ppob_3_o_m         ppob_1          ppob_1dorm     \n##  Min.   :0.6825   Min.   :0.1680   Min.   :0.00611   Min.   :0.08567  \n##  1st Qu.:0.7781   1st Qu.:0.3425   1st Qu.:0.03234   1st Qu.:0.20592  \n##  Median :0.8183   Median :0.3925   Median :0.04198   Median :0.22763  \n##  Mean   :0.8133   Mean   :0.3934   Mean   :0.04413   Mean   :0.23367  \n##  3rd Qu.:0.8569   3rd Qu.:0.4406   3rd Qu.:0.05341   3rd Qu.:0.26855  \n##  Max.   :0.9150   Max.   :0.5947   Max.   :0.09461   Max.   :0.38234  \n##    ppob_2dorm       ppob_3dorm       pviv_ocu5_       pviv_ocu7_      \n##  Min.   :0.4875   Min.   :0.7781   Min.   :0.0654   Min.   :0.009724  \n##  1st Qu.:0.5883   1st Qu.:0.8581   1st Qu.:0.2434   1st Qu.:0.056245  \n##  Median :0.6237   Median :0.8848   Median :0.2835   Median :0.069583  \n##  Mean   :0.6307   Mean   :0.8808   Mean   :0.2799   Mean   :0.069614  \n##  3rd Qu.:0.6757   3rd Qu.:0.9042   3rd Qu.:0.3209   3rd Qu.:0.082869  \n##  Max.   :0.8006   Max.   :0.9484   Max.   :0.4420   Max.   :0.155107  \n##    pviv_ocu9_           analf            sbasc             vhac      \n##  Min.   :0.002354   Min.   :0.3534   Min.   : 5.535   Min.   : 3.95  \n##  1st Qu.:0.015350   1st Qu.:1.6505   1st Qu.:19.915   1st Qu.:16.40  \n##  Median :0.020738   Median :1.9798   Median :22.973   Median :21.19  \n##  Mean   :0.020961   Mean   :2.3555   Mean   :23.160   Mean   :21.01  \n##  3rd Qu.:0.025183   3rd Qu.:2.8267   3rd Qu.:27.193   3rd Qu.:25.00  \n##  Max.   :0.060174   Max.   :7.4096   Max.   :41.399   Max.   :38.81  \n##      po2sm          idh2015             im          gm_2020         \n##  Min.   :28.45   Min.   :0.6240   Min.   :53.57   Length:76         \n##  1st Qu.:61.27   1st Qu.:0.7288   1st Qu.:57.36   Class :character  \n##  Median :67.40   Median :0.7585   Median :58.46   Mode  :character  \n##  Mean   :66.38   Mean   :0.7629   Mean   :58.47                     \n##  3rd Qu.:72.76   3rd Qu.:0.7945   3rd Qu.:59.65                     \n##  Max.   :86.67   Max.   :0.9290   Max.   :62.36                     \n##       grad            grad_h           grad_m           poind        \n##  Min.   : 8.080   Min.   : 8.140   Min.   : 8.030   Min.   :0.06475  \n##  1st Qu.: 9.557   1st Qu.: 9.658   1st Qu.: 9.490   1st Qu.:0.11379  \n##  Median :10.025   Median :10.160   Median : 9.945   Median :0.17993  \n##  Mean   :10.252   Mean   :10.376   Mean   :10.139   Mean   :0.19532  \n##  3rd Qu.:10.840   3rd Qu.:10.967   3rd Qu.:10.730   3rd Qu.:0.26407  \n##  Max.   :14.550   Max.   :14.930   Max.   :14.220   Max.   :0.44291  \n##      pocom             poss            tmind            tmcom       \n##  Min.   :0.1269   Min.   :0.2113   Min.   : 1.636   Min.   : 1.380  \n##  1st Qu.:0.3363   1st Qu.:0.3033   1st Qu.: 3.166   1st Qu.: 1.876  \n##  Median :0.4358   Median :0.3610   Median : 5.086   Median : 2.139  \n##  Mean   :0.4085   Mean   :0.3961   Mean   : 9.086   Mean   : 2.759  \n##  3rd Qu.:0.4970   3rd Qu.:0.4176   3rd Qu.: 9.195   3rd Qu.: 3.009  \n##  Max.   :0.6867   Max.   :0.7686   Max.   :48.067   Max.   :10.310  \n##       tmss            rmind            rmcom             rmss        \n##  Min.   : 1.670   Min.   : 10.26   Min.   : 4.083   Min.   :  2.586  \n##  1st Qu.: 2.072   1st Qu.: 30.95   1st Qu.:14.085   1st Qu.: 16.312  \n##  Median : 2.775   Median : 63.61   Median :21.002   Median : 28.922  \n##  Mean   : 5.765   Mean   : 67.19   Mean   :24.502   Mean   : 39.904  \n##  3rd Qu.: 4.604   3rd Qu.: 88.88   3rd Qu.:31.630   3rd Qu.: 52.806  \n##  Max.   :41.086   Max.   :177.45   Max.   :67.438   Max.   :255.589  \n##       den                   geometry \n##  Min.   :  123.2   POINT        :76  \n##  1st Qu.:  463.5   epsg:NA      : 0  \n##  Median : 1830.4   +proj=lcc ...: 0  \n##  Mean   : 4227.5                     \n##  3rd Qu.: 6216.9                     \n##  Max.   :17519.3\ntm_shape(zmvm_cov_sf) +\n  tm_borders() +\n  tm_shape(zmvm_cntrds) +\n  tm_dots()\ntm_shape(zmvm_cov_sf) +\n  tm_borders() +\n  tm_shape(zmvm_cntrds) +\n  tm_dots(size=0.2,col=\"red\")\nsf::st_write(obj=zmvm_cntrds, \"zmvm_cntrds\", driver = \"ESRI Shapefile\")\nsummary(zmvm_cov_sf$pos_hab)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   1.720   3.337   5.117   6.964   8.636  22.700\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\",title=\"Casos positivos covid\",breaks=c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0),palette=\"YlOrBr\")+\n  tm_layout(title = \"Cortes personalizados\", title.position = c(\"center\",\"top\"))\nget.var <- function(varnom,df) {#Definición de la función\n  v <- df[varnom] %>% st_set_geometry(NULL) #Extracción de la variable de interés y remoción de sus características geográficas\n  v <- unname(v[,1]) #Selección de la columna del data frame extraído que contiene la variable de interés y elimina su nombre pues sólo queremos un vector.\n  return(v) #Resultado de la función: la variable como vector sin sus características espaciales\n}\npos_hab<-get.var(\"pos_hab\",zmvm_cov_sf)\npos_hab##  [1] 14.364996 16.983175 13.352654 16.163929 17.363687 17.313543 14.943518\n##  [8] 17.404756 18.701249 13.677709 20.559563 10.224540 14.717256 22.700331\n## [15] 11.088257  5.882283  4.938574  5.239423  2.570694  3.351482  4.267922\n## [22]  2.382445  7.896182  2.088929  5.500198  5.891309  5.601076  9.197806\n## [29]  8.113844  3.295057  3.676214  3.250036  3.583416  6.155522  2.124319\n## [36]  2.787239  3.293624  6.120050  1.760416  5.184329  2.941489  3.873824\n## [43]  2.507745  5.423064  7.354686  2.224869  3.992095  8.598203  6.385731\n## [50]  4.385253  3.702180  3.693010  5.894044  4.147922  9.766454  3.853830\n## [57]  5.546263  4.766342  8.751090  1.719690  3.024390  2.372319  2.380410\n## [64]  3.876611  4.272596  7.134726  6.920539  5.049320  4.917293  2.582625\n## [71]  3.918632  3.399002  2.473636  2.822012  6.913242 13.935302\npercentiles <- c(0.0,0.01,0.1,0.5,0.9,0.99,1.0)\nvarperc <- quantile(pos_hab,percentiles)\nvarperc##        0%        1%       10%       50%       90%       99%      100% \n##  1.719690  1.750234  2.428041  5.116824 15.553724 21.094755 22.700331\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\",title=\"Índice de marginación 2010\", breaks=varperc, palette=\"-RdBu\",labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))+\n  tmap::tm_borders() +\n  tmap::tm_layout(title = \"Mapa de percentiles\", title.position = c(\"center\",\"bottom\"))\n  percentmap <- function(varnom,df,titulo.leyenda=NA,titulo.principal=\"Mapa de percentiles\"){ #Definición de la función y sus argumentos\n  #Elementos preliminares\n  percent <- c(0,.01,.1,.5,.9,.99,1) #Vector que contiene los percentiles de interés para el mapa\n  var <- get.var(varnom,df) #Extracción de la variable de la base de datos con la función anterior\n  varperc <- quantile(var,percent) #Cálculo de los percentiles de la variable extraída\n  #Especificaciones del mapa\n  tm_shape(df) +\n     tm_fill(varnom,title=titulo.leyenda,breaks=varperc,palette=\"-RdBu\", labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(title = titulo.principal, title.position = c(\"center\",\"bottom\"))\n}\npercentmap(\"pos_hab\",zmvm_cov_sf)\npercentmap(\"pos_hab\",zmvm_cov_sf, titulo.leyenda = \"Categorías\", titulo.principal = \"El título que yo quiera\" )"},{"path":"mapas-coropléticos-en-r.html","id":"argumentos-de-personalización-dentro-de-la-función-de-relleno","chapter":"2 Mapas coropléticos en R","heading":"2.5.1 Argumentos de personalización dentro de la función de relleno","text":"","code":""},{"path":"mapas-coropléticos-en-r.html","id":"paleta-de-colores","chapter":"2 Mapas coropléticos en R","heading":"2.5.1.1 Paleta de colores","text":"Nos concentraremos ahora en revisar cómo podemos hacer “mapas la medida,” es decir, personalizar prácticamente cada parte del mapa, desde la paleta de colores hasta títulos y leyendas. Comencemos revisando las posibilidades de personalización dentro de la función de relleno, tm_fill(). Veremos primero cómo cambiar la paleta de colores de relleno través del argumento palette=. Por defecto, los colores corresponden una gama cromática de rojos, pero podemos cambiarla una de color naranja:EjercicioPrueba cambiar el esquema de colores por aquellos que sean de tu gusto, ¿cuáles son los colores admitidos por R dentro del argumento de paleta?Una manera alternativa de cambiar la paleta de colores consiste en fijar un color para la clase inicial y uno para la clase final, como si se tratara de colores ancla. Para ello nos servimos de un vector como valor del argumento palette=:Para tener una paleta divergente insertamos otro color en el centro:obstante, la manera más adecuada para la selección de una paleta de colores en términos de comunicación visual es través las propuestas de especialistas, como las de la célebre cartógrafa estadounidense Cynthia Brewer.El uso de una determinada paleta de colores depende de la idea transmitir, del tipo de datos representar, de si el mapa será impreso o digital e incluso de quién va dirigido. El términos generales, se puede hacer uso de tres tipos de paletas: ) secuenciales, ii) divergentes y iii) cuantitativas. Las primeras son adecuadas para ordenar datos en forma ascendente, lo que permite observar la distribución espacial de la variable y facilita la identificación de patrones de asociación, en tanto, las paletas divergentes están diseñadas para hacer énfasis en valores extremos (muy altos o muy bajos) y facilitan su identificación en el espacio, finalmente, las paletas cuantitativas buscan clasificar variables categóricas, sin algún orden de magnitud definido. Una exposición más detallada de las paletas de colores y sus usos puede ser consultada aquí.La doctora Brewer diseñó toda una serie de paletas para la representación de información espacial en función del tipo de variable representar y del número de categorías deseadas para clasificar la información. En R, se cuenta con el paquete RColorBrewer que permite, siguiendo los principios de la doctora Brewer, elegir entre una amplia gama de posibilidades de paletas de colores, ya sea secuenciales, divergentes o cualitativas. Además, el paquete permite crear un esquema de colores personalizado indicando los “colores ancla” y el número de categorías.Ilustremos cómo nos servimos del paquete RColorBrewer para crear una paleta de seis colores, en una gama cromática del azul al verde. La función brewer.pal() es la indicada para ello:Los códigos hexagesimales resultado de la función anterior poco nos dicen sobre colores, entonces, hay que visualizarlos. Para ello hemos de usar la función display.brewer.pal()Ahora, ya que tenemos una paleta que luce más estética, podemos aplicarla nuestro mapa:EjercicioSolicita ayuda del paquete RColorBrewer para ver los diferentes tipos de paletas (con las que se cuenta (secuenciales, divergentes y cualitativas) e intenta hacer algunos mapas con dichas paletas.Solicita ayuda del paquete RColorBrewer para ver los diferentes tipos de paletas (con las que se cuenta (secuenciales, divergentes y cualitativas) e intenta hacer algunos mapas con dichas paletas.Ejecuta el siguiente segmento de código para explorar una aplicación con shiny, otro paquete más de R, que te permitirá ver todas las posibilidades de paletas creadas por la doctora Brewer.Ejecuta el siguiente segmento de código para explorar una aplicación con shiny, otro paquete más de R, que te permitirá ver todas las posibilidades de paletas creadas por la doctora Brewer.","code":"\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = \"Oranges\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = c(\"blue\",\"red\"))\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = c(\"blue\",\"white\", \"red\"))\nRColorBrewer::brewer.pal(6,\"BuGn\")## [1] \"#EDF8FB\" \"#CCECE6\" \"#99D8C9\" \"#66C2A4\" \"#2CA25F\" \"#006D2C\"\nRColorBrewer::display.brewer.pal(6,\"BuGn\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = \"BuGn\" )\ninstall.packages(\"shiny\",\"shinyjs\")#Instala Shiny y sus dependencias\ntmaptools::palette_explorer() #Lanza la aplicación de Shiny para mostrar las paletas Brewer\n#El modo estatico del libro no permite que se pueda correr este codigo, no obstante, con R en ejecución no existe ningún problema"},{"path":"mapas-coropléticos-en-r.html","id":"leyenda","chapter":"2 Mapas coropléticos en R","heading":"2.5.1.2 Leyenda","text":"Para modificar el título de la leyenda, el argumento del caso es justamente title=, dentro de la función tm_fill(), través de una cadena de texto. Por ejemplo:Podemos también agregar elementos informativos adicionales nuestro mapa, como un histograma, través del argumento lógico legend.hist=:Como habrás notado, hay infinidad de elementos que es posible modificar. Particularmente para el caso de la leyenda del mapa, su formato está gobernado por el argumento legend.format= en el que es posible especificar desde las etiquetas, el formato de los números, sufijos o prefijos de las categorías, entre otros tantos elementos. Solicita ayuda sobre la función tm_fill() y observa los argumentos que la integran. Cerramos esta sección mencionando únicamente cómo cambiarla leyenda de nuestro mapa para que las etiquetas de ésta queden en castellano (para que diga “0 5” en vez de “0 5”). Esto lo hacemos añadiendo el argumento citado, legend.format=, en el que especificamos, mediante una lista, cómo debe lucir el texto que separa los valores de cada una de las categorías de nuestra leyenda legend.format = list(text.separator=\"\"):","code":"\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Casos positivos COVID19\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Casos positivos COVID19\",legend.hist = TRUE)\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\",\n                palette = \"BuGn\",\n                legend.format = list(text.separator=\"a\") )"},{"path":"mapas-coropléticos-en-r.html","id":"métodos-de-clasificación-en-los-mápas","chapter":"2 Mapas coropléticos en R","heading":"2.5.1.3 Métodos de clasificación en los mápas","text":"Quizá el elemento de personalización más importante en la edición de mapas tiene que ver con el método de clasificación de la variable que busca ser representada. Todas las alternativas de cómo deben construirse las categorías resultado de la clasificación deben ser especificadas dentro de la función tm_fill().Hay diferentes métodos de clasificación y, por tanto, diferentes tipos de mapas según el método de clasificación. Podemos organizar dichos tipos de mapas de coropletas en tres grandes familias, tal como se muestra en el cuadro 2.1:Cuadro 2.1. Formas de clasificación de familias de mapasTodos estos tipos de mapas pueden ser fácilmente invocados en un programa como GeoDa o QGIS, sin embargo, hasta donde sabemos, en R todos están disponibles y veces la construcción de alguno de ellos exige algunos fundamentos de programación. pesar de ello, R ofrece bastantes alternativas sencillas para construir mapas de clasificación común.EjercicioVe la ayuda de la función tm_fill(), identifica el argumento style=, sigue la documentación sugerida y responde:¿Cuántos métodos de clasificación ofrece R?¿Cuántos métodos de clasificación ofrece R?¿En qué consiste el método de clasificación de k-medias?¿En qué consiste el método de clasificación de k-medias?¿En el trabajo de quiénes está basado el método de clasificación fisher?.¿En el trabajo de quiénes está basado el método de clasificación fisher?.Los mapas que es posible construir en R sin mayores complicaciones se muestran en el cuadro 2.2, así como los valores que debes indicar en el argumento del caso:Cuadro 2.2. Tipos de mapasstyle=quantilestyle=equalstyle=jenksstyle=prettyComo se dijo, los mapas de clasificación común quedarán indicados en los argumentos de la función tm_fill(). Para construir un mapa de 5 cuantiles (quintiles), que es la opción por defecto para el número de categorías:Si deseas cambiar el número de categorías, por ejemplo cuatro, deberás indicar explícitamente su número en el argumento n=:EjerciciosConstruye:Un mapa de Jenks con 6 categorías.Un mapa de Jenks con 6 categorías.Un mapa de intervalos iguales con cuatro categorías.Un mapa de intervalos iguales con cuatro categorías.Un mapa partir de la clasificación por clusters jerárquicos.Un mapa partir de la clasificación por clusters jerárquicos.","code":"\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", style = \"quantile\", title = \"Casos positivos\")+\n  tmap::tm_layout(title = \"Mapa de cuantiles\", title.position = c(\"center\", \"bottom\"))#Más adelante se explica esta función\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap:: tm_borders()+\n  tmap::tm_fill(\"pos_hab\", n= 4, style = \"quantile\", title = \"Casos positivos\")+\n  tmap::tm_layout(title = \"Mapa de cuantiles\", title.position = c(\"center\", \"bottom\"))#Más adelante se explica esta función"},{"path":"mapas-coropléticos-en-r.html","id":"argumentos-de-personalización-del-borde","chapter":"2 Mapas coropléticos en R","heading":"2.5.2 Argumentos de personalización del borde","text":"Se dijo antes que pesar de haber especificado argumento alguno en la función de borde, en ésta operan argumentos por defecto. Es momento de modificar dichos argumentos. Las opciones de borde se especifican dentro de tm_border() donde es posible modificar el color (col=), grosor (lwd=) y tipo de borde (lty=):EjercicioSolicita ayuda de la función tm_fill() y explora qué otras opciones de borde existen. Haz algunos mapas para la variable def_hab cambiando el tipo de borde.","code":"\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders(col=\"black\",lwd=2, lty = 3)"},{"path":"mapas-coropléticos-en-r.html","id":"función-para-elementos-de-diseño-de-salida-tm_layout","chapter":"2 Mapas coropléticos en R","heading":"2.6 Función para elementos de diseño de salida, tm_layout()","text":"Con la combinación de las funciones previamente descritas se puede generar un mapa básico, además, dentro de ellas es posible personalizar múltiples elementos. obstante, un diseño más adecuado y profesional es logrado través de la función tm_layout(), que abarca, entre otras cosas, la posición de la leyenda, el título principal del mapa y tamaños de fuente. Veamos cómo opera.Para cambiar la posición de la leyenda, los argumentos deben colocarse dentro de la función, tm_layout(), través de un vector que indique tanto la orientación vertical como la horizontal de la leyenda. Para la orientación horizontal: \"left\",\"right\" o \"center\" y para la vertical: \"top\", \"center\" o \"bottom\", tal que:Para posicionar la leyenda fuera del mapa, dentro de la función tm_layout() usa el argumento lógico legend.outside y si deseas especificar la posición, el argumento será legend.outside.position que tomará los valores tipo texto de \"top\", \"bottom\", \"right\" o \"left\". Por ejemplo:Para personalizar el título dentro del mapa, en la función tm_layout() hay que agregar el argumento title= y para la posición: title.position=:O bien, si queremos el título afuera del mapa, con un tamaño de fuente diferente y centrado:","code":"\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders()+\n  tmap::tm_layout(legend.position = c(\"right\", \"bottom\"))\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders()+\n  tmap::tm_layout(legend.outside = TRUE,legend.outside.position = \"left\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Casos positivos COVID19\")+\n  tmap::tm_layout(title = \"Casos positivos COVID19 por cada mil habitales\", title.position = c(\"center\",\"top\"))\ntmap::tm_shape(zmvm_cov_sf)+\n tmap:: tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Ingreso 2010\")+\n  tmap::tm_layout(main.title = \"Casos positivos COVID19 por cada mil habitantes\", main.title.position = \"center\", title.size = 1.3)"},{"path":"mapas-coropléticos-en-r.html","id":"mapa-base-interactivo","chapter":"2 Mapas coropléticos en R","heading":"2.7 Mapa base interactivo","text":"En R es posible añadir nuestros mapas temáticos un mapa base para dar contexto nuestra representación. Para ello, es necesario activar una suerte de “modo interactivo,” para ser más exactos, lo que activamos es el modo de visualización para poder desplazarnos sobre los mapas. Para cambiar al modo de visualización:La función que permite agregar mapas base es tm_basemap().Ejercicio¿Cuántos tipos de mapas base es posible usar en R? Revisa la ayuda de la función y familiarízate con sus argumentos.De los argumentos para el mapa base, dos son los básicos: el servidor de donde tomaremos el mapa (consejo: ya dentro de los argumentos de la función, escribe providers$ y observarás todas las opciones disponibles) y transparencia (argumento alpha=) que toma valores de 0 1; el argumento alpha= altera tanto la transparencia del mapa base cuando es usado dentro de la función tm_basemap(), como la transparencia de los colores usados para representar la información si se usa dentro de la función tm_fill(). Ejecuta el siguiente segmento de código y navega sobre el mapa representado:Una vez que terminado de navegar y trabajar con mapas base es necesario desactivar el modo de visualización y regresar al modo estático:","code":"\ntmap::tmap_mode(\"view\")## tmap mode set to interactive viewing\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", alpha=0.7)+\n  tmap::tm_basemap(providers$OpenStreetMap,alpha = 0.5)\ntmap::tmap_mode(\"plot\")## tmap mode set to plotting"},{"path":"mapas-coropléticos-en-r.html","id":"cartograma","chapter":"2 Mapas coropléticos en R","heading":"2.8 Cartograma","text":"Un cartograma es un tipo de mapa que deforma la geometría de las áreas de interés en figuras cuyo tamaño depende de la magnitud de la variable representada. Para elaborar un cartograma con circulos, debemos primero generar la geometría deformada por la variable para luego rellenarla. Para tal efecto, usamos la función cartogram_dorling() que nos permitirá generar un nuevo objeto que contiene las geometrías deformadas. Luego, usaremos dicha geometría como argumento de tm_shape(). Primero creamos la nueva geometría circular:Hay que tener en cuenta que la función sólo admite variables negativas. Ahora bien, este objeto recién creado será usado tal como lo hemos hecho antes:Otro tipo de cartograma, más estético, es aquel que garantiza la contigüidad entre las unidades espaciales después de su deformación. Se procede de forma semejante lo hecho antes, sólo que ahora usamos la función cartogram_cont():Ahora, usamos esta información para construir nuestro cartograma:\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2023-02-06.book built bookdown R package.","code":"\ncartograma_circulos <- cartogram::cartogram_dorling(zmvm_cov_sf,\"pos_hab\")\nclass(cartograma_circulos)## [1] \"sf\"         \"data.frame\"\ntmap::tm_shape(cartograma_circulos) +\n  tmap::tm_borders()+\n  tmap:: tm_fill(\"pos_hab\")\ncartograma_cont <- cartogram_cont(zmvm_cov_sf,\"pos_hab\")\ntmap::tm_shape(cartograma_cont) +\n  tmap::tm_fill(\"pos_hab\") +\n  tmap:: tm_borders()"},{"path":"mapas-coropléticos-en-r.html","id":"centroides-o-coordenadas-geométricas-punto-medio","chapter":"2 Mapas coropléticos en R","heading":"2.9 Centroides o coordenadas geométricas (punto medio)","text":"En el blog de mappingGIS encontramos la siguiente definición de punto medio:Toda geometría vectorial (punto, línea o polígono) contiene un punto central denominado centroide.Calcular el centroide de una geometría suele ser una tarea habitual cuando se trabaja con información espacial, por ejemplo, para contar con un punto de referencia partir del cual contar distancias lineales entre polígonos para definir vecindades, como se verá en el capítulo siguiente. Para crear una nueva capa que contenga los centroides de nuestra geometría usamos en R la función st_centroid(). La función generará una nueva serie de archivos SHP que contendrán el conjunto de variables de la base de datos y los centroides:El conjunto de datos recién creado y que contiene los centroides puede ser representado agregando otra “capa” con la función tm_shape(). En el ejemplo, sólo se superponen las dos capas: la de los polígonos originales y la de los centroides:Como habrás podido notar, hemos agregado otra función que define la forma en que ha de representarse la capa de los centroides: tm_dots(). Ahí, es posible especificar la manera en que deseamos que aparezcan los centroides, por ejemplo, en color rojo y más grandes:Para guardar la capa que contiene los centroides en un nuevo archivo SHP, usamos las siguientes líneas de código:Como siempre, se recomienda revisar la documentación de ayuda de la función st_write() para conocer todos los detalles de los argumentos.","code":"\nzmvm_cntrds <- st_centroid(zmvm_cov_sf)\nsummary(zmvm_cntrds)##     cvemun            cve_ent            cve_mun             nom_zm         \n##  Length:76          Length:76          Length:76          Length:76         \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n##      cve_zm       nom_mun            nom_ent            nom_abr         \n##  Min.   :9.01   Length:76          Length:76          Length:76         \n##  1st Qu.:9.01   Class :character   Class :character   Class :character  \n##  Median :9.01   Mode  :character   Mode  :character   Mode  :character  \n##  Mean   :9.01                                                           \n##  3rd Qu.:9.01                                                           \n##  Max.   :9.01                                                           \n##       ext             pob20            pob20_h          pob20_m      \n##  Min.   :  3.17   Min.   :   4862   Min.   :  2338   Min.   :  2524  \n##  1st Qu.: 37.52   1st Qu.:  31900   1st Qu.: 15621   1st Qu.: 16302  \n##  Median : 76.22   Median : 160445   Median : 78574   Median : 81871  \n##  Mean   :103.51   Mean   : 286902   Mean   :138458   Mean   :148443  \n##  3rd Qu.:157.01   3rd Qu.: 432692   3rd Qu.:206156   3rd Qu.:226858  \n##  Max.   :434.26   Max.   :1835486   Max.   :887651   Max.   :947835  \n##    positivos         defuncione        pos_mil         lag_poshab    \n##  Min.   :   18.0   Min.   :   0.0   Min.   : 1.720   Min.   : 2.382  \n##  1st Qu.:  131.5   1st Qu.:  13.5   1st Qu.: 3.337   1st Qu.: 3.977  \n##  Median :  767.0   Median :  86.0   Median : 5.117   Median : 5.275  \n##  Mean   : 2614.0   Mean   : 269.7   Mean   : 6.964   Mean   : 6.733  \n##  3rd Qu.: 3617.2   3rd Qu.: 387.0   3rd Qu.: 8.636   3rd Qu.: 8.229  \n##  Max.   :18767.0   Max.   :2078.0   Max.   :22.700   Max.   :16.443  \n##     pos_hab          def_hab             ss           ppob_sines      \n##  Min.   : 1.720   Min.   :0.0000   Min.   :0.5480   Min.   :0.004937  \n##  1st Qu.: 3.337   1st Qu.:0.4222   1st Qu.:0.6415   1st Qu.:0.019428  \n##  Median : 5.117   Median :0.6528   Median :0.6763   Median :0.023845  \n##  Mean   : 6.964   Mean   :0.6735   Mean   :0.6793   Mean   :0.026127  \n##  3rd Qu.: 8.636   3rd Qu.:0.8791   3rd Qu.:0.7266   3rd Qu.:0.031717  \n##  Max.   :22.700   Max.   :1.6497   Max.   :0.7980   Max.   :0.064699  \n##    ppob_basi        ppob_media        ppob_sup           ocviv      \n##  Min.   :0.1410   Min.   :0.1645   Min.   :0.07101   Min.   :2.460  \n##  1st Qu.:0.4481   1st Qu.:0.2466   1st Qu.:0.13614   1st Qu.:3.530  \n##  Median :0.5295   Median :0.2644   Median :0.17064   Median :3.745  \n##  Mean   :0.5041   Mean   :0.2624   Mean   :0.20549   Mean   :3.690  \n##  3rd Qu.:0.5718   3rd Qu.:0.2831   3rd Qu.:0.25936   3rd Qu.:3.900  \n##  Max.   :0.6842   Max.   :0.3235   Max.   :0.67480   Max.   :4.520  \n##       occu          pintegra4_       pintegra6_        pintegra8_     \n##  Min.   :0.5600   Min.   :0.3657   Min.   :0.06644   Min.   :0.01745  \n##  1st Qu.:0.8600   1st Qu.:0.6772   1st Qu.:0.22736   1st Qu.:0.07521  \n##  Median :0.9900   Median :0.7161   Median :0.26250   Median :0.08992  \n##  Mean   :0.9692   Mean   :0.7013   Mean   :0.26006   Mean   :0.09188  \n##  3rd Qu.:1.0625   3rd Qu.:0.7474   3rd Qu.:0.29090   3rd Qu.:0.10844  \n##  Max.   :1.2900   Max.   :0.8261   Max.   :0.42810   Max.   :0.20046  \n##    ppob_5_o_m       ppob_3_o_m         ppob_1          ppob_1dorm     \n##  Min.   :0.6825   Min.   :0.1680   Min.   :0.00611   Min.   :0.08567  \n##  1st Qu.:0.7781   1st Qu.:0.3425   1st Qu.:0.03234   1st Qu.:0.20592  \n##  Median :0.8183   Median :0.3925   Median :0.04198   Median :0.22763  \n##  Mean   :0.8133   Mean   :0.3934   Mean   :0.04413   Mean   :0.23367  \n##  3rd Qu.:0.8569   3rd Qu.:0.4406   3rd Qu.:0.05341   3rd Qu.:0.26855  \n##  Max.   :0.9150   Max.   :0.5947   Max.   :0.09461   Max.   :0.38234  \n##    ppob_2dorm       ppob_3dorm       pviv_ocu5_       pviv_ocu7_      \n##  Min.   :0.4875   Min.   :0.7781   Min.   :0.0654   Min.   :0.009724  \n##  1st Qu.:0.5883   1st Qu.:0.8581   1st Qu.:0.2434   1st Qu.:0.056245  \n##  Median :0.6237   Median :0.8848   Median :0.2835   Median :0.069583  \n##  Mean   :0.6307   Mean   :0.8808   Mean   :0.2799   Mean   :0.069614  \n##  3rd Qu.:0.6757   3rd Qu.:0.9042   3rd Qu.:0.3209   3rd Qu.:0.082869  \n##  Max.   :0.8006   Max.   :0.9484   Max.   :0.4420   Max.   :0.155107  \n##    pviv_ocu9_           analf            sbasc             vhac      \n##  Min.   :0.002354   Min.   :0.3534   Min.   : 5.535   Min.   : 3.95  \n##  1st Qu.:0.015350   1st Qu.:1.6505   1st Qu.:19.915   1st Qu.:16.40  \n##  Median :0.020738   Median :1.9798   Median :22.973   Median :21.19  \n##  Mean   :0.020961   Mean   :2.3555   Mean   :23.160   Mean   :21.01  \n##  3rd Qu.:0.025183   3rd Qu.:2.8267   3rd Qu.:27.193   3rd Qu.:25.00  \n##  Max.   :0.060174   Max.   :7.4096   Max.   :41.399   Max.   :38.81  \n##      po2sm          idh2015             im          gm_2020         \n##  Min.   :28.45   Min.   :0.6240   Min.   :53.57   Length:76         \n##  1st Qu.:61.27   1st Qu.:0.7288   1st Qu.:57.36   Class :character  \n##  Median :67.40   Median :0.7585   Median :58.46   Mode  :character  \n##  Mean   :66.38   Mean   :0.7629   Mean   :58.47                     \n##  3rd Qu.:72.76   3rd Qu.:0.7945   3rd Qu.:59.65                     \n##  Max.   :86.67   Max.   :0.9290   Max.   :62.36                     \n##       grad            grad_h           grad_m           poind        \n##  Min.   : 8.080   Min.   : 8.140   Min.   : 8.030   Min.   :0.06475  \n##  1st Qu.: 9.557   1st Qu.: 9.658   1st Qu.: 9.490   1st Qu.:0.11379  \n##  Median :10.025   Median :10.160   Median : 9.945   Median :0.17993  \n##  Mean   :10.252   Mean   :10.376   Mean   :10.139   Mean   :0.19532  \n##  3rd Qu.:10.840   3rd Qu.:10.967   3rd Qu.:10.730   3rd Qu.:0.26407  \n##  Max.   :14.550   Max.   :14.930   Max.   :14.220   Max.   :0.44291  \n##      pocom             poss            tmind            tmcom       \n##  Min.   :0.1269   Min.   :0.2113   Min.   : 1.636   Min.   : 1.380  \n##  1st Qu.:0.3363   1st Qu.:0.3033   1st Qu.: 3.166   1st Qu.: 1.876  \n##  Median :0.4358   Median :0.3610   Median : 5.086   Median : 2.139  \n##  Mean   :0.4085   Mean   :0.3961   Mean   : 9.086   Mean   : 2.759  \n##  3rd Qu.:0.4970   3rd Qu.:0.4176   3rd Qu.: 9.195   3rd Qu.: 3.009  \n##  Max.   :0.6867   Max.   :0.7686   Max.   :48.067   Max.   :10.310  \n##       tmss            rmind            rmcom             rmss        \n##  Min.   : 1.670   Min.   : 10.26   Min.   : 4.083   Min.   :  2.586  \n##  1st Qu.: 2.072   1st Qu.: 30.95   1st Qu.:14.085   1st Qu.: 16.312  \n##  Median : 2.775   Median : 63.61   Median :21.002   Median : 28.922  \n##  Mean   : 5.765   Mean   : 67.19   Mean   :24.502   Mean   : 39.904  \n##  3rd Qu.: 4.604   3rd Qu.: 88.88   3rd Qu.:31.630   3rd Qu.: 52.806  \n##  Max.   :41.086   Max.   :177.45   Max.   :67.438   Max.   :255.589  \n##       den                   geometry \n##  Min.   :  123.2   POINT        :76  \n##  1st Qu.:  463.5   epsg:NA      : 0  \n##  Median : 1830.4   +proj=lcc ...: 0  \n##  Mean   : 4227.5                     \n##  3rd Qu.: 6216.9                     \n##  Max.   :17519.3\ntm_shape(zmvm_cov_sf) +\n  tm_borders() +\n  tm_shape(zmvm_cntrds) +\n  tm_dots()\ntm_shape(zmvm_cov_sf) +\n  tm_borders() +\n  tm_shape(zmvm_cntrds) +\n  tm_dots(size=0.2,col=\"red\")\nsf::st_write(obj=zmvm_cntrds, \"zmvm_cntrds\", driver = \"ESRI Shapefile\")"},{"path":"mapas-coropléticos-en-r.html","id":"tópico-adicional-mapas-de-clasificación-especial-elementos-de-programación","chapter":"2 Mapas coropléticos en R","heading":"2.10 Tópico adicional: Mapas de clasificación especial (elementos de programación)","text":"En esta sección se construyen dos tipos de mapas de clasificación especial, es decir, destinados hacer notar los valores atípicos. obstante, la construcción de dichos mapas en R supone cierto conocimiento sobre programación que está fuera del alcance de estas notas. Si deseas profundizar en el aprendizaje de programación en R, el libro de Roger D. Peng, R Programming Data Science es un excelente material, particularmente el capítulo 14 dedicado las funciones en R (Roger D. Peng 2015).Con esta advertencia, llevamos cabo la exposición de esta sección, esperando motivarte para que tú misma profundices en los tópicos de programación.Se señaló antes que, hasta donde tenemos conocimiento, en R es posible construir mapas de clasificación especial con tmap través de las opciones de estilo, obstante, con algunos elementos de programación es posible solventar esta tarea. En esta sección nos servimos del código proporcionado por Luc Anselin y su equipo quienes, en un esfuerzo de difusión del conocimiento sobre el uso de estas herramientas, pone nuestra disposición una basta cantidad de materiales en la página del Center Spatial Data Science de la Universidad de Chicago (Anselin Morrison 2018).Para construir un mapa de intervalos definidos por el usuario, hay que recurrir al argumento breaks= dentro de la función tm_fill(). Para definir los intervalos de forma adecuada, se recomienda mirar las características resumen de la variable de interés, en este caso el número de casos positivos por COVID19 por cada mil habitantes, pos_hab:Esto nos permitirá conocer los valores que toma la variable de interés y pensar la manera en que deseamos delimitar las categorías de nuestro mapa. Supongamos que deseamos 6 categorías y, dado el rango de nuestra variable (valores entre 1.720 y 22.7), podemos fijar los cortes de los intervalos en 2.0, 6.0, 12., 18.0 y 24.0; además, se requiere incluir también un mínimo y máximo, digamos 1.0 y 25.0, para las categorías.La información, tanto de los cortes como de los máximos y mínimos deber ser especifica en forma de vector: c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0). Debemos además especificar la paleta de colores que deseamos usar, atendiendo lo dicho antes, usaremos una paleta con tres anclas: del amarillo, al naranja y al café, Yellow-Orange-Brown,YlOrBr:Un mapa de percentiles es un tipo espacial de mapa de cuantiles en el que se especifican seis categorías: 0-1%,1-10%, 10-50%,50-90%,90-99% y 99-100%; de ellas las que interesan son las categorías que agrupan el 1% de los valores más bajos (0-1%) y el 1% de los valores más altos (99-100%), es decir, las observaciones extremas. Este tipo de mapas es útil, justamente, para identificar la localización de valores atíticos en el espacio. Para poder construirlo, en R debemos llevar cabo los siguientes pasos:Extraer la variable de nuestro arreglo de datos.Calcular los percentiles de nuestro interés, es decir, 0.0,0.01,0.1,0.5,0.9,0.99,1.0.Construir el mapa con base en los intervalos definidos través de la función tm_fill().Para el paso , lo primero será construir una función que llamaremos get.var, que tendrá dos argumentos, varnom y df, el primero indicará, entre comillas, el nombre de la variable utilizar y el segundo indicará la base de la que proviene. Esta función permite que, al extraer la variable de la base, se eliminen los “aspectos espaciales” asociadas ella. Esta función sólo se requiere construir una vez.Ahora, echando mano de la función creada extraeremos la variable de interés sin sus aspectos espaciales:Para el paso ii, hemos de crear un objeto en el que se guarden los percentiles de interés, un vector de 6 elementos:Ahora, se calculan y guardan valores de los percentiles con base en el par de objetos creados, percentiles y pos_hab, es decir, obtendremos los valores de la variable pos_hab en los percentiles de interés:Ahora, en el paso iii, la construcción del mapa de percentiles es posible uniendo todos los elementos:Con el camino que hemos seguido en la sección previa, es posible ahorrarnos muchos pasos y crear nuestra propia función (que opera en el entorno de trabajo activo de la sesión) para construir mapas de percentiles. Nuestra función se llamará percentmap() y tendrá los siguientes argumentos:varnom: nombre de la variable (especificada como texto entre comillas).df: base de datos que contiene el variable.legtitle: titulo de la leyenda del mapa.mtitle: título del mapa.Para crear la función:La función que hemos creado para hacer nuestros mapas tiene cuatro argumentos: varnom,df,legtitle y mtitle. Los dos últimos tienen valores por omisión, lo que significa que es necesario especificarlos al usar la función. Los dos primeros tienen valor por omisión, por lo que será forzoso especificar dichos argumentos.Ahora, invocando nuestra propia función e indicando los argumentos forzosos, varnom y df tenemos que:Podemos, como es obvio, cambiar los dos argumentos dados por omisión:Los capítulos 1 y 2 de este libro, constituyen lo que suele ser denominado Análisis Exploratorio de Datos, (Exploratory Data Analysis, EDA). El capítulo 1 del e-Handbook Statistical Methods expone con detalle esta concepción en el análisis de información (Croarkin Tobias 2014). Además, en el capítulo 7 del ya citado libro R Data Science también explica con detalle el enfoque EDA usando R.En el siguiente capítulo continuamos con la exploración de la información, pero incorporando una estructura de relaciones en el espacio, por lo que dicho enfoque se le conoce como Análisis Exploratorio de Datos Espaciales. En dicho capítulo será de nuestro interés particular un rasgo que suele estar presente en la información georreferenciada: la autocorrelación espacial.","code":"\nsummary(zmvm_cov_sf$pos_hab)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   1.720   3.337   5.117   6.964   8.636  22.700\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\",title=\"Casos positivos covid\",breaks=c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0),palette=\"YlOrBr\")+\n  tm_layout(title = \"Cortes personalizados\", title.position = c(\"center\",\"top\"))\nget.var <- function(varnom,df) {#Definición de la función\n  v <- df[varnom] %>% st_set_geometry(NULL) #Extracción de la variable de interés y remoción de sus características geográficas\n  v <- unname(v[,1]) #Selección de la columna del data frame extraído que contiene la variable de interés y elimina su nombre pues sólo queremos un vector.\n  return(v) #Resultado de la función: la variable como vector sin sus características espaciales\n}\npos_hab<-get.var(\"pos_hab\",zmvm_cov_sf)\npos_hab##  [1] 14.364996 16.983175 13.352654 16.163929 17.363687 17.313543 14.943518\n##  [8] 17.404756 18.701249 13.677709 20.559563 10.224540 14.717256 22.700331\n## [15] 11.088257  5.882283  4.938574  5.239423  2.570694  3.351482  4.267922\n## [22]  2.382445  7.896182  2.088929  5.500198  5.891309  5.601076  9.197806\n## [29]  8.113844  3.295057  3.676214  3.250036  3.583416  6.155522  2.124319\n## [36]  2.787239  3.293624  6.120050  1.760416  5.184329  2.941489  3.873824\n## [43]  2.507745  5.423064  7.354686  2.224869  3.992095  8.598203  6.385731\n## [50]  4.385253  3.702180  3.693010  5.894044  4.147922  9.766454  3.853830\n## [57]  5.546263  4.766342  8.751090  1.719690  3.024390  2.372319  2.380410\n## [64]  3.876611  4.272596  7.134726  6.920539  5.049320  4.917293  2.582625\n## [71]  3.918632  3.399002  2.473636  2.822012  6.913242 13.935302\npercentiles <- c(0.0,0.01,0.1,0.5,0.9,0.99,1.0)\nvarperc <- quantile(pos_hab,percentiles)\nvarperc##        0%        1%       10%       50%       90%       99%      100% \n##  1.719690  1.750234  2.428041  5.116824 15.553724 21.094755 22.700331\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\",title=\"Índice de marginación 2010\", breaks=varperc, palette=\"-RdBu\",labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))+\n  tmap::tm_borders() +\n  tmap::tm_layout(title = \"Mapa de percentiles\", title.position = c(\"center\",\"bottom\"))\n  percentmap <- function(varnom,df,titulo.leyenda=NA,titulo.principal=\"Mapa de percentiles\"){ #Definición de la función y sus argumentos\n  #Elementos preliminares\n  percent <- c(0,.01,.1,.5,.9,.99,1) #Vector que contiene los percentiles de interés para el mapa\n  var <- get.var(varnom,df) #Extracción de la variable de la base de datos con la función anterior\n  varperc <- quantile(var,percent) #Cálculo de los percentiles de la variable extraída\n  #Especificaciones del mapa\n  tm_shape(df) +\n     tm_fill(varnom,title=titulo.leyenda,breaks=varperc,palette=\"-RdBu\", labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(title = titulo.principal, title.position = c(\"center\",\"bottom\"))\n}\npercentmap(\"pos_hab\",zmvm_cov_sf)\npercentmap(\"pos_hab\",zmvm_cov_sf, titulo.leyenda = \"Categorías\", titulo.principal = \"El título que yo quiera\" )"},{"path":"mapas-coropléticos-en-r.html","id":"una-palabra-de-advertencia","chapter":"2 Mapas coropléticos en R","heading":"2.10.1 Una palabra de advertencia","text":"En esta sección se construyen dos tipos de mapas de clasificación especial, es decir, destinados hacer notar los valores atípicos. obstante, la construcción de dichos mapas en R supone cierto conocimiento sobre programación que está fuera del alcance de estas notas. Si deseas profundizar en el aprendizaje de programación en R, el libro de Roger D. Peng, R Programming Data Science es un excelente material, particularmente el capítulo 14 dedicado las funciones en R (Roger D. Peng 2015).Con esta advertencia, llevamos cabo la exposición de esta sección, esperando motivarte para que tú misma profundices en los tópicos de programación.","code":""},{"path":"mapas-coropléticos-en-r.html","id":"mapa-de-intervalos-personalizados","chapter":"2 Mapas coropléticos en R","heading":"2.10.2 Mapa de intervalos personalizados","text":"Se señaló antes que, hasta donde tenemos conocimiento, en R es posible construir mapas de clasificación especial con tmap través de las opciones de estilo, obstante, con algunos elementos de programación es posible solventar esta tarea. En esta sección nos servimos del código proporcionado por Luc Anselin y su equipo quienes, en un esfuerzo de difusión del conocimiento sobre el uso de estas herramientas, pone nuestra disposición una basta cantidad de materiales en la página del Center Spatial Data Science de la Universidad de Chicago (Anselin Morrison 2018).Para construir un mapa de intervalos definidos por el usuario, hay que recurrir al argumento breaks= dentro de la función tm_fill(). Para definir los intervalos de forma adecuada, se recomienda mirar las características resumen de la variable de interés, en este caso el número de casos positivos por COVID19 por cada mil habitantes, pos_hab:Esto nos permitirá conocer los valores que toma la variable de interés y pensar la manera en que deseamos delimitar las categorías de nuestro mapa. Supongamos que deseamos 6 categorías y, dado el rango de nuestra variable (valores entre 1.720 y 22.7), podemos fijar los cortes de los intervalos en 2.0, 6.0, 12., 18.0 y 24.0; además, se requiere incluir también un mínimo y máximo, digamos 1.0 y 25.0, para las categorías.La información, tanto de los cortes como de los máximos y mínimos deber ser especifica en forma de vector: c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0). Debemos además especificar la paleta de colores que deseamos usar, atendiendo lo dicho antes, usaremos una paleta con tres anclas: del amarillo, al naranja y al café, Yellow-Orange-Brown,YlOrBr:","code":"\nsummary(zmvm_cov_sf$pos_hab)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   1.720   3.337   5.117   6.964   8.636  22.700\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\",title=\"Casos positivos covid\",breaks=c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0),palette=\"YlOrBr\")+\n  tm_layout(title = \"Cortes personalizados\", title.position = c(\"center\",\"top\"))"},{"path":"mapas-coropléticos-en-r.html","id":"mapas-de-valores-extremos","chapter":"2 Mapas coropléticos en R","heading":"2.10.3 Mapas de valores extremos","text":"","code":""},{"path":"mapas-coropléticos-en-r.html","id":"mapa-de-percentiles","chapter":"2 Mapas coropléticos en R","heading":"2.10.3.1 Mapa de percentiles","text":"Un mapa de percentiles es un tipo espacial de mapa de cuantiles en el que se especifican seis categorías: 0-1%,1-10%, 10-50%,50-90%,90-99% y 99-100%; de ellas las que interesan son las categorías que agrupan el 1% de los valores más bajos (0-1%) y el 1% de los valores más altos (99-100%), es decir, las observaciones extremas. Este tipo de mapas es útil, justamente, para identificar la localización de valores atíticos en el espacio. Para poder construirlo, en R debemos llevar cabo los siguientes pasos:Extraer la variable de nuestro arreglo de datos.Calcular los percentiles de nuestro interés, es decir, 0.0,0.01,0.1,0.5,0.9,0.99,1.0.Construir el mapa con base en los intervalos definidos través de la función tm_fill().Para el paso , lo primero será construir una función que llamaremos get.var, que tendrá dos argumentos, varnom y df, el primero indicará, entre comillas, el nombre de la variable utilizar y el segundo indicará la base de la que proviene. Esta función permite que, al extraer la variable de la base, se eliminen los “aspectos espaciales” asociadas ella. Esta función sólo se requiere construir una vez.Ahora, echando mano de la función creada extraeremos la variable de interés sin sus aspectos espaciales:Para el paso ii, hemos de crear un objeto en el que se guarden los percentiles de interés, un vector de 6 elementos:Ahora, se calculan y guardan valores de los percentiles con base en el par de objetos creados, percentiles y pos_hab, es decir, obtendremos los valores de la variable pos_hab en los percentiles de interés:Ahora, en el paso iii, la construcción del mapa de percentiles es posible uniendo todos los elementos:","code":"\nget.var <- function(varnom,df) {#Definición de la función\n  v <- df[varnom] %>% st_set_geometry(NULL) #Extracción de la variable de interés y remoción de sus características geográficas\n  v <- unname(v[,1]) #Selección de la columna del data frame extraído que contiene la variable de interés y elimina su nombre pues sólo queremos un vector.\n  return(v) #Resultado de la función: la variable como vector sin sus características espaciales\n}\npos_hab<-get.var(\"pos_hab\",zmvm_cov_sf)\npos_hab##  [1] 14.364996 16.983175 13.352654 16.163929 17.363687 17.313543 14.943518\n##  [8] 17.404756 18.701249 13.677709 20.559563 10.224540 14.717256 22.700331\n## [15] 11.088257  5.882283  4.938574  5.239423  2.570694  3.351482  4.267922\n## [22]  2.382445  7.896182  2.088929  5.500198  5.891309  5.601076  9.197806\n## [29]  8.113844  3.295057  3.676214  3.250036  3.583416  6.155522  2.124319\n## [36]  2.787239  3.293624  6.120050  1.760416  5.184329  2.941489  3.873824\n## [43]  2.507745  5.423064  7.354686  2.224869  3.992095  8.598203  6.385731\n## [50]  4.385253  3.702180  3.693010  5.894044  4.147922  9.766454  3.853830\n## [57]  5.546263  4.766342  8.751090  1.719690  3.024390  2.372319  2.380410\n## [64]  3.876611  4.272596  7.134726  6.920539  5.049320  4.917293  2.582625\n## [71]  3.918632  3.399002  2.473636  2.822012  6.913242 13.935302\npercentiles <- c(0.0,0.01,0.1,0.5,0.9,0.99,1.0)\nvarperc <- quantile(pos_hab,percentiles)\nvarperc##        0%        1%       10%       50%       90%       99%      100% \n##  1.719690  1.750234  2.428041  5.116824 15.553724 21.094755 22.700331\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\",title=\"Índice de marginación 2010\", breaks=varperc, palette=\"-RdBu\",labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))+\n  tmap::tm_borders() +\n  tmap::tm_layout(title = \"Mapa de percentiles\", title.position = c(\"center\",\"bottom\"))"},{"path":"mapas-coropléticos-en-r.html","id":"construcción-de-una-función-personalizada-para-hacer-mapas-de-percentiles","chapter":"2 Mapas coropléticos en R","heading":"2.10.4 Construcción de una función personalizada para hacer mapas de percentiles","text":"Con el camino que hemos seguido en la sección previa, es posible ahorrarnos muchos pasos y crear nuestra propia función (que opera en el entorno de trabajo activo de la sesión) para construir mapas de percentiles. Nuestra función se llamará percentmap() y tendrá los siguientes argumentos:varnom: nombre de la variable (especificada como texto entre comillas).df: base de datos que contiene el variable.legtitle: titulo de la leyenda del mapa.mtitle: título del mapa.Para crear la función:La función que hemos creado para hacer nuestros mapas tiene cuatro argumentos: varnom,df,legtitle y mtitle. Los dos últimos tienen valores por omisión, lo que significa que es necesario especificarlos al usar la función. Los dos primeros tienen valor por omisión, por lo que será forzoso especificar dichos argumentos.Ahora, invocando nuestra propia función e indicando los argumentos forzosos, varnom y df tenemos que:Podemos, como es obvio, cambiar los dos argumentos dados por omisión:Los capítulos 1 y 2 de este libro, constituyen lo que suele ser denominado Análisis Exploratorio de Datos, (Exploratory Data Analysis, EDA). El capítulo 1 del e-Handbook Statistical Methods expone con detalle esta concepción en el análisis de información (Croarkin Tobias 2014). Además, en el capítulo 7 del ya citado libro R Data Science también explica con detalle el enfoque EDA usando R.En el siguiente capítulo continuamos con la exploración de la información, pero incorporando una estructura de relaciones en el espacio, por lo que dicho enfoque se le conoce como Análisis Exploratorio de Datos Espaciales. En dicho capítulo será de nuestro interés particular un rasgo que suele estar presente en la información georreferenciada: la autocorrelación espacial.","code":"\n  percentmap <- function(varnom,df,titulo.leyenda=NA,titulo.principal=\"Mapa de percentiles\"){ #Definición de la función y sus argumentos\n  #Elementos preliminares\n  percent <- c(0,.01,.1,.5,.9,.99,1) #Vector que contiene los percentiles de interés para el mapa\n  var <- get.var(varnom,df) #Extracción de la variable de la base de datos con la función anterior\n  varperc <- quantile(var,percent) #Cálculo de los percentiles de la variable extraída\n  #Especificaciones del mapa\n  tm_shape(df) +\n     tm_fill(varnom,title=titulo.leyenda,breaks=varperc,palette=\"-RdBu\", labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(title = titulo.principal, title.position = c(\"center\",\"bottom\"))\n}\npercentmap(\"pos_hab\",zmvm_cov_sf)\npercentmap(\"pos_hab\",zmvm_cov_sf, titulo.leyenda = \"Categorías\", titulo.principal = \"El título que yo quiera\" )"},{"path":"análisis-espacial-i-autocorrelación.html","id":"análisis-espacial-i-autocorrelación","chapter":"3 Análisis espacial I: autocorrelación","heading":"3 Análisis espacial I: autocorrelación","text":"Observa el siguiente mapa. En él, se representa la variable índice de de desarrollo humano que calculó el Programa de Naciones Unidad para el Desarrollo (PNUD) en 2015.Figura 3.1. Índice de desarrollo humano para los municipios de la ZMVM, 2015¿Notas algún patrón? ¿Detectas cómo los municipios con un valor alto del índice de desarrollo (Benito Juárez, Alvaro Obregón, Miguel Hidalgo, Coyoacán y otros) se encuentran agrupados en la parte centro-sur de la región considerada? Dicho patrón ilustra la posible presencia de autocorrelación espacial, es decir, que los valores de un indicador en una zona estén rodeados por valores muy semejantes o nada semejantes: en ambos casos decimos que hay autocorrelación. Cuando tratamos con datos de una variable georeferenciada hay altas probabilidades de que dicha variable esté autocorrelacionada y esto hace necesario que sean necesarias ciertas técnicas para su tratamiento, es decir, para su representación y modelación. En este capítulo trataremos con el concepto clave de autocorrelación espacial y veremos través de que instrumentos es posible medirla para, más adelante, incorporar su riqueza informativa una explicación sustantiva de los fenómenos socioterritoriales analizados.Una definición sintética de autocorrelación espacial es la que nos brinda Chasco (2003) como “la relación funcional existente entre los valores que adopta un indicador en una zona del espacio y en zonas vecinas” (Chasco 2003, 49). Por ejemplo, imagina que el barrio de la ciudad donde vives presenta un alto número de contagios por COVID19 y, además, los barrios vecinos tienen también valores altos: en este caso es probable que tengamos autocorrelación espacial positiva.La identificación de autocorrelación espacial es importante como parte del proceso de análisis del fenómeno socioterritoriales al menos por dos cuestiones, una de carácter técnica durante la modelación econométrica y otra de carácter sustantiva en relación con la explicación del fenómeno. Respecto la razón técnica, si existe autocorrelación espacial en nuestros datos lo más probable es que la estimación de los parámetros de un modelo con de mínimos cuadrados ordinarios deje de ser válida, en la medida en que se cumplen los supuestos que requiere dicho procedimiento, específicamente, que los errores o perturbaciones del modelo estén correlacionados; sobre esto abundaremos en el capítulo siguiente cuando hagamos un repaso de los modelos clásicos de regresión lineal.Por otro lado, respecto la razón sustantiva, emerge la pregunta, ¿qué pude estar ocurriendo que hace que un fenómeno aparezca, por ejemplo, agrupado en el espacio, es decir, que se distribuya aleatoriamente en el territorio? ¿Por qué se concentra la actividad económica en determinadas ciudades o por qué algunos servicios sólo se brindan en una zona de la ciudad? Esto respecto fenómenos económicos, pero ¿qué hay con el hecho de que una enfermedad se concentra notoriamente en algunas áreas de la ciudad y en otras? Dicho en otras palabras, ¿qué hay detrás de la formación de un patrón en la forma en que se distribuye un fenómeno en el espacio y cómo puede ser esto explicado? eso nos referimos cuando decimos que hay elementos sustantivos para el análisis al hallar evidencia de autocorrelación espacial.Regresemos la definición brindada de autocorrelación y analicémosla con más cuidado:Relación funcional existente entre los valores que adopta un indicador en una zona del espacio y en zonas vecinas.La definición se integra por tres elementos clave: ) valor de un indicador, ii) relación funcional y iii) zonas vecinas. Para dar sentido nuestra definición pensemos en una afirmación como “los casos positivos de COVID19 en la alcaldía Azcapotzalco están asociados en forma directa con los casos positivos de COVID19 en las alcaldías y municipios vecinos que integran la Zona Metropolitana del Valle de México.” Tendríamos entonces que:Indicador: casos positivos por COVID19.Relación funcional: asociación positiva o directa.Zonas vecinas: alcaldías y municipios vecinos de Azcapotzalco.El primer elemento presenta dificultad alguna puesto que se refiere al valor de una variable en el espacio, tal y como la tenemos en la base de datos que hemos estado utilizando: casos positivos por COVID19 por cada 1 mil habitantes dentro de cuyas observaciones se encuentra Azcapotzalco; en tanto, el segundo elemento de nuestra afirmación es una mera suposición, es decir, que hay una relación positiva; por su parte, el tercer elemento “alcaldías y municipios vecinos” implica un problema: ¿de qué modo es posible establecer qué alcaldías son o vecinas de Azcapotzalco?Hay múltiples maneras que definir si un objeto espacial tiene o vecinos, por ejemplo, podríamos decir que aquellas alcaldías que compartan límites administrativos con la demarcación territorial de nuestro interés serán sus vecinos (vecindad por adyacencia) o también sería posible establecer que las alcaldías vecinas serán aquellas que estén menos de 10 km de distancia del centro económico de la alcaldía (vecindad por umbral de distancia) e incluso podríamos decir que las 3 alcaldías o municipios más cercanos serán los vecinos.Ejercicio¿Se te ocurre algún otro criterio para establecer vecindad?¿Se te ocurre algún otro criterio para establecer vecindad?¿Cómo llamarías un criterio de vecindad donde elijas los 3 vecinos más cercanos?¿Cómo llamarías un criterio de vecindad donde elijas los 3 vecinos más cercanos?¿partir de qué punto en el espacio será más conveniente medir la distancia, desde el centro económico de la alcaldía o municipio (por ejemplo su zona industrial o comercial) o desde la sede de la administración local?¿partir de qué punto en el espacio será más conveniente medir la distancia, desde el centro económico de la alcaldía o municipio (por ejemplo su zona industrial o comercial) o desde la sede de la administración local?¿La distancia más indicada usada como criterio de vecindad será una distancia lineal o una distancia por carretera?¿La distancia más indicada usada como criterio de vecindad será una distancia lineal o una distancia por carretera?Una vez que hemos visto que que existen diferentes criterios de vecindad, debemos pensar en una manera de almacenar dicha información. La Zona Metropolitana del Valle de México tiene 76 unidades espaciales, ¿de qué modo podemos apuntar todas las posibles relaciones de vecindad entre ellas? Las personas interesadas en el análisis espacial han propuesto un ingenioso instrumento matemático para captar y sintetizar cómo un objeto se relaciona con otros, es decir, para captar la estructura espacial del área de interés dada por las relaciones de vecindad. Dicho instrumento es denominado matriz de pesos espaciales; ilustremos esta idea. Piensa en un vecindario o área de estudio compuesto sólo por seis elementos, tal como se ilustra en la figura 3.2:\nFigura 3.1: Vecindario regular\npartir de la disposición de este hipotético vecindario nos interesa construir una matriz de pesos espaciales, el instrumento para captar la estructura espacial de dicho vecindario. De los múltiples criterios de vecindad existentes, comencemos por el de adyacencia o contigüidad. decir de Anselin (2020) “contigüidad significa que dos unidades espaciales comparten un borde común de longitud distinta de cero. Desde el punto de vista operativo, podemos distinguir entre un criterio de contigüidad de tipo torre y de tipo reina, en analogía con los movimientos permitidos para las piezas así nombradas en un tablero de ajedrez. El criterio de la torre define los vecinos por la existencia de un borde común entre dos unidades espaciales. El criterio de la reina es algo más amplio y define los vecinos como unidades espaciales que comparten un borde o un vértice comunes” (Anselin 2020).De este modo, del criterio de vecindad por adyacencia tenemos dos tipos: torre y reina. Para construir nuestra matriz de pesos espaciales elijamos el criterio más amplio, de la reina. ¿Cómo podemos plasmar las relaciones de contigüidad entre los seis elementos de la figura 3.2? Pensemos en un cuadro que tiene tantas filas y columnas como objetos espaciales tiene nuestro vecindario, semejante al que aparece en la figura 3.3:\nFigura 3.2: Matriz ejemplo vacía\nDicho cuadro será nuestra matriz de pesos espaciales y contendrá la estructura espacial del vecindario de la figura 3.2. Para un criterio de vecindad por adyacencia de tipo reina, ¿el elemento 1 y 2, son vecinos? Las unidades espaciales 1 y 2 de nuestro vecindario comparten un borde, por tanto, son vecinos y en el elemento (1,2) de nuestra matriz colocaremos un número 1; lo mismo ocurre entre las unidades espaciales 1 y 3 que, al compartir un lado, son vecinos y por tanto el elemento (1,3) de la matriz será también un 1. ¿Qué pasa entre los objetos 1 y 4? En este caso, hay ni bordes ni vértices en común, por tanto, hay una relación de vecindad, entonces, en el elemento (1,4) habremos de colocar un 0 que indica ausencia de vecindad. En síntesis, dado determinado criterio de vecindad, si dos objetos espaciales son vecinos, la relación de vecindad se indica mediante un número 1, en tanto, cuando hay relación de vecindad su ausencia se indica colocando un 0. Hagamos esto para cada celda de la matriz hasta llenarla completamente y obtener algo parecido lo que aparece en la figura 3.4.\nFigura 3.3: Matriz ejemplo llena\nResumamos lo dicho hasta este punto. El cuadro que acabamos de llenar es conocido como matriz de pesos espaciales5 y es el instrumento que nos permite sintetizar las relaciones espaciales o estructura de vecindad que corresponde determinado criterio. Habrá, por tanto, diversos tipos de matrices en función del criterio de vecindad elegido. Esta matriz se denotada por la letra mayúscula \\(W\\) y está integrada por los elementos \\(w_{ij}\\) que toman el valor de 1 cuando el elemento \\(j\\) y el elemento \\(\\) son vecinos y 0 (cero) en cualquier otro caso. Este instrumento es uno de los más importantes en econometría espacial ya que permite construir los estadísticos de autocorrelación espacial y es la manera en que podemos incorporar al espacio como variable partir de lo que denominamos “rezago espacial,” como más adelante veremos en este y en el siguiente capítulo. Las características de la matriz de pesos espaciales son:Es una matriz que en la diagonal principal contiene sólo ceros, es decir, se asume que por definición hay interacciones dentro de un mismo elemento (lo que necesariamente es cierto y que dependerá de la escala de análisis).Es una matriz simétrica, es decir, se asume que hay interacción de “ida y vuelta,” por lo que con un instrumento de estas características es posible asumir efectos de interacción en un solo sentido.Es una matriz cuadrada, es decir, de dimensiones \\(n \\cdot n\\), donde \\(n\\) es el número de objetos espaciales.Acabamos de ilustrar la lógica con la que puede ser construida una matriz de pesos espaciales partir de una retícula regular con apenas seis elementos. Veamos ahora cómo obtener matrices de pesos espaciales sirviéndonos de R, ya que desarrollar los pasos anteriores para un vecindario compuesto por 76 objetos espaciales que son las alcaldías y municipios que integran el Valle de México es tarea para una máquina, para nosotros.En R hay muchas rutas para desarrollar la misma tarea. Presentamos en este capítulo dos rutas: la primera se sirve del paquete spdep de Roger Bivand, en tanto que la segunda sigue la propuesta de Xun Li y su paquete rgeoda, una librería para llevar cabo análisis espacial basado en la funcionalidades del software GeoDa.Roger Bivand y un equipo de colaboradores desarrollaron el paquete spdep para la construcción de matrices de pesos espaciales y el análisis espacial. En este enfoque la función poly2nb() nos permite calcular estructuras espaciales dadas partir de dos tipos de vecindad por adyacencia, una más estricta (argumento queen=FALSE) y que la otra (argumento queen=TRUE). En la documentación de la función podemos leer que: “si es VERDADERO, TRUE, un solo punto límite compartido cumple la condición de contigüidad; si es FALSO, FALSE, se requiere más de un punto compartido; ten en cuenta que más de un punto límite compartido significa necesariamente una línea límite compartida.” Además, para cargar la base de datos espacial recurriremos al paquete rgdal.Lo dicho en el párrafo anterior es relevante en el sentido de que estos criterios son exactamente los mismos que definimos antes (contigüidad reina y torre). Procedamos pues la instalación de los paquetes, en caso de que aún estén en nuestro sistema:Primero, llamemos las librerías y carguemos la base de datos:Ahora, construyamos un objeto que llamaremos mTRUE, dicho objeto contendrá los elementos que definen la estructura espacial, es decir, será nuestra matriz de pesos espaciales. Esto lo haremos con la función poly2nb():El segmento de código anterior genera un objeto de tipo nb. Verifica sus características con class() y str() . Además, nota que en la sección de ambiente de trabajo (cuadrante superior derecho, en la pestaña ambiente) aparece el objeto creado. Da clic en la imagen de la lupa para visualizarlo e intenta interpretar el resultado de la ventana.Ahora, llama al objeto y presta atención sobre los resultados que aparecen en la consola:En la consola aparecen los siguientes elementos:Objeto de lista de vecinos:Número de regiones: 76. Corresponde al número de alcaldías y municipios que componen la Zona Metropolitana del Valle de México.Número de enlaces distintos de cero: 380. Es el número de elementos de una matriz de 76x76 que registran relación de vecindad, dicho en otras palabras, es el total de números uno de la matriz.Porcentaje de pesos distintos de cero: 6.57. Resultado de dividir 380 entre (76x76).Número promedio de vínculos: 5. Número de vecinos que en promedio tiene cada municipio o alcaldía.Ahora bien, para construir una lista de vecindad con base en un criterio más estricto, es decir, queen = FALSE, procedemos como:Ahora, llama dicho objeto y contrasta con los resultados anteriores.Ejercicio¿Qué objeto, mTRUE o mFALSE, tiene el mayor número de vínculos diferentes de cero?¿Qué objeto, mTRUE o mFALSE, tiene el mayor número de vínculos diferentes de cero?¿Por qué crees que esto es así?¿Por qué crees que esto es así?Estas listas, que contienen nuestras estructuras espaciales almacenadas en los objeto de tipo nb llamados mTRUE y mFALSE, pueden representarse visualmente través de un gráfico de conectividad que representa la estructura espacial definida por cada criterio través de líneas que unen los municipios considerados vecinos.Para visualizar el mapa de conectividad recurriremos la función plot() y se superpondran dos gráficas: una sólo con los bordes o límites nivel municipal y otra con los centroides y la estructura espacial. Aquí se muestra el mapa de conectividad resultado de la matriz mTRUEPara comparar ambas estructuras espaciales, mTRUE y mFALSE podemos superponer las dos gráficas y asignar colores diferentes:Como puedes observar, son estructuras muy parecidas, aunque aun así es posible notar sus diferencias.diferencia del enfoque previo, con el paquete rgeoda es necesario cargar la base de datos espacial través del paquete sf. Si los instalado:Para cargarlos:Ahora bien, carguemos la base de datos espacial con sf:EjercicioObserva tu ambiente de trabajo (ventana superior derecha, en la pestaña “ambiente”). ¿De qué tipo es el objeto covid_zmvm_sf y de cuál covid_zmvm?Observa tu ambiente de trabajo (ventana superior derecha, en la pestaña “ambiente”). ¿De qué tipo es el objeto covid_zmvm_sf y de cuál covid_zmvm?¿qué crees que se deban dichas diferencias?¿qué crees que se deban dichas diferencias?través del paquete rgeoda es posible construir, como en GeoDa, cuatro tipos de matrices de pesos:Matrices basadas contigüidadMatrices basadas en distanciaMatrices de k-vecinos más cercanosMatrices de pesos por kernelDe ellas, aquí construiremos sólo las tres primeras.En rgeoda hay dos matrices de pesos espaciales basadas en contigüidad, tal y como lo expisimos en la primera sección de este capítulo, las de tipo reina y las de tipo torre. Para construir una matriz de tipo reina recurrimos la función queen_weight() y para una de tipo torre usamos rook_weights(), ambas funciones tienen cuatro argumentos:sf_obj=: nuestra cartografía en formato sf, tal y como la hemos ya cargado.order=: orden de contigüidad, donde si es igual 1 se indica que sólo los objetos espaciales inmediatos serán vecinos y si es mayor que uno indicará una vecindad de orden superior (si es 2 indica que los vecinos de mis vecinos serán mis vecinos, si es tres los vecinos de mis vecinos de mis vecinos serán mis propios vecinos, y así sucesivamente).include_lower_order=: indica si los vecinos de ordenes inferiores incluidos en la estructura de vecindad.precision_threshold=: este argumento modifica la precisión de la geometría y se usará en caso de que, habiendo observaciones aisladas, se detecte que algún objeto tiene vecinos.Así, para construir la matriz de tipo reina:En tanto, una matriz de contigüidad con el criterio de tipo torre::Los objetos recién creados y que contienen las estructuras de vecindad pueden ser grabadas en un archivo fuera del ambiente del trabajo, para hacerlas permanentes. Para ello usamos la función save_weights(), esta función tiene cuatro argumentos obligatorios: nombre de la matriz hacerse permanente (gda_w=), el identificador único para cada objeto espacial (id_variable=), la ruta donde se almacenará el archivo de salida y el nombre del archivo (out_path=) y el nombre de la capa de entrada (layer_name=):Si bien se dijo antes, en la sección dedicada comentar qué es una matriz de pesos espaciales, que se definía la estructura espacial través de una matriz, computacionalmente esto es así, como seguro habrás notado al tratar de leer las matrices construidas con spdep en la sección anterior. Identifica los archivos creados con las funciones previas, en la sección de archivos en la sección inferior derecha (pestaña files) y selecciona q_1.gal. Notarás que, en efecto, el archivo es una matriz, sino una lista. El hecho de que se usen listas y matrices para el proceso de cómputo de las estructuras espaciales es porque las listas son más conveniente en términos de la cantidad ocupada de recursos del sistema. La razón detrás de ello es que las matrices de pesos espaciales son matrices dispersas, es decir, matrices que contienen muchos elementos que son cero.Del archivo que acabas de abrir, q_1.gal, veámos con detenimiento su contenido para comprender mejor cómo se almacena la información. Reproducimos en seguida las tres primeras líneas:0 76 | covid_zmvm_sf | cvemun | | |\n09010 | 5 | | | |\n09014 | 09008 | 09016 | 09003 | 09012 |Línea 1: en la primera columna aparece el número de objetos espaciales (76), en la segunda columna el nombre del archivo del que proviene la estructura (covid_zmvm_sf) y en la tercera la clave de identificación única para cada objeto (cvemun).Línea 2: en la primera columna se indica el objeto espacial su clave (09010), mientras que en la segunda columna se indica el número de vecinos que dicho objeto tiene, en este caso, cinco.Línea 3: en esta fila aparece, para cada columna, la clave de identificación de los cinco vecinos de 09010: 09014 09008 09016 09003 09012.El resto de las líneas tiene la misma interpretación que las líneas 2 y 3: número de vecinos del objeto listado e identificación de los vecinos. Una vez que hemos comprendido la estructura de dicho archivo, pidamos un resumen del objeto queen_w:De dicho resumen es posible apuntar que: nuestro vecindario, la Zona Metropolitana del Valle de México, tiene 76 unidades espaciales (number observations), la matriz construida es simétrica (symmetric: TRUE), los municipios y alcaldías tienen al menos un vecino (# min neighbors: 1), el número máximo de vecinos es de 9 (# max neighbors: 9), la media de 4.68 (# mean neighbors), la mediana de 4 (# median neighbors) y que hay observaciones sin vecinos (isolates: FALSE). El elemento que hemos indicado es sparsity: que tiene un valor de 0.0616. Dicho valor corresponde la proporción de elementos diferentes cero en una matriz de 76x76 objetos, es decir, nuestra matriz sólo tiene 6.16% son diferentes de cero.En `rgeoda`` hay dos tipos de matrices que recurren la distancia: la matriz de úmbral de distancia mínima y la matriz de k-vecinos más cercanos. Para el caso de la matriz de umbral, se considera que dos objetos espaciales, en el caso aquí analizado, dos municipios o alcaldías, son vecinos siempre que estén dentro de cierto umbral de distancia dado. Para el caso de la matriz de k-vecinos más cercanos, la idea es que un objeto espacial tendrá como vecinos los k-objetos más cercanos.Para construir una matriz de pesos espaciales basada este criterio tenemos proceder en dos pasos: ) definir un umbral de distancia mínimo en el cual todas las unidades espaciales tienen al menos un vecino, ii) usar dicho umbral para hallar cada uno de los vecinos, dado ese umbral.Para definir el umbral usamos la función min_distthreshold() del paquete `rgeoda``:La distancia mínima para que cada uno de los 76 municipios y alcaldías tenga un vecino es 13,943.63 metros. Sabemos que las unidades del resultado anterior son metros puesto que en el archivo con extensión .prj (el archivo que contiene la información sobre la proyección cartográfica utilizada) así se indica. Ahora bien, definido el umbral ya podemos construir la matriz:Cuando decimos k-vecinos más cercanos, con ello queremos decir que se identificará determinado número “k” de vecinos más cercanos: los 2 más cercanos (k=2), los 8 más cercanos (k=8), etcétera. Para ello nos servimos de la función knn_weights(). Si quiseramos una matriz con los 4 vecinos más cercanos, tenemos que:diferencia de las matrices anteriores, ésta es simétrica y, como te podrás dar cuenta, hace que cada objeto espacial tenga exactamente el mismo número de vecinos, en este caso, cuatro.La construcción de un rezago espacial, también llamada variable espacialmente rezagada, es un elemento clave para poder operacionalizar y, por tanto, medir la autocorrelación. Pero, ¿qué es un rezago espacial? Imagina que vives en un barrio de la Ciudad de México que tiene 8 barrios vecinos, mismos que calculaste con alguno de los criterios de que vimos antes y que apuntaste en una matriz de pesos espaciales. Supón ahora que tu barrio tiene 5 casos de COVID19 y quieres comparar dicho dato con el de los 8 barrios vecinos, ¿cómo lo harías? Una alternativa útil es sintetizar la información de los ocho barrios en un sólo indicador que sume y pondere los datos de los casos positivos de los vecinos.Veamos esto con más cuidado y definamos formalmente rezago espacial. Antes dijimos que cada uno de los elementos \\(w_{ij}\\) de la matriz de pesos espaciales \\(W\\) pueden tomar como valores ceros o unos. La matriz \\(W\\) puede escribirse como:\\[\nW=\n\\begin{pmatrix}\nw_{11} & w_{12} & \\cdots  & w_{1n}\\\\\nw_{21} & w_{22} & \\cdots  & w_{2n}\\\\\n\\vdots & \\vdots & \\ddots  & \\vdots \\\\\nw_{n1} & w_{n2} & \\dots  & w_{nn}\\\\\n\\end{pmatrix}\n\\]obstante, es posible expresar dicha matriz \\(W\\) de una forma diferente, normalizandola por filas. Normalizar una matriz de pesos espaciales por filas implica dividir cada elemento \\(w_{ij}\\) de una fila entre la suma de elementos diferentes cero de dicha fila, por lo que los elementos de una matriz de pesos espaciales estandarizada por fila es:\\[w_{ij(s)}=\\frac {w_{ij}} {\\sum{w_{ij}}}\\]\nNuestra matriz de pesos espaciales estandarizada por filas, \\(W_s\\), es pues una transformación de la matriz original que hace que todas las filas sumen en total 1:$$\nw_{11(s)} + w_{12(s)} + + w_{1n(s)}= 1\\\nw_{21(s)} + w_{22(s)} + + w_{2n(s)}=1\\\nw_{n1(s)} + w_{n2(s)} + + w_{nn(s)}=1$$Regresemos nuestra definición sobre la autocorrelación espacial: relación funcional existente entre los valores que adopta un indicador en una zona del espacio con respecto al valor de sus zonas vecinas, dicho valor es lo que llamamos rezago espacial. Así pues, un rezago espacial es definido como el promedio ponderado del valor de la variable de los vecinos (Chasco 2003, 61; Anselin 2020). Siguiendo Anselin (2020), “el rezago espacial de \\(y\\) del objeto espacial \\(\\) es expresado como \\(Wy_{}\\):\\[\n\\begin{aligned}\nWy_i &=  w_{i1(s)}y_1+w_{i2(s)}y_2+...+w_{(s)}y_n \\\\\nWy_i &=  \\sum_{j=1}^nw_{ij(s)}y_j \\\\\n\\end{aligned}\n\\]Donde \\(w_{ij(s)}\\) es cada uno de los elementos de la matriz de pesos estandarizada por fila y \\(y_n\\) es el valor de la variable de interés. Así pues, el rezago espacial pondera la variable de interés través del número de vecinos que cada unidad espacial posee.De nueva cuenta, es posible construir rezagos espaciales ya sea con el paquete spded o rgeoda. Primero ilustraremos la alternativa con spdep y en seguida con rgeoda.Para construir con spdep el rezago espacial de una variable, digamos de los casos positivos por COVID19, debemos primero estandarizar los objetos que contienen las estructuras espaciales que previamente construimos: mTRUE y mFALSE. Este proceso corre cuenta de la función nb2listw() del paquete spdep:Notaras cómo en el ambiente de trabajo se ha creado un objeto nuevo de tipo listw. Ábrelo y observa su contenido.Ejercicio¿Explica por qué se le denomina matriz (lista) estandarizada?¿Explica por qué se le denomina matriz (lista) estandarizada?¿cuanto es igual la suma de cada renglón de la lista?¿cuanto es igual la suma de cada renglón de la lista?¿Cómo se relaciona la forma en que aparecen enlistados los elementos con el número de vecinos que tiene cada objeto espacial?¿Cómo se relaciona la forma en que aparecen enlistados los elementos con el número de vecinos que tiene cada objeto espacial?Construiremos un rezago espacial de la variable pos_hab, número de casos positivos por COVID19, con ayuda de la función lag.listw() del paquete spdep. Indicamos dos argumentos en la función: la estructura espacial dada por la matriz estandarizada, mTRUE.est, y la variable de la que deseamos el rezago espacial, pos_hab, esto es guardado en un nuevo objeto, lag_poshab, tal y como se muestra en el siguiente segmento de código:Para lograr apreciar mejor el rezago espacial, construiremos una tabla de dos columnas que almacenaremos en el objeto df, la primera contendrá la variable original y la segunda el rezago espacial, luego pediremos que nos muestre los primeros registros de la tabla con la función head() en un formato estilizado través de la función kable() del paquete knitr:El valor de la segunda columna, lag_poshab, es el rezago espacial. ¿Cómo interpretamos dicho valor? Veamos con cuidado. El primer valor listado de pos_hab, 14.365, es el número de casos positivos de COVID19 por cada 1 mil habitantes y dicho valor pertenece la alcaldía Álvaro Obregón, en tanto, el primer valor listado en la columna lag_poshab es el rezago espacial y asciende 15.992, este valor es el promedio ponderado de casos positivos por COVID19 en los vecinos Álvaro Obregón.Para identificar los vecinos de cada objeto espacial (municipio o alcaldía), así como el respectivo valor del rezago espacial hay que extraer dicha información utilizando una notación de dobles corchetes. Vayamos por pasos. Primero, para conocer cual es el primer elemento u objeto espacial de nuestra base usamos la notación del doble corchete sobre el objeto covid_zmvm:Una vez que sabemos que dicho objeto es la alcaldía llamada Álvaro Obregón, pidamos R que nos muestre cuáles son sus vecinos. Para ello, usando de nuevo la notación de doble corchete sobre la estructura espacial, el objeto mTRUE:Los números anteriores corresponden los vecinos de Álvaro Obregón, pero, ¿cómo saber su nombre y el número de casos positivos de cada uno? El siguiente segmento de código nos dará los nombres de los vecinos y el número de casos positivos en cada uno.Los valores previamente listados corresponden tanto al nombre como al número de casos positivos de los vecinos de Álvaro Obregón. Así, el rezago espacial de Álvaro Obregón es el resultado de sumar los valores de la tabla anterior y multiplicarlos por \\(\\frac {1}{6}\\), es decir, 15.99.Ejercicio¿Por que para el caso de Álvaro Obregón hubo que multiplicar por un sexto?¿Por que para el caso de Álvaro Obregón hubo que multiplicar por un sexto?Obtén un cuadro con las lista de vecinos y los valores de casos positivos para el objeto espacial 35Obtén un cuadro con las lista de vecinos y los valores de casos positivos para el objeto espacial 35Con el paquete rgeoda calcular rezagos espaciales es sencillo con la función spatial_lag(), en la que hay que especificar dos argumentos: gda_w= la matriz de pesos espaciales y df= la variable sobre la que se desea el rezago espacial, que proviene del objeto de tipo sf. Por ejemplo, para la variable casos positivos, pos_hab y con una matriz de tipo reina, queen_w:Para mejor mirar el rezago espacial, incorporémoslo un nuevo dataframe con la variable pos_hab, de forma emejante como lo hicimos antes:EjercicioCon el paquete rgeoda, construye rezagos espaciales con las otras estructuras espaciales: torre, distancia mínima, k-vecinos.Con el paquete rgeoda, construye rezagos espaciales con las otras estructuras espaciales: torre, distancia mínima, k-vecinos.¿Por qué en cada caso es diferente el valor rezago?¿Por qué en cada caso es diferente el valor rezago?Una vez que hemos abordado y resuelto el problema de cómo definir vecindad partir de la matriz de pesos espaciales y que hemos operacionalizado la definición de valor de la variable en los vecinos través de la noción de rezago espacial, tenemos todos los elementos que integran la definición de autocorrelación espacial: ) relación funcional, ii) valor de una variable y iii) relación de vecindad.Ahora bien, ¿cómo medimos la autocorrelación? Es decir, cómo sabemos si, por ejemplo, la variable casos positivos por COVID19 está autocorrelacionada. Al principio de este capítulo dijimos que apreciar ciertos patrones de agrupamiento en un mapa, como el del índice de desarrollo humano, era un posible indicio de autocorrelación espacial. Es momento de formalizar dicho indicio través de un indicador apropiado. Debemos pues construir un estadístico, un estadístico de autocorrelación espacial.El estadístico de asociación espacial más socorrido es el propuesto por Patrick Moran: la de Moran. La de Moran es un coeficiente de correlación lineal que “incorpora al espacio,” es decir, mide la asociación lineal entre una variable (digamos casos positivos por COVID19) y su rezago espacial (el valor promedio de los casos positivos de los vecinos). Como todo coeficiente de asociación, el valor de la de Moran se encuentra entre -1 y 1.Si el valor de la de Moran es positivo decimos que hay signos de concentración o patrones de aglomeración pues existe autocorrelación espacial positiva, por lo que existen unidades espaciales (alcaldías, municipios) que tienen valores altos en la variable medida que están rodeadas por otras unidades espaciales que tienen valores también altos. Decir que existe autocorrelación espacial positiva también implica afirmar que hay unidades espaciales con valores bajos rodeadas de otras que tienen también valores bajos.Por otro lado, si la de Moran es negativa esto es evidencia de otro tipo de patrones, ya de aglomeración sun ode dispersión o repulsión: una alcaldía o municipio que tiene valores altos está rodeada de vecinos con valores bajos y viceversa.Hasta donde sabemos, el paquete rgeoda aún incorpora una función para construir automáticamente la de Moran, por lo que el enfoque aquí presentado corresponde, para simplificar, al del paquete spdep.Para calcular en R el coeficiente o estadístico de Moran necesitamos recurrir la función moran.test() del paquete spdep. Los argumentos de la función deben especificar el nombre de la variable y el tipo de estructura espacial dado por la matriz de pesos usada; adicionalmente, se puede indicar qué hacer en caso de que existan islas (objetos espaciales sin vecinos) con el argumento zero.policy.EjercicioSolicita ayuda del paquete spdep y responde, ¿para qué sirve el argumento randomization de la función moran.test()?Solicita ayuda del paquete spdep y responde, ¿para qué sirve el argumento randomization de la función moran.test()?De nuevo, en la ayuda de la función, ¿con cuál de los argumentos es posible cambiar la hipótesis alternativa de la evaluación de autocorrelación espacial en la prueba de Moran?De nuevo, en la ayuda de la función, ¿con cuál de los argumentos es posible cambiar la hipótesis alternativa de la evaluación de autocorrelación espacial en la prueba de Moran?Construyamos el estadístico de Moran para la variable pos_hab usando la estructura espacial llamada mTRUE.est:Del indicador obtenido nos interesan tres elementos, como es usual: la magnitud del coeficiente, su sentido y su significancia estadística. En los resultados que aparecen en tu consola identifica cada uno de ellos y responde:Ejercicio¿cuánto asciende el coeficiente estimado? ¿Podría decirse que es alto o bajo?¿cuánto asciende el coeficiente estimado? ¿Podría decirse que es alto o bajo?¿La relación identificada es positiva o negativa?¿La relación identificada es positiva o negativa?Un coeficiente como el obtenido, de 0.656, indica que hay una relación positiva entre los valores de los casos positivos de COVID19 y los valores de los casos positivos por COVID19 en los entornos vecinos (sentido de la asociación), además, podríamos decir que es relativamente alto en la medida en que está más próximo uno que cero (magnitud de la relación). Ahora, ¿qué hay con su significancia estadística? ¿Cómo podemos saber que dicho resultado,. el 0.656, es producto de una coincidencia sino un resultado sistemático o consistente? Para ello, evaluaremos el siguiente juego de hipótesis sobre el índice de Moran:\\(Ho: =0\\), es decir, ausencia de autocorrelación o, de forma equivalente, distribución espacial aleatoria.\\(Ha:\\neq 0\\), es decir, presencia de autocorrelación o, de forma equivalente, distribución espacial aleatoria.Con la información disponible evaluamos dichas hipótesis. Observa como el p-valor obtenido en extremadamente pequeño (p-value < 2.2e-16); si fijamos un nivel de significancia, \\(\\alpha=0.05\\), se rechaza la hipótesis nula en favor de la hipótesis alternativa, por tanto, la variable se distribuye de forma aleatoria en el espacio, sino que muestra indicios de autocorrelación espacial positiva relativamente alta (0.656).EjercicioConstruya una de Moran con la estructura espacial dada por la matriz donde queen=FALSE responda:¿La asociación espacial es positiva o negativa?¿La asociación espacial es positiva o negativa?¿Consideras que es alta o baja?¿Consideras que es alta o baja?¿Dirías que dicha relación es producto del azar o que existe un comportamiento sistemático?¿Dirías que dicha relación es producto del azar o que existe un comportamiento sistemático?Una forma creativa de expresar gráficamente la autocorrelación es través de un diagrama de dispersión cuyo eje \\(x\\) corresponde la variable de interés, casos positivos de COVID19 por cada 1 mil habitantes, y el eje \\(y\\) su rezago espacial; además, al agregar una recta de ajuste sobre los datos estandarizados lograremos que la pendiente de dicha recta corresponda exactamente al valor de la de Moran. Una de las características del diagrama de Moran es que se descompone en cuatro cuadrantes, tal como aparece en la figura 3.5.\nFigura 3.4: Diagrama de dispersión de Moran\nPara representar el diagrama de Moran recurrimos la función moran.plot() del paquete spdep que para una matriz de tipo reina con el argumento queen=TRUE y la base de datos de tipo SpatialPolygonsDataFrame:En el diagrama de Moran recién construido notamos varias cosas:\n) Una parte importante de las observaciones caen en el cuadrante y III, por lo que la relación funcional que predomina entre el conjunto de puntos es positiva.\nii) Que la relación funcional dominante sea positiva implica que haya observaciones en los cuadrantes II y IV.Ejercicio¿Es posible construir un diagrama de Moran usando el paquete ggplot2? De ser así, ¿cómo lo harías?El índice de Moran que recién hemos calculado permite evaluar la existencia de un patrón espacial completo o global, es decir, para el conjunto de todas las observaciones. Por ello, proporciona información de la ubicación de las agrupaciones o clusters de alcaldías y municipios. Por ello se dice que la de Moran es una medida de autocorrelación espacial global: nos dice que hay patrones de concentración o dispersión pero nos dice qué municipios o alcaldías específicos es posible atribuir dichas fuerzas de aglomeración.Para subsanar esta situación, Anselin (1995) propuso la versión local de la de Moran: el indicador local de asociación espacial o LISA (local indicator spatial association).El LISA:Proporciona un estadístico para cada ubicación con un nivel de significancia yEstablece una relación proporcional entre el estadístico local y el global.Es decir, nos permite identificar que unidades espaciales (alcaldías o municipios) es posible atribuir la autocorrelación espacial de forma específica y con qué intensidad, en relación con el indicador global. La representación del LISA se hace con arreglo dos mapas:El mapa de cluster o mapa de agrupaciones que permite clasificar las áreas (alcaldías y municipios) con presencia de autocorrelación espacial según el tipo de asociación identificada. Este mapa permite la clasificación de las áreas estadísticamente significativas en clusters o agrupamientos (alto-alto y bajo-bajo) y de áreas que se constituyen como observaciones espaciales atípicas o spatial outliers (agrupamientos bajo-alto y alto-bajo).El mapa de significancia: muestra las ubicaciones con la de Moran local que son representativas en diferentes niveles de significancia.El Índice local de Moran toma la forma de:\\[I_i = \\frac{(x_i-\\bar{X})}{S_i^2}{\\sum_{j=1,j \\neq }^{n}w_{ij}(x_j-\\bar{X})}\\]\nDonde:\\[\nS_i^2= \\frac{\\sum_{j=1,j \\neq }^{n}(x_j-\\bar{X}) ^2}{n-1}\n\\]\nAdemás, \\(x_i\\) es el valor de la variable de interés, \\(\\bar{X}\\) es el promedio de dicha variable, \\(w_{ij}\\) es cada uno de los elementos de la matriz de pesos espaciales y \\(n\\) es el número de objetos espaciales.Para calcular un índice de Moran local en R usamos el paquete rgeoda, que es el enfoque más sencillo de ejecutar. Echaremos mano de la función local_moran(), que requiere dos argumentos forzosos: la estructura espacial(w=) y la variable para la que se desea el LISA (df=). Primero, guardemos la variable de la que deseamos un LISA en un objeto independiente:Luego, usemos dicha variable para construir el LISA:Dentro del objeto lisa_poshab hay múltiples elementos, que serán útiles más adelante. De momento, llamemos los valores del indicador local de asociación espacial través de la función lisa_values(), es decir, los valores del índice computado para cada uno de los 76 municipios y alcaldías que componen la ZMVM:Para representar en un mapa los valores del LISA, es necesario añadirlos la base de datos original en formato sf:Luego, con las funciones aprendidas en el capítulo anterior, podemos presentar un mapa de quintiles que represente el valor del indicador local de asociación espacial:El mapa permite observar la manera en que varía la correlación espacial nivel local para la variable pos_hab, casos positivos por COVID19 por cada 1 mil habitantes, pues proporciona un valor de correlación para cada municipio y alcaldía: es claro que la mayor parte de los valores altos del LISA se encuentran en la Ciudad de México (tonos en azul).Necesitamos otros instrumentos que nos permitan identificar si los valores de la de Moran local son o significativos, lo que haremos través del mapa de cluster y su respectivo mapa de significancia. Esto nos permitirá identificar agrupaciones o núcleos de cluster significativos, así como observaciones espaciales atípicas.Dentro del objeto creado, lisa_poshab, hay múltiples elementos, mismos que pueden ser llamados por funciones específicas, entre las que se encuentran:lisa_clusters(): que los valores de clasificación de cada cluster.lisa_colors(): brinda los colores asociados cada uno de los clusters computados.lisa_labels(): proporciona las etiquetas de los clusters computados.Así pues, el mapa los construimos en dos pasos: ) primero obtendremos algunos elementos preliminares para construir el mapa (colores, etiquetas y valores de agrupamiento) y ii) construiremos el mapa con las funciones básicas de R_Ahora, con dichos elementos auxiliares, construimos el mapa de cluster:Junto con el mapa anterior es común presentar el mapa de significancia, un tipo de representación que indica el nivel de significancia inividual para cada una de las observaciones (municipios y alcaldías). Para ello, también diviimos el procedimiento en dos partes:El par de mapas anteriores permiten identificar agrupamientos de valores significativos al 5% o menos, es decid, núcleos de cluster que muestran municipios y alcaldías con valores altos de tasas positivas de COVID19 rodeados de vecinos con valores altos (agrupamiento Alto-Alto), así como agrupamientos de valores bajos (cuadrante Bajo-Bajo), además de observaciones espaciales atípicas (cuadrante Alto-bajo y Bajo-Alto).En síntesis, hasta este punto hemos visto en este capítulo:Cómo definir estructuras de relación espacial través de diversos criterios,Cómo identificar autocorrelación espacial global través de la de Moran,Cómo evaluar la significancia estadística de la de Moran,Cómo identificar agrupaciones locales través del indicador LISA.En el capítulo 5 nos adentraremos en cómo incorporar la riqueza que proporciona el análisis espacial en un modelo econométrico. Mientras tanto, en el capítulo 4 llevaremos cabo un repaso de elementos básicos sobre los modelos de regresión lineal clásica con mínimos cuadrados ordinarios.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2023-02-06.book built bookdown R package.","code":"## Reading layer `covid_zmvm' from data source \n##   `C:\\Repositorios\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 76 features and 57 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5\n## Projected CRS: Lambert_Conformal_Conic\ninstall.packages(c(\"spdep\", \"rgdal\"))\nlibrary(rgdal)\nlibrary(spdep)\ncovid_zmvm <-rgdal::readOGR(\"base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp\")## OGR data source with driver: ESRI Shapefile \n## Source: \"C:\\Repositorios\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp\", layer: \"covid_zmvm\"\n## with 76 features\n## It has 57 fields\nmTRUE <- spdep::poly2nb(covid_zmvm)\nmTRUE## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 384 \n## Percentage nonzero weights: 6.648199 \n## Average number of links: 5.052632\nmFALSE<- spdep::poly2nb(covid_zmvm, queen = FALSE)\nmFALSE## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 372 \n## Percentage nonzero weights: 6.440443 \n## Average number of links: 4.894737\nplot(covid_zmvm, border = 'lightgrey')\nplot(mTRUE, coordinates(covid_zmvm), add=TRUE, col='lightblue')\nplot(covid_zmvm, border = 'lightgrey')\nplot(mTRUE, coordinates(covid_zmvm), add=TRUE, col='blue')\nplot(mFALSE, coordinates(covid_zmvm), add=TRUE, col='lightgreen')\ninstall.packages(c(\"rgeoda\", \"sf\"))\nlibrary(sf)\nlibrary(rgeoda)\ncovid_zmvm_sf <- sf::st_read(\"base de datos/covid_zmvm shp/covid_zmvm.shp\")## Reading layer `covid_zmvm' from data source \n##   `C:\\Repositorios\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 76 features and 57 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5\n## Projected CRS: Lambert_Conformal_Conic\nqueen_w <- rgeoda::queen_weights(covid_zmvm_sf)\nrook_w <- rgeoda::rook_weights(covid_zmvm_sf)\n#Para la matriz reina\nrgeoda::save_weights(gda_w=queen_w,\n             id_variable=covid_zmvm_sf['cvemun'],\n             out_path = 'base de datos/covid_zmvm shp/q_1.gal',\n             layer_name = 'covid_zmvm_sf')## [1] TRUE\n#Para la matriz toree\nrgeoda::save_weights(rook_w,\n             covid_zmvm_sf['cvemun'],\n             out_path = 'base de datos/covid_zmvm shp/r_1.gal', \n             layer_name = 'covid_zmvm_sf')## [1] TRUE\nsummary(queen_w)##                      name              value\n## 1 number of observations:                 76\n## 2          is symmetric:                TRUE\n## 3               sparsity: 0.0616343490304709\n## 4        # min neighbors:                  1\n## 5        # max neighbors:                  9\n## 6       # mean neighbors:   4.68421052631579\n## 7     # median neighbors:                  4\n## 8           has isolates:              FALSE\numbral <- rgeoda::min_distthreshold(covid_zmvm_sf)\numbral## [1] 13943.63\ndist_w <- rgeoda::distance_weights(covid_zmvm_sf, umbral)\nsummary(dist_w)##                      name              value\n## 1 number of observations:                 76\n## 2          is symmetric:                TRUE\n## 3               sparsity: 0.0772160664819945\n## 4        # min neighbors:                  1\n## 5        # max neighbors:                 11\n## 6       # mean neighbors:   5.86842105263158\n## 7     # median neighbors:                5.5\n## 8           has isolates:              FALSE\nk4_w <- knn_weights(covid_zmvm_sf, 4)\nsummary(k4_w)##                      name              value\n## 1 number of observations:                 76\n## 2          is symmetric:               FALSE\n## 3               sparsity: 0.0526315789473684\n## 4        # min neighbors:                  4\n## 5        # max neighbors:                  4\n## 6       # mean neighbors:                  4\n## 7     # median neighbors:                  4\n## 8           has isolates:              FALSE\nmTRUE.est <- spdep::nb2listw(mTRUE)\nmTRUE.est## Characteristics of weights list object:\n## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 384 \n## Percentage nonzero weights: 6.648199 \n## Average number of links: 5.052632 \n## \n## Weights style: W \n## Weights constants summary:\n##    n   nn S0       S1       S2\n## W 76 5776 76 35.56142 319.6623\nmFALSE.est <- spdep::nb2listw(mFALSE)\nmFALSE.est## Characteristics of weights list object:\n## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 372 \n## Percentage nonzero weights: 6.440443 \n## Average number of links: 4.894737 \n## \n## Weights style: W \n## Weights constants summary:\n##    n   nn S0       S1       S2\n## W 76 5776 76 36.56499 319.6716\nlag_poshab <- spdep::lag.listw(mTRUE.est, covid_zmvm$pos_hab)\n#Crea un nuevo arreglo de datos donde se almacena la variable original y el rezago espacial\ndf <- base::data.frame(pos_hab = covid_zmvm$pos_hab, lag_poshab)\n\nlibrary(knitr)\n\n#Coloca los primeros valores de ambas variables en una tabla, requiere instalación y carga del paquete  knitr\nkable(head(df))\ncovid_zmvm$nom_mun[[1]] #Devuelve el nombre del municipio identificado con el número 1## [1] \"Álvaro Obregón\"\nmTRUE[[1]]#Devuelve los números de identificación de los municipios y alcaldías vecinas del Álvaro Obregón, la observación identificada con el número 1.## [1]  2  7  9 10 11 15\n#Crea un objeto que contiene el nombre de las alcaldías y municipios vecinos de Álvaro Obregón\nAlcaldías <- covid_zmvm$nom_mun[mTRUE[[1]]]\n\n#Crea un objeto que contiene el valor de la variable pos_hab (casos positivos por mil habitantes) para cada observación vecina de Álvaro Obregón\nCasos_covid <- covid_zmvm$pos_hab[mTRUE[[1]]]\n\n# Guarda los dos objetos anteriores en un dataframe y lo muestra en una tabla\ndb <- data.frame(Alcaldías, Casos_covid)\nkable(db)\nlag <- rgeoda::spatial_lag(queen_w,\n                   covid_zmvm_sf['pos_hab'])\nhead(lag)##   Spatial.Lag\n## 1    15.45044\n## 2    15.57109\n## 3    13.91179\n## 4    12.22961\n## 5    13.20082\n## 6    11.51118\ndf <- base::data.frame(pos_hab = covid_zmvm_sf$pos_hab, lag)\nkable(head(df))\nspdep::moran.test(covid_zmvm$pos_hab, mTRUE.est)## \n##  Moran I test under randomisation\n## \n## data:  covid_zmvm$pos_hab  \n## weights: mTRUE.est    \n## \n## Moran I statistic standard deviate = 8.8625, p-value < 2.2e-16\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##       0.656334129      -0.013333333       0.005709563\nspdep::moran.plot(((covid_zmvm$pos_hab)-mean(covid_zmvm$pos_hab))/(sd(covid_zmvm$pos_hab)),\n                  listw = mTRUE.est, \n                  xlab=\"Casos positivos\",\n                  ylab=\"Rezago espacial de los casos positivos\",\n                  main=\"Diagrama de Moran para casos positivos\",\n                  col=\"lightblue\")\npos_hab=covid_zmvm_sf[\"pos_hab\"]\nlisa_poshab <- local_moran(w=queen_w,\n                           df=pos_hab)\nlisaval_poshab <- lisa_values(lisa_poshab)\nmapa.lisa <- base::cbind(covid_zmvm_sf, lisaval_poshab)\ntmap::tm_shape(mapa.lisa) +\n  tmap::tm_fill(col = \"lisaval_poshab\", style = \"quantile\",\n                palette = \"Spectral\", midpoint= NA,\n                title = \"I de Moran local\") +\n  tmap::tm_borders()\n#Elementos preliminares\nlisa_colores <- lisa_colors(lisa_poshab)\nlisa_etiq <- c(\"No significativo\", \"Alto-Alto\", \"Bajo-Bajo\", \"Bajo-Alto\", \"Alto-Bajo\", \"No definido\", \"Aislado\")\nlisa_clusters <- lisa_clusters(lisa_poshab)\n#Mapa de cluster\nplot(st_geometry(covid_zmvm_sf), \n     col=sapply(lisa_clusters, function(x){return(lisa_colores[[x+1]])}), \n     border = \"#333333\", lwd=0.2)\ntitle(main = \"Moran Local de pos_hab\")\nlegend('bottomleft', legend = lisa_etiq, fill = lisa_colores, border = \"#eeeeee\")\n#Elementos preliminares\nlisa_p <- lisa_pvalues(lisa_poshab)\np_etiq <- c(\"No significativo\", \"p <= 0.05\", \"p <= 0.01\", \"p <= 0.001\")\np_colores <- c(\"#eeeeee\", \"#84f576\", \"#53c53c\", \"#348124\")\n#Mapa de significancia\nplot(st_geometry(covid_zmvm_sf), \n     col=sapply(lisa_p, function(x){\n       if (x <= 0.001) return(p_colores[4])\n       else if (x <= 0.01) return(p_colores[3])\n       else if (x <= 0.05) return (p_colores[2])\n       else return(p_colores[1])\n       }), \n     border = \"#333333\", lwd=0.2)\ntitle(main = \"Mapa de significancia de los casos positivos\")\nlegend('bottomleft', legend = p_etiq, fill = p_colores, border = \"#eeeeee\")"},{"path":"análisis-espacial-i-autocorrelación.html","id":"autocorrelación-espacial-y-definición-de-vecindad","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.1 Autocorrelación espacial y definición de vecindad","text":"Una definición sintética de autocorrelación espacial es la que nos brinda Chasco (2003) como “la relación funcional existente entre los valores que adopta un indicador en una zona del espacio y en zonas vecinas” (Chasco 2003, 49). Por ejemplo, imagina que el barrio de la ciudad donde vives presenta un alto número de contagios por COVID19 y, además, los barrios vecinos tienen también valores altos: en este caso es probable que tengamos autocorrelación espacial positiva.La identificación de autocorrelación espacial es importante como parte del proceso de análisis del fenómeno socioterritoriales al menos por dos cuestiones, una de carácter técnica durante la modelación econométrica y otra de carácter sustantiva en relación con la explicación del fenómeno. Respecto la razón técnica, si existe autocorrelación espacial en nuestros datos lo más probable es que la estimación de los parámetros de un modelo con de mínimos cuadrados ordinarios deje de ser válida, en la medida en que se cumplen los supuestos que requiere dicho procedimiento, específicamente, que los errores o perturbaciones del modelo estén correlacionados; sobre esto abundaremos en el capítulo siguiente cuando hagamos un repaso de los modelos clásicos de regresión lineal.Por otro lado, respecto la razón sustantiva, emerge la pregunta, ¿qué pude estar ocurriendo que hace que un fenómeno aparezca, por ejemplo, agrupado en el espacio, es decir, que se distribuya aleatoriamente en el territorio? ¿Por qué se concentra la actividad económica en determinadas ciudades o por qué algunos servicios sólo se brindan en una zona de la ciudad? Esto respecto fenómenos económicos, pero ¿qué hay con el hecho de que una enfermedad se concentra notoriamente en algunas áreas de la ciudad y en otras? Dicho en otras palabras, ¿qué hay detrás de la formación de un patrón en la forma en que se distribuye un fenómeno en el espacio y cómo puede ser esto explicado? eso nos referimos cuando decimos que hay elementos sustantivos para el análisis al hallar evidencia de autocorrelación espacial.Regresemos la definición brindada de autocorrelación y analicémosla con más cuidado:Relación funcional existente entre los valores que adopta un indicador en una zona del espacio y en zonas vecinas.La definición se integra por tres elementos clave: ) valor de un indicador, ii) relación funcional y iii) zonas vecinas. Para dar sentido nuestra definición pensemos en una afirmación como “los casos positivos de COVID19 en la alcaldía Azcapotzalco están asociados en forma directa con los casos positivos de COVID19 en las alcaldías y municipios vecinos que integran la Zona Metropolitana del Valle de México.” Tendríamos entonces que:Indicador: casos positivos por COVID19.Relación funcional: asociación positiva o directa.Zonas vecinas: alcaldías y municipios vecinos de Azcapotzalco.El primer elemento presenta dificultad alguna puesto que se refiere al valor de una variable en el espacio, tal y como la tenemos en la base de datos que hemos estado utilizando: casos positivos por COVID19 por cada 1 mil habitantes dentro de cuyas observaciones se encuentra Azcapotzalco; en tanto, el segundo elemento de nuestra afirmación es una mera suposición, es decir, que hay una relación positiva; por su parte, el tercer elemento “alcaldías y municipios vecinos” implica un problema: ¿de qué modo es posible establecer qué alcaldías son o vecinas de Azcapotzalco?Hay múltiples maneras que definir si un objeto espacial tiene o vecinos, por ejemplo, podríamos decir que aquellas alcaldías que compartan límites administrativos con la demarcación territorial de nuestro interés serán sus vecinos (vecindad por adyacencia) o también sería posible establecer que las alcaldías vecinas serán aquellas que estén menos de 10 km de distancia del centro económico de la alcaldía (vecindad por umbral de distancia) e incluso podríamos decir que las 3 alcaldías o municipios más cercanos serán los vecinos.Ejercicio¿Se te ocurre algún otro criterio para establecer vecindad?¿Se te ocurre algún otro criterio para establecer vecindad?¿Cómo llamarías un criterio de vecindad donde elijas los 3 vecinos más cercanos?¿Cómo llamarías un criterio de vecindad donde elijas los 3 vecinos más cercanos?¿partir de qué punto en el espacio será más conveniente medir la distancia, desde el centro económico de la alcaldía o municipio (por ejemplo su zona industrial o comercial) o desde la sede de la administración local?¿partir de qué punto en el espacio será más conveniente medir la distancia, desde el centro económico de la alcaldía o municipio (por ejemplo su zona industrial o comercial) o desde la sede de la administración local?¿La distancia más indicada usada como criterio de vecindad será una distancia lineal o una distancia por carretera?¿La distancia más indicada usada como criterio de vecindad será una distancia lineal o una distancia por carretera?Una vez que hemos visto que que existen diferentes criterios de vecindad, debemos pensar en una manera de almacenar dicha información. La Zona Metropolitana del Valle de México tiene 76 unidades espaciales, ¿de qué modo podemos apuntar todas las posibles relaciones de vecindad entre ellas? Las personas interesadas en el análisis espacial han propuesto un ingenioso instrumento matemático para captar y sintetizar cómo un objeto se relaciona con otros, es decir, para captar la estructura espacial del área de interés dada por las relaciones de vecindad. Dicho instrumento es denominado matriz de pesos espaciales; ilustremos esta idea. Piensa en un vecindario o área de estudio compuesto sólo por seis elementos, tal como se ilustra en la figura 3.2:\nFigura 3.1: Vecindario regular\npartir de la disposición de este hipotético vecindario nos interesa construir una matriz de pesos espaciales, el instrumento para captar la estructura espacial de dicho vecindario. De los múltiples criterios de vecindad existentes, comencemos por el de adyacencia o contigüidad. decir de Anselin (2020) “contigüidad significa que dos unidades espaciales comparten un borde común de longitud distinta de cero. Desde el punto de vista operativo, podemos distinguir entre un criterio de contigüidad de tipo torre y de tipo reina, en analogía con los movimientos permitidos para las piezas así nombradas en un tablero de ajedrez. El criterio de la torre define los vecinos por la existencia de un borde común entre dos unidades espaciales. El criterio de la reina es algo más amplio y define los vecinos como unidades espaciales que comparten un borde o un vértice comunes” (Anselin 2020).De este modo, del criterio de vecindad por adyacencia tenemos dos tipos: torre y reina. Para construir nuestra matriz de pesos espaciales elijamos el criterio más amplio, de la reina. ¿Cómo podemos plasmar las relaciones de contigüidad entre los seis elementos de la figura 3.2? Pensemos en un cuadro que tiene tantas filas y columnas como objetos espaciales tiene nuestro vecindario, semejante al que aparece en la figura 3.3:\nFigura 3.2: Matriz ejemplo vacía\nDicho cuadro será nuestra matriz de pesos espaciales y contendrá la estructura espacial del vecindario de la figura 3.2. Para un criterio de vecindad por adyacencia de tipo reina, ¿el elemento 1 y 2, son vecinos? Las unidades espaciales 1 y 2 de nuestro vecindario comparten un borde, por tanto, son vecinos y en el elemento (1,2) de nuestra matriz colocaremos un número 1; lo mismo ocurre entre las unidades espaciales 1 y 3 que, al compartir un lado, son vecinos y por tanto el elemento (1,3) de la matriz será también un 1. ¿Qué pasa entre los objetos 1 y 4? En este caso, hay ni bordes ni vértices en común, por tanto, hay una relación de vecindad, entonces, en el elemento (1,4) habremos de colocar un 0 que indica ausencia de vecindad. En síntesis, dado determinado criterio de vecindad, si dos objetos espaciales son vecinos, la relación de vecindad se indica mediante un número 1, en tanto, cuando hay relación de vecindad su ausencia se indica colocando un 0. Hagamos esto para cada celda de la matriz hasta llenarla completamente y obtener algo parecido lo que aparece en la figura 3.4.\nFigura 3.3: Matriz ejemplo llena\nResumamos lo dicho hasta este punto. El cuadro que acabamos de llenar es conocido como matriz de pesos espaciales5 y es el instrumento que nos permite sintetizar las relaciones espaciales o estructura de vecindad que corresponde determinado criterio. Habrá, por tanto, diversos tipos de matrices en función del criterio de vecindad elegido. Esta matriz se denotada por la letra mayúscula \\(W\\) y está integrada por los elementos \\(w_{ij}\\) que toman el valor de 1 cuando el elemento \\(j\\) y el elemento \\(\\) son vecinos y 0 (cero) en cualquier otro caso. Este instrumento es uno de los más importantes en econometría espacial ya que permite construir los estadísticos de autocorrelación espacial y es la manera en que podemos incorporar al espacio como variable partir de lo que denominamos “rezago espacial,” como más adelante veremos en este y en el siguiente capítulo. Las características de la matriz de pesos espaciales son:Es una matriz que en la diagonal principal contiene sólo ceros, es decir, se asume que por definición hay interacciones dentro de un mismo elemento (lo que necesariamente es cierto y que dependerá de la escala de análisis).Es una matriz simétrica, es decir, se asume que hay interacción de “ida y vuelta,” por lo que con un instrumento de estas características es posible asumir efectos de interacción en un solo sentido.Es una matriz cuadrada, es decir, de dimensiones \\(n \\cdot n\\), donde \\(n\\) es el número de objetos espaciales.Acabamos de ilustrar la lógica con la que puede ser construida una matriz de pesos espaciales partir de una retícula regular con apenas seis elementos. Veamos ahora cómo obtener matrices de pesos espaciales sirviéndonos de R, ya que desarrollar los pasos anteriores para un vecindario compuesto por 76 objetos espaciales que son las alcaldías y municipios que integran el Valle de México es tarea para una máquina, para nosotros.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2023-02-06.book built bookdown R package.","code":""},{"path":"análisis-espacial-i-autocorrelación.html","id":"matrices-de-pesos-espaciales-en-r","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.2 Matrices de pesos espaciales en R","text":"En R hay muchas rutas para desarrollar la misma tarea. Presentamos en este capítulo dos rutas: la primera se sirve del paquete spdep de Roger Bivand, en tanto que la segunda sigue la propuesta de Xun Li y su paquete rgeoda, una librería para llevar cabo análisis espacial basado en la funcionalidades del software GeoDa.Roger Bivand y un equipo de colaboradores desarrollaron el paquete spdep para la construcción de matrices de pesos espaciales y el análisis espacial. En este enfoque la función poly2nb() nos permite calcular estructuras espaciales dadas partir de dos tipos de vecindad por adyacencia, una más estricta (argumento queen=FALSE) y que la otra (argumento queen=TRUE). En la documentación de la función podemos leer que: “si es VERDADERO, TRUE, un solo punto límite compartido cumple la condición de contigüidad; si es FALSO, FALSE, se requiere más de un punto compartido; ten en cuenta que más de un punto límite compartido significa necesariamente una línea límite compartida.” Además, para cargar la base de datos espacial recurriremos al paquete rgdal.Lo dicho en el párrafo anterior es relevante en el sentido de que estos criterios son exactamente los mismos que definimos antes (contigüidad reina y torre). Procedamos pues la instalación de los paquetes, en caso de que aún estén en nuestro sistema:Primero, llamemos las librerías y carguemos la base de datos:Ahora, construyamos un objeto que llamaremos mTRUE, dicho objeto contendrá los elementos que definen la estructura espacial, es decir, será nuestra matriz de pesos espaciales. Esto lo haremos con la función poly2nb():El segmento de código anterior genera un objeto de tipo nb. Verifica sus características con class() y str() . Además, nota que en la sección de ambiente de trabajo (cuadrante superior derecho, en la pestaña ambiente) aparece el objeto creado. Da clic en la imagen de la lupa para visualizarlo e intenta interpretar el resultado de la ventana.Ahora, llama al objeto y presta atención sobre los resultados que aparecen en la consola:En la consola aparecen los siguientes elementos:Objeto de lista de vecinos:Número de regiones: 76. Corresponde al número de alcaldías y municipios que componen la Zona Metropolitana del Valle de México.Número de enlaces distintos de cero: 380. Es el número de elementos de una matriz de 76x76 que registran relación de vecindad, dicho en otras palabras, es el total de números uno de la matriz.Porcentaje de pesos distintos de cero: 6.57. Resultado de dividir 380 entre (76x76).Número promedio de vínculos: 5. Número de vecinos que en promedio tiene cada municipio o alcaldía.Ahora bien, para construir una lista de vecindad con base en un criterio más estricto, es decir, queen = FALSE, procedemos como:Ahora, llama dicho objeto y contrasta con los resultados anteriores.Ejercicio¿Qué objeto, mTRUE o mFALSE, tiene el mayor número de vínculos diferentes de cero?¿Qué objeto, mTRUE o mFALSE, tiene el mayor número de vínculos diferentes de cero?¿Por qué crees que esto es así?¿Por qué crees que esto es así?Estas listas, que contienen nuestras estructuras espaciales almacenadas en los objeto de tipo nb llamados mTRUE y mFALSE, pueden representarse visualmente través de un gráfico de conectividad que representa la estructura espacial definida por cada criterio través de líneas que unen los municipios considerados vecinos.Para visualizar el mapa de conectividad recurriremos la función plot() y se superpondran dos gráficas: una sólo con los bordes o límites nivel municipal y otra con los centroides y la estructura espacial. Aquí se muestra el mapa de conectividad resultado de la matriz mTRUEPara comparar ambas estructuras espaciales, mTRUE y mFALSE podemos superponer las dos gráficas y asignar colores diferentes:Como puedes observar, son estructuras muy parecidas, aunque aun así es posible notar sus diferencias.diferencia del enfoque previo, con el paquete rgeoda es necesario cargar la base de datos espacial través del paquete sf. Si los instalado:Para cargarlos:Ahora bien, carguemos la base de datos espacial con sf:EjercicioObserva tu ambiente de trabajo (ventana superior derecha, en la pestaña “ambiente”). ¿De qué tipo es el objeto covid_zmvm_sf y de cuál covid_zmvm?Observa tu ambiente de trabajo (ventana superior derecha, en la pestaña “ambiente”). ¿De qué tipo es el objeto covid_zmvm_sf y de cuál covid_zmvm?¿qué crees que se deban dichas diferencias?¿qué crees que se deban dichas diferencias?través del paquete rgeoda es posible construir, como en GeoDa, cuatro tipos de matrices de pesos:Matrices basadas contigüidadMatrices basadas en distanciaMatrices de k-vecinos más cercanosMatrices de pesos por kernelDe ellas, aquí construiremos sólo las tres primeras.En rgeoda hay dos matrices de pesos espaciales basadas en contigüidad, tal y como lo expisimos en la primera sección de este capítulo, las de tipo reina y las de tipo torre. Para construir una matriz de tipo reina recurrimos la función queen_weight() y para una de tipo torre usamos rook_weights(), ambas funciones tienen cuatro argumentos:sf_obj=: nuestra cartografía en formato sf, tal y como la hemos ya cargado.order=: orden de contigüidad, donde si es igual 1 se indica que sólo los objetos espaciales inmediatos serán vecinos y si es mayor que uno indicará una vecindad de orden superior (si es 2 indica que los vecinos de mis vecinos serán mis vecinos, si es tres los vecinos de mis vecinos de mis vecinos serán mis propios vecinos, y así sucesivamente).include_lower_order=: indica si los vecinos de ordenes inferiores incluidos en la estructura de vecindad.precision_threshold=: este argumento modifica la precisión de la geometría y se usará en caso de que, habiendo observaciones aisladas, se detecte que algún objeto tiene vecinos.Así, para construir la matriz de tipo reina:En tanto, una matriz de contigüidad con el criterio de tipo torre::Los objetos recién creados y que contienen las estructuras de vecindad pueden ser grabadas en un archivo fuera del ambiente del trabajo, para hacerlas permanentes. Para ello usamos la función save_weights(), esta función tiene cuatro argumentos obligatorios: nombre de la matriz hacerse permanente (gda_w=), el identificador único para cada objeto espacial (id_variable=), la ruta donde se almacenará el archivo de salida y el nombre del archivo (out_path=) y el nombre de la capa de entrada (layer_name=):Si bien se dijo antes, en la sección dedicada comentar qué es una matriz de pesos espaciales, que se definía la estructura espacial través de una matriz, computacionalmente esto es así, como seguro habrás notado al tratar de leer las matrices construidas con spdep en la sección anterior. Identifica los archivos creados con las funciones previas, en la sección de archivos en la sección inferior derecha (pestaña files) y selecciona q_1.gal. Notarás que, en efecto, el archivo es una matriz, sino una lista. El hecho de que se usen listas y matrices para el proceso de cómputo de las estructuras espaciales es porque las listas son más conveniente en términos de la cantidad ocupada de recursos del sistema. La razón detrás de ello es que las matrices de pesos espaciales son matrices dispersas, es decir, matrices que contienen muchos elementos que son cero.Del archivo que acabas de abrir, q_1.gal, veámos con detenimiento su contenido para comprender mejor cómo se almacena la información. Reproducimos en seguida las tres primeras líneas:0 76 | covid_zmvm_sf | cvemun | | |\n09010 | 5 | | | |\n09014 | 09008 | 09016 | 09003 | 09012 |Línea 1: en la primera columna aparece el número de objetos espaciales (76), en la segunda columna el nombre del archivo del que proviene la estructura (covid_zmvm_sf) y en la tercera la clave de identificación única para cada objeto (cvemun).Línea 2: en la primera columna se indica el objeto espacial su clave (09010), mientras que en la segunda columna se indica el número de vecinos que dicho objeto tiene, en este caso, cinco.Línea 3: en esta fila aparece, para cada columna, la clave de identificación de los cinco vecinos de 09010: 09014 09008 09016 09003 09012.El resto de las líneas tiene la misma interpretación que las líneas 2 y 3: número de vecinos del objeto listado e identificación de los vecinos. Una vez que hemos comprendido la estructura de dicho archivo, pidamos un resumen del objeto queen_w:De dicho resumen es posible apuntar que: nuestro vecindario, la Zona Metropolitana del Valle de México, tiene 76 unidades espaciales (number observations), la matriz construida es simétrica (symmetric: TRUE), los municipios y alcaldías tienen al menos un vecino (# min neighbors: 1), el número máximo de vecinos es de 9 (# max neighbors: 9), la media de 4.68 (# mean neighbors), la mediana de 4 (# median neighbors) y que hay observaciones sin vecinos (isolates: FALSE). El elemento que hemos indicado es sparsity: que tiene un valor de 0.0616. Dicho valor corresponde la proporción de elementos diferentes cero en una matriz de 76x76 objetos, es decir, nuestra matriz sólo tiene 6.16% son diferentes de cero.En `rgeoda`` hay dos tipos de matrices que recurren la distancia: la matriz de úmbral de distancia mínima y la matriz de k-vecinos más cercanos. Para el caso de la matriz de umbral, se considera que dos objetos espaciales, en el caso aquí analizado, dos municipios o alcaldías, son vecinos siempre que estén dentro de cierto umbral de distancia dado. Para el caso de la matriz de k-vecinos más cercanos, la idea es que un objeto espacial tendrá como vecinos los k-objetos más cercanos.Para construir una matriz de pesos espaciales basada este criterio tenemos proceder en dos pasos: ) definir un umbral de distancia mínimo en el cual todas las unidades espaciales tienen al menos un vecino, ii) usar dicho umbral para hallar cada uno de los vecinos, dado ese umbral.Para definir el umbral usamos la función min_distthreshold() del paquete `rgeoda``:La distancia mínima para que cada uno de los 76 municipios y alcaldías tenga un vecino es 13,943.63 metros. Sabemos que las unidades del resultado anterior son metros puesto que en el archivo con extensión .prj (el archivo que contiene la información sobre la proyección cartográfica utilizada) así se indica. Ahora bien, definido el umbral ya podemos construir la matriz:Cuando decimos k-vecinos más cercanos, con ello queremos decir que se identificará determinado número “k” de vecinos más cercanos: los 2 más cercanos (k=2), los 8 más cercanos (k=8), etcétera. Para ello nos servimos de la función knn_weights(). Si quiseramos una matriz con los 4 vecinos más cercanos, tenemos que:diferencia de las matrices anteriores, ésta es simétrica y, como te podrás dar cuenta, hace que cada objeto espacial tenga exactamente el mismo número de vecinos, en este caso, cuatro.","code":"\ninstall.packages(c(\"spdep\", \"rgdal\"))\nlibrary(rgdal)\nlibrary(spdep)\ncovid_zmvm <-rgdal::readOGR(\"base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp\")## OGR data source with driver: ESRI Shapefile \n## Source: \"C:\\Repositorios\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp\", layer: \"covid_zmvm\"\n## with 76 features\n## It has 57 fields\nmTRUE <- spdep::poly2nb(covid_zmvm)\nmTRUE## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 384 \n## Percentage nonzero weights: 6.648199 \n## Average number of links: 5.052632\nmFALSE<- spdep::poly2nb(covid_zmvm, queen = FALSE)\nmFALSE## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 372 \n## Percentage nonzero weights: 6.440443 \n## Average number of links: 4.894737\nplot(covid_zmvm, border = 'lightgrey')\nplot(mTRUE, coordinates(covid_zmvm), add=TRUE, col='lightblue')\nplot(covid_zmvm, border = 'lightgrey')\nplot(mTRUE, coordinates(covid_zmvm), add=TRUE, col='blue')\nplot(mFALSE, coordinates(covid_zmvm), add=TRUE, col='lightgreen')\ninstall.packages(c(\"rgeoda\", \"sf\"))\nlibrary(sf)\nlibrary(rgeoda)\ncovid_zmvm_sf <- sf::st_read(\"base de datos/covid_zmvm shp/covid_zmvm.shp\")## Reading layer `covid_zmvm' from data source \n##   `C:\\Repositorios\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 76 features and 57 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5\n## Projected CRS: Lambert_Conformal_Conic\nqueen_w <- rgeoda::queen_weights(covid_zmvm_sf)\nrook_w <- rgeoda::rook_weights(covid_zmvm_sf)\n#Para la matriz reina\nrgeoda::save_weights(gda_w=queen_w,\n             id_variable=covid_zmvm_sf['cvemun'],\n             out_path = 'base de datos/covid_zmvm shp/q_1.gal',\n             layer_name = 'covid_zmvm_sf')## [1] TRUE\n#Para la matriz toree\nrgeoda::save_weights(rook_w,\n             covid_zmvm_sf['cvemun'],\n             out_path = 'base de datos/covid_zmvm shp/r_1.gal', \n             layer_name = 'covid_zmvm_sf')## [1] TRUE\nsummary(queen_w)##                      name              value\n## 1 number of observations:                 76\n## 2          is symmetric:                TRUE\n## 3               sparsity: 0.0616343490304709\n## 4        # min neighbors:                  1\n## 5        # max neighbors:                  9\n## 6       # mean neighbors:   4.68421052631579\n## 7     # median neighbors:                  4\n## 8           has isolates:              FALSE\numbral <- rgeoda::min_distthreshold(covid_zmvm_sf)\numbral## [1] 13943.63\ndist_w <- rgeoda::distance_weights(covid_zmvm_sf, umbral)\nsummary(dist_w)##                      name              value\n## 1 number of observations:                 76\n## 2          is symmetric:                TRUE\n## 3               sparsity: 0.0772160664819945\n## 4        # min neighbors:                  1\n## 5        # max neighbors:                 11\n## 6       # mean neighbors:   5.86842105263158\n## 7     # median neighbors:                5.5\n## 8           has isolates:              FALSE\nk4_w <- knn_weights(covid_zmvm_sf, 4)\nsummary(k4_w)##                      name              value\n## 1 number of observations:                 76\n## 2          is symmetric:               FALSE\n## 3               sparsity: 0.0526315789473684\n## 4        # min neighbors:                  4\n## 5        # max neighbors:                  4\n## 6       # mean neighbors:                  4\n## 7     # median neighbors:                  4\n## 8           has isolates:              FALSE"},{"path":"análisis-espacial-i-autocorrelación.html","id":"construcción-de-matrices-con-spdep","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.2.1 Construcción de matrices con spdep","text":"Roger Bivand y un equipo de colaboradores desarrollaron el paquete spdep para la construcción de matrices de pesos espaciales y el análisis espacial. En este enfoque la función poly2nb() nos permite calcular estructuras espaciales dadas partir de dos tipos de vecindad por adyacencia, una más estricta (argumento queen=FALSE) y que la otra (argumento queen=TRUE). En la documentación de la función podemos leer que: “si es VERDADERO, TRUE, un solo punto límite compartido cumple la condición de contigüidad; si es FALSO, FALSE, se requiere más de un punto compartido; ten en cuenta que más de un punto límite compartido significa necesariamente una línea límite compartida.” Además, para cargar la base de datos espacial recurriremos al paquete rgdal.Lo dicho en el párrafo anterior es relevante en el sentido de que estos criterios son exactamente los mismos que definimos antes (contigüidad reina y torre). Procedamos pues la instalación de los paquetes, en caso de que aún estén en nuestro sistema:Primero, llamemos las librerías y carguemos la base de datos:Ahora, construyamos un objeto que llamaremos mTRUE, dicho objeto contendrá los elementos que definen la estructura espacial, es decir, será nuestra matriz de pesos espaciales. Esto lo haremos con la función poly2nb():El segmento de código anterior genera un objeto de tipo nb. Verifica sus características con class() y str() . Además, nota que en la sección de ambiente de trabajo (cuadrante superior derecho, en la pestaña ambiente) aparece el objeto creado. Da clic en la imagen de la lupa para visualizarlo e intenta interpretar el resultado de la ventana.Ahora, llama al objeto y presta atención sobre los resultados que aparecen en la consola:En la consola aparecen los siguientes elementos:Objeto de lista de vecinos:Número de regiones: 76. Corresponde al número de alcaldías y municipios que componen la Zona Metropolitana del Valle de México.Número de enlaces distintos de cero: 380. Es el número de elementos de una matriz de 76x76 que registran relación de vecindad, dicho en otras palabras, es el total de números uno de la matriz.Porcentaje de pesos distintos de cero: 6.57. Resultado de dividir 380 entre (76x76).Número promedio de vínculos: 5. Número de vecinos que en promedio tiene cada municipio o alcaldía.Ahora bien, para construir una lista de vecindad con base en un criterio más estricto, es decir, queen = FALSE, procedemos como:Ahora, llama dicho objeto y contrasta con los resultados anteriores.Ejercicio¿Qué objeto, mTRUE o mFALSE, tiene el mayor número de vínculos diferentes de cero?¿Qué objeto, mTRUE o mFALSE, tiene el mayor número de vínculos diferentes de cero?¿Por qué crees que esto es así?¿Por qué crees que esto es así?Estas listas, que contienen nuestras estructuras espaciales almacenadas en los objeto de tipo nb llamados mTRUE y mFALSE, pueden representarse visualmente través de un gráfico de conectividad que representa la estructura espacial definida por cada criterio través de líneas que unen los municipios considerados vecinos.Para visualizar el mapa de conectividad recurriremos la función plot() y se superpondran dos gráficas: una sólo con los bordes o límites nivel municipal y otra con los centroides y la estructura espacial. Aquí se muestra el mapa de conectividad resultado de la matriz mTRUEPara comparar ambas estructuras espaciales, mTRUE y mFALSE podemos superponer las dos gráficas y asignar colores diferentes:Como puedes observar, son estructuras muy parecidas, aunque aun así es posible notar sus diferencias.","code":"\ninstall.packages(c(\"spdep\", \"rgdal\"))\nlibrary(rgdal)\nlibrary(spdep)\ncovid_zmvm <-rgdal::readOGR(\"base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp\")## OGR data source with driver: ESRI Shapefile \n## Source: \"C:\\Repositorios\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp\", layer: \"covid_zmvm\"\n## with 76 features\n## It has 57 fields\nmTRUE <- spdep::poly2nb(covid_zmvm)\nmTRUE## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 384 \n## Percentage nonzero weights: 6.648199 \n## Average number of links: 5.052632\nmFALSE<- spdep::poly2nb(covid_zmvm, queen = FALSE)\nmFALSE## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 372 \n## Percentage nonzero weights: 6.440443 \n## Average number of links: 4.894737\nplot(covid_zmvm, border = 'lightgrey')\nplot(mTRUE, coordinates(covid_zmvm), add=TRUE, col='lightblue')\nplot(covid_zmvm, border = 'lightgrey')\nplot(mTRUE, coordinates(covid_zmvm), add=TRUE, col='blue')\nplot(mFALSE, coordinates(covid_zmvm), add=TRUE, col='lightgreen')"},{"path":"análisis-espacial-i-autocorrelación.html","id":"construcción-de-matrices-con-rgeoda","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.2.2 Construcción de matrices con rgeoda","text":"diferencia del enfoque previo, con el paquete rgeoda es necesario cargar la base de datos espacial través del paquete sf. Si los instalado:Para cargarlos:Ahora bien, carguemos la base de datos espacial con sf:EjercicioObserva tu ambiente de trabajo (ventana superior derecha, en la pestaña “ambiente”). ¿De qué tipo es el objeto covid_zmvm_sf y de cuál covid_zmvm?Observa tu ambiente de trabajo (ventana superior derecha, en la pestaña “ambiente”). ¿De qué tipo es el objeto covid_zmvm_sf y de cuál covid_zmvm?¿qué crees que se deban dichas diferencias?¿qué crees que se deban dichas diferencias?través del paquete rgeoda es posible construir, como en GeoDa, cuatro tipos de matrices de pesos:Matrices basadas contigüidadMatrices basadas en distanciaMatrices de k-vecinos más cercanosMatrices de pesos por kernelDe ellas, aquí construiremos sólo las tres primeras.","code":"\ninstall.packages(c(\"rgeoda\", \"sf\"))\nlibrary(sf)\nlibrary(rgeoda)\ncovid_zmvm_sf <- sf::st_read(\"base de datos/covid_zmvm shp/covid_zmvm.shp\")## Reading layer `covid_zmvm' from data source \n##   `C:\\Repositorios\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 76 features and 57 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5\n## Projected CRS: Lambert_Conformal_Conic"},{"path":"análisis-espacial-i-autocorrelación.html","id":"matrices-basadas-en-contigüidad","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.2.2.1 Matrices basadas en contigüidad","text":"En rgeoda hay dos matrices de pesos espaciales basadas en contigüidad, tal y como lo expisimos en la primera sección de este capítulo, las de tipo reina y las de tipo torre. Para construir una matriz de tipo reina recurrimos la función queen_weight() y para una de tipo torre usamos rook_weights(), ambas funciones tienen cuatro argumentos:sf_obj=: nuestra cartografía en formato sf, tal y como la hemos ya cargado.order=: orden de contigüidad, donde si es igual 1 se indica que sólo los objetos espaciales inmediatos serán vecinos y si es mayor que uno indicará una vecindad de orden superior (si es 2 indica que los vecinos de mis vecinos serán mis vecinos, si es tres los vecinos de mis vecinos de mis vecinos serán mis propios vecinos, y así sucesivamente).include_lower_order=: indica si los vecinos de ordenes inferiores incluidos en la estructura de vecindad.precision_threshold=: este argumento modifica la precisión de la geometría y se usará en caso de que, habiendo observaciones aisladas, se detecte que algún objeto tiene vecinos.Así, para construir la matriz de tipo reina:En tanto, una matriz de contigüidad con el criterio de tipo torre::Los objetos recién creados y que contienen las estructuras de vecindad pueden ser grabadas en un archivo fuera del ambiente del trabajo, para hacerlas permanentes. Para ello usamos la función save_weights(), esta función tiene cuatro argumentos obligatorios: nombre de la matriz hacerse permanente (gda_w=), el identificador único para cada objeto espacial (id_variable=), la ruta donde se almacenará el archivo de salida y el nombre del archivo (out_path=) y el nombre de la capa de entrada (layer_name=):Si bien se dijo antes, en la sección dedicada comentar qué es una matriz de pesos espaciales, que se definía la estructura espacial través de una matriz, computacionalmente esto es así, como seguro habrás notado al tratar de leer las matrices construidas con spdep en la sección anterior. Identifica los archivos creados con las funciones previas, en la sección de archivos en la sección inferior derecha (pestaña files) y selecciona q_1.gal. Notarás que, en efecto, el archivo es una matriz, sino una lista. El hecho de que se usen listas y matrices para el proceso de cómputo de las estructuras espaciales es porque las listas son más conveniente en términos de la cantidad ocupada de recursos del sistema. La razón detrás de ello es que las matrices de pesos espaciales son matrices dispersas, es decir, matrices que contienen muchos elementos que son cero.Del archivo que acabas de abrir, q_1.gal, veámos con detenimiento su contenido para comprender mejor cómo se almacena la información. Reproducimos en seguida las tres primeras líneas:0 76 | covid_zmvm_sf | cvemun | | |\n09010 | 5 | | | |\n09014 | 09008 | 09016 | 09003 | 09012 |Línea 1: en la primera columna aparece el número de objetos espaciales (76), en la segunda columna el nombre del archivo del que proviene la estructura (covid_zmvm_sf) y en la tercera la clave de identificación única para cada objeto (cvemun).Línea 2: en la primera columna se indica el objeto espacial su clave (09010), mientras que en la segunda columna se indica el número de vecinos que dicho objeto tiene, en este caso, cinco.Línea 3: en esta fila aparece, para cada columna, la clave de identificación de los cinco vecinos de 09010: 09014 09008 09016 09003 09012.El resto de las líneas tiene la misma interpretación que las líneas 2 y 3: número de vecinos del objeto listado e identificación de los vecinos. Una vez que hemos comprendido la estructura de dicho archivo, pidamos un resumen del objeto queen_w:De dicho resumen es posible apuntar que: nuestro vecindario, la Zona Metropolitana del Valle de México, tiene 76 unidades espaciales (number observations), la matriz construida es simétrica (symmetric: TRUE), los municipios y alcaldías tienen al menos un vecino (# min neighbors: 1), el número máximo de vecinos es de 9 (# max neighbors: 9), la media de 4.68 (# mean neighbors), la mediana de 4 (# median neighbors) y que hay observaciones sin vecinos (isolates: FALSE). El elemento que hemos indicado es sparsity: que tiene un valor de 0.0616. Dicho valor corresponde la proporción de elementos diferentes cero en una matriz de 76x76 objetos, es decir, nuestra matriz sólo tiene 6.16% son diferentes de cero.","code":"\nqueen_w <- rgeoda::queen_weights(covid_zmvm_sf)\nrook_w <- rgeoda::rook_weights(covid_zmvm_sf)\n#Para la matriz reina\nrgeoda::save_weights(gda_w=queen_w,\n             id_variable=covid_zmvm_sf['cvemun'],\n             out_path = 'base de datos/covid_zmvm shp/q_1.gal',\n             layer_name = 'covid_zmvm_sf')## [1] TRUE\n#Para la matriz toree\nrgeoda::save_weights(rook_w,\n             covid_zmvm_sf['cvemun'],\n             out_path = 'base de datos/covid_zmvm shp/r_1.gal', \n             layer_name = 'covid_zmvm_sf')## [1] TRUE\nsummary(queen_w)##                      name              value\n## 1 number of observations:                 76\n## 2          is symmetric:                TRUE\n## 3               sparsity: 0.0616343490304709\n## 4        # min neighbors:                  1\n## 5        # max neighbors:                  9\n## 6       # mean neighbors:   4.68421052631579\n## 7     # median neighbors:                  4\n## 8           has isolates:              FALSE"},{"path":"análisis-espacial-i-autocorrelación.html","id":"matrices-basadas-en-distancia","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.2.2.2 Matrices basadas en distancia","text":"En `rgeoda`` hay dos tipos de matrices que recurren la distancia: la matriz de úmbral de distancia mínima y la matriz de k-vecinos más cercanos. Para el caso de la matriz de umbral, se considera que dos objetos espaciales, en el caso aquí analizado, dos municipios o alcaldías, son vecinos siempre que estén dentro de cierto umbral de distancia dado. Para el caso de la matriz de k-vecinos más cercanos, la idea es que un objeto espacial tendrá como vecinos los k-objetos más cercanos.","code":""},{"path":"análisis-espacial-i-autocorrelación.html","id":"matriz-basada-en-umbral-distancia","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.2.2.2.1 Matriz basada en umbral distancia","text":"Para construir una matriz de pesos espaciales basada este criterio tenemos proceder en dos pasos: ) definir un umbral de distancia mínimo en el cual todas las unidades espaciales tienen al menos un vecino, ii) usar dicho umbral para hallar cada uno de los vecinos, dado ese umbral.Para definir el umbral usamos la función min_distthreshold() del paquete `rgeoda``:La distancia mínima para que cada uno de los 76 municipios y alcaldías tenga un vecino es 13,943.63 metros. Sabemos que las unidades del resultado anterior son metros puesto que en el archivo con extensión .prj (el archivo que contiene la información sobre la proyección cartográfica utilizada) así se indica. Ahora bien, definido el umbral ya podemos construir la matriz:","code":"\numbral <- rgeoda::min_distthreshold(covid_zmvm_sf)\numbral## [1] 13943.63\ndist_w <- rgeoda::distance_weights(covid_zmvm_sf, umbral)\nsummary(dist_w)##                      name              value\n## 1 number of observations:                 76\n## 2          is symmetric:                TRUE\n## 3               sparsity: 0.0772160664819945\n## 4        # min neighbors:                  1\n## 5        # max neighbors:                 11\n## 6       # mean neighbors:   5.86842105263158\n## 7     # median neighbors:                5.5\n## 8           has isolates:              FALSE"},{"path":"análisis-espacial-i-autocorrelación.html","id":"matriz-de-k-vecinos-más-cercanos","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.2.2.2.2 Matriz de k-vecinos más cercanos","text":"Cuando decimos k-vecinos más cercanos, con ello queremos decir que se identificará determinado número “k” de vecinos más cercanos: los 2 más cercanos (k=2), los 8 más cercanos (k=8), etcétera. Para ello nos servimos de la función knn_weights(). Si quiseramos una matriz con los 4 vecinos más cercanos, tenemos que:diferencia de las matrices anteriores, ésta es simétrica y, como te podrás dar cuenta, hace que cada objeto espacial tenga exactamente el mismo número de vecinos, en este caso, cuatro.","code":"\nk4_w <- knn_weights(covid_zmvm_sf, 4)\nsummary(k4_w)##                      name              value\n## 1 number of observations:                 76\n## 2          is symmetric:               FALSE\n## 3               sparsity: 0.0526315789473684\n## 4        # min neighbors:                  4\n## 5        # max neighbors:                  4\n## 6       # mean neighbors:                  4\n## 7     # median neighbors:                  4\n## 8           has isolates:              FALSE"},{"path":"análisis-espacial-i-autocorrelación.html","id":"variables-espacialmente-rezagadas","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.3 Variables espacialmente rezagadas","text":"La construcción de un rezago espacial, también llamada variable espacialmente rezagada, es un elemento clave para poder operacionalizar y, por tanto, medir la autocorrelación. Pero, ¿qué es un rezago espacial? Imagina que vives en un barrio de la Ciudad de México que tiene 8 barrios vecinos, mismos que calculaste con alguno de los criterios de que vimos antes y que apuntaste en una matriz de pesos espaciales. Supón ahora que tu barrio tiene 5 casos de COVID19 y quieres comparar dicho dato con el de los 8 barrios vecinos, ¿cómo lo harías? Una alternativa útil es sintetizar la información de los ocho barrios en un sólo indicador que sume y pondere los datos de los casos positivos de los vecinos.Veamos esto con más cuidado y definamos formalmente rezago espacial. Antes dijimos que cada uno de los elementos \\(w_{ij}\\) de la matriz de pesos espaciales \\(W\\) pueden tomar como valores ceros o unos. La matriz \\(W\\) puede escribirse como:\\[\nW=\n\\begin{pmatrix}\nw_{11} & w_{12} & \\cdots  & w_{1n}\\\\\nw_{21} & w_{22} & \\cdots  & w_{2n}\\\\\n\\vdots & \\vdots & \\ddots  & \\vdots \\\\\nw_{n1} & w_{n2} & \\dots  & w_{nn}\\\\\n\\end{pmatrix}\n\\]obstante, es posible expresar dicha matriz \\(W\\) de una forma diferente, normalizandola por filas. Normalizar una matriz de pesos espaciales por filas implica dividir cada elemento \\(w_{ij}\\) de una fila entre la suma de elementos diferentes cero de dicha fila, por lo que los elementos de una matriz de pesos espaciales estandarizada por fila es:\\[w_{ij(s)}=\\frac {w_{ij}} {\\sum{w_{ij}}}\\]\nNuestra matriz de pesos espaciales estandarizada por filas, \\(W_s\\), es pues una transformación de la matriz original que hace que todas las filas sumen en total 1:$$\nw_{11(s)} + w_{12(s)} + + w_{1n(s)}= 1\\\nw_{21(s)} + w_{22(s)} + + w_{2n(s)}=1\\\nw_{n1(s)} + w_{n2(s)} + + w_{nn(s)}=1$$Regresemos nuestra definición sobre la autocorrelación espacial: relación funcional existente entre los valores que adopta un indicador en una zona del espacio con respecto al valor de sus zonas vecinas, dicho valor es lo que llamamos rezago espacial. Así pues, un rezago espacial es definido como el promedio ponderado del valor de la variable de los vecinos (Chasco 2003, 61; Anselin 2020). Siguiendo Anselin (2020), “el rezago espacial de \\(y\\) del objeto espacial \\(\\) es expresado como \\(Wy_{}\\):\\[\n\\begin{aligned}\nWy_i &=  w_{i1(s)}y_1+w_{i2(s)}y_2+...+w_{(s)}y_n \\\\\nWy_i &=  \\sum_{j=1}^nw_{ij(s)}y_j \\\\\n\\end{aligned}\n\\]Donde \\(w_{ij(s)}\\) es cada uno de los elementos de la matriz de pesos estandarizada por fila y \\(y_n\\) es el valor de la variable de interés. Así pues, el rezago espacial pondera la variable de interés través del número de vecinos que cada unidad espacial posee.De nueva cuenta, es posible construir rezagos espaciales ya sea con el paquete spded o rgeoda. Primero ilustraremos la alternativa con spdep y en seguida con rgeoda.Para construir con spdep el rezago espacial de una variable, digamos de los casos positivos por COVID19, debemos primero estandarizar los objetos que contienen las estructuras espaciales que previamente construimos: mTRUE y mFALSE. Este proceso corre cuenta de la función nb2listw() del paquete spdep:Notaras cómo en el ambiente de trabajo se ha creado un objeto nuevo de tipo listw. Ábrelo y observa su contenido.Ejercicio¿Explica por qué se le denomina matriz (lista) estandarizada?¿Explica por qué se le denomina matriz (lista) estandarizada?¿cuanto es igual la suma de cada renglón de la lista?¿cuanto es igual la suma de cada renglón de la lista?¿Cómo se relaciona la forma en que aparecen enlistados los elementos con el número de vecinos que tiene cada objeto espacial?¿Cómo se relaciona la forma en que aparecen enlistados los elementos con el número de vecinos que tiene cada objeto espacial?Construiremos un rezago espacial de la variable pos_hab, número de casos positivos por COVID19, con ayuda de la función lag.listw() del paquete spdep. Indicamos dos argumentos en la función: la estructura espacial dada por la matriz estandarizada, mTRUE.est, y la variable de la que deseamos el rezago espacial, pos_hab, esto es guardado en un nuevo objeto, lag_poshab, tal y como se muestra en el siguiente segmento de código:Para lograr apreciar mejor el rezago espacial, construiremos una tabla de dos columnas que almacenaremos en el objeto df, la primera contendrá la variable original y la segunda el rezago espacial, luego pediremos que nos muestre los primeros registros de la tabla con la función head() en un formato estilizado través de la función kable() del paquete knitr:El valor de la segunda columna, lag_poshab, es el rezago espacial. ¿Cómo interpretamos dicho valor? Veamos con cuidado. El primer valor listado de pos_hab, 14.365, es el número de casos positivos de COVID19 por cada 1 mil habitantes y dicho valor pertenece la alcaldía Álvaro Obregón, en tanto, el primer valor listado en la columna lag_poshab es el rezago espacial y asciende 15.992, este valor es el promedio ponderado de casos positivos por COVID19 en los vecinos Álvaro Obregón.Para identificar los vecinos de cada objeto espacial (municipio o alcaldía), así como el respectivo valor del rezago espacial hay que extraer dicha información utilizando una notación de dobles corchetes. Vayamos por pasos. Primero, para conocer cual es el primer elemento u objeto espacial de nuestra base usamos la notación del doble corchete sobre el objeto covid_zmvm:Una vez que sabemos que dicho objeto es la alcaldía llamada Álvaro Obregón, pidamos R que nos muestre cuáles son sus vecinos. Para ello, usando de nuevo la notación de doble corchete sobre la estructura espacial, el objeto mTRUE:Los números anteriores corresponden los vecinos de Álvaro Obregón, pero, ¿cómo saber su nombre y el número de casos positivos de cada uno? El siguiente segmento de código nos dará los nombres de los vecinos y el número de casos positivos en cada uno.Los valores previamente listados corresponden tanto al nombre como al número de casos positivos de los vecinos de Álvaro Obregón. Así, el rezago espacial de Álvaro Obregón es el resultado de sumar los valores de la tabla anterior y multiplicarlos por \\(\\frac {1}{6}\\), es decir, 15.99.Ejercicio¿Por que para el caso de Álvaro Obregón hubo que multiplicar por un sexto?¿Por que para el caso de Álvaro Obregón hubo que multiplicar por un sexto?Obtén un cuadro con las lista de vecinos y los valores de casos positivos para el objeto espacial 35Obtén un cuadro con las lista de vecinos y los valores de casos positivos para el objeto espacial 35Con el paquete rgeoda calcular rezagos espaciales es sencillo con la función spatial_lag(), en la que hay que especificar dos argumentos: gda_w= la matriz de pesos espaciales y df= la variable sobre la que se desea el rezago espacial, que proviene del objeto de tipo sf. Por ejemplo, para la variable casos positivos, pos_hab y con una matriz de tipo reina, queen_w:Para mejor mirar el rezago espacial, incorporémoslo un nuevo dataframe con la variable pos_hab, de forma emejante como lo hicimos antes:EjercicioCon el paquete rgeoda, construye rezagos espaciales con las otras estructuras espaciales: torre, distancia mínima, k-vecinos.Con el paquete rgeoda, construye rezagos espaciales con las otras estructuras espaciales: torre, distancia mínima, k-vecinos.¿Por qué en cada caso es diferente el valor rezago?¿Por qué en cada caso es diferente el valor rezago?","code":"\nmTRUE.est <- spdep::nb2listw(mTRUE)\nmTRUE.est## Characteristics of weights list object:\n## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 384 \n## Percentage nonzero weights: 6.648199 \n## Average number of links: 5.052632 \n## \n## Weights style: W \n## Weights constants summary:\n##    n   nn S0       S1       S2\n## W 76 5776 76 35.56142 319.6623\nmFALSE.est <- spdep::nb2listw(mFALSE)\nmFALSE.est## Characteristics of weights list object:\n## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 372 \n## Percentage nonzero weights: 6.440443 \n## Average number of links: 4.894737 \n## \n## Weights style: W \n## Weights constants summary:\n##    n   nn S0       S1       S2\n## W 76 5776 76 36.56499 319.6716\nlag_poshab <- spdep::lag.listw(mTRUE.est, covid_zmvm$pos_hab)\n#Crea un nuevo arreglo de datos donde se almacena la variable original y el rezago espacial\ndf <- base::data.frame(pos_hab = covid_zmvm$pos_hab, lag_poshab)\n\nlibrary(knitr)\n\n#Coloca los primeros valores de ambas variables en una tabla, requiere instalación y carga del paquete  knitr\nkable(head(df))\ncovid_zmvm$nom_mun[[1]] #Devuelve el nombre del municipio identificado con el número 1## [1] \"Álvaro Obregón\"\nmTRUE[[1]]#Devuelve los números de identificación de los municipios y alcaldías vecinas del Álvaro Obregón, la observación identificada con el número 1.## [1]  2  7  9 10 11 15\n#Crea un objeto que contiene el nombre de las alcaldías y municipios vecinos de Álvaro Obregón\nAlcaldías <- covid_zmvm$nom_mun[mTRUE[[1]]]\n\n#Crea un objeto que contiene el valor de la variable pos_hab (casos positivos por mil habitantes) para cada observación vecina de Álvaro Obregón\nCasos_covid <- covid_zmvm$pos_hab[mTRUE[[1]]]\n\n# Guarda los dos objetos anteriores en un dataframe y lo muestra en una tabla\ndb <- data.frame(Alcaldías, Casos_covid)\nkable(db)\nlag <- rgeoda::spatial_lag(queen_w,\n                   covid_zmvm_sf['pos_hab'])\nhead(lag)##   Spatial.Lag\n## 1    15.45044\n## 2    15.57109\n## 3    13.91179\n## 4    12.22961\n## 5    13.20082\n## 6    11.51118\ndf <- base::data.frame(pos_hab = covid_zmvm_sf$pos_hab, lag)\nkable(head(df))"},{"path":"análisis-espacial-i-autocorrelación.html","id":"rezagos-espaciales-con-spdep","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.3.1 Rezagos espaciales con spdep","text":"Para construir con spdep el rezago espacial de una variable, digamos de los casos positivos por COVID19, debemos primero estandarizar los objetos que contienen las estructuras espaciales que previamente construimos: mTRUE y mFALSE. Este proceso corre cuenta de la función nb2listw() del paquete spdep:Notaras cómo en el ambiente de trabajo se ha creado un objeto nuevo de tipo listw. Ábrelo y observa su contenido.Ejercicio¿Explica por qué se le denomina matriz (lista) estandarizada?¿Explica por qué se le denomina matriz (lista) estandarizada?¿cuanto es igual la suma de cada renglón de la lista?¿cuanto es igual la suma de cada renglón de la lista?¿Cómo se relaciona la forma en que aparecen enlistados los elementos con el número de vecinos que tiene cada objeto espacial?¿Cómo se relaciona la forma en que aparecen enlistados los elementos con el número de vecinos que tiene cada objeto espacial?Construiremos un rezago espacial de la variable pos_hab, número de casos positivos por COVID19, con ayuda de la función lag.listw() del paquete spdep. Indicamos dos argumentos en la función: la estructura espacial dada por la matriz estandarizada, mTRUE.est, y la variable de la que deseamos el rezago espacial, pos_hab, esto es guardado en un nuevo objeto, lag_poshab, tal y como se muestra en el siguiente segmento de código:Para lograr apreciar mejor el rezago espacial, construiremos una tabla de dos columnas que almacenaremos en el objeto df, la primera contendrá la variable original y la segunda el rezago espacial, luego pediremos que nos muestre los primeros registros de la tabla con la función head() en un formato estilizado través de la función kable() del paquete knitr:El valor de la segunda columna, lag_poshab, es el rezago espacial. ¿Cómo interpretamos dicho valor? Veamos con cuidado. El primer valor listado de pos_hab, 14.365, es el número de casos positivos de COVID19 por cada 1 mil habitantes y dicho valor pertenece la alcaldía Álvaro Obregón, en tanto, el primer valor listado en la columna lag_poshab es el rezago espacial y asciende 15.992, este valor es el promedio ponderado de casos positivos por COVID19 en los vecinos Álvaro Obregón.Para identificar los vecinos de cada objeto espacial (municipio o alcaldía), así como el respectivo valor del rezago espacial hay que extraer dicha información utilizando una notación de dobles corchetes. Vayamos por pasos. Primero, para conocer cual es el primer elemento u objeto espacial de nuestra base usamos la notación del doble corchete sobre el objeto covid_zmvm:Una vez que sabemos que dicho objeto es la alcaldía llamada Álvaro Obregón, pidamos R que nos muestre cuáles son sus vecinos. Para ello, usando de nuevo la notación de doble corchete sobre la estructura espacial, el objeto mTRUE:Los números anteriores corresponden los vecinos de Álvaro Obregón, pero, ¿cómo saber su nombre y el número de casos positivos de cada uno? El siguiente segmento de código nos dará los nombres de los vecinos y el número de casos positivos en cada uno.Los valores previamente listados corresponden tanto al nombre como al número de casos positivos de los vecinos de Álvaro Obregón. Así, el rezago espacial de Álvaro Obregón es el resultado de sumar los valores de la tabla anterior y multiplicarlos por \\(\\frac {1}{6}\\), es decir, 15.99.Ejercicio¿Por que para el caso de Álvaro Obregón hubo que multiplicar por un sexto?¿Por que para el caso de Álvaro Obregón hubo que multiplicar por un sexto?Obtén un cuadro con las lista de vecinos y los valores de casos positivos para el objeto espacial 35Obtén un cuadro con las lista de vecinos y los valores de casos positivos para el objeto espacial 35","code":"\nmTRUE.est <- spdep::nb2listw(mTRUE)\nmTRUE.est## Characteristics of weights list object:\n## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 384 \n## Percentage nonzero weights: 6.648199 \n## Average number of links: 5.052632 \n## \n## Weights style: W \n## Weights constants summary:\n##    n   nn S0       S1       S2\n## W 76 5776 76 35.56142 319.6623\nmFALSE.est <- spdep::nb2listw(mFALSE)\nmFALSE.est## Characteristics of weights list object:\n## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 372 \n## Percentage nonzero weights: 6.440443 \n## Average number of links: 4.894737 \n## \n## Weights style: W \n## Weights constants summary:\n##    n   nn S0       S1       S2\n## W 76 5776 76 36.56499 319.6716\nlag_poshab <- spdep::lag.listw(mTRUE.est, covid_zmvm$pos_hab)\n#Crea un nuevo arreglo de datos donde se almacena la variable original y el rezago espacial\ndf <- base::data.frame(pos_hab = covid_zmvm$pos_hab, lag_poshab)\n\nlibrary(knitr)\n\n#Coloca los primeros valores de ambas variables en una tabla, requiere instalación y carga del paquete  knitr\nkable(head(df))\ncovid_zmvm$nom_mun[[1]] #Devuelve el nombre del municipio identificado con el número 1## [1] \"Álvaro Obregón\"\nmTRUE[[1]]#Devuelve los números de identificación de los municipios y alcaldías vecinas del Álvaro Obregón, la observación identificada con el número 1.## [1]  2  7  9 10 11 15\n#Crea un objeto que contiene el nombre de las alcaldías y municipios vecinos de Álvaro Obregón\nAlcaldías <- covid_zmvm$nom_mun[mTRUE[[1]]]\n\n#Crea un objeto que contiene el valor de la variable pos_hab (casos positivos por mil habitantes) para cada observación vecina de Álvaro Obregón\nCasos_covid <- covid_zmvm$pos_hab[mTRUE[[1]]]\n\n# Guarda los dos objetos anteriores en un dataframe y lo muestra en una tabla\ndb <- data.frame(Alcaldías, Casos_covid)\nkable(db)"},{"path":"análisis-espacial-i-autocorrelación.html","id":"rezagos-espaciales-con-rgeoda","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.3.2 Rezagos espaciales con rgeoda","text":"Con el paquete rgeoda calcular rezagos espaciales es sencillo con la función spatial_lag(), en la que hay que especificar dos argumentos: gda_w= la matriz de pesos espaciales y df= la variable sobre la que se desea el rezago espacial, que proviene del objeto de tipo sf. Por ejemplo, para la variable casos positivos, pos_hab y con una matriz de tipo reina, queen_w:Para mejor mirar el rezago espacial, incorporémoslo un nuevo dataframe con la variable pos_hab, de forma emejante como lo hicimos antes:EjercicioCon el paquete rgeoda, construye rezagos espaciales con las otras estructuras espaciales: torre, distancia mínima, k-vecinos.Con el paquete rgeoda, construye rezagos espaciales con las otras estructuras espaciales: torre, distancia mínima, k-vecinos.¿Por qué en cada caso es diferente el valor rezago?¿Por qué en cada caso es diferente el valor rezago?","code":"\nlag <- rgeoda::spatial_lag(queen_w,\n                   covid_zmvm_sf['pos_hab'])\nhead(lag)##   Spatial.Lag\n## 1    15.45044\n## 2    15.57109\n## 3    13.91179\n## 4    12.22961\n## 5    13.20082\n## 6    11.51118\ndf <- base::data.frame(pos_hab = covid_zmvm_sf$pos_hab, lag)\nkable(head(df))"},{"path":"análisis-espacial-i-autocorrelación.html","id":"coeficiente-de-correlación-espacial-la-i-de-moran","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.4 Coeficiente de correlación espacial: la I de Moran","text":"Una vez que hemos abordado y resuelto el problema de cómo definir vecindad partir de la matriz de pesos espaciales y que hemos operacionalizado la definición de valor de la variable en los vecinos través de la noción de rezago espacial, tenemos todos los elementos que integran la definición de autocorrelación espacial: ) relación funcional, ii) valor de una variable y iii) relación de vecindad.Ahora bien, ¿cómo medimos la autocorrelación? Es decir, cómo sabemos si, por ejemplo, la variable casos positivos por COVID19 está autocorrelacionada. Al principio de este capítulo dijimos que apreciar ciertos patrones de agrupamiento en un mapa, como el del índice de desarrollo humano, era un posible indicio de autocorrelación espacial. Es momento de formalizar dicho indicio través de un indicador apropiado. Debemos pues construir un estadístico, un estadístico de autocorrelación espacial.El estadístico de asociación espacial más socorrido es el propuesto por Patrick Moran: la de Moran. La de Moran es un coeficiente de correlación lineal que “incorpora al espacio,” es decir, mide la asociación lineal entre una variable (digamos casos positivos por COVID19) y su rezago espacial (el valor promedio de los casos positivos de los vecinos). Como todo coeficiente de asociación, el valor de la de Moran se encuentra entre -1 y 1.Si el valor de la de Moran es positivo decimos que hay signos de concentración o patrones de aglomeración pues existe autocorrelación espacial positiva, por lo que existen unidades espaciales (alcaldías, municipios) que tienen valores altos en la variable medida que están rodeadas por otras unidades espaciales que tienen valores también altos. Decir que existe autocorrelación espacial positiva también implica afirmar que hay unidades espaciales con valores bajos rodeadas de otras que tienen también valores bajos.Por otro lado, si la de Moran es negativa esto es evidencia de otro tipo de patrones, ya de aglomeración sun ode dispersión o repulsión: una alcaldía o municipio que tiene valores altos está rodeada de vecinos con valores bajos y viceversa.Hasta donde sabemos, el paquete rgeoda aún incorpora una función para construir automáticamente la de Moran, por lo que el enfoque aquí presentado corresponde, para simplificar, al del paquete spdep.Para calcular en R el coeficiente o estadístico de Moran necesitamos recurrir la función moran.test() del paquete spdep. Los argumentos de la función deben especificar el nombre de la variable y el tipo de estructura espacial dado por la matriz de pesos usada; adicionalmente, se puede indicar qué hacer en caso de que existan islas (objetos espaciales sin vecinos) con el argumento zero.policy.EjercicioSolicita ayuda del paquete spdep y responde, ¿para qué sirve el argumento randomization de la función moran.test()?Solicita ayuda del paquete spdep y responde, ¿para qué sirve el argumento randomization de la función moran.test()?De nuevo, en la ayuda de la función, ¿con cuál de los argumentos es posible cambiar la hipótesis alternativa de la evaluación de autocorrelación espacial en la prueba de Moran?De nuevo, en la ayuda de la función, ¿con cuál de los argumentos es posible cambiar la hipótesis alternativa de la evaluación de autocorrelación espacial en la prueba de Moran?Construyamos el estadístico de Moran para la variable pos_hab usando la estructura espacial llamada mTRUE.est:Del indicador obtenido nos interesan tres elementos, como es usual: la magnitud del coeficiente, su sentido y su significancia estadística. En los resultados que aparecen en tu consola identifica cada uno de ellos y responde:Ejercicio¿cuánto asciende el coeficiente estimado? ¿Podría decirse que es alto o bajo?¿cuánto asciende el coeficiente estimado? ¿Podría decirse que es alto o bajo?¿La relación identificada es positiva o negativa?¿La relación identificada es positiva o negativa?Un coeficiente como el obtenido, de 0.656, indica que hay una relación positiva entre los valores de los casos positivos de COVID19 y los valores de los casos positivos por COVID19 en los entornos vecinos (sentido de la asociación), además, podríamos decir que es relativamente alto en la medida en que está más próximo uno que cero (magnitud de la relación). Ahora, ¿qué hay con su significancia estadística? ¿Cómo podemos saber que dicho resultado,. el 0.656, es producto de una coincidencia sino un resultado sistemático o consistente? Para ello, evaluaremos el siguiente juego de hipótesis sobre el índice de Moran:\\(Ho: =0\\), es decir, ausencia de autocorrelación o, de forma equivalente, distribución espacial aleatoria.\\(Ha:\\neq 0\\), es decir, presencia de autocorrelación o, de forma equivalente, distribución espacial aleatoria.Con la información disponible evaluamos dichas hipótesis. Observa como el p-valor obtenido en extremadamente pequeño (p-value < 2.2e-16); si fijamos un nivel de significancia, \\(\\alpha=0.05\\), se rechaza la hipótesis nula en favor de la hipótesis alternativa, por tanto, la variable se distribuye de forma aleatoria en el espacio, sino que muestra indicios de autocorrelación espacial positiva relativamente alta (0.656).EjercicioConstruya una de Moran con la estructura espacial dada por la matriz donde queen=FALSE responda:¿La asociación espacial es positiva o negativa?¿La asociación espacial es positiva o negativa?¿Consideras que es alta o baja?¿Consideras que es alta o baja?¿Dirías que dicha relación es producto del azar o que existe un comportamiento sistemático?¿Dirías que dicha relación es producto del azar o que existe un comportamiento sistemático?","code":"\nspdep::moran.test(covid_zmvm$pos_hab, mTRUE.est)## \n##  Moran I test under randomisation\n## \n## data:  covid_zmvm$pos_hab  \n## weights: mTRUE.est    \n## \n## Moran I statistic standard deviate = 8.8625, p-value < 2.2e-16\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##       0.656334129      -0.013333333       0.005709563"},{"path":"análisis-espacial-i-autocorrelación.html","id":"diagrama-de-moran","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.4.1 Diagrama de Moran","text":"Una forma creativa de expresar gráficamente la autocorrelación es través de un diagrama de dispersión cuyo eje \\(x\\) corresponde la variable de interés, casos positivos de COVID19 por cada 1 mil habitantes, y el eje \\(y\\) su rezago espacial; además, al agregar una recta de ajuste sobre los datos estandarizados lograremos que la pendiente de dicha recta corresponda exactamente al valor de la de Moran. Una de las características del diagrama de Moran es que se descompone en cuatro cuadrantes, tal como aparece en la figura 3.5.\nFigura 3.4: Diagrama de dispersión de Moran\nPara representar el diagrama de Moran recurrimos la función moran.plot() del paquete spdep que para una matriz de tipo reina con el argumento queen=TRUE y la base de datos de tipo SpatialPolygonsDataFrame:En el diagrama de Moran recién construido notamos varias cosas:\n) Una parte importante de las observaciones caen en el cuadrante y III, por lo que la relación funcional que predomina entre el conjunto de puntos es positiva.\nii) Que la relación funcional dominante sea positiva implica que haya observaciones en los cuadrantes II y IV.","code":"\nspdep::moran.plot(((covid_zmvm$pos_hab)-mean(covid_zmvm$pos_hab))/(sd(covid_zmvm$pos_hab)),\n                  listw = mTRUE.est, \n                  xlab=\"Casos positivos\",\n                  ylab=\"Rezago espacial de los casos positivos\",\n                  main=\"Diagrama de Moran para casos positivos\",\n                  col=\"lightblue\")"},{"path":"análisis-espacial-i-autocorrelación.html","id":"múltiples-elementos-de-personalización-de-ésta-y-otras-gráficas-asociadas-al-paquete-base-de-r-pueden-revisarse-en-la-documentación-de-la-función-par.","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.5 Múltiples elementos de personalización de ésta y otras gráficas asociadas al paquete base de R pueden revisarse en la documentación de la función par().","text":"Ejercicio¿Es posible construir un diagrama de Moran usando el paquete ggplot2? De ser así, ¿cómo lo harías?","code":""},{"path":"análisis-espacial-i-autocorrelación.html","id":"índice-de-moran-local-y-mapa-de-clusters","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.6 Índice de Moran local y mapa de clusters","text":"El índice de Moran que recién hemos calculado permite evaluar la existencia de un patrón espacial completo o global, es decir, para el conjunto de todas las observaciones. Por ello, proporciona información de la ubicación de las agrupaciones o clusters de alcaldías y municipios. Por ello se dice que la de Moran es una medida de autocorrelación espacial global: nos dice que hay patrones de concentración o dispersión pero nos dice qué municipios o alcaldías específicos es posible atribuir dichas fuerzas de aglomeración.Para subsanar esta situación, Anselin (1995) propuso la versión local de la de Moran: el indicador local de asociación espacial o LISA (local indicator spatial association).El LISA:Proporciona un estadístico para cada ubicación con un nivel de significancia yEstablece una relación proporcional entre el estadístico local y el global.Es decir, nos permite identificar que unidades espaciales (alcaldías o municipios) es posible atribuir la autocorrelación espacial de forma específica y con qué intensidad, en relación con el indicador global. La representación del LISA se hace con arreglo dos mapas:El mapa de cluster o mapa de agrupaciones que permite clasificar las áreas (alcaldías y municipios) con presencia de autocorrelación espacial según el tipo de asociación identificada. Este mapa permite la clasificación de las áreas estadísticamente significativas en clusters o agrupamientos (alto-alto y bajo-bajo) y de áreas que se constituyen como observaciones espaciales atípicas o spatial outliers (agrupamientos bajo-alto y alto-bajo).El mapa de significancia: muestra las ubicaciones con la de Moran local que son representativas en diferentes niveles de significancia.El Índice local de Moran toma la forma de:\\[I_i = \\frac{(x_i-\\bar{X})}{S_i^2}{\\sum_{j=1,j \\neq }^{n}w_{ij}(x_j-\\bar{X})}\\]\nDonde:\\[\nS_i^2= \\frac{\\sum_{j=1,j \\neq }^{n}(x_j-\\bar{X}) ^2}{n-1}\n\\]\nAdemás, \\(x_i\\) es el valor de la variable de interés, \\(\\bar{X}\\) es el promedio de dicha variable, \\(w_{ij}\\) es cada uno de los elementos de la matriz de pesos espaciales y \\(n\\) es el número de objetos espaciales.Para calcular un índice de Moran local en R usamos el paquete rgeoda, que es el enfoque más sencillo de ejecutar. Echaremos mano de la función local_moran(), que requiere dos argumentos forzosos: la estructura espacial(w=) y la variable para la que se desea el LISA (df=). Primero, guardemos la variable de la que deseamos un LISA en un objeto independiente:Luego, usemos dicha variable para construir el LISA:Dentro del objeto lisa_poshab hay múltiples elementos, que serán útiles más adelante. De momento, llamemos los valores del indicador local de asociación espacial través de la función lisa_values(), es decir, los valores del índice computado para cada uno de los 76 municipios y alcaldías que componen la ZMVM:Para representar en un mapa los valores del LISA, es necesario añadirlos la base de datos original en formato sf:Luego, con las funciones aprendidas en el capítulo anterior, podemos presentar un mapa de quintiles que represente el valor del indicador local de asociación espacial:El mapa permite observar la manera en que varía la correlación espacial nivel local para la variable pos_hab, casos positivos por COVID19 por cada 1 mil habitantes, pues proporciona un valor de correlación para cada municipio y alcaldía: es claro que la mayor parte de los valores altos del LISA se encuentran en la Ciudad de México (tonos en azul).Necesitamos otros instrumentos que nos permitan identificar si los valores de la de Moran local son o significativos, lo que haremos través del mapa de cluster y su respectivo mapa de significancia. Esto nos permitirá identificar agrupaciones o núcleos de cluster significativos, así como observaciones espaciales atípicas.Dentro del objeto creado, lisa_poshab, hay múltiples elementos, mismos que pueden ser llamados por funciones específicas, entre las que se encuentran:lisa_clusters(): que los valores de clasificación de cada cluster.lisa_colors(): brinda los colores asociados cada uno de los clusters computados.lisa_labels(): proporciona las etiquetas de los clusters computados.Así pues, el mapa los construimos en dos pasos: ) primero obtendremos algunos elementos preliminares para construir el mapa (colores, etiquetas y valores de agrupamiento) y ii) construiremos el mapa con las funciones básicas de R_Ahora, con dichos elementos auxiliares, construimos el mapa de cluster:Junto con el mapa anterior es común presentar el mapa de significancia, un tipo de representación que indica el nivel de significancia inividual para cada una de las observaciones (municipios y alcaldías). Para ello, también diviimos el procedimiento en dos partes:El par de mapas anteriores permiten identificar agrupamientos de valores significativos al 5% o menos, es decid, núcleos de cluster que muestran municipios y alcaldías con valores altos de tasas positivas de COVID19 rodeados de vecinos con valores altos (agrupamiento Alto-Alto), así como agrupamientos de valores bajos (cuadrante Bajo-Bajo), además de observaciones espaciales atípicas (cuadrante Alto-bajo y Bajo-Alto).En síntesis, hasta este punto hemos visto en este capítulo:Cómo definir estructuras de relación espacial través de diversos criterios,Cómo identificar autocorrelación espacial global través de la de Moran,Cómo evaluar la significancia estadística de la de Moran,Cómo identificar agrupaciones locales través del indicador LISA.En el capítulo 5 nos adentraremos en cómo incorporar la riqueza que proporciona el análisis espacial en un modelo econométrico. Mientras tanto, en el capítulo 4 llevaremos cabo un repaso de elementos básicos sobre los modelos de regresión lineal clásica con mínimos cuadrados ordinarios.","code":"\npos_hab=covid_zmvm_sf[\"pos_hab\"]\nlisa_poshab <- local_moran(w=queen_w,\n                           df=pos_hab)\nlisaval_poshab <- lisa_values(lisa_poshab)\nmapa.lisa <- base::cbind(covid_zmvm_sf, lisaval_poshab)\ntmap::tm_shape(mapa.lisa) +\n  tmap::tm_fill(col = \"lisaval_poshab\", style = \"quantile\",\n                palette = \"Spectral\", midpoint= NA,\n                title = \"I de Moran local\") +\n  tmap::tm_borders()\n#Elementos preliminares\nlisa_colores <- lisa_colors(lisa_poshab)\nlisa_etiq <- c(\"No significativo\", \"Alto-Alto\", \"Bajo-Bajo\", \"Bajo-Alto\", \"Alto-Bajo\", \"No definido\", \"Aislado\")\nlisa_clusters <- lisa_clusters(lisa_poshab)\n#Mapa de cluster\nplot(st_geometry(covid_zmvm_sf), \n     col=sapply(lisa_clusters, function(x){return(lisa_colores[[x+1]])}), \n     border = \"#333333\", lwd=0.2)\ntitle(main = \"Moran Local de pos_hab\")\nlegend('bottomleft', legend = lisa_etiq, fill = lisa_colores, border = \"#eeeeee\")\n#Elementos preliminares\nlisa_p <- lisa_pvalues(lisa_poshab)\np_etiq <- c(\"No significativo\", \"p <= 0.05\", \"p <= 0.01\", \"p <= 0.001\")\np_colores <- c(\"#eeeeee\", \"#84f576\", \"#53c53c\", \"#348124\")\n#Mapa de significancia\nplot(st_geometry(covid_zmvm_sf), \n     col=sapply(lisa_p, function(x){\n       if (x <= 0.001) return(p_colores[4])\n       else if (x <= 0.01) return(p_colores[3])\n       else if (x <= 0.05) return (p_colores[2])\n       else return(p_colores[1])\n       }), \n     border = \"#333333\", lwd=0.2)\ntitle(main = \"Mapa de significancia de los casos positivos\")\nlegend('bottomleft', legend = p_etiq, fill = p_colores, border = \"#eeeeee\")"},{"path":"modelos-de-regresión-lineal.html","id":"modelos-de-regresión-lineal","chapter":"4 Modelos de regresión lineal","heading":"4 Modelos de regresión lineal","text":"Hasta este punto hemos hecho un recorrido que va desde los elementos más básicos sobre la exploración de información (capítulo 1), pasando por la representación de información espacial través de mapas coropléticos (capítulo 2), hasta la exploración de patrones de asociación de una variable en el espacio, la llamada autocorrelación (capítulo 3).Antes de avanzar en el conocimiento de las herramientas que nos permitirán elaborar algunos modelos básicos de econometría espacial que incorporen los patrones espaciales que hemos identificado (capítulo 5), en este capítulo hacemos un pequeño paréntesis en el tratamiento de la información espacial para llevar cabo un repaso elemental, desde una perspectiva práctica, del modelo clásico de regresión lineal. Esto nos permitirá que tengas en la mente dos cosas: ) qué es lo que se busca través de modelos econométricos y ii) cuáles son los supuestos detrás del modelo clásico de regresión lineal estimado con el método de mínimos cuadrados ordinarios (MCO).En términos de su origen etimológico, econometría significa medición econonómica. Pero su alcance se extiende más allá de la simple medición ya que través de ella es posible contribuir dar soporte empírico la teoría económica través del uso de herramientas estadísticas como los modelos de regresión y sus estimaciones (Gujarati Porter 2011). Veamos esto con un poco más de detalle.En el proceso de investigación podemos plantearnos preguntas con un grado de complejidad diverso. Habrá preguntas que, al ser respondidas, nos permitan tener una descripción general del fenómeno, habrá otras que nos permitan explorar la relación entre dos fenómenos, o bien, preguntas cuya respuesta nos permita predecir el comportamiento futuro del elemento estudiado. Así pues, hay diferentes tipos de preguntas de investigación, dependiendo de su grado de complejidad. En su libro “El arte de la ciencia de datos,” Roger Peng y Elizabeth Matsui(2017), abordan sistemáticamente esta cuestión.\nCuando, por ejemplo, nos preguntamos “¿el cambio en una variable provoca el cambio en otra diferente?” este tipo de pregunta es llamada pregunta de análisis causal. Con el análisis causal se busca saber si un par de variables están asociadas, más aún, si el cambio en una variable causa cambios en la otra.Responder este tipo de preguntas exige el dominio de diversas herramientas estadísticas, cuyo abordaje está fuera del alcance de este capítulo. obstante, la econometría través del análisis de regresión nos brinda una oportunidad que, cuando es bien utilizada, permite aproximarnos al análisis causal. partir de una relación causal anclada en la teoría (modelo teórico), es preciso indicar el tipo de relación específico entre las variables del caso (modelo matemático), para luego expresar dicha relación en una relación exacta sino sujeta variaciones individuales (modelo econométrico) (Gujarati Porter 2011: 3-5).La regresión como instrumento analítico nos brinda elementos para acercarnos la comprensión de la asociación entre dos variables, aunque es posible hablar en sentido estricto descubrir relaciones de causalidad como resultado del análisis de regresión. Así, “El análisis de regresión solo puede abordar los problemas de correlación. puede abordar el problema de la necesidad (causalidad). Por tanto, nuestras expectativas de descubrir relaciones de causa y efecto partir de la regresión deberían ser modestas” (Montgomery, Peck., Vining. 2012, 81:3).Entonces, ¿de qué modo el análisis de regresión se convierte potencialmente en un elemento que nos permite analizar causalidad? Lo que resultará fundamental en el proceso de investigación que recurre esta herramienta es pues la pregunta que se plantee el investigador, preguntas que deben ser planteadas desde el conocimiento científico previamente existente. Dicho en otros términos, el análisis de causalidad que nos da la regresión, sí lo puede ofrecer la teoría existente en determinado campo de conocimiento: ¿qué dice la teoría sobre la relación entre la variable y la variable B?¿Qué asociaciones entre pares de variables recuerdas de tus cursos teoría económica? Quizá los siguientes ejemplos te resulten familiares: ) la asociación entre la tasa de interés y eficiencia marginal del capital en la teoría de la inversión de Keynes, ii) la relación entre la composición orgánica del capital y tasa de ganancia en la perspectiva de Marx sobre el comportamiento de largo plazo del capitalismo (la teoría de la tasa de ganancia decreciente), iii) los rendimientos decrecientes en el producto al añadir una unidad adicional de trabajo, ceteris paribus, en la teoría del productor de los neoclásicos. o bien, preguntas de otra naturaleza como “¿una educación formal de más años está asociada un ingreso personal más elevado?” o bien, “¿la población que vive barrios con viviendas de menores dimensiones está más expuesta al COVID19?”En el proceso de investigación científica, las preguntas planteadas deberán hacerse desde el conocimiento previamente existente, es decir, se plantean desde determinados cuerpos teóricos reconocidos y validados, o bien, partir de los resultados de investigaciones previas. Antes del método está la pregunta de investigación que establece la línea de causalidad entre las variables postuladas en el modelo. Esta base teórica ha de ser complementada con los resultados arrojados en el análisis exploratorio.Así pues, es relevante la siguiente advertencia:“Para establecer la causalidad, la relación entre los regresores y la variable de respuesta debe tener una base fuera de los datos de la muestra; por ejemplo, la relación puede ser sugerida por consideraciones teóricas. El análisis de regresión puede ayudar confirmar una relación de causa y efecto, pero puede ser la única base de tal afirmación” (Montgomery, Peck., Vining. 2012, 81:3).Los elementos anteriores, la metodología econométrica clásica, son explicados con detalle en el primer capítulo del libro Econometría Básica de Gujarati y Porter, del que existen varias ediciones y que se recomienda ampliamente consultar.cabe duda que la especificación de un buen modelo econométrico tiene algo de artesanal, es decir, serán pocas las ocasiones en las que tendremos que proponer diferentes alternativas de modelo, probar con múltiples variables y llevar cabo un sin número de ajustes. Conviene desde este momento eliminar la idea de que “solo hay que aplicar una técnica”: todas las técnicas requieren práctica y el desarrollo de habilidad en su uso pues siempre enfrentaremos un mismo problema en las mismas condiciones. este respecto puede ser apuntado que:“La construcción de un modelo de regresión es un proceso iterativo. Comienza utilizando conocimiento teórico del proceso que se está estudiando y los datos disponibles para especificar un modelo de regresión inicial. Las visualizaciones de datos gráficos suelen ser muy útiles para especificar el modelo inicial. Luego, los parámetros del modelo se estiman, normalmente por mínimos cuadrados (…). Luego debe evaluarse la adecuación del modelo. Consiste en buscar posibles errores de especificación de la fórmula del modelo, como incluir variables importantes, o incluir variables innecesarias, o datos inusuales o inapropiados. Si el modelo es inadecuado, entonces debe proponerse nuevamente uno diferente y estimar nuevamente los parámetros. Este proceso puede repetirse varias veces hasta obtener un modelo adecuado. Finalmente, la validación del modelo debe llevarse cabo para asegurar que el modelo producirá resultados que sean aceptables en la aplicación final” (Montgomery, Peck., Vining. 2012, 81:3–10).\nFigura 4.1: Construcción de un modelo de regresión\nEn este capítulo buscamos que recuerdes algunos de los elementos fundamentales del análisis de regresión, como sus supuestos básicos y la interpretación de los resultados que nos ofrece este instrumento. Para ilustrar dichos elementos nos serviremos de la base de datos sobre COVID19 que contiene información sobre las condiciones sociodemodráficas y económicas de los municipios en la Zona Metropolitana del Valle de México, la misma base que hemos explorado en los capítulos previos. La pregunta que nos interesa responder es de carácter preliminar y atiende cuerpo teórico alguno, por lo que los resultados son solo ilustrativos y tienen solo fines didácticos. La pregunta planteada es: ¿de qué manera se asocian las condiciones sociales, demográficas y económicas, tanto con los casos positivos de COVID19 por cada 1 mil habitantes, como con las muertes provocadas por este mal por cada 1 mil habitantes? Para lograr dar solución la pregunta propondremos algunos modelos econométricos y buscaremos interpretarlos.Un modelo es una representación simplificada de la realidad. En el caso particular de un modelo econométrico y de la regresión lineal lo que se pretende encontrar la recta que mejor se ajusta los datos observados.\nLa técnica más usual para lograrlo son los llamados mínimos cuadrados ordinarios, MCO, justamente poque este método minimiza la suma de los residuales al cuadrado.Veamos esta idea de forma interactiva través de una sencilla ilustración en R. Para ello, crearemos una función que tendrá un solo parámetro al cual llamaremos beta y que será el valor de la pendiente de una recta que buscaremos ajustar un conjunto de datos. Lo que pretendemos pues es encontrar el mejor valor de beta, es decir, el valor que permita minimizar la distancia entre los puntos dados por los pares ordenados (variable \\(x\\) y \\(y\\)) y la recta, en otras palabras, buscamos un valor de beta que minimice el error.Para usar la aplicación interactiva, debes instalar los siguientes paquetes:Ahora bien, antes de ejecutar los segmentos de código que aparecen más abajo, leélos e intenta comprender qué es lo que hace cada línea:Se usa function() justamente para crear para crear una función que llevará por nombre Gráfica (function() es una función como cualquier otra en R, semejante la función para calcular una media, mean(), o para crear una gráfica,plot()).Dentro del paréntesis colocamos los argumentos con los que trabajará nuestra función y, entre las llaves, {}, indicamos los elementos con los que vamos operar, incluidos los argumentos.Nuestra función tomará información de la base de datos llamada Galton, que viene con el paquete HistData, recuerda que Galton (1886) sentó las bases del análisis de regresión través de un estudio de lo que podríamos aquí llamar informalmente “herencia genética,” en otras palabras, cómo la altura de padres e hijos adultos está asociada.Después de haber leído el código, cópialo y pégalo en tu consola6:Una vez que ejecutado el código, observa la sección de ambiente en RStudio y deberás ver la función creada, Gráfica(). Vamos interactuar con ella y para ello ejecutaremos el siguiente segmento de código:Verás en la sección de gráficos el resultado del código anterior: una gráfica con círculos de diferente tamaño que representan la altura de padres y sus hijos adultos; además, identificarás una línea (nuestra recta de ajuste). Como encabezado encontraras el nombre del único argumento de nuestra función, beta. Recuerda: beta es el valor de la pendiente de esa recta. Junto al valor del argumento de nuestra función aparece la suma de los residuales al cuadrado, o Error Cuadrático Medio (Mean Squared Error, MSE). ¿Logras observar el engrane en la parte superior izquierda de la gráfica? Usa el deslizador que aparece cuando das clic en él para elegir diferentes valores para beta y seleccionar aquel que te permita obtener el menor valor de MSE: ese valor de beta es el que buscamos con los MCO.Como verás, lo que buscamos es justamente un modelo que describa nuestros datos. Hay muchos tipos de modelos para describir el comportamiento entre dos variables (curvas cuadráticas, curvas exponenciales o curvas polinomiales). Aquí buscamos expresar la relación entre nuestro par de variables de la forma más simple posible: través de una línea recta.Te recomendamos ampliamente el material de David Dalpiaz, Applied Statistics R, particularmente el capítulo 7 en caso de que quieras recordar con todo detalle los fundamentos estadísticos de la regresión (Dalpiaz 2022).Ya que tenemos una noción básica de lo que buscamos con una regresión, es decir, un modelo lineal que describa el comportamiento de nuestros datos, es momento de dar una definición más precisa: “El análisis de regresión es una técnica estadística para investigar y modelar la relación entre variables” (Montgomery, Peck., Vining. 2012). Un modelo de regresión lineal simple es un modelo con una sola regresora (una variable \\(x\\)) cuya relación con la variable de respuesta (\\(y\\)) está dada por una línea recta:\\[y=\\beta_0+\\beta_1x+u \\]En donde \\(\\beta_0\\) y \\(\\beta_1\\) son constates desconocidas, el intercepto y la pendiente, respectivamente; en tanto, \\(u\\) es el componente de error aleatorio (que se asume con una distribución normal con media cero y varianza constante, \\(N(\\mu=0,\\sigma_u^2=\\sigma^2)\\)), que es la “falla” de nuestro modelo, es decir, la diferencia entre el valor observado de \\(y\\) y el valor estimado por nuestro modelo.¿Cómo deben ser entendidos los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\), el intercepto y la pendiente? “La pendiente, \\(\\beta_1\\) se puede interpretar como el cambio en la media de \\(y\\) para un cambio unitario en \\(x\\)” (Montgomery, Peck., Vining. 2012, 81:3). En tanto, “si el rango de datos en \\(x\\) incluye \\(x = 0\\), entonces la intersección \\(\\beta_0\\) es la media de la distribución de la variable de respuesta \\(y\\) cuando \\(x = 0\\) (es decir, la media de la variable \\(y\\) cuando \\(x=0\\)) Si el rango de \\(x\\) incluye cero, entonces \\(β_0\\) tiene una interpretación práctica.”Una regresión lineal es pues el método que nos permite evaluar la relación lineal entre variables numéricas y se le llama precisamente lineal por la linealidad de sus parámetros (las betas que, como notarás, están elevadas al exponente 1). Lo que buscamos con el análisis de regresión es encontrar los valores estimados de \\(\\beta_0\\) y \\(\\beta_1\\), es decir, \\(\\hat\\beta_0\\) y \\(\\hat\\beta_1\\), que nos permitan describir con una línea recta el par de variables consideradas y el método más usual para hallarlas son los Mínimos Cuadrados Ordinarios, MCO.Los estimadores de MCO ordinarios, tienen tres propiedades: son lineales, insesgados y de varianza mínima.En esta sección enlistamos los elementos sobre los que se funda la estimación de un modelo clásico de regresión lineal con mínimos cuadrados ordinarios y haremos énfasis en uno de ellos que se vincula directamente con los contenidos del capítulo siguiente: la autocorrelación. Los supuestos sobre los que se funda en modelo clásico de regresión lineal son:El modelo es líneal en los parámetros, aunque necesariamente en sus variables.Los valores de la variable o variables explicativas son fijos, es decir, aleatorios, o bien, independientes del término de error.La media de los términos de error, dado el valor de la variable explicativa, es cero.La varianza de los errores es constante (homoscedasticidad).Los errores o perturbaciones son independientes entre sí (hay autocorrelación).El número de observaciones \\(n\\) debe ser mayor que el número de parámetros estimar.La variable o variables explicativas debe tener suficiente variabilidad.Detengámonos un momento en el primer supuesto, relativo la linealiadad de los parámetros. Trata de responder la siguiente pregunta:EjercicioDe las siguientes expresiones, ¿cuál corresponde un modelo líneal en los parámetros?\\(Y=\\beta_1+\\beta_2X +\\beta_3X^2\\)\\(Y=e^{\\beta_1+\\beta_2X}\\)$Y=_1+_2X $Como habrás podido notar, todas las opciones corresponden modelos lineales en los parámetros, es decir, en las “betas.” importa si en en el modelo las variables explicativas, las \\(X\\) aparecen elevadas algún exponente o ellas mismas son exponentes.Respecto al resto de los supuestos, necesariamente éstos se cumplen cuando se recurre la econometría. Para un exámen cuidadoso de la violación de estos supuestos y qué hacer en cada caso, remitimos la estudiante al clásico libro de Gujarati y Porter (2011), donde en la tabla 3.4 podrá encontrar una ruta que le guie en cómo atender cada caso. Aquí sólo haremos mención con un poco más de detalle sobre el supuesto v o supuesto de autocorrelación de los errores, en la medida en que se vincula directamente con el contenido del siguiete capítulo.El supuesto v o supuesto de autocorrelación establece que dos errores, \\(u_i\\) y \\(u_j\\), donde los subíndices \\(j\\) e \\(\\) corresponden perturbaciones de dos observaciones diferentes, están asociados, es decir, que la covarianza entre ellos es igual cero. Si se grafican los errores de un modelo en un diagrama de dispersión, lo que esperamos es que haya ningún patrón sistemático. Reproducimos aquí la figura 3.6 de la obra de (Gujarati Porter 2011), en donde los paneles y b dan cuenta de un comportamiento sistemático o aleatorio en los errores, lo que es indicio de autocorrelación o dependencia entre los errores, en cambio, en el panel c se observa ningún patrón sistemático, esto es justamente lo que se busca en los errores del modelo, que los errores se distribuyan aleatoriamente.\nFigura 4.2: Patrones de correlación. Fuente: Gujarati y Porter, 2011, p. 67\nDe forma simple, con el supuesto de autocorrelación “se afirma que se considerará el efecto sistemático, si existe, de \\(X_t\\) sobre \\(Y_t\\), sin preocuparse por las demás influencias que podrían actuar sobre \\(Y\\) como resultado de las posibles correlaciones entre las \\(u\\)” (Gujarati Porter 2011, 67).Como vimos en el capítulo previo, los datos espaciales suelen presentar entre sus características autocorrelación. Estimar con MCO un modelo econométrico con este tipo de datos muy probablemente violará este supuesto, por lo que se recurre técnicas específicas para la modelación con datos espaciales, esto será objeto del capítulo 5 donde veremos algunos modelos para solventar este problema. En el resto del capítulo se presenta la manera de construir e interpretar modelos econométricos lineales en R través de MCO.Para ilustrar cómo emplear una regresión lineal en R usaremos de nuevo la base de datos covid_zmvm, conformada por 54 variables de 76 observaciones, municipios y alcaldías, que conforman la Zona Metropolitana del Valle de México. Carguemos nuestra base de datos:La base recién cargada es de tamaño considerable, por lo que te recomendamos revisar el diccionario que la acompaña para que comprendas el significado las etiquetas usadas para cada variable. Para mejor comprender el tipo de variables de la base, podemos dividirlas en tres categorías:COVID19: cuatro variables sobre casos positivos y defunciones por COVID19 durante la primera ola de contagios, en términos absolutos y relativos.Sociodemográficas: 32 variables asociadas las características de la población y sus condiciones de viviendaEstructura económica: nueve variables que describen la estructura económica y remuneraciones medias por gran sector de actividad.Para ilustrar el método de regresión con MCO y estimar los parámetros \\(\\beta_1\\) y \\(\\beta_0\\) propondremos un modelo de regresión lineal simple, es decir, compuesto solo por una variable explicativa o predictoria, \\(x\\), y una sola variable explicar o variable de respuesta, \\(y\\). Con base en la información que integra la base, la variable que buscamos explicar con nuestro modelo será casos positivos COVID19 por cada 1 mil habitantes, pos_hab, explicada través del porcentaje de población con acceso servicios de salud, ss. El modelo estimar tomaría la forma de:\\[poshab_i=\\beta_0+\\beta_1ss_i+u_i\\]\nEl subíndice \\(\\) da cuenta de cada una de las 76 unidades espaciales que conforman la Zona Metropolitana del Valle de México. Para estimar un modelo como el anterior, en R recurrimos la función lm(), linear model, del paquete stats que se instala desde que añades R tu equipo de cómputo. La función usada cuenta con dos argumentos data= y formula= que indican la fuente de datos y la expresión estimar, respectivamente. El resultado lo guardaremos en un nuevo objeto que nombraremos modelo_simple:Nota cómo la especificación del modelo tiene la forma “variable de respuesta ~ variable explicativa.” Observa la parte superior derecha de tu ambiente de trabajo, donde aparece el objeto que se ha creado, éste es de tipo lm, una lista con 12 elementos. Da clic en el icono de la lente de aumento y podrás observar que el resultado de la función lm() contiene muchos datos de interés.EjercicioRevisa la documentación de la función lm() y responde:¿Es posible ponderar el proceso de ajuste con base en alguna variable? De ser así, ¿cuál es el argumento que lo permite?¿Es posible ponderar el proceso de ajuste con base en alguna variable? De ser así, ¿cuál es el argumento que lo permite?Si alguna de las variables usadas en el modelo tiene registros vacíos, ¿qué argumento debes recurrir para corregir esta situación?Si alguna de las variables usadas en el modelo tiene registros vacíos, ¿qué argumento debes recurrir para corregir esta situación?¿Qué elemento del objeto de tipo lm contiene los coeficientes estimados?¿Qué elemento del objeto de tipo lm contiene los coeficientes estimados?¿Al menos cuántos componentes podría contener la lista de tipo lm?¿Al menos cuántos componentes podría contener la lista de tipo lm?¿Para qué sirven las funciones coef(), resid() y fitted() aplicadas un objeto de tipo lm?¿Para qué sirven las funciones coef(), resid() y fitted() aplicadas un objeto de tipo lm?Para observar los resultados de nuestra estimación basta con llamar al objeto nuestra consola:Una correcta interpretación de los resultados atraviesa por recordar cómo están medidas cada una de las variables usadas en el modelo. La variable pos_hab es el número de casos positivos por cada 1 mil habitantes; tomemos un municipio en particular, por ejemplo, Isidro Fabela, en el Estado de México, que tiene 1.71 casos por cada 1 mil habitantes. En tanto, la variable ss es la proporción de población con afiliación servicios de salud de cada municipio o alcaldía, que para Isidro Fabela es de 78.6, es decir, la proporción de población con acceso servicios de salud es de 78.6%. Veamos un par de gráficas para recordar cómo se comportan este par de variables:El valor de \\(\\hat \\beta_1\\), el coeficiente de la variable ss, es de 0.3949, lo que significa que cuando la proporción de población con acceso servicios de salud aumenta en 1 punto porcentual (recuerda que así lo estamos midiendo), el número de casos positivos por cada 1 mil habitantes aumenta 0.39, en promedio. Dicho en otras palabras, el parámetro estimado de la pendiente, \\(\\hat \\beta_1\\), nos indica cómo la media de los casos positivos, \\(y\\), es afectada por un cambio en \\(x\\), el porcentaje de personas con acceso servicios de salud. En este caso, el coeficiente estimado \\(\\hat \\beta_0\\) carece de interpretación razonable. Así, el modelo estimado, usando los valores de los coeficientes obtenidos es:\\[\\hat y=\\hat \\beta_0+\\hat\\beta_1x\\]\nEs decir,\\[\\hat {poshab}=-19.8622+0.3949ss\\]Adicionalmente, podemos construir un diagrama de dispersión entre las variables pos_hab y ss añadiendo un ajuste líneal, es decir, la recta de nuestro modelo regresión:Como puedes observar, entre ambas variables existe una relación positiva que puedes identificar por la línea con pendiente positiva que representa la recta de regresión que hemos obtenido, cuya pendiente es igual al valor de \\(\\hat \\beta_1\\), es decir, 0.3949. La distancia vertical entre cada punto y la recta de ajuste de la regresión es el error del modelo, es decir \\(u_i\\)Un elemento de interés para el análisis en la regresión lineal estimada son los errores observados, \\(u_i\\), puesto que los supuestos sobre los que se basa el método de mínimos cuadrados ordinarios son, en buena medida, sobre dichos errores. Para lograr comprender mejor qué son los errores y su importancia analítica, conviene imaginar nuestro modelo como “Respuesta = Predicción + Error,” es decir, \\(y=\\hat y+u\\). De esta manera podemos expresar el error como:\\[u_i=y_i-\\hat {y_i}\\]\nLos errores observados, \\(u_i\\), y los valores estimados, \\(\\hat y_i\\), de nuestro modelo son almacenados en el elemento residuals y fitted.values, respectivamente, dentro de la lista lm. Da clic en la lente de aumento al final de la fila que contiene el objeto de tipo lm en la sección de ambiente en tu entorno de trabajo para observarlos. Para poder analizar mejor los errores del modelo, vamos crear un nuevo objeto que contenga, además de nuestras variables de interés, pos_hab y ss, la clave municipal, cvemun, los errores observados y los valores ajustados o predichos. Para ello, primero generamos una tabla con la función data.frame() que contendrán las variables mencionadas (aquí, los dobles corchetes, [[]], son utilizados para extraer los elementos solicitados):Luego, creamos una lista que contiene las etiquetas de los nombres deseados para cada variable:Y, finalmente, asignamos los nombres deseados la tabla creadaEjercicio\nCon los datos de la tabla anterior:¿Cómo esperarías que luzca un diagrama de dispersión entre los valores observados, pos_hab, y los valores ajustados, \\(\\hat {poshab}\\)? Elabora dicho diagrama.¿Cómo esperarías que luzca un diagrama de dispersión entre los valores observados, pos_hab, y los valores ajustados, \\(\\hat {poshab}\\)? Elabora dicho diagrama.Grafica los errores, primero respecto un índice del 1 al 76 y luego respecto los valores ajustados, ¿puedes observar algún patrón en ellos?Grafica los errores, primero respecto un índice del 1 al 76 y luego respecto los valores ajustados, ¿puedes observar algún patrón en ellos?El supuesto de linealidad de nuestro modelo puede ser verificado partir de una exploración visual de un diagrama de dispersión entre los errores del modelo y la variable explicativa, nuestra \\(x\\), es decir, el porcentaje de población con acceso servicios de salud. Si nuestro modelo es verdaderamente un modelo lineal, en la gráfica debería mostrarse patrón alguno. Veamos:Claramente hay un patrón, por lo que probablemente un modelo lineal como el propuesto sería la mejor opción. ¿Recuerdas cómo luce el diagrama de dispersión entre las dos variables de nuestro modelo?EjercicioElabora un diagrama de dispersión entre pos_hab y ss y añade un ajuste lineal.Entonces, parece que el modelo lineal propuesto capta con precisión la relación entre este par de variables, por lo que un modelo lineal podría ser una mejor opción; sin embargo, esta alternativa de modelación es objeto de estas notas, pero en el capítulo 14 de la obra de Gujarati y Porter (2011) puedes revisar con detalle una ruta para solventar esta situación.Otro de los supuestos sobre los que se basa la estimación de modelos con MCO es que los errores deberían tener una distribución normal, lo que se cumplirá cuando haya observaciones atípicas, los llamados outliers. Para verificar esto visualmente, es posible recurrir un histograma de los errores, o bien, una gráfica de normalidad:¿Notas los valores extremos en el histograma anterior? Una gráfica cuantil-cuantil o gráfica QQ, contrasta la distribución de una variable con una distribución teórica: si la distribución de la variable fuera idéntica la distribución teórica, los puntos se alinearían sobre la línea de 45°. En este caso, la distribución teórica probar es una distribución normal:EjercicioConsulta la ayuda de las funciones geom_qq() y stat_qq_line(), ¿qué otras distribuciones es posible verificar y con qué argumentos?Finalmente, el supuesto de varianza constante de los errores también puede explorarse visualmente través de una gráfica donde se debería lograr ver variabilidad constante de los errores alrededor de la recta de ajuste: este es el denominado supuesto de homoscedasticidad. Exploremos si los errores de nuestro modelo cumplen o con este supuesto través de una gráfica de dispersión:Patrones visuales de “tipo ventilador” o “embudo” deberían ser observados cuando los errores son homoscedásticos, lo que es el caso. Los ejercicios gráficos hechos aquí con los errores para verificar si se cumplen o los supuestos de un modelo de MCO son, desde luego, la única ruta para verificar los supuestos, analizar si una regresión cumple con los supuestos necesarios requiere de práctica. En esta página encontrarás una serie de ejemplos de cuando una regresión cumple o con los supuestos enlistados antes y cómo lucen las gráficas mencionadas en cada caso. Sin embargo, como recordarás, existen pruebas formales que te permitirán determinar si el modelo cumple o con los supuestos del caso.Ejercicio¿Cuál es el nombre de la prueba más popular para evaluar normalidad y cómo se interpreta?¿Cuál es el nombre de la prueba más popular para evaluar normalidad y cómo se interpreta?Menciona una prueba para evaluar homoscedasticidad y cómo es interpretada.Menciona una prueba para evaluar homoscedasticidad y cómo es interpretada.El \\(R^2\\) o coeficiente de determinación es una medida de bondad de ajuste, es decir, un número de nos indica qué tanto nuestro modelo es capaz de explicar la variable de interés. El \\(R^2\\) es la proporción de la variabilidad de \\(y\\) que es explicada por \\(x\\). Valores más cercanos 1 significan que una mayor parte de la variabilidad de \\(y\\) es explicada por \\(x\\). Solicita de nuevo un resumen del objeto modelo_simple e identifica la sección Multiple R-squared:En nuestro caso, hemos obtenido un \\(R^2\\) de 0.2252, lo que puede ser interpretado de la siguiente manera: poco más de la quinta parte de la variabilidad de los casos positivos por cada 1 mil habitantes es explicada por el porcentaje de población con acceso servicios de salud.través de nuestro modelo estimado (también llamado ajustado) es posible hacer predicciones de \\(y\\), es decir, de los casos positivos por cada 1 mil habitantes, siempre que los valores de la variable predictora estén dentro del rango original.Ejercicio¿Cuál es el rango de la variable predictora, es decir de \\(x\\)?Hagamos una predicción de los casos positivos cuando \\(x\\) toma el valor de 60%:\\[\\hat {poshab}=-19.8622+0.3949(60)\\]Con independencia de la consistencia y validez de nuestro modelo, situación que de momento estamos evaluando, ¿cuál es el valor promedio de los casos positivos de COVID19 por cada 1 mil habitantes cuando el porcentaje de población con acceso servicios de salud es de 60%? Bastará resolver la expresión anterior para responder la pregunta: 3.83 casos por cada 1 mil habitantes, aproximadamente; es decir, si un municipio tiene un porcentaje de población con acceso servicios de salud de 60%, nuestro modelo estima un total de 3.83 casos positivos de COVID19.Alternativamente, en R contamos con la función predict() para llevar cabo, justamente, predicciones. La función requiere de dos argumentos forzosos: ) el modelo usado para la predicción (argumento object=) y ii) los valores de la regresión (argumento newdata=). Así:En este caso, estamos indicando los nuevos datos de forma manual para la variable de interés, ss.Uno de los aspectos más importantes del análisis de regresión es la interpretación de los coeficientes obtenidos. Será de particular interés para nosotros conocer, de cada coeficiente: ) el sentido de su asociación con la variable dependiente, es decir, el signo del coeficiente; ii) la magnitud de la asociación lineal, qué tan grande es dicho coeficiente en términos de las unidades de la variable y iii) su significancia estadística, es decir, si hay evidencia de que el verdadero valor del estimador se corresponde con el estimado.Los dos primeros elementos, sentido y magnitud, ya fueron comentados con anterioridad y pueden ser conocidos llamando al objeto que contiene los resultados de nuestro modelo:R despliega en tu consola el valor de los dos coeficientes estimados, en este caso \\(\\hat \\beta_0\\) es negativo y \\(\\hat \\beta_1\\) es positivo. El punto iii consiste en verificar si, en términos estadísticos, nuestros coeficientes estimados son iguales o determinado valor, concretamente, buscamos llevar cabo una prueba de hipótesis sobre dichos coeficientes.La prueba de hipótesis se refiere la evaluación de si el valor de un estadístico, un valor observado u obtenido través de un cálculo, se acerca lo suficiente un valor hipotético. Por ejemplo, algún estudio puede sugerir que la relación entre la educación formal y el ingreso personal es de 0.25, es decir, que medida que crece un año la educación formal el ingreso personal se incrementa en 0.25 unidades, este valor llamémoslo \\(\\nu\\). Quiza en un estudio más reciente se encontró que \\(\\nu\\) es igual 0.19, ¿está este último valor lo suficientemente cerca del 0.25 original para decir que se rechaza el planteamiento original?Siguiendo Gujarati y Damodar (2011: 113), “la hipótesis planteada se conoce como hipótesis nula, y se denota con el símbolo \\(Ho\\). La hipótesis nula suele probarse frente una hipótesis alternativa (también conocida como hipótesis mantenida) denotada con \\(Ha\\).” Así, con relación al ajemplo anterior, la \\(Ho: \\nu=0.19\\) y la \\(Ha: \\nu \\neq 0.19\\).Sinteticemos el procedimiento de prueba de hipótesis desde una perspectiva netamente práctica y simplificada, desde el ángulo de la prueba de significancia:Definir el juego de hipótesis verificar, la hipótesis nula (Ho) y la hipótesis alternativa (Ha).Definir un nivel de significancia, conocido como \\(\\alpha\\), valor con base en el cual tomarás una decisión en torno las hipótesis propuestas. Además, recuerda que (1-\\(\\alpha\\)) define el nivel de confianza con el que la decisión sobre el juego de hipótesis verificar es tomada.Comparar el valor p asociado cada coeficiente estimado con el nivel de \\(\\alpha\\) elegido y decidir con arreglo los siguientes criterios:\n. Si el valor-p < \\(\\alpha\\): se rechaza Ho en favor de Ha.\nb. Si el valor-p > \\(\\alpha\\): se rechaza Ho.El juego de hipótesis sobre la significancia individual de los estimadores calculados:\\[H_o:\\hat\\beta_i=0 \\\\\nH_a:\\hat\\beta_i\\=0 \\]Ahora bien, \\(\\alpha\\) usualmente toma valores de 0.1, 0.05 y 0.01, lo que equivale niveles de confianza de 0.9, 0.95 y 0.99, es decir, de 90%, 95% y 99%, respectivamente; en tanto, el valor-p para tomar la decisión lo tomamos del resumen del objeto modelo_simple. Para observar los valores-p de cada uno de los coeficientes estimados solicitamos un resumen del objeto modelo_simple con la función summary():Observa con cuidado los resultados que ahora se presentan como una tabla en su consola. Identifica las columnas estimate y Pr(>|t|): en esta última aparece el valor-p y es el elemento con base en el cual tomaremos la decisión sobre la significancia estadística de los coeficientes obtenidos.Veamos los valores de los coeficientes estimados: el intercepto, \\(\\hat \\beta_0\\) es igual -19.8 y el coeficiente vinculado ss, \\(\\hat \\beta_1\\), tiene un valor de 0.394. Ahora bien, recuerda el juego de hipótesis verificar: \\(H_o:\\hat\\beta_i=0\\) y \\(H_a:\\hat\\beta_i\\=0\\) (paso 1), elijamos un nivel de significancia, \\(\\alpha\\), de 0.05 (paso 2) y comparemos el valor p asociado cada coeficiente con \\(\\alpha\\) (paso 3). El siguiente cuadro sintentiza lo anterior:Así, decimos que para el caso del coeficiente estimado de ss hay evidencia suficiente para afirmar que su verdadero valor es diferente de cero, por lo que decimos que es estadísticamente significativo, o lo que es lo mismo, hay evidencia suficiente con un nivel de confianza de 95% para sostener que entre el número de casos positivos por cada 1 mil habitantes y el porcentaje de población con acceso afiliación servicios de salud presentan una asociación lineal.Antes de concluir esta sección, es importante tener en mente los siguientes puntos cuando usemos regresiones:Los modelos de regresión son útiles para llevar cabo intrapolación, es decir, para obtener valores en el rango de nuestras variables explicativas. obstante, deben ser usados con cuidado cuando lo que se pretende es extrapolar datos, es decir, conocer los valores de \\(y\\) que están en el rango original de \\(x\\).Como el ajuste de mínimos cuadrados depende en buena medida de los valores de \\(x\\) y con este método cada punto \\(x\\) tiene la misma ponderación, la pendiente de la regresión está mayormente influenciada por los valores más extremos de \\(x\\). Cuando se tienen valores inusuales se podría proceder eliminarlos, o bien, recurrir técnicas diferentes los mínimos cuadrados ordinarios que sean menos sensibles estos puntos.Que la regresión tuviera como resultado que dos variables están asociadas, significa que entre ellas haya una relación causal, tal como se indicó al inicio de este capítulo. Causalidad implica asociación, pero lo opuesto es necesariamente cierto.La regresión lineal múltiple es, en esencia, una extensión lógica de todos los elementos descritos antes: “Un modelo de regresión que involucra más de una variable regresora se llama modelo de regresión múltiple” (Montgomery, Peck., Vining. 2012, 81:67). Así, por ejemplo, un modelo con dos regresoras, \\(x_1\\) y \\(x_2\\) luciría como:\\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+u\\]Esta expresión que incluye tres variables, dos independientes y una dependiente, aún es posible representarse en espacio tridimensional con los ejes \\(y\\), \\(x_1\\) y \\(x_2\\). El parámetro \\(\\beta_0\\) es el intercepto del plano de la regresión que, cuando el origen está incluido en el rango de \\(x_1\\) y \\(x_2\\), \\(\\beta_0\\) indica la media de \\(y\\) cuando \\(x_1=x_2=0\\). En tanto, \\(\\beta_1\\) es el cambio esperado \\(y\\) cuando \\(x_1\\) varia en una unidad, siempre que \\(x_2\\) permanezca constante, análogamente para \\(\\beta_2\\). ¿Qué variables de nuestra base incluirías en un modelo que incluya dos regresoras?Ejercicio¿Recuerdas cómo construir una matriz de diagramas de dispersión través del paquete GGally? Explora el conjunto de variables de la base covid_zmvm y construye una matriz de diagramas de dispersión solo con 4 o 5 variables que presenten el mayor nivel de correlación tanto con los casos positivos como con las defunciones por cada 1 mil habitantes, pos_hab y def_hab.Un modelo con buen nivel de ajuste debería procurar que las variables explicativas propuestas presenten una alta correlación con la variable explicar. Como podrás haberte dado cuenta, las variables que presentan el mayor nivel de asociación lineal negativa son: ppob_basi, pocom, occu, sbasc, ppob_5_o_m. En tanto, las variables que muestran el mayor nivel de asociación lineal positiva son: poss, grad_m, ppob_sup, grad, grad_h, tmss y rmss.Otro principio recomendable la hora de proponer un modelo, amén de su consistencia teórica, es su simpleza: ¿qué relevancia tendrá incluir en un modelo el grado promedio de escolaridad total, el de hombres y el de mujeres por separado? Bastaría incluir solo una de ellas. Por otro lado, ¿qué tan oportuno sería incluir simultáneamente variables que buscan recoger el mismo aspecto de la realidad? Esto es así porque tanto las variables grad, ppob_basi y sbasc refieren el nivel de educación formal de la población, así, bastaría usar alternativamente alguna de ellas, algo parecido pasa con occu y ppob_5_o_m que expresan las condiciones de vivienda y habitación de las personas. En tanto, parece que de los grandes sectores: industria, comercio y servicios, las variables de éste último son las que muestran un mayor nivel de asociación lineal: poss, tmss y rmss, ¿será relevante incluir todas en el modelo?Con base en lo dicho antes, propongamos un primer modelo, lo más sencillo posible:\\[poshab_i=\\beta_0+\\beta_1grad_i+\\beta_2occu_i+\\beta_3poss_i+\\beta_4pocom_i+\\epsilon_i\\]\nDe nuevo, través de la función lm() estimaremos este modelo y al resultado lo guardaremos en un nuevo objeto llamado modelo_multiEjercicio¿Qué pasa con las variables propuestas en este modelo? ¿Cuáles resultaron estadísticamente significativas y por qué?¿Qué pasa con las variables propuestas en este modelo? ¿Cuáles resultaron estadísticamente significativas y por qué?Aquí hemos expuesto una versión simplificada en extremo de cómo tomar decisiones través de las pruebas de hipótesis, puedes remitirte cualquier libro de econometría y responder, ¿de qué modo la prueba t (t value, penúltima columna del cuadro de resumen) nos permitiría llegar las mismas conclusiones?Aquí hemos expuesto una versión simplificada en extremo de cómo tomar decisiones través de las pruebas de hipótesis, puedes remitirte cualquier libro de econometría y responder, ¿de qué modo la prueba t (t value, penúltima columna del cuadro de resumen) nos permitiría llegar las mismas conclusiones?Seguramente notaste que cuando se llamó el objeto modelo_multi tu consola aparecieron otros tantos elementos ,además de los coeficientes, entre ellos uno llamado F-statistic , el estadístico F. En este contexto es utilizado para evaluar si la particular combinación de variables propuesta contribuye o explicar la variable de interés. En términos concretos, la prueba F y el valor p asociado ella, permiten evaluar el siguiente juego de hipótesis:\\[\n\\begin{aligned}\nHo&: \\hat\\beta_1=\\hat\\beta_2=...=\\hat\\beta_i=0 \\\\\nHa&: al.menos.una. \\hat\\beta_i\\=0 \\\\\n\\end{aligned}\n\\]es decir, al menos uno de los coeficientes estimados, en su particular combinación, es diferente de cero. Así, con un valor p de casi cero (2.112e-09), resulta claro que se rechaza Ho en favor de Ha, es decir, que al menos uno de los coeficientes estimados es diferente de 0 en el modelo propuesto y que esta particular combinación de variables explicativas tiene algo interesante para decirnos.La primera parte de este capítulo llamó la atención sobre el llamado análisis causal, por lo que dedicaremos unas cuantas palabras para cerrar la discusión con la que comenzamos. La realidad social es compleja y si bien se han extendido las técnicas experimentales, incluso en disciplinas como la economía, la mayor parte de la información con la que cotidianamente trabaja un científico social son datos observados, experimentales.Esto implica que el uso de técnicas de análisis de información como la regresión en ciencias sociales tienen un alcance limitado en términos del análisis de causalidad; obstante, el análisis de regresión sí nos permite estudiar rigurosamente la correlación entre variables. Que exista correlación entre dos variables significa que sus movimientos están asociados, por ejemplo, que cuando el ingreso personal es alto, se consume más en servicios culturales, pero eso implica que una elevación del ingreso cause el incremento en el consumo de los servicios culturales.En este sentido, lo que permite anclar la correlación identificada con la técnica una línea de causalidad es la reflexión teórica, es decir, un estudio cuidadoso de las teóricas propuestas para la explicación de un fenómeno. Por eso, como se dijo en la primera parte de este capítulo, el análisis de regresión debe ser tomado con cautela.Dejemos hasta aquí este somero repaso de los aspectos más elementales del análisis de regresión. Es primordial que por tu propia cuenta estudies los materiales sugeridos, pues este capítulo en modo alguno suple un curso formal de los tópicos básicos de econometría. En el siguiente capítulo, retomamos el hilo del tratamiento de la información espacial través de su incorporación los modelos econométricos.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2023-02-06.book built bookdown R package.","code":"\n#Instalar dos paquetes\ninstall.packages(c(\"manipulate\", \"UsingR\"))\n#Para llamar las librerías que recién instalaste\nlibrary(manipulate)\nlibrary(UsingR)\n\n#Crear función llamada \"Gráfica\"\nGráfica <- function(beta){\n  y <- Galton$child - mean(Galton$child) #Crea un vector llamado \"y\" con las desviaciones de la media de las alturas de los hijos.\n  x <- Galton$parent - mean(Galton$parent) #Crea un vector llamado \"x\" con las desviaciones de la media de las alturas de los padres.\n  freqData <- as.data.frame(table(x, y)) #Crea un arreglo de datos llamado freqData con los vectores anteriores\n  names(freqData) <- c(\"child\", \"parent\", \"freq\") #Asigna nombres a las columnas del data frame anterior\n  plot(                 \n    as.numeric(as.vector(freqData$parent)),\n    as.numeric(as.vector(freqData$child)),\n    pch = 21, col = \"black\", bg = \"lightblue\",\n    cex = .15 * freqData$freq,\n    xlab = \"Padres\",\n    ylab = \"Hijos\"\n  )#Crea un diagrama de dispersión con varios elementos de formato, el más importante es que el tamaño de los símbolos usados dependerá de la frecuencia. \n  abline(0, beta, lwd = 3)#Añade una línea en función del valor elegido de beta\n  points(0, 0, cex = 2, pch = 19) #Añade puntos\n  mse <- mean( (y - beta * x)^2 ) #Muestra el valor de la suma de los residuales al cuadrado \n  title(paste(\"beta = \", beta, \"mse = \", round(mse, 3)))} #Añade títulos a la gráfica\nmanipulate(Gráfica(beta), beta = slider(-1.5, 1.5, step = 0.02))\nlibrary(readxl)\ncovid_zmvm <- read_excel(path=\"base de datos\\\\covid_zmvm.xlsx\")\nmodelo_simple <- stats::lm (formula=pos_hab ~ ss, data = covid_zmvm)\nmodelo_simple## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Coefficients:\n## (Intercept)           ss  \n##    -19.8622       0.3949\nlibrary(tidyverse)\n\ncovid_zmvm %>% \n  ggplot2::ggplot()+geom_dotplot(aes(pos_hab))\ncovid_zmvm %>% \n  ggplot2::ggplot()+geom_dotplot(aes(poss))\ncovid_zmvm %>% ggplot()+\n  geom_point(aes(x=ss,y=pos_hab))+\n  geom_smooth(aes(x=ss,y=pos_hab),method = \"lm\")+\n  labs(x=\"Población con acceso a servicios de salud (%)\", y=\"Casos positivos por COVID19\")## `geom_smooth()` using formula = 'y ~ x'\ntabla <- data.frame(covid_zmvm$cvemun, covid_zmvm$ss, covid_zmvm$pos_hab, as.data.frame(modelo_simple[[\"fitted.values\"]]),\n                    as.data.frame(modelo_simple[[\"residuals\"]]))\nnombres <- c(\"cvemun\",\"ss\", \"pos_hab\",\"ajustados\", \"errores\") \ncolnames(tabla)<-nombres\ntabla %>% \n  ggplot()+\n  geom_point(aes(x=pos_hab,y=errores))\ntabla %>%\n  ggplot()+\n  geom_histogram(aes(errores))## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\ntabla %>% \n  ggplot()+\n  geom_qq(aes(sample=errores))+\n  geom_qq_line(aes(sample=errores))\ntabla %>%\n  ggplot()+\n  geom_point(aes(ajustados,errores))\nsummary(modelo_simple)## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.4391 -2.7744 -0.6709  1.5324 14.9969 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -19.86224    5.80867  -3.419  0.00102 ** \n## ss            0.39490    0.08516   4.637 1.49e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.572 on 74 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2147 \n## F-statistic:  21.5 on 1 and 74 DF,  p-value: 1.488e-05\nstats::predict(object = modelo_simple, newdata=data.frame(ss=60))##        1 \n## 3.831836\nmodelo_simple## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Coefficients:\n## (Intercept)           ss  \n##    -19.8622       0.3949\nbase::summary(modelo_simple)## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.4391 -2.7744 -0.6709  1.5324 14.9969 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -19.86224    5.80867  -3.419  0.00102 ** \n## ss            0.39490    0.08516   4.637 1.49e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.572 on 74 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2147 \n## F-statistic:  21.5 on 1 and 74 DF,  p-value: 1.488e-05\nmodelo_multi <- stats::lm (formula=pos_hab ~ grad+occu+poss+pocom, data = covid_zmvm)\nsummary(modelo_multi)## \n## Call:\n## stats::lm(formula = pos_hab ~ grad + occu + poss + pocom, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.0017 -2.4621 -0.7526  1.2428 16.6659 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  1.43576   15.07149   0.095 0.924374    \n## grad         0.09555    1.01112   0.094 0.924981    \n## occu        -4.25136    7.65251  -0.556 0.580264    \n## poss        21.80677    5.91896   3.684 0.000445 ***\n## pocom        0.07430    6.71860   0.011 0.991208    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.843 on 71 degrees of freedom\n## Multiple R-squared:  0.4748, Adjusted R-squared:  0.4452 \n## F-statistic: 16.04 on 4 and 71 DF,  p-value: 2.112e-09"},{"path":"modelos-de-regresión-lineal.html","id":"causalidad-y-correlación-el-alcance-del-análisis-de-regresión","chapter":"4 Modelos de regresión lineal","heading":"4.1 Causalidad y correlación: el alcance del análisis de regresión","text":"En términos de su origen etimológico, econometría significa medición econonómica. Pero su alcance se extiende más allá de la simple medición ya que través de ella es posible contribuir dar soporte empírico la teoría económica través del uso de herramientas estadísticas como los modelos de regresión y sus estimaciones (Gujarati Porter 2011). Veamos esto con un poco más de detalle.En el proceso de investigación podemos plantearnos preguntas con un grado de complejidad diverso. Habrá preguntas que, al ser respondidas, nos permitan tener una descripción general del fenómeno, habrá otras que nos permitan explorar la relación entre dos fenómenos, o bien, preguntas cuya respuesta nos permita predecir el comportamiento futuro del elemento estudiado. Así pues, hay diferentes tipos de preguntas de investigación, dependiendo de su grado de complejidad. En su libro “El arte de la ciencia de datos,” Roger Peng y Elizabeth Matsui(2017), abordan sistemáticamente esta cuestión.\nCuando, por ejemplo, nos preguntamos “¿el cambio en una variable provoca el cambio en otra diferente?” este tipo de pregunta es llamada pregunta de análisis causal. Con el análisis causal se busca saber si un par de variables están asociadas, más aún, si el cambio en una variable causa cambios en la otra.Responder este tipo de preguntas exige el dominio de diversas herramientas estadísticas, cuyo abordaje está fuera del alcance de este capítulo. obstante, la econometría través del análisis de regresión nos brinda una oportunidad que, cuando es bien utilizada, permite aproximarnos al análisis causal. partir de una relación causal anclada en la teoría (modelo teórico), es preciso indicar el tipo de relación específico entre las variables del caso (modelo matemático), para luego expresar dicha relación en una relación exacta sino sujeta variaciones individuales (modelo econométrico) (Gujarati Porter 2011: 3-5).La regresión como instrumento analítico nos brinda elementos para acercarnos la comprensión de la asociación entre dos variables, aunque es posible hablar en sentido estricto descubrir relaciones de causalidad como resultado del análisis de regresión. Así, “El análisis de regresión solo puede abordar los problemas de correlación. puede abordar el problema de la necesidad (causalidad). Por tanto, nuestras expectativas de descubrir relaciones de causa y efecto partir de la regresión deberían ser modestas” (Montgomery, Peck., Vining. 2012, 81:3).Entonces, ¿de qué modo el análisis de regresión se convierte potencialmente en un elemento que nos permite analizar causalidad? Lo que resultará fundamental en el proceso de investigación que recurre esta herramienta es pues la pregunta que se plantee el investigador, preguntas que deben ser planteadas desde el conocimiento científico previamente existente. Dicho en otros términos, el análisis de causalidad que nos da la regresión, sí lo puede ofrecer la teoría existente en determinado campo de conocimiento: ¿qué dice la teoría sobre la relación entre la variable y la variable B?¿Qué asociaciones entre pares de variables recuerdas de tus cursos teoría económica? Quizá los siguientes ejemplos te resulten familiares: ) la asociación entre la tasa de interés y eficiencia marginal del capital en la teoría de la inversión de Keynes, ii) la relación entre la composición orgánica del capital y tasa de ganancia en la perspectiva de Marx sobre el comportamiento de largo plazo del capitalismo (la teoría de la tasa de ganancia decreciente), iii) los rendimientos decrecientes en el producto al añadir una unidad adicional de trabajo, ceteris paribus, en la teoría del productor de los neoclásicos. o bien, preguntas de otra naturaleza como “¿una educación formal de más años está asociada un ingreso personal más elevado?” o bien, “¿la población que vive barrios con viviendas de menores dimensiones está más expuesta al COVID19?”En el proceso de investigación científica, las preguntas planteadas deberán hacerse desde el conocimiento previamente existente, es decir, se plantean desde determinados cuerpos teóricos reconocidos y validados, o bien, partir de los resultados de investigaciones previas. Antes del método está la pregunta de investigación que establece la línea de causalidad entre las variables postuladas en el modelo. Esta base teórica ha de ser complementada con los resultados arrojados en el análisis exploratorio.Así pues, es relevante la siguiente advertencia:“Para establecer la causalidad, la relación entre los regresores y la variable de respuesta debe tener una base fuera de los datos de la muestra; por ejemplo, la relación puede ser sugerida por consideraciones teóricas. El análisis de regresión puede ayudar confirmar una relación de causa y efecto, pero puede ser la única base de tal afirmación” (Montgomery, Peck., Vining. 2012, 81:3).Los elementos anteriores, la metodología econométrica clásica, son explicados con detalle en el primer capítulo del libro Econometría Básica de Gujarati y Porter, del que existen varias ediciones y que se recomienda ampliamente consultar.","code":""},{"path":"modelos-de-regresión-lineal.html","id":"lo-artesanal-de-la-econometría","chapter":"4 Modelos de regresión lineal","heading":"4.2 Lo “artesanal” de la econometría","text":"cabe duda que la especificación de un buen modelo econométrico tiene algo de artesanal, es decir, serán pocas las ocasiones en las que tendremos que proponer diferentes alternativas de modelo, probar con múltiples variables y llevar cabo un sin número de ajustes. Conviene desde este momento eliminar la idea de que “solo hay que aplicar una técnica”: todas las técnicas requieren práctica y el desarrollo de habilidad en su uso pues siempre enfrentaremos un mismo problema en las mismas condiciones. este respecto puede ser apuntado que:“La construcción de un modelo de regresión es un proceso iterativo. Comienza utilizando conocimiento teórico del proceso que se está estudiando y los datos disponibles para especificar un modelo de regresión inicial. Las visualizaciones de datos gráficos suelen ser muy útiles para especificar el modelo inicial. Luego, los parámetros del modelo se estiman, normalmente por mínimos cuadrados (…). Luego debe evaluarse la adecuación del modelo. Consiste en buscar posibles errores de especificación de la fórmula del modelo, como incluir variables importantes, o incluir variables innecesarias, o datos inusuales o inapropiados. Si el modelo es inadecuado, entonces debe proponerse nuevamente uno diferente y estimar nuevamente los parámetros. Este proceso puede repetirse varias veces hasta obtener un modelo adecuado. Finalmente, la validación del modelo debe llevarse cabo para asegurar que el modelo producirá resultados que sean aceptables en la aplicación final” (Montgomery, Peck., Vining. 2012, 81:3–10).\nFigura 4.1: Construcción de un modelo de regresión\nEn este capítulo buscamos que recuerdes algunos de los elementos fundamentales del análisis de regresión, como sus supuestos básicos y la interpretación de los resultados que nos ofrece este instrumento. Para ilustrar dichos elementos nos serviremos de la base de datos sobre COVID19 que contiene información sobre las condiciones sociodemodráficas y económicas de los municipios en la Zona Metropolitana del Valle de México, la misma base que hemos explorado en los capítulos previos. La pregunta que nos interesa responder es de carácter preliminar y atiende cuerpo teórico alguno, por lo que los resultados son solo ilustrativos y tienen solo fines didácticos. La pregunta planteada es: ¿de qué manera se asocian las condiciones sociales, demográficas y económicas, tanto con los casos positivos de COVID19 por cada 1 mil habitantes, como con las muertes provocadas por este mal por cada 1 mil habitantes? Para lograr dar solución la pregunta propondremos algunos modelos econométricos y buscaremos interpretarlos.","code":""},{"path":"modelos-de-regresión-lineal.html","id":"qué-buscamos-a-través-de-una-regresión-con-mínimos-cuadrados-ordinarios","chapter":"4 Modelos de regresión lineal","heading":"4.3 Qué buscamos a través de una regresión (con mínimos cuadrados ordinarios)","text":"Un modelo es una representación simplificada de la realidad. En el caso particular de un modelo econométrico y de la regresión lineal lo que se pretende encontrar la recta que mejor se ajusta los datos observados.\nLa técnica más usual para lograrlo son los llamados mínimos cuadrados ordinarios, MCO, justamente poque este método minimiza la suma de los residuales al cuadrado.Veamos esta idea de forma interactiva través de una sencilla ilustración en R. Para ello, crearemos una función que tendrá un solo parámetro al cual llamaremos beta y que será el valor de la pendiente de una recta que buscaremos ajustar un conjunto de datos. Lo que pretendemos pues es encontrar el mejor valor de beta, es decir, el valor que permita minimizar la distancia entre los puntos dados por los pares ordenados (variable \\(x\\) y \\(y\\)) y la recta, en otras palabras, buscamos un valor de beta que minimice el error.Para usar la aplicación interactiva, debes instalar los siguientes paquetes:Ahora bien, antes de ejecutar los segmentos de código que aparecen más abajo, leélos e intenta comprender qué es lo que hace cada línea:Se usa function() justamente para crear para crear una función que llevará por nombre Gráfica (function() es una función como cualquier otra en R, semejante la función para calcular una media, mean(), o para crear una gráfica,plot()).Dentro del paréntesis colocamos los argumentos con los que trabajará nuestra función y, entre las llaves, {}, indicamos los elementos con los que vamos operar, incluidos los argumentos.Nuestra función tomará información de la base de datos llamada Galton, que viene con el paquete HistData, recuerda que Galton (1886) sentó las bases del análisis de regresión través de un estudio de lo que podríamos aquí llamar informalmente “herencia genética,” en otras palabras, cómo la altura de padres e hijos adultos está asociada.Después de haber leído el código, cópialo y pégalo en tu consola6:Una vez que ejecutado el código, observa la sección de ambiente en RStudio y deberás ver la función creada, Gráfica(). Vamos interactuar con ella y para ello ejecutaremos el siguiente segmento de código:Verás en la sección de gráficos el resultado del código anterior: una gráfica con círculos de diferente tamaño que representan la altura de padres y sus hijos adultos; además, identificarás una línea (nuestra recta de ajuste). Como encabezado encontraras el nombre del único argumento de nuestra función, beta. Recuerda: beta es el valor de la pendiente de esa recta. Junto al valor del argumento de nuestra función aparece la suma de los residuales al cuadrado, o Error Cuadrático Medio (Mean Squared Error, MSE). ¿Logras observar el engrane en la parte superior izquierda de la gráfica? Usa el deslizador que aparece cuando das clic en él para elegir diferentes valores para beta y seleccionar aquel que te permita obtener el menor valor de MSE: ese valor de beta es el que buscamos con los MCO.Como verás, lo que buscamos es justamente un modelo que describa nuestros datos. Hay muchos tipos de modelos para describir el comportamiento entre dos variables (curvas cuadráticas, curvas exponenciales o curvas polinomiales). Aquí buscamos expresar la relación entre nuestro par de variables de la forma más simple posible: través de una línea recta.Te recomendamos ampliamente el material de David Dalpiaz, Applied Statistics R, particularmente el capítulo 7 en caso de que quieras recordar con todo detalle los fundamentos estadísticos de la regresión (Dalpiaz 2022).","code":"\n#Instalar dos paquetes\ninstall.packages(c(\"manipulate\", \"UsingR\"))\n#Para llamar las librerías que recién instalaste\nlibrary(manipulate)\nlibrary(UsingR)\n\n#Crear función llamada \"Gráfica\"\nGráfica <- function(beta){\n  y <- Galton$child - mean(Galton$child) #Crea un vector llamado \"y\" con las desviaciones de la media de las alturas de los hijos.\n  x <- Galton$parent - mean(Galton$parent) #Crea un vector llamado \"x\" con las desviaciones de la media de las alturas de los padres.\n  freqData <- as.data.frame(table(x, y)) #Crea un arreglo de datos llamado freqData con los vectores anteriores\n  names(freqData) <- c(\"child\", \"parent\", \"freq\") #Asigna nombres a las columnas del data frame anterior\n  plot(                 \n    as.numeric(as.vector(freqData$parent)),\n    as.numeric(as.vector(freqData$child)),\n    pch = 21, col = \"black\", bg = \"lightblue\",\n    cex = .15 * freqData$freq,\n    xlab = \"Padres\",\n    ylab = \"Hijos\"\n  )#Crea un diagrama de dispersión con varios elementos de formato, el más importante es que el tamaño de los símbolos usados dependerá de la frecuencia. \n  abline(0, beta, lwd = 3)#Añade una línea en función del valor elegido de beta\n  points(0, 0, cex = 2, pch = 19) #Añade puntos\n  mse <- mean( (y - beta * x)^2 ) #Muestra el valor de la suma de los residuales al cuadrado \n  title(paste(\"beta = \", beta, \"mse = \", round(mse, 3)))} #Añade títulos a la gráfica\nmanipulate(Gráfica(beta), beta = slider(-1.5, 1.5, step = 0.02))"},{"path":"modelos-de-regresión-lineal.html","id":"modelo-de-regresión-lineal-simple","chapter":"4 Modelos de regresión lineal","heading":"4.4 Modelo de regresión lineal simple","text":"Ya que tenemos una noción básica de lo que buscamos con una regresión, es decir, un modelo lineal que describa el comportamiento de nuestros datos, es momento de dar una definición más precisa: “El análisis de regresión es una técnica estadística para investigar y modelar la relación entre variables” (Montgomery, Peck., Vining. 2012). Un modelo de regresión lineal simple es un modelo con una sola regresora (una variable \\(x\\)) cuya relación con la variable de respuesta (\\(y\\)) está dada por una línea recta:\\[y=\\beta_0+\\beta_1x+u \\]En donde \\(\\beta_0\\) y \\(\\beta_1\\) son constates desconocidas, el intercepto y la pendiente, respectivamente; en tanto, \\(u\\) es el componente de error aleatorio (que se asume con una distribución normal con media cero y varianza constante, \\(N(\\mu=0,\\sigma_u^2=\\sigma^2)\\)), que es la “falla” de nuestro modelo, es decir, la diferencia entre el valor observado de \\(y\\) y el valor estimado por nuestro modelo.¿Cómo deben ser entendidos los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\), el intercepto y la pendiente? “La pendiente, \\(\\beta_1\\) se puede interpretar como el cambio en la media de \\(y\\) para un cambio unitario en \\(x\\)” (Montgomery, Peck., Vining. 2012, 81:3). En tanto, “si el rango de datos en \\(x\\) incluye \\(x = 0\\), entonces la intersección \\(\\beta_0\\) es la media de la distribución de la variable de respuesta \\(y\\) cuando \\(x = 0\\) (es decir, la media de la variable \\(y\\) cuando \\(x=0\\)) Si el rango de \\(x\\) incluye cero, entonces \\(β_0\\) tiene una interpretación práctica.”Una regresión lineal es pues el método que nos permite evaluar la relación lineal entre variables numéricas y se le llama precisamente lineal por la linealidad de sus parámetros (las betas que, como notarás, están elevadas al exponente 1). Lo que buscamos con el análisis de regresión es encontrar los valores estimados de \\(\\beta_0\\) y \\(\\beta_1\\), es decir, \\(\\hat\\beta_0\\) y \\(\\hat\\beta_1\\), que nos permitan describir con una línea recta el par de variables consideradas y el método más usual para hallarlas son los Mínimos Cuadrados Ordinarios, MCO.Los estimadores de MCO ordinarios, tienen tres propiedades: son lineales, insesgados y de varianza mínima.En esta sección enlistamos los elementos sobre los que se funda la estimación de un modelo clásico de regresión lineal con mínimos cuadrados ordinarios y haremos énfasis en uno de ellos que se vincula directamente con los contenidos del capítulo siguiente: la autocorrelación. Los supuestos sobre los que se funda en modelo clásico de regresión lineal son:El modelo es líneal en los parámetros, aunque necesariamente en sus variables.Los valores de la variable o variables explicativas son fijos, es decir, aleatorios, o bien, independientes del término de error.La media de los términos de error, dado el valor de la variable explicativa, es cero.La varianza de los errores es constante (homoscedasticidad).Los errores o perturbaciones son independientes entre sí (hay autocorrelación).El número de observaciones \\(n\\) debe ser mayor que el número de parámetros estimar.La variable o variables explicativas debe tener suficiente variabilidad.Detengámonos un momento en el primer supuesto, relativo la linealiadad de los parámetros. Trata de responder la siguiente pregunta:EjercicioDe las siguientes expresiones, ¿cuál corresponde un modelo líneal en los parámetros?\\(Y=\\beta_1+\\beta_2X +\\beta_3X^2\\)\\(Y=e^{\\beta_1+\\beta_2X}\\)$Y=_1+_2X $Como habrás podido notar, todas las opciones corresponden modelos lineales en los parámetros, es decir, en las “betas.” importa si en en el modelo las variables explicativas, las \\(X\\) aparecen elevadas algún exponente o ellas mismas son exponentes.Respecto al resto de los supuestos, necesariamente éstos se cumplen cuando se recurre la econometría. Para un exámen cuidadoso de la violación de estos supuestos y qué hacer en cada caso, remitimos la estudiante al clásico libro de Gujarati y Porter (2011), donde en la tabla 3.4 podrá encontrar una ruta que le guie en cómo atender cada caso. Aquí sólo haremos mención con un poco más de detalle sobre el supuesto v o supuesto de autocorrelación de los errores, en la medida en que se vincula directamente con el contenido del siguiete capítulo.El supuesto v o supuesto de autocorrelación establece que dos errores, \\(u_i\\) y \\(u_j\\), donde los subíndices \\(j\\) e \\(\\) corresponden perturbaciones de dos observaciones diferentes, están asociados, es decir, que la covarianza entre ellos es igual cero. Si se grafican los errores de un modelo en un diagrama de dispersión, lo que esperamos es que haya ningún patrón sistemático. Reproducimos aquí la figura 3.6 de la obra de (Gujarati Porter 2011), en donde los paneles y b dan cuenta de un comportamiento sistemático o aleatorio en los errores, lo que es indicio de autocorrelación o dependencia entre los errores, en cambio, en el panel c se observa ningún patrón sistemático, esto es justamente lo que se busca en los errores del modelo, que los errores se distribuyan aleatoriamente.\nFigura 4.2: Patrones de correlación. Fuente: Gujarati y Porter, 2011, p. 67\nDe forma simple, con el supuesto de autocorrelación “se afirma que se considerará el efecto sistemático, si existe, de \\(X_t\\) sobre \\(Y_t\\), sin preocuparse por las demás influencias que podrían actuar sobre \\(Y\\) como resultado de las posibles correlaciones entre las \\(u\\)” (Gujarati Porter 2011, 67).Como vimos en el capítulo previo, los datos espaciales suelen presentar entre sus características autocorrelación. Estimar con MCO un modelo econométrico con este tipo de datos muy probablemente violará este supuesto, por lo que se recurre técnicas específicas para la modelación con datos espaciales, esto será objeto del capítulo 5 donde veremos algunos modelos para solventar este problema. En el resto del capítulo se presenta la manera de construir e interpretar modelos econométricos lineales en R través de MCO.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2023-02-06.book built bookdown R package.","code":""},{"path":"modelos-de-regresión-lineal.html","id":"supuestos-del-módelo-clásico-de-regresión-lineal","chapter":"4 Modelos de regresión lineal","heading":"4.4.1 Supuestos del módelo clásico de regresión lineal","text":"En esta sección enlistamos los elementos sobre los que se funda la estimación de un modelo clásico de regresión lineal con mínimos cuadrados ordinarios y haremos énfasis en uno de ellos que se vincula directamente con los contenidos del capítulo siguiente: la autocorrelación. Los supuestos sobre los que se funda en modelo clásico de regresión lineal son:El modelo es líneal en los parámetros, aunque necesariamente en sus variables.Los valores de la variable o variables explicativas son fijos, es decir, aleatorios, o bien, independientes del término de error.La media de los términos de error, dado el valor de la variable explicativa, es cero.La varianza de los errores es constante (homoscedasticidad).Los errores o perturbaciones son independientes entre sí (hay autocorrelación).El número de observaciones \\(n\\) debe ser mayor que el número de parámetros estimar.La variable o variables explicativas debe tener suficiente variabilidad.Detengámonos un momento en el primer supuesto, relativo la linealiadad de los parámetros. Trata de responder la siguiente pregunta:EjercicioDe las siguientes expresiones, ¿cuál corresponde un modelo líneal en los parámetros?\\(Y=\\beta_1+\\beta_2X +\\beta_3X^2\\)\\(Y=e^{\\beta_1+\\beta_2X}\\)$Y=_1+_2X $Como habrás podido notar, todas las opciones corresponden modelos lineales en los parámetros, es decir, en las “betas.” importa si en en el modelo las variables explicativas, las \\(X\\) aparecen elevadas algún exponente o ellas mismas son exponentes.Respecto al resto de los supuestos, necesariamente éstos se cumplen cuando se recurre la econometría. Para un exámen cuidadoso de la violación de estos supuestos y qué hacer en cada caso, remitimos la estudiante al clásico libro de Gujarati y Porter (2011), donde en la tabla 3.4 podrá encontrar una ruta que le guie en cómo atender cada caso. Aquí sólo haremos mención con un poco más de detalle sobre el supuesto v o supuesto de autocorrelación de los errores, en la medida en que se vincula directamente con el contenido del siguiete capítulo.El supuesto v o supuesto de autocorrelación establece que dos errores, \\(u_i\\) y \\(u_j\\), donde los subíndices \\(j\\) e \\(\\) corresponden perturbaciones de dos observaciones diferentes, están asociados, es decir, que la covarianza entre ellos es igual cero. Si se grafican los errores de un modelo en un diagrama de dispersión, lo que esperamos es que haya ningún patrón sistemático. Reproducimos aquí la figura 3.6 de la obra de (Gujarati Porter 2011), en donde los paneles y b dan cuenta de un comportamiento sistemático o aleatorio en los errores, lo que es indicio de autocorrelación o dependencia entre los errores, en cambio, en el panel c se observa ningún patrón sistemático, esto es justamente lo que se busca en los errores del modelo, que los errores se distribuyan aleatoriamente.\nFigura 4.2: Patrones de correlación. Fuente: Gujarati y Porter, 2011, p. 67\nDe forma simple, con el supuesto de autocorrelación “se afirma que se considerará el efecto sistemático, si existe, de \\(X_t\\) sobre \\(Y_t\\), sin preocuparse por las demás influencias que podrían actuar sobre \\(Y\\) como resultado de las posibles correlaciones entre las \\(u\\)” (Gujarati Porter 2011, 67).Como vimos en el capítulo previo, los datos espaciales suelen presentar entre sus características autocorrelación. Estimar con MCO un modelo econométrico con este tipo de datos muy probablemente violará este supuesto, por lo que se recurre técnicas específicas para la modelación con datos espaciales, esto será objeto del capítulo 5 donde veremos algunos modelos para solventar este problema. En el resto del capítulo se presenta la manera de construir e interpretar modelos econométricos lineales en R través de MCO.","code":""},{"path":"modelos-de-regresión-lineal.html","id":"regresión-lineal-simple-en-r","chapter":"4 Modelos de regresión lineal","heading":"4.5 Regresión lineal simple en R","text":"Para ilustrar cómo emplear una regresión lineal en R usaremos de nuevo la base de datos covid_zmvm, conformada por 54 variables de 76 observaciones, municipios y alcaldías, que conforman la Zona Metropolitana del Valle de México. Carguemos nuestra base de datos:La base recién cargada es de tamaño considerable, por lo que te recomendamos revisar el diccionario que la acompaña para que comprendas el significado las etiquetas usadas para cada variable. Para mejor comprender el tipo de variables de la base, podemos dividirlas en tres categorías:COVID19: cuatro variables sobre casos positivos y defunciones por COVID19 durante la primera ola de contagios, en términos absolutos y relativos.Sociodemográficas: 32 variables asociadas las características de la población y sus condiciones de viviendaEstructura económica: nueve variables que describen la estructura económica y remuneraciones medias por gran sector de actividad.Para ilustrar el método de regresión con MCO y estimar los parámetros \\(\\beta_1\\) y \\(\\beta_0\\) propondremos un modelo de regresión lineal simple, es decir, compuesto solo por una variable explicativa o predictoria, \\(x\\), y una sola variable explicar o variable de respuesta, \\(y\\). Con base en la información que integra la base, la variable que buscamos explicar con nuestro modelo será casos positivos COVID19 por cada 1 mil habitantes, pos_hab, explicada través del porcentaje de población con acceso servicios de salud, ss. El modelo estimar tomaría la forma de:\\[poshab_i=\\beta_0+\\beta_1ss_i+u_i\\]\nEl subíndice \\(\\) da cuenta de cada una de las 76 unidades espaciales que conforman la Zona Metropolitana del Valle de México. Para estimar un modelo como el anterior, en R recurrimos la función lm(), linear model, del paquete stats que se instala desde que añades R tu equipo de cómputo. La función usada cuenta con dos argumentos data= y formula= que indican la fuente de datos y la expresión estimar, respectivamente. El resultado lo guardaremos en un nuevo objeto que nombraremos modelo_simple:Nota cómo la especificación del modelo tiene la forma “variable de respuesta ~ variable explicativa.” Observa la parte superior derecha de tu ambiente de trabajo, donde aparece el objeto que se ha creado, éste es de tipo lm, una lista con 12 elementos. Da clic en el icono de la lente de aumento y podrás observar que el resultado de la función lm() contiene muchos datos de interés.EjercicioRevisa la documentación de la función lm() y responde:¿Es posible ponderar el proceso de ajuste con base en alguna variable? De ser así, ¿cuál es el argumento que lo permite?¿Es posible ponderar el proceso de ajuste con base en alguna variable? De ser así, ¿cuál es el argumento que lo permite?Si alguna de las variables usadas en el modelo tiene registros vacíos, ¿qué argumento debes recurrir para corregir esta situación?Si alguna de las variables usadas en el modelo tiene registros vacíos, ¿qué argumento debes recurrir para corregir esta situación?¿Qué elemento del objeto de tipo lm contiene los coeficientes estimados?¿Qué elemento del objeto de tipo lm contiene los coeficientes estimados?¿Al menos cuántos componentes podría contener la lista de tipo lm?¿Al menos cuántos componentes podría contener la lista de tipo lm?¿Para qué sirven las funciones coef(), resid() y fitted() aplicadas un objeto de tipo lm?¿Para qué sirven las funciones coef(), resid() y fitted() aplicadas un objeto de tipo lm?Para observar los resultados de nuestra estimación basta con llamar al objeto nuestra consola:Una correcta interpretación de los resultados atraviesa por recordar cómo están medidas cada una de las variables usadas en el modelo. La variable pos_hab es el número de casos positivos por cada 1 mil habitantes; tomemos un municipio en particular, por ejemplo, Isidro Fabela, en el Estado de México, que tiene 1.71 casos por cada 1 mil habitantes. En tanto, la variable ss es la proporción de población con afiliación servicios de salud de cada municipio o alcaldía, que para Isidro Fabela es de 78.6, es decir, la proporción de población con acceso servicios de salud es de 78.6%. Veamos un par de gráficas para recordar cómo se comportan este par de variables:El valor de \\(\\hat \\beta_1\\), el coeficiente de la variable ss, es de 0.3949, lo que significa que cuando la proporción de población con acceso servicios de salud aumenta en 1 punto porcentual (recuerda que así lo estamos midiendo), el número de casos positivos por cada 1 mil habitantes aumenta 0.39, en promedio. Dicho en otras palabras, el parámetro estimado de la pendiente, \\(\\hat \\beta_1\\), nos indica cómo la media de los casos positivos, \\(y\\), es afectada por un cambio en \\(x\\), el porcentaje de personas con acceso servicios de salud. En este caso, el coeficiente estimado \\(\\hat \\beta_0\\) carece de interpretación razonable. Así, el modelo estimado, usando los valores de los coeficientes obtenidos es:\\[\\hat y=\\hat \\beta_0+\\hat\\beta_1x\\]\nEs decir,\\[\\hat {poshab}=-19.8622+0.3949ss\\]Adicionalmente, podemos construir un diagrama de dispersión entre las variables pos_hab y ss añadiendo un ajuste líneal, es decir, la recta de nuestro modelo regresión:Como puedes observar, entre ambas variables existe una relación positiva que puedes identificar por la línea con pendiente positiva que representa la recta de regresión que hemos obtenido, cuya pendiente es igual al valor de \\(\\hat \\beta_1\\), es decir, 0.3949. La distancia vertical entre cada punto y la recta de ajuste de la regresión es el error del modelo, es decir \\(u_i\\)Un elemento de interés para el análisis en la regresión lineal estimada son los errores observados, \\(u_i\\), puesto que los supuestos sobre los que se basa el método de mínimos cuadrados ordinarios son, en buena medida, sobre dichos errores. Para lograr comprender mejor qué son los errores y su importancia analítica, conviene imaginar nuestro modelo como “Respuesta = Predicción + Error,” es decir, \\(y=\\hat y+u\\). De esta manera podemos expresar el error como:\\[u_i=y_i-\\hat {y_i}\\]\nLos errores observados, \\(u_i\\), y los valores estimados, \\(\\hat y_i\\), de nuestro modelo son almacenados en el elemento residuals y fitted.values, respectivamente, dentro de la lista lm. Da clic en la lente de aumento al final de la fila que contiene el objeto de tipo lm en la sección de ambiente en tu entorno de trabajo para observarlos. Para poder analizar mejor los errores del modelo, vamos crear un nuevo objeto que contenga, además de nuestras variables de interés, pos_hab y ss, la clave municipal, cvemun, los errores observados y los valores ajustados o predichos. Para ello, primero generamos una tabla con la función data.frame() que contendrán las variables mencionadas (aquí, los dobles corchetes, [[]], son utilizados para extraer los elementos solicitados):Luego, creamos una lista que contiene las etiquetas de los nombres deseados para cada variable:Y, finalmente, asignamos los nombres deseados la tabla creadaEjercicio\nCon los datos de la tabla anterior:¿Cómo esperarías que luzca un diagrama de dispersión entre los valores observados, pos_hab, y los valores ajustados, \\(\\hat {poshab}\\)? Elabora dicho diagrama.¿Cómo esperarías que luzca un diagrama de dispersión entre los valores observados, pos_hab, y los valores ajustados, \\(\\hat {poshab}\\)? Elabora dicho diagrama.Grafica los errores, primero respecto un índice del 1 al 76 y luego respecto los valores ajustados, ¿puedes observar algún patrón en ellos?Grafica los errores, primero respecto un índice del 1 al 76 y luego respecto los valores ajustados, ¿puedes observar algún patrón en ellos?El supuesto de linealidad de nuestro modelo puede ser verificado partir de una exploración visual de un diagrama de dispersión entre los errores del modelo y la variable explicativa, nuestra \\(x\\), es decir, el porcentaje de población con acceso servicios de salud. Si nuestro modelo es verdaderamente un modelo lineal, en la gráfica debería mostrarse patrón alguno. Veamos:Claramente hay un patrón, por lo que probablemente un modelo lineal como el propuesto sería la mejor opción. ¿Recuerdas cómo luce el diagrama de dispersión entre las dos variables de nuestro modelo?EjercicioElabora un diagrama de dispersión entre pos_hab y ss y añade un ajuste lineal.Entonces, parece que el modelo lineal propuesto capta con precisión la relación entre este par de variables, por lo que un modelo lineal podría ser una mejor opción; sin embargo, esta alternativa de modelación es objeto de estas notas, pero en el capítulo 14 de la obra de Gujarati y Porter (2011) puedes revisar con detalle una ruta para solventar esta situación.Otro de los supuestos sobre los que se basa la estimación de modelos con MCO es que los errores deberían tener una distribución normal, lo que se cumplirá cuando haya observaciones atípicas, los llamados outliers. Para verificar esto visualmente, es posible recurrir un histograma de los errores, o bien, una gráfica de normalidad:¿Notas los valores extremos en el histograma anterior? Una gráfica cuantil-cuantil o gráfica QQ, contrasta la distribución de una variable con una distribución teórica: si la distribución de la variable fuera idéntica la distribución teórica, los puntos se alinearían sobre la línea de 45°. En este caso, la distribución teórica probar es una distribución normal:EjercicioConsulta la ayuda de las funciones geom_qq() y stat_qq_line(), ¿qué otras distribuciones es posible verificar y con qué argumentos?Finalmente, el supuesto de varianza constante de los errores también puede explorarse visualmente través de una gráfica donde se debería lograr ver variabilidad constante de los errores alrededor de la recta de ajuste: este es el denominado supuesto de homoscedasticidad. Exploremos si los errores de nuestro modelo cumplen o con este supuesto través de una gráfica de dispersión:Patrones visuales de “tipo ventilador” o “embudo” deberían ser observados cuando los errores son homoscedásticos, lo que es el caso. Los ejercicios gráficos hechos aquí con los errores para verificar si se cumplen o los supuestos de un modelo de MCO son, desde luego, la única ruta para verificar los supuestos, analizar si una regresión cumple con los supuestos necesarios requiere de práctica. En esta página encontrarás una serie de ejemplos de cuando una regresión cumple o con los supuestos enlistados antes y cómo lucen las gráficas mencionadas en cada caso. Sin embargo, como recordarás, existen pruebas formales que te permitirán determinar si el modelo cumple o con los supuestos del caso.Ejercicio¿Cuál es el nombre de la prueba más popular para evaluar normalidad y cómo se interpreta?¿Cuál es el nombre de la prueba más popular para evaluar normalidad y cómo se interpreta?Menciona una prueba para evaluar homoscedasticidad y cómo es interpretada.Menciona una prueba para evaluar homoscedasticidad y cómo es interpretada.El \\(R^2\\) o coeficiente de determinación es una medida de bondad de ajuste, es decir, un número de nos indica qué tanto nuestro modelo es capaz de explicar la variable de interés. El \\(R^2\\) es la proporción de la variabilidad de \\(y\\) que es explicada por \\(x\\). Valores más cercanos 1 significan que una mayor parte de la variabilidad de \\(y\\) es explicada por \\(x\\). Solicita de nuevo un resumen del objeto modelo_simple e identifica la sección Multiple R-squared:En nuestro caso, hemos obtenido un \\(R^2\\) de 0.2252, lo que puede ser interpretado de la siguiente manera: poco más de la quinta parte de la variabilidad de los casos positivos por cada 1 mil habitantes es explicada por el porcentaje de población con acceso servicios de salud.través de nuestro modelo estimado (también llamado ajustado) es posible hacer predicciones de \\(y\\), es decir, de los casos positivos por cada 1 mil habitantes, siempre que los valores de la variable predictora estén dentro del rango original.Ejercicio¿Cuál es el rango de la variable predictora, es decir de \\(x\\)?Hagamos una predicción de los casos positivos cuando \\(x\\) toma el valor de 60%:\\[\\hat {poshab}=-19.8622+0.3949(60)\\]Con independencia de la consistencia y validez de nuestro modelo, situación que de momento estamos evaluando, ¿cuál es el valor promedio de los casos positivos de COVID19 por cada 1 mil habitantes cuando el porcentaje de población con acceso servicios de salud es de 60%? Bastará resolver la expresión anterior para responder la pregunta: 3.83 casos por cada 1 mil habitantes, aproximadamente; es decir, si un municipio tiene un porcentaje de población con acceso servicios de salud de 60%, nuestro modelo estima un total de 3.83 casos positivos de COVID19.Alternativamente, en R contamos con la función predict() para llevar cabo, justamente, predicciones. La función requiere de dos argumentos forzosos: ) el modelo usado para la predicción (argumento object=) y ii) los valores de la regresión (argumento newdata=). Así:En este caso, estamos indicando los nuevos datos de forma manual para la variable de interés, ss.Uno de los aspectos más importantes del análisis de regresión es la interpretación de los coeficientes obtenidos. Será de particular interés para nosotros conocer, de cada coeficiente: ) el sentido de su asociación con la variable dependiente, es decir, el signo del coeficiente; ii) la magnitud de la asociación lineal, qué tan grande es dicho coeficiente en términos de las unidades de la variable y iii) su significancia estadística, es decir, si hay evidencia de que el verdadero valor del estimador se corresponde con el estimado.Los dos primeros elementos, sentido y magnitud, ya fueron comentados con anterioridad y pueden ser conocidos llamando al objeto que contiene los resultados de nuestro modelo:R despliega en tu consola el valor de los dos coeficientes estimados, en este caso \\(\\hat \\beta_0\\) es negativo y \\(\\hat \\beta_1\\) es positivo. El punto iii consiste en verificar si, en términos estadísticos, nuestros coeficientes estimados son iguales o determinado valor, concretamente, buscamos llevar cabo una prueba de hipótesis sobre dichos coeficientes.La prueba de hipótesis se refiere la evaluación de si el valor de un estadístico, un valor observado u obtenido través de un cálculo, se acerca lo suficiente un valor hipotético. Por ejemplo, algún estudio puede sugerir que la relación entre la educación formal y el ingreso personal es de 0.25, es decir, que medida que crece un año la educación formal el ingreso personal se incrementa en 0.25 unidades, este valor llamémoslo \\(\\nu\\). Quiza en un estudio más reciente se encontró que \\(\\nu\\) es igual 0.19, ¿está este último valor lo suficientemente cerca del 0.25 original para decir que se rechaza el planteamiento original?Siguiendo Gujarati y Damodar (2011: 113), “la hipótesis planteada se conoce como hipótesis nula, y se denota con el símbolo \\(Ho\\). La hipótesis nula suele probarse frente una hipótesis alternativa (también conocida como hipótesis mantenida) denotada con \\(Ha\\).” Así, con relación al ajemplo anterior, la \\(Ho: \\nu=0.19\\) y la \\(Ha: \\nu \\neq 0.19\\).Sinteticemos el procedimiento de prueba de hipótesis desde una perspectiva netamente práctica y simplificada, desde el ángulo de la prueba de significancia:Definir el juego de hipótesis verificar, la hipótesis nula (Ho) y la hipótesis alternativa (Ha).Definir un nivel de significancia, conocido como \\(\\alpha\\), valor con base en el cual tomarás una decisión en torno las hipótesis propuestas. Además, recuerda que (1-\\(\\alpha\\)) define el nivel de confianza con el que la decisión sobre el juego de hipótesis verificar es tomada.Comparar el valor p asociado cada coeficiente estimado con el nivel de \\(\\alpha\\) elegido y decidir con arreglo los siguientes criterios:\n. Si el valor-p < \\(\\alpha\\): se rechaza Ho en favor de Ha.\nb. Si el valor-p > \\(\\alpha\\): se rechaza Ho.El juego de hipótesis sobre la significancia individual de los estimadores calculados:\\[H_o:\\hat\\beta_i=0 \\\\\nH_a:\\hat\\beta_i\\=0 \\]Ahora bien, \\(\\alpha\\) usualmente toma valores de 0.1, 0.05 y 0.01, lo que equivale niveles de confianza de 0.9, 0.95 y 0.99, es decir, de 90%, 95% y 99%, respectivamente; en tanto, el valor-p para tomar la decisión lo tomamos del resumen del objeto modelo_simple. Para observar los valores-p de cada uno de los coeficientes estimados solicitamos un resumen del objeto modelo_simple con la función summary():Observa con cuidado los resultados que ahora se presentan como una tabla en su consola. Identifica las columnas estimate y Pr(>|t|): en esta última aparece el valor-p y es el elemento con base en el cual tomaremos la decisión sobre la significancia estadística de los coeficientes obtenidos.Veamos los valores de los coeficientes estimados: el intercepto, \\(\\hat \\beta_0\\) es igual -19.8 y el coeficiente vinculado ss, \\(\\hat \\beta_1\\), tiene un valor de 0.394. Ahora bien, recuerda el juego de hipótesis verificar: \\(H_o:\\hat\\beta_i=0\\) y \\(H_a:\\hat\\beta_i\\=0\\) (paso 1), elijamos un nivel de significancia, \\(\\alpha\\), de 0.05 (paso 2) y comparemos el valor p asociado cada coeficiente con \\(\\alpha\\) (paso 3). El siguiente cuadro sintentiza lo anterior:Así, decimos que para el caso del coeficiente estimado de ss hay evidencia suficiente para afirmar que su verdadero valor es diferente de cero, por lo que decimos que es estadísticamente significativo, o lo que es lo mismo, hay evidencia suficiente con un nivel de confianza de 95% para sostener que entre el número de casos positivos por cada 1 mil habitantes y el porcentaje de población con acceso afiliación servicios de salud presentan una asociación lineal.","code":"\nlibrary(readxl)\ncovid_zmvm <- read_excel(path=\"base de datos\\\\covid_zmvm.xlsx\")\nmodelo_simple <- stats::lm (formula=pos_hab ~ ss, data = covid_zmvm)\nmodelo_simple## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Coefficients:\n## (Intercept)           ss  \n##    -19.8622       0.3949\nlibrary(tidyverse)\n\ncovid_zmvm %>% \n  ggplot2::ggplot()+geom_dotplot(aes(pos_hab))\ncovid_zmvm %>% \n  ggplot2::ggplot()+geom_dotplot(aes(poss))\ncovid_zmvm %>% ggplot()+\n  geom_point(aes(x=ss,y=pos_hab))+\n  geom_smooth(aes(x=ss,y=pos_hab),method = \"lm\")+\n  labs(x=\"Población con acceso a servicios de salud (%)\", y=\"Casos positivos por COVID19\")## `geom_smooth()` using formula = 'y ~ x'\ntabla <- data.frame(covid_zmvm$cvemun, covid_zmvm$ss, covid_zmvm$pos_hab, as.data.frame(modelo_simple[[\"fitted.values\"]]),\n                    as.data.frame(modelo_simple[[\"residuals\"]]))\nnombres <- c(\"cvemun\",\"ss\", \"pos_hab\",\"ajustados\", \"errores\") \ncolnames(tabla)<-nombres\ntabla %>% \n  ggplot()+\n  geom_point(aes(x=pos_hab,y=errores))\ntabla %>%\n  ggplot()+\n  geom_histogram(aes(errores))## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\ntabla %>% \n  ggplot()+\n  geom_qq(aes(sample=errores))+\n  geom_qq_line(aes(sample=errores))\ntabla %>%\n  ggplot()+\n  geom_point(aes(ajustados,errores))\nsummary(modelo_simple)## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.4391 -2.7744 -0.6709  1.5324 14.9969 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -19.86224    5.80867  -3.419  0.00102 ** \n## ss            0.39490    0.08516   4.637 1.49e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.572 on 74 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2147 \n## F-statistic:  21.5 on 1 and 74 DF,  p-value: 1.488e-05\nstats::predict(object = modelo_simple, newdata=data.frame(ss=60))##        1 \n## 3.831836\nmodelo_simple## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Coefficients:\n## (Intercept)           ss  \n##    -19.8622       0.3949\nbase::summary(modelo_simple)## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.4391 -2.7744 -0.6709  1.5324 14.9969 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -19.86224    5.80867  -3.419  0.00102 ** \n## ss            0.39490    0.08516   4.637 1.49e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.572 on 74 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2147 \n## F-statistic:  21.5 on 1 and 74 DF,  p-value: 1.488e-05"},{"path":"modelos-de-regresión-lineal.html","id":"una-exploración-visual-de-los-errores","chapter":"4 Modelos de regresión lineal","heading":"4.5.1 Una exploración visual de los errores","text":"Un elemento de interés para el análisis en la regresión lineal estimada son los errores observados, \\(u_i\\), puesto que los supuestos sobre los que se basa el método de mínimos cuadrados ordinarios son, en buena medida, sobre dichos errores. Para lograr comprender mejor qué son los errores y su importancia analítica, conviene imaginar nuestro modelo como “Respuesta = Predicción + Error,” es decir, \\(y=\\hat y+u\\). De esta manera podemos expresar el error como:\\[u_i=y_i-\\hat {y_i}\\]\nLos errores observados, \\(u_i\\), y los valores estimados, \\(\\hat y_i\\), de nuestro modelo son almacenados en el elemento residuals y fitted.values, respectivamente, dentro de la lista lm. Da clic en la lente de aumento al final de la fila que contiene el objeto de tipo lm en la sección de ambiente en tu entorno de trabajo para observarlos. Para poder analizar mejor los errores del modelo, vamos crear un nuevo objeto que contenga, además de nuestras variables de interés, pos_hab y ss, la clave municipal, cvemun, los errores observados y los valores ajustados o predichos. Para ello, primero generamos una tabla con la función data.frame() que contendrán las variables mencionadas (aquí, los dobles corchetes, [[]], son utilizados para extraer los elementos solicitados):Luego, creamos una lista que contiene las etiquetas de los nombres deseados para cada variable:Y, finalmente, asignamos los nombres deseados la tabla creadaEjercicio\nCon los datos de la tabla anterior:¿Cómo esperarías que luzca un diagrama de dispersión entre los valores observados, pos_hab, y los valores ajustados, \\(\\hat {poshab}\\)? Elabora dicho diagrama.¿Cómo esperarías que luzca un diagrama de dispersión entre los valores observados, pos_hab, y los valores ajustados, \\(\\hat {poshab}\\)? Elabora dicho diagrama.Grafica los errores, primero respecto un índice del 1 al 76 y luego respecto los valores ajustados, ¿puedes observar algún patrón en ellos?Grafica los errores, primero respecto un índice del 1 al 76 y luego respecto los valores ajustados, ¿puedes observar algún patrón en ellos?El supuesto de linealidad de nuestro modelo puede ser verificado partir de una exploración visual de un diagrama de dispersión entre los errores del modelo y la variable explicativa, nuestra \\(x\\), es decir, el porcentaje de población con acceso servicios de salud. Si nuestro modelo es verdaderamente un modelo lineal, en la gráfica debería mostrarse patrón alguno. Veamos:Claramente hay un patrón, por lo que probablemente un modelo lineal como el propuesto sería la mejor opción. ¿Recuerdas cómo luce el diagrama de dispersión entre las dos variables de nuestro modelo?EjercicioElabora un diagrama de dispersión entre pos_hab y ss y añade un ajuste lineal.Entonces, parece que el modelo lineal propuesto capta con precisión la relación entre este par de variables, por lo que un modelo lineal podría ser una mejor opción; sin embargo, esta alternativa de modelación es objeto de estas notas, pero en el capítulo 14 de la obra de Gujarati y Porter (2011) puedes revisar con detalle una ruta para solventar esta situación.Otro de los supuestos sobre los que se basa la estimación de modelos con MCO es que los errores deberían tener una distribución normal, lo que se cumplirá cuando haya observaciones atípicas, los llamados outliers. Para verificar esto visualmente, es posible recurrir un histograma de los errores, o bien, una gráfica de normalidad:¿Notas los valores extremos en el histograma anterior? Una gráfica cuantil-cuantil o gráfica QQ, contrasta la distribución de una variable con una distribución teórica: si la distribución de la variable fuera idéntica la distribución teórica, los puntos se alinearían sobre la línea de 45°. En este caso, la distribución teórica probar es una distribución normal:EjercicioConsulta la ayuda de las funciones geom_qq() y stat_qq_line(), ¿qué otras distribuciones es posible verificar y con qué argumentos?Finalmente, el supuesto de varianza constante de los errores también puede explorarse visualmente través de una gráfica donde se debería lograr ver variabilidad constante de los errores alrededor de la recta de ajuste: este es el denominado supuesto de homoscedasticidad. Exploremos si los errores de nuestro modelo cumplen o con este supuesto través de una gráfica de dispersión:Patrones visuales de “tipo ventilador” o “embudo” deberían ser observados cuando los errores son homoscedásticos, lo que es el caso. Los ejercicios gráficos hechos aquí con los errores para verificar si se cumplen o los supuestos de un modelo de MCO son, desde luego, la única ruta para verificar los supuestos, analizar si una regresión cumple con los supuestos necesarios requiere de práctica. En esta página encontrarás una serie de ejemplos de cuando una regresión cumple o con los supuestos enlistados antes y cómo lucen las gráficas mencionadas en cada caso. Sin embargo, como recordarás, existen pruebas formales que te permitirán determinar si el modelo cumple o con los supuestos del caso.Ejercicio¿Cuál es el nombre de la prueba más popular para evaluar normalidad y cómo se interpreta?¿Cuál es el nombre de la prueba más popular para evaluar normalidad y cómo se interpreta?Menciona una prueba para evaluar homoscedasticidad y cómo es interpretada.Menciona una prueba para evaluar homoscedasticidad y cómo es interpretada.","code":"\ntabla <- data.frame(covid_zmvm$cvemun, covid_zmvm$ss, covid_zmvm$pos_hab, as.data.frame(modelo_simple[[\"fitted.values\"]]),\n                    as.data.frame(modelo_simple[[\"residuals\"]]))\nnombres <- c(\"cvemun\",\"ss\", \"pos_hab\",\"ajustados\", \"errores\") \ncolnames(tabla)<-nombres\ntabla %>% \n  ggplot()+\n  geom_point(aes(x=pos_hab,y=errores))\ntabla %>%\n  ggplot()+\n  geom_histogram(aes(errores))## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\ntabla %>% \n  ggplot()+\n  geom_qq(aes(sample=errores))+\n  geom_qq_line(aes(sample=errores))\ntabla %>%\n  ggplot()+\n  geom_point(aes(ajustados,errores))"},{"path":"modelos-de-regresión-lineal.html","id":"coeficiente-de-determinación","chapter":"4 Modelos de regresión lineal","heading":"4.5.2 Coeficiente de determinación","text":"El \\(R^2\\) o coeficiente de determinación es una medida de bondad de ajuste, es decir, un número de nos indica qué tanto nuestro modelo es capaz de explicar la variable de interés. El \\(R^2\\) es la proporción de la variabilidad de \\(y\\) que es explicada por \\(x\\). Valores más cercanos 1 significan que una mayor parte de la variabilidad de \\(y\\) es explicada por \\(x\\). Solicita de nuevo un resumen del objeto modelo_simple e identifica la sección Multiple R-squared:En nuestro caso, hemos obtenido un \\(R^2\\) de 0.2252, lo que puede ser interpretado de la siguiente manera: poco más de la quinta parte de la variabilidad de los casos positivos por cada 1 mil habitantes es explicada por el porcentaje de población con acceso servicios de salud.","code":"\nsummary(modelo_simple)## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.4391 -2.7744 -0.6709  1.5324 14.9969 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -19.86224    5.80867  -3.419  0.00102 ** \n## ss            0.39490    0.08516   4.637 1.49e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.572 on 74 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2147 \n## F-statistic:  21.5 on 1 and 74 DF,  p-value: 1.488e-05"},{"path":"modelos-de-regresión-lineal.html","id":"una-aproximación-a-la-predicción","chapter":"4 Modelos de regresión lineal","heading":"4.5.3 Una aproximación a la predicción","text":"través de nuestro modelo estimado (también llamado ajustado) es posible hacer predicciones de \\(y\\), es decir, de los casos positivos por cada 1 mil habitantes, siempre que los valores de la variable predictora estén dentro del rango original.Ejercicio¿Cuál es el rango de la variable predictora, es decir de \\(x\\)?Hagamos una predicción de los casos positivos cuando \\(x\\) toma el valor de 60%:\\[\\hat {poshab}=-19.8622+0.3949(60)\\]Con independencia de la consistencia y validez de nuestro modelo, situación que de momento estamos evaluando, ¿cuál es el valor promedio de los casos positivos de COVID19 por cada 1 mil habitantes cuando el porcentaje de población con acceso servicios de salud es de 60%? Bastará resolver la expresión anterior para responder la pregunta: 3.83 casos por cada 1 mil habitantes, aproximadamente; es decir, si un municipio tiene un porcentaje de población con acceso servicios de salud de 60%, nuestro modelo estima un total de 3.83 casos positivos de COVID19.Alternativamente, en R contamos con la función predict() para llevar cabo, justamente, predicciones. La función requiere de dos argumentos forzosos: ) el modelo usado para la predicción (argumento object=) y ii) los valores de la regresión (argumento newdata=). Así:En este caso, estamos indicando los nuevos datos de forma manual para la variable de interés, ss.","code":"\nstats::predict(object = modelo_simple, newdata=data.frame(ss=60))##        1 \n## 3.831836"},{"path":"modelos-de-regresión-lineal.html","id":"significancia-individual-de-los-coeficientes-obtenidos","chapter":"4 Modelos de regresión lineal","heading":"4.5.4 Significancia individual de los coeficientes obtenidos","text":"Uno de los aspectos más importantes del análisis de regresión es la interpretación de los coeficientes obtenidos. Será de particular interés para nosotros conocer, de cada coeficiente: ) el sentido de su asociación con la variable dependiente, es decir, el signo del coeficiente; ii) la magnitud de la asociación lineal, qué tan grande es dicho coeficiente en términos de las unidades de la variable y iii) su significancia estadística, es decir, si hay evidencia de que el verdadero valor del estimador se corresponde con el estimado.Los dos primeros elementos, sentido y magnitud, ya fueron comentados con anterioridad y pueden ser conocidos llamando al objeto que contiene los resultados de nuestro modelo:R despliega en tu consola el valor de los dos coeficientes estimados, en este caso \\(\\hat \\beta_0\\) es negativo y \\(\\hat \\beta_1\\) es positivo. El punto iii consiste en verificar si, en términos estadísticos, nuestros coeficientes estimados son iguales o determinado valor, concretamente, buscamos llevar cabo una prueba de hipótesis sobre dichos coeficientes.La prueba de hipótesis se refiere la evaluación de si el valor de un estadístico, un valor observado u obtenido través de un cálculo, se acerca lo suficiente un valor hipotético. Por ejemplo, algún estudio puede sugerir que la relación entre la educación formal y el ingreso personal es de 0.25, es decir, que medida que crece un año la educación formal el ingreso personal se incrementa en 0.25 unidades, este valor llamémoslo \\(\\nu\\). Quiza en un estudio más reciente se encontró que \\(\\nu\\) es igual 0.19, ¿está este último valor lo suficientemente cerca del 0.25 original para decir que se rechaza el planteamiento original?Siguiendo Gujarati y Damodar (2011: 113), “la hipótesis planteada se conoce como hipótesis nula, y se denota con el símbolo \\(Ho\\). La hipótesis nula suele probarse frente una hipótesis alternativa (también conocida como hipótesis mantenida) denotada con \\(Ha\\).” Así, con relación al ajemplo anterior, la \\(Ho: \\nu=0.19\\) y la \\(Ha: \\nu \\neq 0.19\\).Sinteticemos el procedimiento de prueba de hipótesis desde una perspectiva netamente práctica y simplificada, desde el ángulo de la prueba de significancia:Definir el juego de hipótesis verificar, la hipótesis nula (Ho) y la hipótesis alternativa (Ha).Definir un nivel de significancia, conocido como \\(\\alpha\\), valor con base en el cual tomarás una decisión en torno las hipótesis propuestas. Además, recuerda que (1-\\(\\alpha\\)) define el nivel de confianza con el que la decisión sobre el juego de hipótesis verificar es tomada.Comparar el valor p asociado cada coeficiente estimado con el nivel de \\(\\alpha\\) elegido y decidir con arreglo los siguientes criterios:\n. Si el valor-p < \\(\\alpha\\): se rechaza Ho en favor de Ha.\nb. Si el valor-p > \\(\\alpha\\): se rechaza Ho.El juego de hipótesis sobre la significancia individual de los estimadores calculados:\\[H_o:\\hat\\beta_i=0 \\\\\nH_a:\\hat\\beta_i\\=0 \\]Ahora bien, \\(\\alpha\\) usualmente toma valores de 0.1, 0.05 y 0.01, lo que equivale niveles de confianza de 0.9, 0.95 y 0.99, es decir, de 90%, 95% y 99%, respectivamente; en tanto, el valor-p para tomar la decisión lo tomamos del resumen del objeto modelo_simple. Para observar los valores-p de cada uno de los coeficientes estimados solicitamos un resumen del objeto modelo_simple con la función summary():Observa con cuidado los resultados que ahora se presentan como una tabla en su consola. Identifica las columnas estimate y Pr(>|t|): en esta última aparece el valor-p y es el elemento con base en el cual tomaremos la decisión sobre la significancia estadística de los coeficientes obtenidos.Veamos los valores de los coeficientes estimados: el intercepto, \\(\\hat \\beta_0\\) es igual -19.8 y el coeficiente vinculado ss, \\(\\hat \\beta_1\\), tiene un valor de 0.394. Ahora bien, recuerda el juego de hipótesis verificar: \\(H_o:\\hat\\beta_i=0\\) y \\(H_a:\\hat\\beta_i\\=0\\) (paso 1), elijamos un nivel de significancia, \\(\\alpha\\), de 0.05 (paso 2) y comparemos el valor p asociado cada coeficiente con \\(\\alpha\\) (paso 3). El siguiente cuadro sintentiza lo anterior:Así, decimos que para el caso del coeficiente estimado de ss hay evidencia suficiente para afirmar que su verdadero valor es diferente de cero, por lo que decimos que es estadísticamente significativo, o lo que es lo mismo, hay evidencia suficiente con un nivel de confianza de 95% para sostener que entre el número de casos positivos por cada 1 mil habitantes y el porcentaje de población con acceso afiliación servicios de salud presentan una asociación lineal.","code":"\nmodelo_simple## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Coefficients:\n## (Intercept)           ss  \n##    -19.8622       0.3949\nbase::summary(modelo_simple)## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.4391 -2.7744 -0.6709  1.5324 14.9969 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -19.86224    5.80867  -3.419  0.00102 ** \n## ss            0.39490    0.08516   4.637 1.49e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.572 on 74 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2147 \n## F-statistic:  21.5 on 1 and 74 DF,  p-value: 1.488e-05"},{"path":"modelos-de-regresión-lineal.html","id":"algunos-elementos-de-cuidado-con-las-regresiones-lineales","chapter":"4 Modelos de regresión lineal","heading":"4.6 Algunos elementos de cuidado con las regresiones lineales","text":"Antes de concluir esta sección, es importante tener en mente los siguientes puntos cuando usemos regresiones:Los modelos de regresión son útiles para llevar cabo intrapolación, es decir, para obtener valores en el rango de nuestras variables explicativas. obstante, deben ser usados con cuidado cuando lo que se pretende es extrapolar datos, es decir, conocer los valores de \\(y\\) que están en el rango original de \\(x\\).Como el ajuste de mínimos cuadrados depende en buena medida de los valores de \\(x\\) y con este método cada punto \\(x\\) tiene la misma ponderación, la pendiente de la regresión está mayormente influenciada por los valores más extremos de \\(x\\). Cuando se tienen valores inusuales se podría proceder eliminarlos, o bien, recurrir técnicas diferentes los mínimos cuadrados ordinarios que sean menos sensibles estos puntos.Que la regresión tuviera como resultado que dos variables están asociadas, significa que entre ellas haya una relación causal, tal como se indicó al inicio de este capítulo. Causalidad implica asociación, pero lo opuesto es necesariamente cierto.","code":""},{"path":"modelos-de-regresión-lineal.html","id":"regresión-lineal-múltiple-una-propuesta-de-modelo","chapter":"4 Modelos de regresión lineal","heading":"4.7 Regresión lineal múltiple: una propuesta de modelo","text":"La regresión lineal múltiple es, en esencia, una extensión lógica de todos los elementos descritos antes: “Un modelo de regresión que involucra más de una variable regresora se llama modelo de regresión múltiple” (Montgomery, Peck., Vining. 2012, 81:67). Así, por ejemplo, un modelo con dos regresoras, \\(x_1\\) y \\(x_2\\) luciría como:\\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+u\\]Esta expresión que incluye tres variables, dos independientes y una dependiente, aún es posible representarse en espacio tridimensional con los ejes \\(y\\), \\(x_1\\) y \\(x_2\\). El parámetro \\(\\beta_0\\) es el intercepto del plano de la regresión que, cuando el origen está incluido en el rango de \\(x_1\\) y \\(x_2\\), \\(\\beta_0\\) indica la media de \\(y\\) cuando \\(x_1=x_2=0\\). En tanto, \\(\\beta_1\\) es el cambio esperado \\(y\\) cuando \\(x_1\\) varia en una unidad, siempre que \\(x_2\\) permanezca constante, análogamente para \\(\\beta_2\\). ¿Qué variables de nuestra base incluirías en un modelo que incluya dos regresoras?Ejercicio¿Recuerdas cómo construir una matriz de diagramas de dispersión través del paquete GGally? Explora el conjunto de variables de la base covid_zmvm y construye una matriz de diagramas de dispersión solo con 4 o 5 variables que presenten el mayor nivel de correlación tanto con los casos positivos como con las defunciones por cada 1 mil habitantes, pos_hab y def_hab.Un modelo con buen nivel de ajuste debería procurar que las variables explicativas propuestas presenten una alta correlación con la variable explicar. Como podrás haberte dado cuenta, las variables que presentan el mayor nivel de asociación lineal negativa son: ppob_basi, pocom, occu, sbasc, ppob_5_o_m. En tanto, las variables que muestran el mayor nivel de asociación lineal positiva son: poss, grad_m, ppob_sup, grad, grad_h, tmss y rmss.Otro principio recomendable la hora de proponer un modelo, amén de su consistencia teórica, es su simpleza: ¿qué relevancia tendrá incluir en un modelo el grado promedio de escolaridad total, el de hombres y el de mujeres por separado? Bastaría incluir solo una de ellas. Por otro lado, ¿qué tan oportuno sería incluir simultáneamente variables que buscan recoger el mismo aspecto de la realidad? Esto es así porque tanto las variables grad, ppob_basi y sbasc refieren el nivel de educación formal de la población, así, bastaría usar alternativamente alguna de ellas, algo parecido pasa con occu y ppob_5_o_m que expresan las condiciones de vivienda y habitación de las personas. En tanto, parece que de los grandes sectores: industria, comercio y servicios, las variables de éste último son las que muestran un mayor nivel de asociación lineal: poss, tmss y rmss, ¿será relevante incluir todas en el modelo?Con base en lo dicho antes, propongamos un primer modelo, lo más sencillo posible:\\[poshab_i=\\beta_0+\\beta_1grad_i+\\beta_2occu_i+\\beta_3poss_i+\\beta_4pocom_i+\\epsilon_i\\]\nDe nuevo, través de la función lm() estimaremos este modelo y al resultado lo guardaremos en un nuevo objeto llamado modelo_multiEjercicio¿Qué pasa con las variables propuestas en este modelo? ¿Cuáles resultaron estadísticamente significativas y por qué?¿Qué pasa con las variables propuestas en este modelo? ¿Cuáles resultaron estadísticamente significativas y por qué?Aquí hemos expuesto una versión simplificada en extremo de cómo tomar decisiones través de las pruebas de hipótesis, puedes remitirte cualquier libro de econometría y responder, ¿de qué modo la prueba t (t value, penúltima columna del cuadro de resumen) nos permitiría llegar las mismas conclusiones?Aquí hemos expuesto una versión simplificada en extremo de cómo tomar decisiones través de las pruebas de hipótesis, puedes remitirte cualquier libro de econometría y responder, ¿de qué modo la prueba t (t value, penúltima columna del cuadro de resumen) nos permitiría llegar las mismas conclusiones?Seguramente notaste que cuando se llamó el objeto modelo_multi tu consola aparecieron otros tantos elementos ,además de los coeficientes, entre ellos uno llamado F-statistic , el estadístico F. En este contexto es utilizado para evaluar si la particular combinación de variables propuesta contribuye o explicar la variable de interés. En términos concretos, la prueba F y el valor p asociado ella, permiten evaluar el siguiente juego de hipótesis:\\[\n\\begin{aligned}\nHo&: \\hat\\beta_1=\\hat\\beta_2=...=\\hat\\beta_i=0 \\\\\nHa&: al.menos.una. \\hat\\beta_i\\=0 \\\\\n\\end{aligned}\n\\]es decir, al menos uno de los coeficientes estimados, en su particular combinación, es diferente de cero. Así, con un valor p de casi cero (2.112e-09), resulta claro que se rechaza Ho en favor de Ha, es decir, que al menos uno de los coeficientes estimados es diferente de 0 en el modelo propuesto y que esta particular combinación de variables explicativas tiene algo interesante para decirnos.","code":"\nmodelo_multi <- stats::lm (formula=pos_hab ~ grad+occu+poss+pocom, data = covid_zmvm)\nsummary(modelo_multi)## \n## Call:\n## stats::lm(formula = pos_hab ~ grad + occu + poss + pocom, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.0017 -2.4621 -0.7526  1.2428 16.6659 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  1.43576   15.07149   0.095 0.924374    \n## grad         0.09555    1.01112   0.094 0.924981    \n## occu        -4.25136    7.65251  -0.556 0.580264    \n## poss        21.80677    5.91896   3.684 0.000445 ***\n## pocom        0.07430    6.71860   0.011 0.991208    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.843 on 71 degrees of freedom\n## Multiple R-squared:  0.4748, Adjusted R-squared:  0.4452 \n## F-statistic: 16.04 on 4 and 71 DF,  p-value: 2.112e-09"},{"path":"modelos-de-regresión-lineal.html","id":"significancia-conjunta-de-los-coeficientes-calculados","chapter":"4 Modelos de regresión lineal","heading":"4.7.1 Significancia conjunta de los coeficientes calculados","text":"Seguramente notaste que cuando se llamó el objeto modelo_multi tu consola aparecieron otros tantos elementos ,además de los coeficientes, entre ellos uno llamado F-statistic , el estadístico F. En este contexto es utilizado para evaluar si la particular combinación de variables propuesta contribuye o explicar la variable de interés. En términos concretos, la prueba F y el valor p asociado ella, permiten evaluar el siguiente juego de hipótesis:\\[\n\\begin{aligned}\nHo&: \\hat\\beta_1=\\hat\\beta_2=...=\\hat\\beta_i=0 \\\\\nHa&: al.menos.una. \\hat\\beta_i\\=0 \\\\\n\\end{aligned}\n\\]es decir, al menos uno de los coeficientes estimados, en su particular combinación, es diferente de cero. Así, con un valor p de casi cero (2.112e-09), resulta claro que se rechaza Ho en favor de Ha, es decir, que al menos uno de los coeficientes estimados es diferente de 0 en el modelo propuesto y que esta particular combinación de variables explicativas tiene algo interesante para decirnos.","code":""},{"path":"modelos-de-regresión-lineal.html","id":"alcance-de-la-regresión-y-en-análisis-causal","chapter":"4 Modelos de regresión lineal","heading":"4.8 Alcance de la regresión y en análisis causal","text":"La primera parte de este capítulo llamó la atención sobre el llamado análisis causal, por lo que dedicaremos unas cuantas palabras para cerrar la discusión con la que comenzamos. La realidad social es compleja y si bien se han extendido las técnicas experimentales, incluso en disciplinas como la economía, la mayor parte de la información con la que cotidianamente trabaja un científico social son datos observados, experimentales.Esto implica que el uso de técnicas de análisis de información como la regresión en ciencias sociales tienen un alcance limitado en términos del análisis de causalidad; obstante, el análisis de regresión sí nos permite estudiar rigurosamente la correlación entre variables. Que exista correlación entre dos variables significa que sus movimientos están asociados, por ejemplo, que cuando el ingreso personal es alto, se consume más en servicios culturales, pero eso implica que una elevación del ingreso cause el incremento en el consumo de los servicios culturales.En este sentido, lo que permite anclar la correlación identificada con la técnica una línea de causalidad es la reflexión teórica, es decir, un estudio cuidadoso de las teóricas propuestas para la explicación de un fenómeno. Por eso, como se dijo en la primera parte de este capítulo, el análisis de regresión debe ser tomado con cautela.Dejemos hasta aquí este somero repaso de los aspectos más elementales del análisis de regresión. Es primordial que por tu propia cuenta estudies los materiales sugeridos, pues este capítulo en modo alguno suple un curso formal de los tópicos básicos de econometría. En el siguiente capítulo, retomamos el hilo del tratamiento de la información espacial través de su incorporación los modelos econométricos.","code":""},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"análisis-espacial-ii-modelos-econométricos-espaciales","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5 Análisis espacial II: modelos econométricos espaciales","text":"En el capítulo 3 nos hemos referido la necesidad de identificar la presencia de autocorrelación espacial en el conjunto de información utilizado. Mencionamos dos razones por las que conviene saber si la información usada presenta este rasgo: una de carácter técnico y otra que llamamos sustantiva. En términos de la segunda, la presencia de autocorrelación espacial significa que el fenómeno de interés se distribuye de manera aleatoria en el espacio por lo que algo está pasando y debe ser investigado, ¿por qué la variable exhibe patrones en su distribución en el espacio? Por otro lado, en términos de la razón de carácter técnico, que está desvinculada necesariamente de la razón previa, al estimar un modelo clásico de regresión lineal usando datos espaciales través del método de Mínimos Cuadrados Ordinarios seguramente se violaría uno de sus supuestos, específicamente, aquel que tiene que ver con la independencia de los términos de error. Por tanto, por razones de la técnica utilizada y del interés de la investigadora, es necesario recurrir un modelo econométrico espacial. Al hacerlo, se puede resolver el problema de la violación del citado supuesto y la vez brindar elementos para buscar las causas que explican la formación de patrones en el espacio.En este capítulo nos dedicamos mostrar dos de las alternativas de modelos econométricos espaciales, es decir, modelos econométricos que incorporan diversos tipos de interacciones que se suceden en el espacio. Es crucial que, antes de abordar este capítulo, hayas asimilado los contenidos del capítulo 3 y 4, en la medida en que lo expuesto aquí se basa en los conocimientos previos.La econometría espacial puede ser entendida como una rama de la econometría que incluye “…aquellos métodos y técnicas que, sustentados en una representación formal de la estructura de la dependencia y heterogeneidad espacial, provee el medio para llevar cabo la adecuada especificación, estimación, prueba de hipótesis y predicción para modelos en la ciencia regional” (Anselin, 1988, citado por Quintana y Mendoza (2016: 356).Desde una perspectiva teórica, la econometría espacial se ha desarrollado para resolver dos problemas que presenta la modelación econométrica con datos espaciales, relacionados con la violación de los supuestos del modelo clásico de regresión lineal que se expusieron en el capítulo previo: la heteroscedasticiadad (varianza constante de los errores) y la autocorrelación (la dependencia entre los términos de error). obstante, la econometría espacial se ha ocupado fundamentalmente del segundo problema, puesto que la heteroscedasticidad ha podido ser abordada en el marco de la econometría convencional.existe una única manera de incorporar en un modelo econométrico los efectos de interacción que se dan en el espacio y, por tanto, existe una amplia variedad de modelos econométricos espaciales, dependiendo del tipo de interacciones que en cada uno se incorpore.Existen múltiples modelos econométricos espaciales que buscan resolver el problema de la autocorrelación espacial. En todos los casos se trata de captar las interacciones que se dan en el espacio entre las variables consideradas. Estas interacciones son de diferente naturaleza, las puede haber en la varible dependiente y entonces se las denomina interacciones endógenas, las puede haber en la o las variables independientes y entonces se les llama interacciones exógenas, o bien, se pueden incorporar través del término de error de la regresión.El modelo que incorpora todas las posibles interacciones espaciales recibe el nombre de Modelo General Anidado (General Nesting Spatial Model (Elhorst 2006, 7) y puede ser escrito, en términos matriciales, como:\\[\n\\begin{aligned}\nY &= \\rho WY +\\alpha1_N+X\\beta+WX\\theta+u  \\\\\nu &=\\lambda Wu+\\epsilon  \\\\\n\\end{aligned}\n\\]Siguiendo Elhorst (2006: 10), se le denomina modelo general justamente por le hecho de que incorporara todos los tipos de efectos espaciales posibles. Veamos con cuidado la expresión anterior para distinguir cada uno de sus términos y ver con claridad los diferentes tipos de efectos espaciales.El término identificado como \\(Y\\) es otro que el vector que contiene la variable dependiente de nuestro modelo, por ejemplo, el número de casos positivos por COVID19; \\(X\\) corresponde la matriz de variables independientes que pueden ser, por ejemplo, el acceso los servicios de salud y la proporción promedio de ocupantes por vivienda; el término \\(\\beta\\) corresponde al conjunto de parámetros ser estimados, además, seguro que te resulta familiar el término \\(W\\) que es otra cosa que la estructura espacial definida partir de la matriz de pesos espaciales; finalmente, listemos los diferentes efectos espaciales que aparecen en la expresión:efectos de interacción endógenos, es decir, cuando la variable dependiente reaparece como independiente pero través de su rezago espacial (\\(WY\\)), aquí el sentido y magnitud de la interacción está dado por el parámetro \\(\\rho\\), llamado coeficiente autoregresivo espacial;efectos de interacción exógenos, es decir, cuando las variables independientes son incluidas en forma de sus rezagos espaciales (\\(WX\\)) en la que la relación es captada por \\(\\theta\\), también un parámetro estimar;efectos de interacción dados por los términos de error de la regresión (\\(Wu\\)) e interesa el sentido y magnitud de \\(\\lambda\\), el coeficiente estimar, que es llamado coeficiente de autocorrelación espacial.partir del modelo general es posible derivar otros, más simples, si se suponen determinados valores para los coeficientes \\(\\rho\\), \\(\\theta\\) y \\(\\lambda\\). Toda la constelación de modelos econométricos espaciales aparece en la figura 5.1 (Elhorst 2006, 7). Por ejemplo, si partiendo del modelo general se asume que \\(\\lambda=0\\), se obtiene el denominado modelo espacial de Durbin; en tanto si \\(\\rho=\\theta=\\lambda=0\\), llegamos al conocido modelo clásico de regresión lineal (MCO). Observa con cuidado cada una de las diferentes alternativas de modelación econométrica espacial y trata de reflexionar sobre sus diferencias.\nFigura 5.1: Clasificación de modelos espaciales, Elhorst (2006)\nCada uno de los modelos econométricos espaciales de la figura buscan responder diferentes preguntas en relación cómo la variable de interés, nuestra variable dependiente, se ve afectada por las interacciones espaciales que hemos mencionado. Por ejemplo, quizá como parte de la exploración de la pandemia por COVID19 nos dimos cuenta de que el número de casos positivos por este mal está asociado con el acceso los servicios de salud en los municipios vecinos, es decir, el rezago espacial de la población con acceso servicios de salud, tal como se aprecia en el siguiente diagrama de dispersión:Como puedes observar, el diagrama de dispersión muestra que existe asociación lineal positiva entre el valor promedio de la población con acceso servicios de salud en los municipios vecinos (el rezago espacial en el eje X) y el número de casos positivos por COVID19 (en el eje Y), por tanto, podría ser pertinente proponer un modelo que incluya dicho efecto espacial, es decir, un efecto espacial dado por la variable independiente (efecto espacial exógeno).Ejercicio¿Cuál es el nombre de los modelos econométricos espaciales que incorporan dicho efecto?Una relación como la anterior quizá pueda ser modelada través de un modelo espacial de Durbin, \\(Y=\\rho WY+\\alpha 1n+X\\beta+WX \\theta+\\epsilon\\), puesto que la investigadora pudiera interesarle cómo los valores de las variables independientes en los espacios vecinos inciden en el fenómeno de interés.Así pues, la elección del tipo de modelo espacial depende, en lo fundamental, de dos elementos: ) la evidencia de asociación espacial entre el conjunto de variables disponibles y que se revela través del análisis exploratorio de datos espaciales (definición de criterios de vecindad, construcción de índices de Moran) y ii) los intereses y propósitos de la investigación.De toda la gama de modelos econométricos espaciales que es posible construir, aquí sólo nos ocuparemos de dos de ellos, el modelo de rezago espacial y el modelo de error espacial.Para ello, seguimos la ruta metodológica propuesta por Anselin y Rey (2014) que consiste en comenzar con un modelo lineal espacial estimado con mínimos cuadrados ordinarios (MCO) que luego es extendido con la incorporación de interacciones espaciales, ya sea incluyendo \\(\\rho\\), el coeficiente autoregresivo espacial o \\(\\lambda\\), el término de autocorrelación espacial, tal y como se muestra en la figura 5.2.\nFigura 5.2: De un modelo con MCO un modelo espacial\nLa estimación de un modelo econométrico de error o rezago espacial, siguiendo Anselin y Rey (2014), puede ser dividida en los siguientes pasos:Estimar un modelo clásico de regresión lineal con mínimos cuadrados ordinarios.Evaluar la presencia de autocorrelación espacial en los errores del modelo.Evaluar, alternativamente, la pertinencia de un modelo de rezago o de error espacial.Estimar el modelo de rezago y de error espacial y apuntar cuál de ellos capta mejor la relación propuesta.El punto ha sido abordado en el capítulo previo, sin embargo regresaremos ello más adelante de forma práctica para describir la secuencia general. Sobre el punto ii nos detendremos un momento para explicarlo con más detalle. Bien podrías pensar que la evaluación de la autocorrelación ha quedado plenamente cubierta en el capítulo 3 cuando nos referimos al análisis exploratorio de datos espaciales y usamos la de Moran; sin embargo, es necesario distinguir entre autocorrelación en la variable de interés y autocorrelación en los errores del modelo, lo primero lo hicimos en el citado capítulo, mientras que lo segundo corresponde la secuencia para la selección del modelo econométrico espacial.Si se ha encontrado evidencia de autocorrelación en la variable través de los instrumentos de exploración, como los diversos mapas y la de Moran, estos elementos nos llevan pesar que al usar dicha variable en un modelo econométrico lineal simple estimado con MCO, muy probablemente los términos de error estén correlacionados y, por tanto, se violaría dicho supuesto. Para verificarlo, ahora estimaremos la de Moran sobre los errores del modelo: en esto justamente consiste el paso ii.Ahora bien, si hallamos evidencia de que los errores del modelo lineal están autocorrelacionadaso, sabremos que lo mejor es estimar un modelo econométrico espacial. Pero, ¿cuál es mejor, el modelo de error espacial o el de rezago? Para ello recurriremos un estadístico denominado Prueba de los Multiplicadores de Lagrange o Prueba de Puntaje de Rao (Rao score test), que fue desarrollada por Anselin finales de la década de los ochenta del siglo pasado (Anselin Rey 2014: 105). Las pruebas nos permitirán decidir qué modelo es mejor, evaluando entre el modelo lineal y el de rezago, por un lado, y el modelo lineal y el de error, por otro.Así, cuando se evalúa entre el modelo lineal y el modelo de rezago lo que en el fondo hacemos es probar estadísticamente si \\(\\rho\\), el coeficiente autoregresivo espacial, realmente está presente en la relación propuesta. Recordemos la estructura del modelo de rezago:\\[\ny=\\rho Wy+X\\beta+u \n\\]De modo que, formalmente, el juego de hipótesis evaluar y que nos permitirán decidir entre el modelo lineal y el modelo de rezago es:\\[\n\\begin {aligned}\nHo &: \\rho=0 \\\\\nHa &: \\rho\\=0 \\\\\n\\end {aligned}\n\\]Según lo que nos indique el multiplicador de Lagrange y su valor-p asociado, determinado nivel de significancia, rechazar la hipótesis nula significará que, entre el modelo lineal y el modelo de rezago, el mejor modelo es un modelo de rezago espacial.En esta misma línea de razonamiento, del modelo de error nos interesa saber si \\(\\lambda\\) es diferente de 0 y, por tanto, si juega algún papel relevante en la relación propuesta. Recuerda que el modelo de error luce como:\\[\n\\begin{aligned}\nY &= \\alpha1_N+X\\beta+u  \\\\\nu &=\\lambda Wu+\\epsilon  \\\\\n\\end{aligned}\n\\]Formalmente, las hipótesis verificar son:\\[\n\\begin {aligned}\nHo &: \\lambda=0 \\\\\nHa &: \\lambda\\=0 \\\\\n\\end {aligned}\n\\]Igualmente, el estadístico usado para evaluar nuestras hipótesis es el multiplicador de Lagrange y que, dado determinado nivel de significancia, rechazar la hipótesis nula significa que, entre el modelo lineal y el modelo de error, el mejor modelo es uno de error espacial.Pero, ¿qué ocurre cuando en las pruebas anteriores son concluyentes? Es decir, cuando tanto la prueba sobre la versión de rezago y de error han resultado ambas significativas? Sin abudar en los detalles técnicos de esta situación, esto podría pasar porque el estadístico del multiplicador de Lagrange sobre el modelo de rezago, \\(LM_{\\rho}\\), es sensible la presencia autocorrelación espacial en el término de error, lo mismo que el estadístico del multiplicador de Lagrange sobre el modelo de error, \\(LM_{\\lambda}\\) que es sensible la presencia autocorrelación espacial en el rezago. Para decirlo de forma hipersimplificada, estas pruebas podrían dar un “falso positivo.”Exclusivamente cuando en ambos casos se rechazan las respectivas hipótesis nulas, es decir que \\(\\lambda\\=0\\) y \\(\\rho\\=0\\), habremos de recurrir las llamadas “versiones robustas” del multiplicador de Lagrange. Las versiones robustas de dichos indicadores implican “que el estadístico original es corregido mediante la influencia potencial de ‘otras’ alternativas” (Anselin y Rey, 2014: 105). Usar la versión robusta del multiplicador de Lagrange que evalúa entre la alternativa del modelo lineal y el modelo de rezago implica que dicha prueba determina con mayor precisión si hay presencia de autocorrelación y esta es captada través del coeficiente autoregresivo espacial, excluyendo los efectos que podrían estar dados por el coeficiente de autocorrelación espacial; reciprocamente para la prueba en su versión robusta del multiplicador de Lagrange sobre la alternativa del modelo lineal respecto al modelo de error.Usemos una analogía para mejor comprender lo anterior. Imagina que intentas pasar por un colador un poco de harina que tiene algunos terrones porque ha estado algún tiempo en la despensa. Si los orificios del colador son demasiado grandes dejarán pasar los terrones y pensaríamos que nuestra harina estaba libre de terrones. obstante, si usamos un colador con orificios más finos, los terrones pasarán y sabremos que la harina, en efecto, tenía terrones. Algo semejante ocurre con los multiplicadores de Lagrange en sus versiones estándar y robustas: los orificios del colador en las pruebas robustas son más finos, permitiéndonos discriminar con mayor seguridad entre uno y otro modelo, para tener evidencia concluyente.Así, por ejemplo, si el multiplicador de Lagrange en su versión robusta que evalúa entre un modelo lineal y un modelo de rezago sostiene que el mejor modelo es el modelo de rezago, mientras que el multipilcador de Lagrange en su versión robusta que evalúa entre un modelo lineal y un modelo de rezago sostiene que el mejor modelo es el modelo lineal, la conclusión la que arribamos, la luz ofrecida por ambas pruebas, es que el mejor modelo es el modelo de rezago espacial. Para simplificar, basta recordar lo recomendado por Anselin (2005, 197): “lo importante recordar (en la selección entre modelos espaciales) es considerar las versiones robustas de los estadísticos (los multiplicadores de Lagrange) sólo cuando las versiones estándar son ambas significativas.Finalmente, ya que tenemos claro qué modelo capta mejor la relación propuesta, procederemos estimar y analizar dicho modelo. Todos los pasos que hemos tratado de describir hasta aquí, son sintetizados por Anselin y Rey (2014) en el siguiente esquema:\nFigura 5.3: Árbol de decisión de regresión espacial, Anselin y Rey (2014)\nEn las siguientes sección se ilustra cómo seguir esta hoja de ruta para estimar los dos modelos que incluyen efectos espaciales: el de error y el de rezago y decidir cuál es el mejor.En esta sección exponemos cómo, través de algunos paquetes y funciones en R, podemos recorrer el camino propuesto en la sección anterior, que nos llevará decidir cuál es el mejor modelo que capta las interacciones que se dan en el espacio, uno de error o de rezago. obstante, antes e ello, aprovecharemos para hacer un brevísimo recordatorio de algunas de las herramientas mostradas en los capítulos previos, fin de mostrar su uso de forma integrada en el contexto del planteamiento de un modelo econométrico espacial.Antes de emprender la tarea de la estimación de los modelos espaciales siguiendo la secuencia anterior, llevaremos cabo un recuento de diversos elementos que nos permitirán poner en contexto el problema de la selección del modelo espacial y, al mismo tiempo, recapitular algunos de los tópicos tratados lo largo de este libro.Hemos estado analizando un conjunto de variables sociodemográficas y económicas con la intensión de indagar cuál es su relación con la situación de la pandemia por COVID19 durante la primera ola en una región del centro de México llamada Zona Metropolitana del Valle de México.En el primer capítulo de este material expusimos una serie de herramientas para tener una primera aproximación con el conjunto de datos. Esas herramientas, entre otros aspectos, integran un enfoque denominado Análisis Exploratorio de Datos (EDA) y tiene por objeto, antes de proponer cualquier modelo, conocer la estructura subyacente de la información e indagar en la asociación entre variables para responder y plantear algunas preguntas de carácter preliminar en torno al objeto de estudio construido.Por ejemplo, través de un diagrama de caja del número de casos positivos es posible observar la distribución de la variable e identificar observaciones típicas, es decir, valores extremadamente altos o bajos:Los puntos en color rojo son observaciones extremadamente altas, en el rango de la información utilizada. Del mismo modo, una gráfica de la densidad de probabilidad de la variable nos indica cómo ésta se distribuye:La “cola” de esta gráfica que se extiende la derecha (sesgo positivo) indica, como la anterior, que hay valores muy altos. Esto nos lleva pensar en las razones de este comportamiento, ¿por qué hay unidades territoriales, municipios en este caso, que presentan tal magnitud de casos? El EDA, justamente, busca despertar nuestra curiosidad e imaginación en torno al fenómeno analizado.Otro tipo de pregunta que nos permite responder de forma preliminar el EDA es, por ejemplo, la siguiente: ¿existe asociación entre el número de casos positivos por COVID19 y el acceso los servicios de salud? Una manera sencilla de acercarse la respuesta de tal pregunta es través de un diagrama de dispersión:El gráfico anterior indica que, en efecto, parece haber una relación lineal positiva entre ambas variables, ¿por qué crees que pueda existir esta relación, qué la fundamenta? Habrá pues que indagar y reflexionar sobre las posibles explicaciones que se hayan dado una relación como la identificada.Por otro lado, el interés central de este libro ha sido brindar algunas herramientas básicas para el tratamiento de información espacial: el espacio importa, dónde ocurren los fenómenos y cómo se comportan o distribuyen en el territorialmente es un elemento clave para comprender tales fenómenos cabalmente.Por ejemplo, ¿hay algún patrón en cómo se dieron los casos positivos por COVID19 en el Valle de México? Para tener una primera idea de esto, podemos construir un mapa, como los expuestos en el capítulo 2, cuando tratamos con los mapas de coropletas:Es valido sostener, al menos inicialmente que, en efecto, hay una concentración de los casos positivos en la zona sur del área estudiada, la Ciudad de México, donde se muestran los colores en azul más intenso. Esta concentración identificada visualmente, nos lleva pensar en la necesidad de evaluar formalmente si los datos sobre los casos positivos por COVID19 muestran patrones sistemáticos de agrupamiento en el territorio. Para ello, podemos recurrir la prueba de autocorrelación espacial que vimos en el capítulo 3, la de Moran. Como recordarás del capítulo 3, para construir el estadístico de asociación espacial de Moran hay que definir una estructura espacial, es decir, definir entre qué unidades espaciales hay una relación de vecindad, misma que es recogida en la llamada matriz de pesos espaciales. Con una relación de vecindad de tipo reina, la de Moran y el diagrama de Moran sobre el número de casos positivos luce como:La información que nos proporciona la de Moran indica que los casos positivos por COVID19 en el Valle de México muestran un patrón de agrupamiento sistemático en las unidades territoriales analizadas: el coeficiente es positivo, relativamente alto y estadisticamente significativo. Así pues, un nivel aún exploratorio, tanto el mapa como la de Moran dan cuenta de que la variable explicar presenta autocorrelación espacial.Hay varios elementos interesantes respecto la evidencia anterior. El primero es que lo más probable es que, si estimamos un modelo econométrico lineal buscando explicar los casos positivos, se violaría el supuesto de independencia en los términos de error de la regresión que exige un modelo lineal clásico, por lo sería el más apropiado para explicar dicho fenómeno. El segundo, y quizá el más interesante, es que debemos reflexionar sobre el hecho de que la variable explicar presenta patrones de asociación en el espacio: ¿por qué los casos positivos de COVID19 son más altos en algunos grupos de municipios de la ciudad que en otros?La respuesta esta pregunta es compleja y, desde luego, requiere una reflexión cuidadosa sobre las teorías y trabajos previos de especialistas en el tema que han tratado sobre los patrones espaciales en la transmisión de una enfermedad, como el clásico trabajo de Cliff y sus colaboradores (Cliff et al. 1981) sobre la dinámica espacial de las pandemias en Islandia, aunque ciertamente se ha aceptado que las epidemias pueden y deben analizarse desde una perspectiva espacial (Rezaeian et al. 2007).El caso es que hay tanto evidencia en los datos, como en trabajos previos que nos llevan pensar en la pertinencia de un modelo econométrico espacial que incorpore el comportamiento que hemos identificado en un nivel exploratorio: la autocorrelación espacial en los casos positivos por COVID19. Ahora entonces, cuál es el mejor modelo que podemos usar, ¿uno de rezago espacial o de error espacial? Seguiremos pues la secuencia de la sección anterior para decidir, mientras ilustramos el proceso través de los paquetes y funciones que nos ofrece R.Ahora que tenemos claro que requerimos un modelo espacial si lo que prentedemos es explicar los casos positivos por COVID19 en el Valle de México, veamos cómo llevar cabo esto través de R.Los paquetes utilizados para la estimación de modelos econométricos espaciales son:spatialreg: un paquete que contiene las funciones para calcular los modelos econométricos espaciales que hemos comentado antes.spdep: es el mismo paquete que usamos en el capítulo 3 y que contiene funciones para evaluar dependencia espacial y construir matrices de pesos espaciales.Como es usual, la instalación desde la consola puede hacerse con:Además, echaremos mano de otros tantos paquetes previamente trabajados, por lo que deberán ser llamados junto con los recién instalados:La base de datos usar en este ejercicio es la misma que hemos usando antes y que puedes descargar aquí. Para cargar nuestra base de datos:De acuerdo lo dicho en las secciones previas, primero debemos estimar un modelo clásico de regresión lineal con mínimos cuadrados ordinarios. Usaremos el mismo modelo del capítulo sobre regresión lineal en su versión simple: un modelo que busca explicar los casos positivos por COVID-19 por cada 1 mil habitantes (pos_hab) mediante la población con acceso servicios de salud (ss), la especificación de dicho modelo lineal luce como:\\[\nposhab_i=\\beta_0+\\beta_1ss_i+u_i\n\\]\nLa variable que buscamos explicar con nuestro modelo será casos positivos COVID19 por cada 1 mil habitantes, pos_hab, explicada través del porcentaje de población con acceso servicios de salud, ss. La estimación del modelo lineal, tal y como se expuso en el capítulo 4, es:Ejercicio¿Qué puedes decir sobre la significancia individual del coeficiente estimado? ¿Qué sobre la bondad de ajuste del modelo?Una vez que hemos estimado el modelo lineal través del MCO, hemos completado el paso de nuestra secuencia general. Debemos ahora evaluar la presencia de autocorrelación en los térinos de error del modelo, es decir, el paso ii. De nueva cuenta insistimos en la diferencia entre evaluar la autocorrelación sobre la variable de interés (lo que hicimos hace un momento con la de Moran y su diagrama sobre la variable pos_hab) y evaluar la autocorrelación sobre los errores del modelo, lo que haremos en este momento.Verificamos la presencia de autocorrelación espacial de los errores través de la de Moran, lo que nos permitirá responder la pregunta ¿los errores de nuestro modelo están correlacionados? Para ello, nos servimos de la función moran.test() del paquete spdep. Como recordarás, la función requiere dos argumentos: el vector número evaluar (en este caso los errores del modelo MCO) y la estructura espacial.En el capítulo relativo al análisis exploratorio de datos espaciales vimos que la definición de una estructura espacial atraviesa por la construcción de una matriz de pesos espaciales, recuerda que través de la función poly2nb() del paquete spdep construimos nuestra lista de vecinos con base en los criterios de contigüidad de tipo reina (queen=TRUE):Ahora, el objeto de tipo nb debe ser trasformado uno que contenga los pesos espaciales, es decir, la matriz ponderada: un objeto de tipo listw:Ahora, alculemos la de Moran sobre los términos de error:EjercicioCon base en los resultados de la de Moran, ¿dirías que existe autocorrelación espacial en los errores?Lo usual es que cuando la variable de interés presenta autocorrelación, esta característica termine por afectar los resultados de la estimación por mínimos cuadrados ordinarios, tal y como ha ocurrido en este caso en donde los errores del modelo lineal están correlacionados.Así pues, es necesario, siguiendo la ruta marcada por Anselin y Rey (2014), evaluar, alternativamente, la pertinencia de un modelo de rezago o de error espacial, es decir, el paso iii. Como dijimos antes, esto lo haremos con base en las pruebas sobre las dos alternativas propuestas (modelo de rezago o modelo de error), través de los multiplicadores de Lagrange, primero observando sus versiones estándar y, sólo si éstas son concluyentes, después, sus versiones robustasLas pruebas son llamadas con la función lm.LMtests() del paquete spdep que requiere de los siguientes argumentos: el modelo lineal evaluar (model=), la estructura espacial (listw=) y los pruebas solicitadas (test=):Revisa la ayuda relacionada con la función lm.LMtests() para familiarizarte con todos sus argumentos. En este caso, solicitamos las 4 pruebas de las que hicimos mención previamente: la prueba del multiplicador de Lagrange sobre la alternativa del modelo de rezago (LMlag), la prueba del multiplicador de Lagrange sobre la alternativa del modelo de error (LMerr), así como sus respectivas versiones robustas (RLMlag,RLMerr).Interpretemos los resultados. Primero, contrastemos las versiones estándar de los multiplicadores, que aparecen el la siguiente tabla:Tabla 1. Multiplicadores de Lagrange, versiones estándarTanto el estadístico vinculado al modelo de rezago, como el del error son estadísticamente significativos, es decir, en cada caso se rechaza la hipótesis nula de que \\(\\rho=0\\) y \\(\\lambda=0\\), respectivamente, por lo que tanto un modelo de rezago como de error son plausibles: tenemos pues, con las versiones estándar, pruebas concluyentes de qué modelo es mejor. En este caso, hay que mirar los resultados de las versiones robustas para tomar la decisión final:En este caso, sólo el estadístico vinculado al modelo de rezago, RLMlag, resulta significativo, es decir, se rechaza la hipótesis nula de que \\(\\rho=0\\); en tanto, para el caso del estadístico vinculado al modelo de error, RLMerr, podemos rechazar la hipótesis nula de que \\(\\lambda=0\\). la luz de la evidencia anterior, la mejor alternativa de modelo econométrico espacial es el modelo de rezago, por las siguientes razones:La variable explicar mostró, través de mapas de clasificación común y elestadístico de Moran, evidencia de autocorrelación espacial.Los términos de error del modelo lineal mostraban también autocorrelación, por lo que dicho modelo viola, al menos, uno de sus supuestos.Las versiones robustas de los multiplicadores de Lagrange mostraron que, entre un modelo MCO y uno de rezago, el mejor era el de rezago, y que entre un modelo MCO y uno de error es mejor el de MCO, pero por las razones y ii, y por la versión robusta sobre el modelo de rezago, resulta que el mejor modelo es el de rezago espacial.Revisa de nueva cuenta la figura 5.3 para mejor entender nuestra decisión. Finalmente, los tanto el modelo de rezago como el de error serán estimados, con ello, llegamos al paso iv y habremos terminado la secuencia.El modelo de rezago espacial se invoca con al función lagsarlm() del paquete spatialreg en la que la estimación corre cargo del método de máxima verosimilitud. En la función se especifica el modelo (formula=), la base de datos (data=) y la estructura espacial (listw=).De la salida debemos llamar la atención sobre los coeficientes de interés: \\(\\rho\\), el coeficiente autoregresivo espacial, que en este caso es positivo, significativo y alto (0.83). ¿Qué es lo que dicho valor significa? La interpretación que podría darse este coeficiente es que cuando aumenta en una unidad el número de casos positivos en los municipios y alcaldías vecinas, el número de casos positivos por COVID19 por cada 1 mil habitantes en el municipio de interés aumentará en 0.83. Hemos pues encontrado evidencia de que hay interacciones espaciales entre el estado y dinámica de la pandemía por COVID19 en la región estudiada: lo que pasa en las áreas territoriales vecinas afecta la unidad territorial de referencia. Esto podría tener implicaciones sobre las medidas de política sanitaria en la contención de la enfermedad y en el posible desarrollo futuro de ésta.Por supuesto, hay que decir, que esto es sólo un ejercicio ilustrativo y que una interpretación más fina atraviesa por un concienzudo y sistemático estudio del carácter espacial de la transmisión de las enfermedades, lo que el autor de estas líneas está lejos de lograr, por lo que los resultados deben interpretarse con cautela y sólo con fines didácticos.Las líneas anteriores dan cuenta del elemento sustantivo del fenómeno de interés: la pandemia tiene un fuerte componente espacial y lo hemos logrado captar través del coeficiente autorregresivo espacial en el modelo de rezago. Pero, ¿con este modelo hemos logrado resolver el problema de la autocorrelación en los errores? Los términos de error del modelo de rezago que recién hemos estimado pueden ser visualizados, guardados y evaluados para ver si presentan autocorrelación:EjercicioCon base en los resultados de la de Moran sobre los términos de error del modelo de rezago, ¿dirías que persite el problema de autocorrelación espacial en los errores en el modelo de rezago?El modelo de error espacial es llamado con la función errorsarlm() y los argumentos son los mismos:Aquí, el parámetro de interés estimado es \\(\\lambda\\), el coeficiente de autocorrelación espacial, que resultó positivo, alto y estadísticamente significativo, también (0.87). En este caso, las posibilidades de interpretación sustantiva del coeficiente obtenido son mucho más limitadas, ya que los efectos espaciales están modelados por el término de error.Recordemos que los términos de error incluyen los efectos de todas aquellas variables que están siendo incluidas en nuestro modelo. En este caso, entonces, es posible brindar una lectura explícita de la variable o variable que determinan las interacciones espaciales. Sólo podemos contentarnos con decir que el problema de la dependencia entre los términos de error ha sido resuelto y que el componente espacial es considerablemente alto.Con la siguiente línea, se verifica que se ha resuelto el problema de la autocorrelación en los errores:Con estos resultados hemos completado la ruta más elemental para la estimación de modelos econométricos espaciales; sin embargo, debemos olvidar que existen todas las alternativas que apuntamos en la figura 5.1 y que fueron discutidas aquí.Hay una serie de tópicos que han quedado fuera de estas notas, tales como los modelos que buscan resolver la heterogeneidad espacial, aquellos que atienden la violación del supuesto de homoscedasticidad en las perturbaciones, o bien, la incorporación de la dimensión temporal nuestro análisis, través de instrumentos como los modelos de panel, por lo que este material es sino sólo un abordaje introductorio, una invitación que profundices por tu propia cuenta en el manejo de las herramientas para el tratamiento de datos espaciales.Cerramos este libro recomendado una serie de materiales para cada uno de los capítulos que hemos tratado aquí.Para el Capítulo 1, donde tuvimos oportunidad de presentar los elementos básicos de R y RStudio mediante el análisis exploratorio de datos, recomendamos consultar el curso gratuito ofrecido por la Universidad Nacional Autónoma de México través de la plataforma Coursera Introducción Data Science: Programación Estadística con R, así como la versión en castellano del excelente libro de Wickham y Grolemund R para Ciencia de Datos.Para el Capítulo 2, centrado en la elaboración de mapas coropléticos, hay varios materiales que puedes consultad, como blogs o los servidores que contiene ejercicios y prácticas, como RPubs, aunque también existen los materiales desarrollados por los propios autores de la paquería, aunque están en castellano.En el Capítulo 3, que trató sobre los elementos básicos del análisis exploratorio de datos espaciales, existen los materiales desarrollados por los colegas del Centro de Estudios de Desarrollo Regional y Urbano Sustentable quienes, en su canal de YouTube, desarrollan una serie de prácticas introductorias de exploración de información espacial, aunque con otro Softeare (GeoDa).Para el Capítulo 4 y Capítulo 5](https://jaime-pru.github.io/Analisis-de-datos-espaciales/%C3%A1lisis-espacial-ii-modelos-econom%C3%A9tricos-espaciales.html) existe un sin número de materiales para el análisis de regresión, como el libro electrónico de Luis Quintana y Miguel . Mendoza “Econometría Aplicada Utilizando R,” que puedes descargar libremente desde aquí, del que el último capítulo aborda el tema de econometría espacial.Esperamos que estas notas contribuyan alimentar tu curiosidad sobre el tratamiento y análisis de información espacial y que sean un primer paso en tu formación como profesional.","code":"## `geom_smooth()` using formula = 'y ~ x'## `geom_smooth()` using formula = 'y ~ x'## Reading layer `covid_zmvm' from data source \n##   `C:\\Repositorios\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 76 features and 57 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5\n## Projected CRS: Lambert_Conformal_Conic\ntmap::tm_shape(base) +\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", \n                style = \"quantile\", \n                n=6,\n                title = \"Casos positivos COVID19\",\n                \n                palette = \"GnBu\",\n                legend.format = list(text.separator=\"-\",digits=2))+\n  tmap::tm_layout(main.title = \"Positivos\", main.title.position = \"center\", title.size = 0.5)## \n##  Moran I test under randomisation\n## \n## data:  covid_zmvm$pos_hab  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = 8.8625, p-value < 2.2e-16\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##       0.656334129      -0.013333333       0.005709563\n#Instalar dos paquetes\ninstall.packages(c(\"spdep\", \"spatialreg\" ))\n#Paquetes recién instalados\nlibrary(spdep)\nlibrary(spatialreg)\n#Otros paquetes usados\nlibrary(rgdal)\nlibrary(sp)\ncovid_zmvm <-rgdal::readOGR(\"base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp\")## Warning: OGR support is provided by the sf and terra packages among others## Warning: OGR support is provided by the sf and terra packages among others## Warning: OGR support is provided by the sf and terra packages among others## Warning: OGR support is provided by the sf and terra packages among others## Warning: OGR support is provided by the sf and terra packages among others## Warning: OGR support is provided by the sf and terra packages among others## Warning: OGR support is provided by the sf and terra packages among others## OGR data source with driver: ESRI Shapefile \n## Source: \"C:\\Repositorios\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp\", layer: \"covid_zmvm\"\n## with 76 features\n## It has 57 fields\nmodelo_mco <- stats::lm (formula=pos_hab ~ ss, data = covid_zmvm)\nsummary(modelo_mco)## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.4391 -2.7744 -0.6709  1.5324 14.9969 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -19.862      5.809  -3.419  0.00102 ** \n## ss            39.490      8.516   4.637 1.49e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.572 on 74 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2147 \n## F-statistic:  21.5 on 1 and 74 DF,  p-value: 1.488e-05\nmTRUE <- spdep::poly2nb(covid_zmvm)\nmTRUE.pesos<-spdep::nb2listw(mTRUE)\nspdep::moran.test(modelo_mco$residuals, mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  modelo_mco$residuals  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = 6.9825, p-value = 1.45e-12\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##       0.512928287      -0.013333333       0.005680411\nspdep::lm.LMtests(model=modelo_mco, listw=mTRUE.pesos, test=c(\"LMlag\",\"LMerr\", \"RLMlag\",\"RLMerr\"))## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## LMlag = 55.313, df = 1, p-value = 1.028e-13\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## LMerr = 42.733, df = 1, p-value = 6.275e-11\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## RLMlag = 13.358, df = 1, p-value = 0.0002573\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## RLMerr = 0.77786, df = 1, p-value = 0.3778\nlibrary(spatialreg)\nrezago <- spatialreg::lagsarlm(formula=pos_hab ~ ss, data=covid_zmvm, listw=mTRUE.pesos)\nsummary(rezago)## \n## Call:spatialreg::lagsarlm(formula = pos_hab ~ ss, data = covid_zmvm, \n##     listw = mTRUE.pesos)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -5.72501 -1.71081 -0.31629  1.12054 11.64378 \n## \n## Type: lag \n## Coefficients: (asymptotic standard errors) \n##             Estimate Std. Error z value Pr(>|z|)\n## (Intercept)  -8.8906     3.4221 -2.5980 0.009377\n## ss           14.9811     5.1378  2.9159 0.003547\n## \n## Rho: 0.83629, LR test value: 62.655, p-value: 2.4425e-15\n## Asymptotic standard error: 0.056938\n##     z-value: 14.688, p-value: < 2.22e-16\n## Wald statistic: 215.73, p-value: < 2.22e-16\n## \n## Log likelihood: -191.0071 for lag model\n## ML residual variance (sigma squared): 7.1381, (sigma: 2.6717)\n## Number of observations: 76 \n## Number of parameters estimated: 4 \n## AIC: 390.01, (AIC for lm: 450.67)\n## LM test for residual autocorrelation\n## test value: 0.10973, p-value: 0.74045\nspdep::moran.test(rezago[[\"residuals\"]], mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  rezago[[\"residuals\"]]  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = -0.030802, p-value = 0.5123\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##      -0.015615667      -0.013333333       0.005490469\nerror <-  spatialreg::errorsarlm(formula=pos_hab ~ ss, data=covid_zmvm, listw=mTRUE.pesos)\nsummary(error)## \n## Call:spatialreg::errorsarlm(formula = pos_hab ~ ss, data = covid_zmvm, \n##     listw = mTRUE.pesos)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -5.80220 -1.62230 -0.39269  1.11193 11.55180 \n## \n## Type: error \n## Coefficients: (asymptotic standard errors) \n##             Estimate Std. Error z value Pr(>|z|)\n## (Intercept) -0.66237    5.03921 -0.1314  0.89543\n## ss          12.54484    6.30980  1.9882  0.04679\n## \n## Lambda: 0.87101, LR test value: 58.512, p-value: 2.0206e-14\n## Asymptotic standard error: 0.050779\n##     z-value: 17.153, p-value: < 2.22e-16\n## Wald statistic: 294.22, p-value: < 2.22e-16\n## \n## Log likelihood: -193.0788 for error model\n## ML residual variance (sigma squared): 7.3069, (sigma: 2.7031)\n## Number of observations: 76 \n## Number of parameters estimated: 4 \n## AIC: 394.16, (AIC for lm: 450.67)\nspdep::moran.test(error[[\"residuals\"]], mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  error[[\"residuals\"]]  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = 0.18736, p-value = 0.4257\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##      0.0005636417     -0.0133333333      0.0055017348"},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"econometría-espacial","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.1 Econometría espacial","text":"La econometría espacial puede ser entendida como una rama de la econometría que incluye “…aquellos métodos y técnicas que, sustentados en una representación formal de la estructura de la dependencia y heterogeneidad espacial, provee el medio para llevar cabo la adecuada especificación, estimación, prueba de hipótesis y predicción para modelos en la ciencia regional” (Anselin, 1988, citado por Quintana y Mendoza (2016: 356).Desde una perspectiva teórica, la econometría espacial se ha desarrollado para resolver dos problemas que presenta la modelación econométrica con datos espaciales, relacionados con la violación de los supuestos del modelo clásico de regresión lineal que se expusieron en el capítulo previo: la heteroscedasticiadad (varianza constante de los errores) y la autocorrelación (la dependencia entre los términos de error). obstante, la econometría espacial se ha ocupado fundamentalmente del segundo problema, puesto que la heteroscedasticidad ha podido ser abordada en el marco de la econometría convencional.existe una única manera de incorporar en un modelo econométrico los efectos de interacción que se dan en el espacio y, por tanto, existe una amplia variedad de modelos econométricos espaciales, dependiendo del tipo de interacciones que en cada uno se incorpore.","code":""},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"diversas-alternativas-de-modelos-econométricos","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.1.1 Diversas alternativas de modelos econométricos","text":"Existen múltiples modelos econométricos espaciales que buscan resolver el problema de la autocorrelación espacial. En todos los casos se trata de captar las interacciones que se dan en el espacio entre las variables consideradas. Estas interacciones son de diferente naturaleza, las puede haber en la varible dependiente y entonces se las denomina interacciones endógenas, las puede haber en la o las variables independientes y entonces se les llama interacciones exógenas, o bien, se pueden incorporar través del término de error de la regresión.El modelo que incorpora todas las posibles interacciones espaciales recibe el nombre de Modelo General Anidado (General Nesting Spatial Model (Elhorst 2006, 7) y puede ser escrito, en términos matriciales, como:\\[\n\\begin{aligned}\nY &= \\rho WY +\\alpha1_N+X\\beta+WX\\theta+u  \\\\\nu &=\\lambda Wu+\\epsilon  \\\\\n\\end{aligned}\n\\]Siguiendo Elhorst (2006: 10), se le denomina modelo general justamente por le hecho de que incorporara todos los tipos de efectos espaciales posibles. Veamos con cuidado la expresión anterior para distinguir cada uno de sus términos y ver con claridad los diferentes tipos de efectos espaciales.El término identificado como \\(Y\\) es otro que el vector que contiene la variable dependiente de nuestro modelo, por ejemplo, el número de casos positivos por COVID19; \\(X\\) corresponde la matriz de variables independientes que pueden ser, por ejemplo, el acceso los servicios de salud y la proporción promedio de ocupantes por vivienda; el término \\(\\beta\\) corresponde al conjunto de parámetros ser estimados, además, seguro que te resulta familiar el término \\(W\\) que es otra cosa que la estructura espacial definida partir de la matriz de pesos espaciales; finalmente, listemos los diferentes efectos espaciales que aparecen en la expresión:efectos de interacción endógenos, es decir, cuando la variable dependiente reaparece como independiente pero través de su rezago espacial (\\(WY\\)), aquí el sentido y magnitud de la interacción está dado por el parámetro \\(\\rho\\), llamado coeficiente autoregresivo espacial;efectos de interacción exógenos, es decir, cuando las variables independientes son incluidas en forma de sus rezagos espaciales (\\(WX\\)) en la que la relación es captada por \\(\\theta\\), también un parámetro estimar;efectos de interacción dados por los términos de error de la regresión (\\(Wu\\)) e interesa el sentido y magnitud de \\(\\lambda\\), el coeficiente estimar, que es llamado coeficiente de autocorrelación espacial.partir del modelo general es posible derivar otros, más simples, si se suponen determinados valores para los coeficientes \\(\\rho\\), \\(\\theta\\) y \\(\\lambda\\). Toda la constelación de modelos econométricos espaciales aparece en la figura 5.1 (Elhorst 2006, 7). Por ejemplo, si partiendo del modelo general se asume que \\(\\lambda=0\\), se obtiene el denominado modelo espacial de Durbin; en tanto si \\(\\rho=\\theta=\\lambda=0\\), llegamos al conocido modelo clásico de regresión lineal (MCO). Observa con cuidado cada una de las diferentes alternativas de modelación econométrica espacial y trata de reflexionar sobre sus diferencias.\nFigura 5.1: Clasificación de modelos espaciales, Elhorst (2006)\nCada uno de los modelos econométricos espaciales de la figura buscan responder diferentes preguntas en relación cómo la variable de interés, nuestra variable dependiente, se ve afectada por las interacciones espaciales que hemos mencionado. Por ejemplo, quizá como parte de la exploración de la pandemia por COVID19 nos dimos cuenta de que el número de casos positivos por este mal está asociado con el acceso los servicios de salud en los municipios vecinos, es decir, el rezago espacial de la población con acceso servicios de salud, tal como se aprecia en el siguiente diagrama de dispersión:Como puedes observar, el diagrama de dispersión muestra que existe asociación lineal positiva entre el valor promedio de la población con acceso servicios de salud en los municipios vecinos (el rezago espacial en el eje X) y el número de casos positivos por COVID19 (en el eje Y), por tanto, podría ser pertinente proponer un modelo que incluya dicho efecto espacial, es decir, un efecto espacial dado por la variable independiente (efecto espacial exógeno).Ejercicio¿Cuál es el nombre de los modelos econométricos espaciales que incorporan dicho efecto?Una relación como la anterior quizá pueda ser modelada través de un modelo espacial de Durbin, \\(Y=\\rho WY+\\alpha 1n+X\\beta+WX \\theta+\\epsilon\\), puesto que la investigadora pudiera interesarle cómo los valores de las variables independientes en los espacios vecinos inciden en el fenómeno de interés.Así pues, la elección del tipo de modelo espacial depende, en lo fundamental, de dos elementos: ) la evidencia de asociación espacial entre el conjunto de variables disponibles y que se revela través del análisis exploratorio de datos espaciales (definición de criterios de vecindad, construcción de índices de Moran) y ii) los intereses y propósitos de la investigación.","code":"## `geom_smooth()` using formula = 'y ~ x'"},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"los-modelos-econométricos-de-error-espacial-y-rezago-espacial","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.2 Los modelos econométricos de error espacial y rezago espacial","text":"De toda la gama de modelos econométricos espaciales que es posible construir, aquí sólo nos ocuparemos de dos de ellos, el modelo de rezago espacial y el modelo de error espacial.Para ello, seguimos la ruta metodológica propuesta por Anselin y Rey (2014) que consiste en comenzar con un modelo lineal espacial estimado con mínimos cuadrados ordinarios (MCO) que luego es extendido con la incorporación de interacciones espaciales, ya sea incluyendo \\(\\rho\\), el coeficiente autoregresivo espacial o \\(\\lambda\\), el término de autocorrelación espacial, tal y como se muestra en la figura 5.2.\nFigura 5.2: De un modelo con MCO un modelo espacial\n","code":""},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"secuencia-general-para-la-estimación","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.2.1 Secuencia general para la estimación","text":"La estimación de un modelo econométrico de error o rezago espacial, siguiendo Anselin y Rey (2014), puede ser dividida en los siguientes pasos:Estimar un modelo clásico de regresión lineal con mínimos cuadrados ordinarios.Evaluar la presencia de autocorrelación espacial en los errores del modelo.Evaluar, alternativamente, la pertinencia de un modelo de rezago o de error espacial.Estimar el modelo de rezago y de error espacial y apuntar cuál de ellos capta mejor la relación propuesta.El punto ha sido abordado en el capítulo previo, sin embargo regresaremos ello más adelante de forma práctica para describir la secuencia general. Sobre el punto ii nos detendremos un momento para explicarlo con más detalle. Bien podrías pensar que la evaluación de la autocorrelación ha quedado plenamente cubierta en el capítulo 3 cuando nos referimos al análisis exploratorio de datos espaciales y usamos la de Moran; sin embargo, es necesario distinguir entre autocorrelación en la variable de interés y autocorrelación en los errores del modelo, lo primero lo hicimos en el citado capítulo, mientras que lo segundo corresponde la secuencia para la selección del modelo econométrico espacial.Si se ha encontrado evidencia de autocorrelación en la variable través de los instrumentos de exploración, como los diversos mapas y la de Moran, estos elementos nos llevan pesar que al usar dicha variable en un modelo econométrico lineal simple estimado con MCO, muy probablemente los términos de error estén correlacionados y, por tanto, se violaría dicho supuesto. Para verificarlo, ahora estimaremos la de Moran sobre los errores del modelo: en esto justamente consiste el paso ii.Ahora bien, si hallamos evidencia de que los errores del modelo lineal están autocorrelacionadaso, sabremos que lo mejor es estimar un modelo econométrico espacial. Pero, ¿cuál es mejor, el modelo de error espacial o el de rezago? Para ello recurriremos un estadístico denominado Prueba de los Multiplicadores de Lagrange o Prueba de Puntaje de Rao (Rao score test), que fue desarrollada por Anselin finales de la década de los ochenta del siglo pasado (Anselin Rey 2014: 105). Las pruebas nos permitirán decidir qué modelo es mejor, evaluando entre el modelo lineal y el de rezago, por un lado, y el modelo lineal y el de error, por otro.Así, cuando se evalúa entre el modelo lineal y el modelo de rezago lo que en el fondo hacemos es probar estadísticamente si \\(\\rho\\), el coeficiente autoregresivo espacial, realmente está presente en la relación propuesta. Recordemos la estructura del modelo de rezago:\\[\ny=\\rho Wy+X\\beta+u \n\\]De modo que, formalmente, el juego de hipótesis evaluar y que nos permitirán decidir entre el modelo lineal y el modelo de rezago es:\\[\n\\begin {aligned}\nHo &: \\rho=0 \\\\\nHa &: \\rho\\=0 \\\\\n\\end {aligned}\n\\]Según lo que nos indique el multiplicador de Lagrange y su valor-p asociado, determinado nivel de significancia, rechazar la hipótesis nula significará que, entre el modelo lineal y el modelo de rezago, el mejor modelo es un modelo de rezago espacial.En esta misma línea de razonamiento, del modelo de error nos interesa saber si \\(\\lambda\\) es diferente de 0 y, por tanto, si juega algún papel relevante en la relación propuesta. Recuerda que el modelo de error luce como:\\[\n\\begin{aligned}\nY &= \\alpha1_N+X\\beta+u  \\\\\nu &=\\lambda Wu+\\epsilon  \\\\\n\\end{aligned}\n\\]Formalmente, las hipótesis verificar son:\\[\n\\begin {aligned}\nHo &: \\lambda=0 \\\\\nHa &: \\lambda\\=0 \\\\\n\\end {aligned}\n\\]Igualmente, el estadístico usado para evaluar nuestras hipótesis es el multiplicador de Lagrange y que, dado determinado nivel de significancia, rechazar la hipótesis nula significa que, entre el modelo lineal y el modelo de error, el mejor modelo es uno de error espacial.Pero, ¿qué ocurre cuando en las pruebas anteriores son concluyentes? Es decir, cuando tanto la prueba sobre la versión de rezago y de error han resultado ambas significativas? Sin abudar en los detalles técnicos de esta situación, esto podría pasar porque el estadístico del multiplicador de Lagrange sobre el modelo de rezago, \\(LM_{\\rho}\\), es sensible la presencia autocorrelación espacial en el término de error, lo mismo que el estadístico del multiplicador de Lagrange sobre el modelo de error, \\(LM_{\\lambda}\\) que es sensible la presencia autocorrelación espacial en el rezago. Para decirlo de forma hipersimplificada, estas pruebas podrían dar un “falso positivo.”Exclusivamente cuando en ambos casos se rechazan las respectivas hipótesis nulas, es decir que \\(\\lambda\\=0\\) y \\(\\rho\\=0\\), habremos de recurrir las llamadas “versiones robustas” del multiplicador de Lagrange. Las versiones robustas de dichos indicadores implican “que el estadístico original es corregido mediante la influencia potencial de ‘otras’ alternativas” (Anselin y Rey, 2014: 105). Usar la versión robusta del multiplicador de Lagrange que evalúa entre la alternativa del modelo lineal y el modelo de rezago implica que dicha prueba determina con mayor precisión si hay presencia de autocorrelación y esta es captada través del coeficiente autoregresivo espacial, excluyendo los efectos que podrían estar dados por el coeficiente de autocorrelación espacial; reciprocamente para la prueba en su versión robusta del multiplicador de Lagrange sobre la alternativa del modelo lineal respecto al modelo de error.Usemos una analogía para mejor comprender lo anterior. Imagina que intentas pasar por un colador un poco de harina que tiene algunos terrones porque ha estado algún tiempo en la despensa. Si los orificios del colador son demasiado grandes dejarán pasar los terrones y pensaríamos que nuestra harina estaba libre de terrones. obstante, si usamos un colador con orificios más finos, los terrones pasarán y sabremos que la harina, en efecto, tenía terrones. Algo semejante ocurre con los multiplicadores de Lagrange en sus versiones estándar y robustas: los orificios del colador en las pruebas robustas son más finos, permitiéndonos discriminar con mayor seguridad entre uno y otro modelo, para tener evidencia concluyente.Así, por ejemplo, si el multiplicador de Lagrange en su versión robusta que evalúa entre un modelo lineal y un modelo de rezago sostiene que el mejor modelo es el modelo de rezago, mientras que el multipilcador de Lagrange en su versión robusta que evalúa entre un modelo lineal y un modelo de rezago sostiene que el mejor modelo es el modelo lineal, la conclusión la que arribamos, la luz ofrecida por ambas pruebas, es que el mejor modelo es el modelo de rezago espacial. Para simplificar, basta recordar lo recomendado por Anselin (2005, 197): “lo importante recordar (en la selección entre modelos espaciales) es considerar las versiones robustas de los estadísticos (los multiplicadores de Lagrange) sólo cuando las versiones estándar son ambas significativas.Finalmente, ya que tenemos claro qué modelo capta mejor la relación propuesta, procederemos estimar y analizar dicho modelo. Todos los pasos que hemos tratado de describir hasta aquí, son sintetizados por Anselin y Rey (2014) en el siguiente esquema:\nFigura 5.3: Árbol de decisión de regresión espacial, Anselin y Rey (2014)\nEn las siguientes sección se ilustra cómo seguir esta hoja de ruta para estimar los dos modelos que incluyen efectos espaciales: el de error y el de rezago y decidir cuál es el mejor.","code":""},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"modelos-espaciales-en-r-un-ejemplo-para-el-valle-de-méxico","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.3 Modelos espaciales en R: un ejemplo para el Valle de México","text":"En esta sección exponemos cómo, través de algunos paquetes y funciones en R, podemos recorrer el camino propuesto en la sección anterior, que nos llevará decidir cuál es el mejor modelo que capta las interacciones que se dan en el espacio, uno de error o de rezago. obstante, antes e ello, aprovecharemos para hacer un brevísimo recordatorio de algunas de las herramientas mostradas en los capítulos previos, fin de mostrar su uso de forma integrada en el contexto del planteamiento de un modelo econométrico espacial.","code":""},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"recapitulación-la-necesidad-de-un-modelo-espacial","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.3.1 Recapitulación: la necesidad de un modelo espacial","text":"Antes de emprender la tarea de la estimación de los modelos espaciales siguiendo la secuencia anterior, llevaremos cabo un recuento de diversos elementos que nos permitirán poner en contexto el problema de la selección del modelo espacial y, al mismo tiempo, recapitular algunos de los tópicos tratados lo largo de este libro.Hemos estado analizando un conjunto de variables sociodemográficas y económicas con la intensión de indagar cuál es su relación con la situación de la pandemia por COVID19 durante la primera ola en una región del centro de México llamada Zona Metropolitana del Valle de México.En el primer capítulo de este material expusimos una serie de herramientas para tener una primera aproximación con el conjunto de datos. Esas herramientas, entre otros aspectos, integran un enfoque denominado Análisis Exploratorio de Datos (EDA) y tiene por objeto, antes de proponer cualquier modelo, conocer la estructura subyacente de la información e indagar en la asociación entre variables para responder y plantear algunas preguntas de carácter preliminar en torno al objeto de estudio construido.Por ejemplo, través de un diagrama de caja del número de casos positivos es posible observar la distribución de la variable e identificar observaciones típicas, es decir, valores extremadamente altos o bajos:Los puntos en color rojo son observaciones extremadamente altas, en el rango de la información utilizada. Del mismo modo, una gráfica de la densidad de probabilidad de la variable nos indica cómo ésta se distribuye:La “cola” de esta gráfica que se extiende la derecha (sesgo positivo) indica, como la anterior, que hay valores muy altos. Esto nos lleva pensar en las razones de este comportamiento, ¿por qué hay unidades territoriales, municipios en este caso, que presentan tal magnitud de casos? El EDA, justamente, busca despertar nuestra curiosidad e imaginación en torno al fenómeno analizado.Otro tipo de pregunta que nos permite responder de forma preliminar el EDA es, por ejemplo, la siguiente: ¿existe asociación entre el número de casos positivos por COVID19 y el acceso los servicios de salud? Una manera sencilla de acercarse la respuesta de tal pregunta es través de un diagrama de dispersión:El gráfico anterior indica que, en efecto, parece haber una relación lineal positiva entre ambas variables, ¿por qué crees que pueda existir esta relación, qué la fundamenta? Habrá pues que indagar y reflexionar sobre las posibles explicaciones que se hayan dado una relación como la identificada.Por otro lado, el interés central de este libro ha sido brindar algunas herramientas básicas para el tratamiento de información espacial: el espacio importa, dónde ocurren los fenómenos y cómo se comportan o distribuyen en el territorialmente es un elemento clave para comprender tales fenómenos cabalmente.Por ejemplo, ¿hay algún patrón en cómo se dieron los casos positivos por COVID19 en el Valle de México? Para tener una primera idea de esto, podemos construir un mapa, como los expuestos en el capítulo 2, cuando tratamos con los mapas de coropletas:Es valido sostener, al menos inicialmente que, en efecto, hay una concentración de los casos positivos en la zona sur del área estudiada, la Ciudad de México, donde se muestran los colores en azul más intenso. Esta concentración identificada visualmente, nos lleva pensar en la necesidad de evaluar formalmente si los datos sobre los casos positivos por COVID19 muestran patrones sistemáticos de agrupamiento en el territorio. Para ello, podemos recurrir la prueba de autocorrelación espacial que vimos en el capítulo 3, la de Moran. Como recordarás del capítulo 3, para construir el estadístico de asociación espacial de Moran hay que definir una estructura espacial, es decir, definir entre qué unidades espaciales hay una relación de vecindad, misma que es recogida en la llamada matriz de pesos espaciales. Con una relación de vecindad de tipo reina, la de Moran y el diagrama de Moran sobre el número de casos positivos luce como:La información que nos proporciona la de Moran indica que los casos positivos por COVID19 en el Valle de México muestran un patrón de agrupamiento sistemático en las unidades territoriales analizadas: el coeficiente es positivo, relativamente alto y estadisticamente significativo. Así pues, un nivel aún exploratorio, tanto el mapa como la de Moran dan cuenta de que la variable explicar presenta autocorrelación espacial.Hay varios elementos interesantes respecto la evidencia anterior. El primero es que lo más probable es que, si estimamos un modelo econométrico lineal buscando explicar los casos positivos, se violaría el supuesto de independencia en los términos de error de la regresión que exige un modelo lineal clásico, por lo sería el más apropiado para explicar dicho fenómeno. El segundo, y quizá el más interesante, es que debemos reflexionar sobre el hecho de que la variable explicar presenta patrones de asociación en el espacio: ¿por qué los casos positivos de COVID19 son más altos en algunos grupos de municipios de la ciudad que en otros?La respuesta esta pregunta es compleja y, desde luego, requiere una reflexión cuidadosa sobre las teorías y trabajos previos de especialistas en el tema que han tratado sobre los patrones espaciales en la transmisión de una enfermedad, como el clásico trabajo de Cliff y sus colaboradores (Cliff et al. 1981) sobre la dinámica espacial de las pandemias en Islandia, aunque ciertamente se ha aceptado que las epidemias pueden y deben analizarse desde una perspectiva espacial (Rezaeian et al. 2007).El caso es que hay tanto evidencia en los datos, como en trabajos previos que nos llevan pensar en la pertinencia de un modelo econométrico espacial que incorpore el comportamiento que hemos identificado en un nivel exploratorio: la autocorrelación espacial en los casos positivos por COVID19. Ahora entonces, cuál es el mejor modelo que podemos usar, ¿uno de rezago espacial o de error espacial? Seguiremos pues la secuencia de la sección anterior para decidir, mientras ilustramos el proceso través de los paquetes y funciones que nos ofrece R.","code":"## `geom_smooth()` using formula = 'y ~ x'## Reading layer `covid_zmvm' from data source \n##   `C:\\Repositorios\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 76 features and 57 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5\n## Projected CRS: Lambert_Conformal_Conic\ntmap::tm_shape(base) +\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", \n                style = \"quantile\", \n                n=6,\n                title = \"Casos positivos COVID19\",\n                \n                palette = \"GnBu\",\n                legend.format = list(text.separator=\"-\",digits=2))+\n  tmap::tm_layout(main.title = \"Positivos\", main.title.position = \"center\", title.size = 0.5)## \n##  Moran I test under randomisation\n## \n## data:  covid_zmvm$pos_hab  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = 8.8625, p-value < 2.2e-16\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##       0.656334129      -0.013333333       0.005709563"},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"estimación-de-los-modelos-espaciales","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.3.2 Estimación de los modelos espaciales","text":"Ahora que tenemos claro que requerimos un modelo espacial si lo que prentedemos es explicar los casos positivos por COVID19 en el Valle de México, veamos cómo llevar cabo esto través de R.Los paquetes utilizados para la estimación de modelos econométricos espaciales son:spatialreg: un paquete que contiene las funciones para calcular los modelos econométricos espaciales que hemos comentado antes.spdep: es el mismo paquete que usamos en el capítulo 3 y que contiene funciones para evaluar dependencia espacial y construir matrices de pesos espaciales.Como es usual, la instalación desde la consola puede hacerse con:Además, echaremos mano de otros tantos paquetes previamente trabajados, por lo que deberán ser llamados junto con los recién instalados:La base de datos usar en este ejercicio es la misma que hemos usando antes y que puedes descargar aquí. Para cargar nuestra base de datos:De acuerdo lo dicho en las secciones previas, primero debemos estimar un modelo clásico de regresión lineal con mínimos cuadrados ordinarios. Usaremos el mismo modelo del capítulo sobre regresión lineal en su versión simple: un modelo que busca explicar los casos positivos por COVID-19 por cada 1 mil habitantes (pos_hab) mediante la población con acceso servicios de salud (ss), la especificación de dicho modelo lineal luce como:\\[\nposhab_i=\\beta_0+\\beta_1ss_i+u_i\n\\]\nLa variable que buscamos explicar con nuestro modelo será casos positivos COVID19 por cada 1 mil habitantes, pos_hab, explicada través del porcentaje de población con acceso servicios de salud, ss. La estimación del modelo lineal, tal y como se expuso en el capítulo 4, es:Ejercicio¿Qué puedes decir sobre la significancia individual del coeficiente estimado? ¿Qué sobre la bondad de ajuste del modelo?Una vez que hemos estimado el modelo lineal través del MCO, hemos completado el paso de nuestra secuencia general. Debemos ahora evaluar la presencia de autocorrelación en los térinos de error del modelo, es decir, el paso ii. De nueva cuenta insistimos en la diferencia entre evaluar la autocorrelación sobre la variable de interés (lo que hicimos hace un momento con la de Moran y su diagrama sobre la variable pos_hab) y evaluar la autocorrelación sobre los errores del modelo, lo que haremos en este momento.Verificamos la presencia de autocorrelación espacial de los errores través de la de Moran, lo que nos permitirá responder la pregunta ¿los errores de nuestro modelo están correlacionados? Para ello, nos servimos de la función moran.test() del paquete spdep. Como recordarás, la función requiere dos argumentos: el vector número evaluar (en este caso los errores del modelo MCO) y la estructura espacial.En el capítulo relativo al análisis exploratorio de datos espaciales vimos que la definición de una estructura espacial atraviesa por la construcción de una matriz de pesos espaciales, recuerda que través de la función poly2nb() del paquete spdep construimos nuestra lista de vecinos con base en los criterios de contigüidad de tipo reina (queen=TRUE):Ahora, el objeto de tipo nb debe ser trasformado uno que contenga los pesos espaciales, es decir, la matriz ponderada: un objeto de tipo listw:Ahora, alculemos la de Moran sobre los términos de error:EjercicioCon base en los resultados de la de Moran, ¿dirías que existe autocorrelación espacial en los errores?Lo usual es que cuando la variable de interés presenta autocorrelación, esta característica termine por afectar los resultados de la estimación por mínimos cuadrados ordinarios, tal y como ha ocurrido en este caso en donde los errores del modelo lineal están correlacionados.Así pues, es necesario, siguiendo la ruta marcada por Anselin y Rey (2014), evaluar, alternativamente, la pertinencia de un modelo de rezago o de error espacial, es decir, el paso iii. Como dijimos antes, esto lo haremos con base en las pruebas sobre las dos alternativas propuestas (modelo de rezago o modelo de error), través de los multiplicadores de Lagrange, primero observando sus versiones estándar y, sólo si éstas son concluyentes, después, sus versiones robustasLas pruebas son llamadas con la función lm.LMtests() del paquete spdep que requiere de los siguientes argumentos: el modelo lineal evaluar (model=), la estructura espacial (listw=) y los pruebas solicitadas (test=):Revisa la ayuda relacionada con la función lm.LMtests() para familiarizarte con todos sus argumentos. En este caso, solicitamos las 4 pruebas de las que hicimos mención previamente: la prueba del multiplicador de Lagrange sobre la alternativa del modelo de rezago (LMlag), la prueba del multiplicador de Lagrange sobre la alternativa del modelo de error (LMerr), así como sus respectivas versiones robustas (RLMlag,RLMerr).Interpretemos los resultados. Primero, contrastemos las versiones estándar de los multiplicadores, que aparecen el la siguiente tabla:Tabla 1. Multiplicadores de Lagrange, versiones estándarTanto el estadístico vinculado al modelo de rezago, como el del error son estadísticamente significativos, es decir, en cada caso se rechaza la hipótesis nula de que \\(\\rho=0\\) y \\(\\lambda=0\\), respectivamente, por lo que tanto un modelo de rezago como de error son plausibles: tenemos pues, con las versiones estándar, pruebas concluyentes de qué modelo es mejor. En este caso, hay que mirar los resultados de las versiones robustas para tomar la decisión final:En este caso, sólo el estadístico vinculado al modelo de rezago, RLMlag, resulta significativo, es decir, se rechaza la hipótesis nula de que \\(\\rho=0\\); en tanto, para el caso del estadístico vinculado al modelo de error, RLMerr, podemos rechazar la hipótesis nula de que \\(\\lambda=0\\). la luz de la evidencia anterior, la mejor alternativa de modelo econométrico espacial es el modelo de rezago, por las siguientes razones:La variable explicar mostró, través de mapas de clasificación común y elestadístico de Moran, evidencia de autocorrelación espacial.Los términos de error del modelo lineal mostraban también autocorrelación, por lo que dicho modelo viola, al menos, uno de sus supuestos.Las versiones robustas de los multiplicadores de Lagrange mostraron que, entre un modelo MCO y uno de rezago, el mejor era el de rezago, y que entre un modelo MCO y uno de error es mejor el de MCO, pero por las razones y ii, y por la versión robusta sobre el modelo de rezago, resulta que el mejor modelo es el de rezago espacial.Revisa de nueva cuenta la figura 5.3 para mejor entender nuestra decisión. Finalmente, los tanto el modelo de rezago como el de error serán estimados, con ello, llegamos al paso iv y habremos terminado la secuencia.","code":"\n#Instalar dos paquetes\ninstall.packages(c(\"spdep\", \"spatialreg\" ))\n#Paquetes recién instalados\nlibrary(spdep)\nlibrary(spatialreg)\n#Otros paquetes usados\nlibrary(rgdal)\nlibrary(sp)\ncovid_zmvm <-rgdal::readOGR(\"base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp\")## Warning: OGR support is provided by the sf and terra packages among others## Warning: OGR support is provided by the sf and terra packages among others## Warning: OGR support is provided by the sf and terra packages among others## Warning: OGR support is provided by the sf and terra packages among others## Warning: OGR support is provided by the sf and terra packages among others## Warning: OGR support is provided by the sf and terra packages among others## Warning: OGR support is provided by the sf and terra packages among others## OGR data source with driver: ESRI Shapefile \n## Source: \"C:\\Repositorios\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp\", layer: \"covid_zmvm\"\n## with 76 features\n## It has 57 fields\nmodelo_mco <- stats::lm (formula=pos_hab ~ ss, data = covid_zmvm)\nsummary(modelo_mco)## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.4391 -2.7744 -0.6709  1.5324 14.9969 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -19.862      5.809  -3.419  0.00102 ** \n## ss            39.490      8.516   4.637 1.49e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.572 on 74 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2147 \n## F-statistic:  21.5 on 1 and 74 DF,  p-value: 1.488e-05\nmTRUE <- spdep::poly2nb(covid_zmvm)\nmTRUE.pesos<-spdep::nb2listw(mTRUE)\nspdep::moran.test(modelo_mco$residuals, mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  modelo_mco$residuals  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = 6.9825, p-value = 1.45e-12\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##       0.512928287      -0.013333333       0.005680411\nspdep::lm.LMtests(model=modelo_mco, listw=mTRUE.pesos, test=c(\"LMlag\",\"LMerr\", \"RLMlag\",\"RLMerr\"))## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## LMlag = 55.313, df = 1, p-value = 1.028e-13\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## LMerr = 42.733, df = 1, p-value = 6.275e-11\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## RLMlag = 13.358, df = 1, p-value = 0.0002573\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## RLMerr = 0.77786, df = 1, p-value = 0.3778"},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"modelo-de-rezago-espacial","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.3.2.1 Modelo de rezago espacial","text":"El modelo de rezago espacial se invoca con al función lagsarlm() del paquete spatialreg en la que la estimación corre cargo del método de máxima verosimilitud. En la función se especifica el modelo (formula=), la base de datos (data=) y la estructura espacial (listw=).De la salida debemos llamar la atención sobre los coeficientes de interés: \\(\\rho\\), el coeficiente autoregresivo espacial, que en este caso es positivo, significativo y alto (0.83). ¿Qué es lo que dicho valor significa? La interpretación que podría darse este coeficiente es que cuando aumenta en una unidad el número de casos positivos en los municipios y alcaldías vecinas, el número de casos positivos por COVID19 por cada 1 mil habitantes en el municipio de interés aumentará en 0.83. Hemos pues encontrado evidencia de que hay interacciones espaciales entre el estado y dinámica de la pandemía por COVID19 en la región estudiada: lo que pasa en las áreas territoriales vecinas afecta la unidad territorial de referencia. Esto podría tener implicaciones sobre las medidas de política sanitaria en la contención de la enfermedad y en el posible desarrollo futuro de ésta.Por supuesto, hay que decir, que esto es sólo un ejercicio ilustrativo y que una interpretación más fina atraviesa por un concienzudo y sistemático estudio del carácter espacial de la transmisión de las enfermedades, lo que el autor de estas líneas está lejos de lograr, por lo que los resultados deben interpretarse con cautela y sólo con fines didácticos.Las líneas anteriores dan cuenta del elemento sustantivo del fenómeno de interés: la pandemia tiene un fuerte componente espacial y lo hemos logrado captar través del coeficiente autorregresivo espacial en el modelo de rezago. Pero, ¿con este modelo hemos logrado resolver el problema de la autocorrelación en los errores? Los términos de error del modelo de rezago que recién hemos estimado pueden ser visualizados, guardados y evaluados para ver si presentan autocorrelación:EjercicioCon base en los resultados de la de Moran sobre los términos de error del modelo de rezago, ¿dirías que persite el problema de autocorrelación espacial en los errores en el modelo de rezago?","code":"\nlibrary(spatialreg)\nrezago <- spatialreg::lagsarlm(formula=pos_hab ~ ss, data=covid_zmvm, listw=mTRUE.pesos)\nsummary(rezago)## \n## Call:spatialreg::lagsarlm(formula = pos_hab ~ ss, data = covid_zmvm, \n##     listw = mTRUE.pesos)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -5.72501 -1.71081 -0.31629  1.12054 11.64378 \n## \n## Type: lag \n## Coefficients: (asymptotic standard errors) \n##             Estimate Std. Error z value Pr(>|z|)\n## (Intercept)  -8.8906     3.4221 -2.5980 0.009377\n## ss           14.9811     5.1378  2.9159 0.003547\n## \n## Rho: 0.83629, LR test value: 62.655, p-value: 2.4425e-15\n## Asymptotic standard error: 0.056938\n##     z-value: 14.688, p-value: < 2.22e-16\n## Wald statistic: 215.73, p-value: < 2.22e-16\n## \n## Log likelihood: -191.0071 for lag model\n## ML residual variance (sigma squared): 7.1381, (sigma: 2.6717)\n## Number of observations: 76 \n## Number of parameters estimated: 4 \n## AIC: 390.01, (AIC for lm: 450.67)\n## LM test for residual autocorrelation\n## test value: 0.10973, p-value: 0.74045\nspdep::moran.test(rezago[[\"residuals\"]], mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  rezago[[\"residuals\"]]  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = -0.030802, p-value = 0.5123\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##      -0.015615667      -0.013333333       0.005490469"},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"modelo-de-error-espacial","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.3.2.2 Modelo de error espacial","text":"El modelo de error espacial es llamado con la función errorsarlm() y los argumentos son los mismos:Aquí, el parámetro de interés estimado es \\(\\lambda\\), el coeficiente de autocorrelación espacial, que resultó positivo, alto y estadísticamente significativo, también (0.87). En este caso, las posibilidades de interpretación sustantiva del coeficiente obtenido son mucho más limitadas, ya que los efectos espaciales están modelados por el término de error.Recordemos que los términos de error incluyen los efectos de todas aquellas variables que están siendo incluidas en nuestro modelo. En este caso, entonces, es posible brindar una lectura explícita de la variable o variable que determinan las interacciones espaciales. Sólo podemos contentarnos con decir que el problema de la dependencia entre los términos de error ha sido resuelto y que el componente espacial es considerablemente alto.Con la siguiente línea, se verifica que se ha resuelto el problema de la autocorrelación en los errores:Con estos resultados hemos completado la ruta más elemental para la estimación de modelos econométricos espaciales; sin embargo, debemos olvidar que existen todas las alternativas que apuntamos en la figura 5.1 y que fueron discutidas aquí.","code":"\nerror <-  spatialreg::errorsarlm(formula=pos_hab ~ ss, data=covid_zmvm, listw=mTRUE.pesos)\nsummary(error)## \n## Call:spatialreg::errorsarlm(formula = pos_hab ~ ss, data = covid_zmvm, \n##     listw = mTRUE.pesos)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -5.80220 -1.62230 -0.39269  1.11193 11.55180 \n## \n## Type: error \n## Coefficients: (asymptotic standard errors) \n##             Estimate Std. Error z value Pr(>|z|)\n## (Intercept) -0.66237    5.03921 -0.1314  0.89543\n## ss          12.54484    6.30980  1.9882  0.04679\n## \n## Lambda: 0.87101, LR test value: 58.512, p-value: 2.0206e-14\n## Asymptotic standard error: 0.050779\n##     z-value: 17.153, p-value: < 2.22e-16\n## Wald statistic: 294.22, p-value: < 2.22e-16\n## \n## Log likelihood: -193.0788 for error model\n## ML residual variance (sigma squared): 7.3069, (sigma: 2.7031)\n## Number of observations: 76 \n## Number of parameters estimated: 4 \n## AIC: 394.16, (AIC for lm: 450.67)\nspdep::moran.test(error[[\"residuals\"]], mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  error[[\"residuals\"]]  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = 0.18736, p-value = 0.4257\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##      0.0005636417     -0.0133333333      0.0055017348"},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"reflexiones-finales","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.4 Reflexiones finales","text":"Hay una serie de tópicos que han quedado fuera de estas notas, tales como los modelos que buscan resolver la heterogeneidad espacial, aquellos que atienden la violación del supuesto de homoscedasticidad en las perturbaciones, o bien, la incorporación de la dimensión temporal nuestro análisis, través de instrumentos como los modelos de panel, por lo que este material es sino sólo un abordaje introductorio, una invitación que profundices por tu propia cuenta en el manejo de las herramientas para el tratamiento de datos espaciales.Cerramos este libro recomendado una serie de materiales para cada uno de los capítulos que hemos tratado aquí.Para el Capítulo 1, donde tuvimos oportunidad de presentar los elementos básicos de R y RStudio mediante el análisis exploratorio de datos, recomendamos consultar el curso gratuito ofrecido por la Universidad Nacional Autónoma de México través de la plataforma Coursera Introducción Data Science: Programación Estadística con R, así como la versión en castellano del excelente libro de Wickham y Grolemund R para Ciencia de Datos.Para el Capítulo 2, centrado en la elaboración de mapas coropléticos, hay varios materiales que puedes consultad, como blogs o los servidores que contiene ejercicios y prácticas, como RPubs, aunque también existen los materiales desarrollados por los propios autores de la paquería, aunque están en castellano.En el Capítulo 3, que trató sobre los elementos básicos del análisis exploratorio de datos espaciales, existen los materiales desarrollados por los colegas del Centro de Estudios de Desarrollo Regional y Urbano Sustentable quienes, en su canal de YouTube, desarrollan una serie de prácticas introductorias de exploración de información espacial, aunque con otro Softeare (GeoDa).Para el Capítulo 4 y Capítulo 5](https://jaime-pru.github.io/Analisis-de-datos-espaciales/%C3%A1lisis-espacial-ii-modelos-econom%C3%A9tricos-espaciales.html) existe un sin número de materiales para el análisis de regresión, como el libro electrónico de Luis Quintana y Miguel . Mendoza “Econometría Aplicada Utilizando R,” que puedes descargar libremente desde aquí, del que el último capítulo aborda el tema de econometría espacial.Esperamos que estas notas contribuyan alimentar tu curiosidad sobre el tratamiento y análisis de información espacial y que sean un primer paso en tu formación como profesional.","code":""},{"path":"sobre-el-autor.html","id":"sobre-el-autor","chapter":"Sobre el autor","heading":"Sobre el autor","text":"Jaime . Prudencio Vázquez ORCIDGoogle Académico es docente universitario desde 2013 en las Área de Investigación, Métodos cuantitativos, Economía Política y Economía Regional. Cursó sus estudios de maestría y doctorado en la Universidad Nacional Autónoma de México, en el campo de Economía Urbana y Regional.Ha impartido talleres sobre análisis de datos espaciales y econometría espacial, tanto en el ámbito universitario como en el marco del Encuentro Nacional sobre Desarrollo Regional de la Asociación Mexicana de Ciencias para el Desarrollo Regional (AMECIDER).Entre 2012 y 2015 colaboró como investigador asociado en el Centro de Estudios para el Desarrollo Alternativo, . C. Ha participado, primero como becario y luego como académico, en diversos proyectos de investigación gestionados por la Coordinación de Investigación Científica de la UNAM para instituciones como INFONAVIT, BANCOMEXT y el Consejo de Investigación y Evaluación de la Política Social (CIEPS) del Estado de México.Actualmente es profesor visitante en el Departamento de Economía, Área de Relaciones Productivas en México de la Universidad Autónoma Metropolitana Unidad Azcapotzalco con actividad docente en la Licenciatura en Economía en el Área de Concentración de Economía de la Innovación: empresas, redes y territorio.Entre sus intereses de investigación se encuentran: Productividad laboral de la manufactura y desarrollo regional; Ciudades, economías de aglomeración y productividad laboral; Disparidad regiones y su medición, temas en los que cuenta con trabajos publicados.Correo electrónico: japv@azc.uam.mx. Twitter.El autor agradece Montserrat Romero Martínez (vmrm@azc.uam.mx) y Alvaro Martínez Rodríguez (amr@azc.uam.mx), ayudantes en el Área de Relaciones Productivas, quienes se encargaron respectivamente de la revisión de los materiales preliminares de este libro y de su edición para su publicación en línea con Bookdown en GitHub.Ciudad de México, enero de 2022.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
