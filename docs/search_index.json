[["index.html", "Análisis de datos espaciales con R Análisis de datos espaciales con R", " Análisis de datos espaciales con R Jaime Alberto Prudencio Vázquez 2023-10-03 Análisis de datos espaciales con R Este obra está bajo una Licencia Creative Commons Atribución-CompartirIgual 4.0 Internacional . "],["prólogo.html", "Prólogo", " Prólogo Las ciencias regionales, un conjunto amplio de especialidades que estudian los fenómenos sociales, económicos y políticos que tienen una dimensión espacial1, no son ya una disciplina menor, pasaron de ser una suerte de espacio para la experimentación de teorías y métodos que no tenían cabida en las corrientes principales de los cuerpos científicos plenamente reconocidos y se han ido abriendo paso y consolidando como una disciplina totalmente reconocida. Las ciencias regionales pueden ser entendidas como un conjunto de campos disciplinares en los que el punto en común es el interés dado al espacio (ya sea este entendido como regiones, ciudades, infraestructuras, medio ambiente) en los fenómenos sociales tales como las disparidades socioeconómicas, el crecimiento urbano y regional o el transporte y la logística (Nijkamp and Ratajczak 2015). Las ciencias regionales han ido adquiriendo “una amplia orientación multidisciplinaria sobre temas regionales y urbanos, combinando y siendo un complemento a la economía regional, geografía social y económica, economía urbana, ciencia del transporte, ciencia ambiental, ciencia política y teoría de la planificación” (Fischer and Nijkamp 2014). La realidad es que las ciencias regionales han evolucionado como un campo de investigación en pleno derecho. Tras un largo recorrido que incluso se podría remontar a los inicios de la economía como ciencia, comenzando con los trabajos de Adam Smith que trataban algunos elementos relacionados con la localización de actividades comerciales, pasando por los estudios clásicos de Von Thünen, Weber, Lösch (todos ellos relacionados con la denominada teoría de la localización), hasta llegar a la consolidación de las ciencias regionales como disciplina gracias al impulso de Isaard que trata de brindar una perspectiva teórica y metodológica coherente con un fuerte soporte empírico desde una perspectiva multidisciplinaria. Las contribuciones de W. Isaard abarcan una multitud de campos tales como la ecología, el estudio de los transportes e incluso el manejo de conflictos sociales. Si bien las ciencias regionales abarcan múltiples líneas temáticas, hay dos de ellas que se constituyen en tópicos clásicos y que podría denominárseles corriente principal de las ciencias regionales: el estudio de las fuerzas de aglomeración y los determinantes de la localización de la actividad. Buena parte de las investigaciones de estos ejes buscan responder a preguntas como ¿por qué las actividades económicas no se distribuyen de forma homogénea en el espacio?, o bien, ¿qué hace que determinadas actividades se localicen en unos sitios y no en otros? En su “Manual de Ciencias regionales” (Handbook of Regional Science, Fischer y Nijkamp señalan que quizá sean dos los elementos clave que constituyen a la ciencia regional: un enfoque multidisciplinario y un fuerte análisis cuantitativo2. Respecto al primer elemento, si bien es cierto que la economía espacial, urbana y regional, han representado pasos alentadores hacia la aproximación del economista con otros científicos sociales, aquellos no suelen mirar hacia otras disciplinas con la frecuencia que exigen las problemáticas sociales. De este modo, aún sigue siendo necesario construir alternativas de formación en la fase final de los estudios de licenciatura que busquen romper la endogamia de la profesión. En tanto, sobre el enfoque cuantitativo que caracteriza a las ciencias regionales, es cierto que en todos los planes de estudio de economía encontramos un sólido repertorio de instrumentos de carácter cuantitativo: matemáticas, estadística y, por supuesto, econometría. Además, resulta indudablemente que el soporte empírico que caracteriza a las ciencias regionales requiere un tipo particular de datos, los datos espaciales, es decir datos que hacen explícita la ubicación de los fenómenos estudiados en la superficie terrestre, y por tanto, de herramientas específicas para el tratamiento de dichos datos. No obstante, las herramientas requeridas para el análisis de la realidad desde una perspectiva espacial siguen siendo escasas dentro de la formación a nivel de licenciatura. Estas notas buscan ser una contribución, por mínima que esta sea, para subsanar dicha situación. Este trabajo está, por tanto, fundamentalmente dirigido a las estudiantes de licenciatura en economía y campos afines interesados en el desarrollo de habilidades técnicas para el análisis cuantitativo que exigen las ciencias regionales y el enfoque espacial de la economía. Así pues, el objetivo de este libro es guiar a la estudiante, desde un enfoque fundamentalmente práctico, en el conocimiento y manejo de técnicas para la exploración, análisis y modelado de información espacial mediante el uso de R, un programa informático de software libre y lenguaje de programación enfocado en el análisis estadístico y visualización de información y RStudio un entorno de desarrollo integrado (IDE) desde donde se puede interactuar con R más eficientemente. Este libro se estructura en este momento, enero de 2023, en 5 capítulos, aunque se busca integrar gradualmente material extra. En el Capítulo 1 se presentan los elementos básicos de R y RStudio a través de la utilización del enfoque del análisis exploratorio de datos, es decir, introducimos de lleno a la estudiante en el uso del software para plantear y resolver preguntas relativas a la estructura de la información utilizada a través de diversas herramientas de visualización y manipulación de la información. En el Capítulo 2 se muestra cómo elaborar diversos tipos de mapas coropléticos y la enorme flexibilidad de personalización de estilos que tiene R para tal efecto. En el Capítulo 3 se presentan las herramientas para llevar a cabo un análisis exploratorio de datos espaciales donde la estudiante encontrará la manera de definir las interrelaciones que se dan en el espacio a través de la construcción de matrices de pesos espaciales y aprenderá sobre la autocorrelación espacial y sus implicaciones en el análisis de la información. En tanto, en el Capítulo 4, se presenta un repaso muy sintético de los modelos de regresión simple enfatizando el problema de la autocorrelación que se puede presentar cuando se estima un modelo lineal con datos espaciales. Finalmente, en el Capítulo 5, mostramos algunas de las diferentes alternativas de modelación econométrica espacial disponibles en R. Todos los ejercicios que se desarrollan en esta versión del libro corresponden a la realidad económica y social de la Zona Metropolitana del Valle de México, en la región centro de este país latonoamericano, y están disponibles para que las estudiantes puedan replicar todos los resultados aquí ilustrados. Así, se guia a la estudiante a través de la exploración de la información epidemiológica relacionada con la primera ola de la pandemia por COVID19 en relación con factores sociodemográficos y económicos de la zona metropolitana más grande de México, en la que residen más de 20 millones de personas. El libro se concibe como un proyecto que puede ser difundido y ampliado, por lo que está licenciado bajo una Licencia Internacional Creative Commons Attribution 4.0. Este proyecto es un proyecto vivo, en permanente construcción y modificación, por lo que todos los comentarios y observaciones serán bienvenidas. Si tienes alguna duda, comentario o encuentras algún error, házmelo saber. Una guía de cómo contribuir a este proyecto puede ser encontrada aquí. Finalmente, es necesario mencionar que este libro no es más que un esfuerzo de recopilación, sistematización y aplicación que se ha hecho a partir de infinidad de materiales que la activa comunidad de R, interesada en el análisis espacial en México y el mundo, comparte desinteresadamente a través de internet. Creemos que así debería ser siempre el conocimiento: libre y abierto, como el software que es usado aquí. Así pues, si algún mérito tiene este material es justamente su sentido didáctico que confiamos permita contribuir a la formación de profesionales interesandas en las ciencias regionales y en la mejora de las condiciones de vida de las mayorías. References "],["r-una-introducción-desde-la-exploración-de-información.html", "1 R: Una introducción desde la exploración de información 1.1 R: Lenguaje de programación y plataforma de análisis 1.2 Códigos, objetos, paquetes 1.3 Cargar bases de datos 1.4 Exploración inicial a través de gráficos 1.5 Manipulación de la información", " 1 R: Una introducción desde la exploración de información En este capítulo buscamos cubrir dos objetivos. El primero corresponde a que la estudiante se familiarice con los diversos elementos que componen la intefaz de trabajo en RStudio y conozca la definición de objeto y sus tipos, en el contexto de los lenguajes de programación. El segundo es que conozca cómo desarrollar un análisis exploratorio de datos, a través de un tipo de objetos específicos (los dataframe). 1.1 R: Lenguaje de programación y plataforma de análisis R es un lenguaje de programación orientado a objetos, libre y de código abierto y además un ambiente enfocado al análisis estadístico y gráfico. Que sea libre significa, a diferencia de otros programas, que cualquier persona puede usarlo, redistribuirlo o modificarlo, sin necesidad de contar con una licencia pago. R es un entorno de software libre para computación estadística y gráficos. Compila y se ejecuta en una amplia variedad de plataformas UNIX, Windows y MacOS. Fuente:  http://cran.r-project.org Usar R en lugar de otras plataformas que requieren licencia pago es recomendable al menos por tres razones: Es libre: en contraste con otros populares programas informáticos como lo es STATA, del que una licencia sencilla ronda los 100 dólares por año. R no es sólo un software, también es un lenguaje de programación; así, al aprenderlo se obtiene un resultado doble. De modo que cuando llegues a manejarlo con profundidad, puedes crear tus propias librerías. No sólo sirve para hacer análisis gráfico y estadístico, con el puedes hacer páginas web, ser usado como Sistema de Información Geográfica e incluso para escribir libros, como éste que ahora estas leyendo. Es un lenguaje con un amplio soporte, es decir, con multitud de guías y materiales para el autoaprendizaje. A lo largo de este libro usaremos R a través de RStudio, un entorno de desarrollo integrado o IDE por sus siglas en inglés. Así que, lo primero será descargar e instalar R y luego descargar e instalar RStudio. R puede ser descargado de aquí. Al seleccionar “download R” hay que elegir cualquier CRAN Mirror (del inglés Comprehensive R Archive Network que son repositorios que contienen una copia del código base de R y sus paquetes). México tiene dos, puedes elegir cualquiera. En tanto, RStudio puede ser descargado de aquí. Ve a la sección “Download” en la parte superior y elije la versión gratuita. El navegador debería seleccionar la versión adecuada para tu equipo, sino es así, selecciona del listado que aparece en la parte inferior. También puedes consultar este video para seguir paso a paso la instalación de ambos programas. Al abrir RStudio se mostrará una pantalla como la que aparece en la Figura 1.1. La pantalla principal de RStudio está dividida en cuatro partes. En caso de que el recuadro superior izquierdo no aparezca, da clic en el ícono de la hoja en blanco con el signo de más (New file, parte superior izquierda de la pantalla) para que se despliegue. Figura 1.1: Panel principal Las citadas cuatro secciones son: Fuente u origen (source): esta sección, zona superior izquierda, aparecerá una vez que des clic en New script. Aquí podrás escribir las líneas de código de tu proyecto y guardarlas para consultarlas cuando lo desees, visualizarás tus bases de datos organizadas en pestañas y varios otros elementos. Consola: localizada en la parte inferior izquierda, es donde aparece el puntero o prompt (&gt;); este es el espacio en el que se insertarán o escribirán las líneas de código para ser ejecutadas por R y en donde se mostrarán parte de los resultados. Cuando el prompt aparezca, R está listo para recibir tus instrucciones. Archivo: en el espacio inferior derecho aparecen una serie de pestañas que muestran las carpetas y archivos de tu equipo, las gráficas realizadas y los paquetes disponibles, así como la ayuda de R y de sus paquetes. Ambiente: por último el área superior derecha, mostrará las variables creadas, bases de datos cargadas y todos los objetos activos en la sesión de R. Esta sección contiene otras dos pestañas: historial y conexiones. La primera ofrece una memoria de todas las instrucciones escritas en la consola, en tanto, la segunda le será útil cuando haga trabajo colaborativo a través de, por ejemplo, GitHub. 1.2 Códigos, objetos, paquetes 1.2.1 Código Los programas informáticos están construidos a partir de código, éste puede ser entendido como un lenguaje a partir del cual se dan las instrucciones a la computadora para que realice alguna acción. Como en todo lenguaje, un aspecto sustantivo es la sintaxis y la ortografía, es decir, el orden y la corrección en la forma en que uno se comunica con el sistema de cómputo, de modo que si se escribe alguna instrucción de forma incorrecta o incompleta (faltas de ortografía o errores de sintaxis) la instrucción no se ejecutará. En este sentido, se debe señalar que R es sensible al uso de mayúsculas y minúsculas, esto quiere decir para nuestro programa no es lo mismo Árbol y árbol. Además, en este programa constantemente estará usando paréntesis y comas en las instrucciones, por lo que hay que poner especial atención para no olvidar colocar alguno de ellos en la sentencia escrita. Otro elemento importante son las llamadas “palabras clave”, Keywords. Éstas son palabras reservadas, es decir, palabras con un significado especial para R y que hacen referencia a instrucciones que pertenecen a la estructura base del programa R por lo que no está recomendado su uso. Algunos ejemplos de palabras reservadas son son: if, else, break. Además, el nombre de las variables no debe comenzar con caracteres especiales (&amp;,#,$,%) o números. 1.2.2 Objetos, operadores y tipos de variables Si bien R es tremendamente potente, puede incluso utilizarse como simple calculadora. Los operadores matemáticos que pueden emplearse en R se muestran en el cuadro 1.1: Cuadro 1.1 Operador Operación Ejemplo Resultado + Suma 5+3 8 - Resta 5-3 2 * Multiplicación 5*3 15 / División 5/3 1.66667 ^ Potencia 5^3 125 %% División entera 5%%3 2 De esta forma si colocas en la consola el número 5 seguido del signo aritmético correspondiente, por ejemplo un signo de más, +, y luego otro número, R devolverá el resultado de la operación: 5+10 ## [1] 15 En R, todos los elementos con los que se interactúa son denominados objetos (por eso se dice que es un lenguaje de programación orientado a objetos), los hay de distintas clases y cada clase se puede identificar por un nombre. Los objetos más comunes en R son: constantes, variables, vectores, listas, matrices y arreglos de datos (data frames). Estos últimos los hay en diferentes versiones dependiendo de algunas de sus características. Los objetos pueden contener diferentes tipos de información como: números enteros (integer), valores numéricos (numeric), números complejos (complex) o valores lógicos (logic) y de cadena/texto (character). Distinguir entre los tipos de información que contiene cada objeto es importante para conocer las operaciones que se pueden hacer entre ellos. Por ejemplo, no es posible hacer operaciones matemáticas con variables tipo character ya que no tiene sentido sumar aritméticamente “casa” y “azul”, pero sí es posible concatenarlos. Para crear objetos, en R se utiliza el signo “menor que” seguido de un guion para formar una suerte de flecha que apunta a la izquierda, &lt;-, éste singo es llamado operador de asignación y precisamente asigna contenido a objetos, es decir, con el operador &lt;- creamos un objeto que tendrá determinado contenido. Ve a la sección consola de tu entorno de trabajo e ingresa las siguientes líneas de instrucciones (o simplemente da click en el ícono de copiar que aparece en la esquina superior derecha del cuadro que contiene el código y pégalo en la consola): #Nota: el símbolo de # representa un comentario y no se ejecuta con el código x &lt;- 3 #Numérico y &lt;- &quot;Hola&quot; #Carácter z &lt;- FALSE # Lógico Tras haber escrito estas líneas de código notarás que en la sección superior derecha de RStudio aparecen, en la pestaña de ambiente, los tres objetos creados, cada uno con un nombre y contenido diferente entre ellos: x es un objeto numérico, y es una cadena de texto o carácter y z contiene un valor lógico. Basta con escribir el nombre del objeto en la consola para ver su contenido. Ahora, vuelve a crear un objeto de nombre x que contenga el resultado de la suma de 5+10: x &lt;- 5+10 Como es posible identificar, si en una sesión de R se utiliza dos veces el mismo nombre para una variable, se sobrescribe la información asignada a esa variable, es decir, sólo permanece el último valor asignado: x ya no contiene la primera asignación, 3, solamente la última, 15. Hay dos funciones muy útiles para conocer el tipo de objeto que tenemos en nuestro entorno de trabajo: la función class() y la función str(). La primera indica el tipo de objeto, en tanto, la segunda nos dice el tipo de objeto y su valor. En el siguiente ejemplo puedes ver que el objeto y es de tipo carácter y su contenido es la expresión “Hello World”. class(y) ## [1] &quot;character&quot; str(y) ## chr &quot;Hola&quot; 1.2.2.1 Vectores Los vectores son una forma de almacenar más de un elemento en un objeto y dichos elementos no tienen que ser necesariamente del mismo tipo, aunque es importante tener en cuenta que no es recomendable combinar diferentes tipos de objetos porque se pueden alterar las clases de cada uno de los elementos originales. Cuando se quiere crear un vector que contiene cadenas de texto, cada cadena (palabra u oración) se debe poner entre comillas, como se ilustra a continuación: x &lt;- c(&quot;a&quot;,&quot;v&quot;) x ## [1] &quot;a&quot; &quot;v&quot; El uso de vectores es útil para modificar las etiquetas de una gráfica o bien los encabezados de una tabla, ya que R tomará los valores de texto almacenados en el vector para utilizarlos en la forma deseada. Aquí se muestran otros tipos de objetos en un vector: x &lt;- c(&quot;a&quot;,&quot;v&quot;) x ## [1] &quot;a&quot; &quot;v&quot; x &lt;- c(TRUE,FALSE) x ## [1] TRUE FALSE x&lt;- c(4+5i,3+2i) x ## [1] 4+5i 3+2i Imagina que deseas crear un vector numérico con una secuencia numérica del 1 al 10. Para hacerlo, deberías proceder como: x&lt;-c(1:10) x ## [1] 1 2 3 4 5 6 7 8 9 10 Es decir, no fue necesario insertar en el vector cada uno de los 10 números, sino que la secuencia continua (del 1 al 10) se señaló con los dos puntos. 1.2.2.2 Matrices Las matrices son un tipo de objeto que se distingue porque entre sus propiedades está el de tener dimensión (filas y columnas). Se puede generar una matriz en R con la función matrix() y se procede como: m &lt;- matrix(data=1:6,nrow = 2,ncol = 3) Las funciones, tanto las previamente citadas como esta, requieren que el usuario indique argumentos que son los elementos que aparecen entre paréntesis. La función matrix() requirió indicar varios: los datos que forman el contenido que tendrá la matriz,data=, el número de filas , nrow= y el número de columnas, ncol=. Recuerda: toda función requiere especificar determinados argumentos para que pueda funcionar. Alternativamente se pueden construir matrices a partir de vectores. Para ello, se pueden usar las funciones cbind() y rbind(). Primero creamos dos vectores, x y y: x&lt;-1:3 x ## [1] 1 2 3 y&lt;-10:12 y ## [1] 10 11 12 La función cbind() permite “unir por columna” (column bind) y rbind() que “une por fila” (row bind). Así, con los vectores previamente construidos podemos tener dos matrices diferentes, esto dependerá si las unimos por fila: cbind(x,y) ## x y ## [1,] 1 10 ## [2,] 2 11 ## [3,] 3 12 o columna: rbind(x,y) ## [,1] [,2] [,3] ## x 1 2 3 ## y 10 11 12 1.2.2.3 Factores Son un tipo especial de vectores y son utilizados para representar información categórica, lo que permite organizarla en niveles para analizarla mejor. Para mejor entender esto, basta recordar la definición de variable. Las “variables” o “atributos” son medidas o características de los elementos que conforman la población. Las medidas son cuantitativas y las características son cuantitativas. Justamente los factores se refieren a variables que recogen cualidades. Por ejemplo, si la variable responde a la pregunta “¿el paciente recibió la vacuna contra COVID-19?” y hay dos posibles respuestas, es recomendable que en lugar de designar cada alernativa como 1 o 2 se usen variables categóricas como “Vacuna aplicada” y “Vacuna no aplicada”, es decir, es recomendable almacenar la información como factor: x &lt;- factor(c(&quot;Vacuna no aplicada&quot;,&quot;Vacuna no aplicada&quot;,&quot;Vacuna no aplicada&quot;,&quot;Vacuna no aplicada&quot;,&quot;Vacuna no aplicada&quot;)) x ## [1] Vacuna no aplicada Vacuna no aplicada Vacuna no aplicada Vacuna no aplicada ## [5] Vacuna no aplicada ## Levels: Vacuna no aplicada 1.2.2.4 Data frame o arreglo de datos Esta estructura de datos es la más usada para realizar análisis en R. Son estructuras de datos de dos dimensiones, es decir, están compuestas por filas y columnas. Los renglones de un data frame admiten datos de distintos tipos, pero sus columnas tienen la restricción de contener datos sólo de un tipo. Para comprender mejor esto piensa en un data frame como si de una hoja de cálculo se tratara: los renglones representan casos, individuos u observaciones, mientras que las columnas representan atributos, rasgos o variables. Una columna con la variable “ingreso” deberá ser del mismo tipo para todos los casos, por ejemplo un valor numérico, en tanto, para el caso o individuo 1 (fila 1) tendremos información relativa no sólo al ingreso, sino al sexo, estatura y edad, es decir, variables categóricas, numéricas y enteros, respectivamente. Cuando arribemos un poco más adelante al proceso de importación de información al entorno de trabajo se presentará cómo luce un arreglo de datos de estas características. 1.2.3 Paquetes Se dijo que R es un programa especializado en el análisis estadístico y la representación gráfica, pero R no sólo se limita a lo que ofrece cuando lo descarga por primera vez, es decir a su paquete base o paquete base. Por ejemplo, dentro del paquete base se cuenta con la función para calcular la media de una muestra, mean(), entre otras tantas. Uno de los elementos que hace de R una potente herramienta es la posibilidad de ampliar su potencial a través de la instalación de paquetes que expanden sus funciones básicas. Existen paquetes de R para múltiples campos disciplinares y especialidades, por ejemplo estas notas se sirven de los paquetes especializados en el análisis espacial, como se observará en los capítulos subsecuentes. De momento mencionemos sólo un par: tmap: ofrece un enfoque flexible, basado en capas y fácil de usar para crear mapas temáticos. spatialreg: paquete para la elaboración de regresiones con componentes espaciales. Los paquetes, que expanden las capaciades de la versión base de R, son elaborados por múltiples colaboradores. Para conocer quines están detrás de ello, podemos usar la función citation(), que nos muestra cómo dar crédito a los autores de los paquetes cuando los usamos. citation(&quot;tmap&quot;) ## To cite tmap/tmaptools in publications use: ## ## Tennekes M (2018). &quot;tmap: Thematic Maps in R.&quot; _Journal of ## Statistical Software_, *84*(6), 1-39. doi:10.18637/jss.v084.i06 ## &lt;https://doi.org/10.18637/jss.v084.i06&gt;. ## ## A BibTeX entry for LaTeX users is ## ## @Article{, ## title = {{tmap}: Thematic Maps in {R}}, ## author = {Martijn Tennekes}, ## journal = {Journal of Statistical Software}, ## year = {2018}, ## volume = {84}, ## number = {6}, ## pages = {1--39}, ## doi = {10.18637/jss.v084.i06}, ## } Para poder hacer uso de los paquetes que amplían el potencial de R es necesario descargarlos, instalarlos y, en cada sesión de trabajo, llamarlos. Para la descarga e instalación podemos ir a Archivo (File) en la cinta de menú que se localiza en la parte superior de tu entorno de trabajo y seleccionar tools/install package. A continuación, deberás colocar el nombre del paquete deseado y dar click en instalar. O bien, alternativamente, puedes ir a la sección de Archivo, en la zona inferior derecha, seleccionar la pestaña Packages y a continuación el ícono de Install. Una tercera manera de instalar paquetes es desde la consola (sección inferior izquierda), para instalar un paquete a la vez: install.packages(&quot;tmap&quot;) install.packages(&quot;spatialreg&quot;) O bien, varios paquetes a la vez: install.packages(c(&quot;tmap&quot;, &quot;spatialreg&quot;)) Tras el proceso de instalación, en tu consola, R informará sobre el resultado de la instalación y te ofrecerá algunos datos sobre la ubicación del paquete en tu equipo. Para poder hacer uso de los paquetes no basta con descargarlos e instalarlos, es necesario “llamarlos” en cada sesión de trabajo de R para ser utilizados. Para ello deberás usar la función library() y como argumento el nombre del paquete: library(tmap) 1.2.4 Solicitar ayuda de un paquete o función de R Cada uno de los paquetes y funciones de R está acompañado por materiales de referencia que explican con detalle su uso, así como los diferentes argumentos en el caso de las funciones. Para solicitar ayuda en R puedes recurrir a la función help() e indicar como argumento el nombre de la función: help(tmap) Alternativamente, para solicitar ayuda puedes escribir un signo de interrogación y el nombre del paquete o dos signos para una función: ?ggplot También puedes buscar ayuda de una función específica, por ejemplo, de las funciones para crear una matriz (matrix())o para calcular una media (mean()): help(matrix) help(mean) En la cinta de menú, en la sección ayuda, Help, encontrarás una serie de materiales muy útiles para familiarizarse con los paquetes instalados. Estos materiales reciben el nombre de “hojas de trucos”, Cheatsheet. Se recomienda ampliamente revisar cada una de ellas, incluidas las versiones en epañol. Además, R es un programa con un sin número de entusiastas usuarios y con un amplio soporte técnico por lo que cuando te encuentres con una dificultad para usar algún paquete o función puedes remitite al sitio de Ayuda del R, o bien, a alguno de los repositorios especializados para presentar y resolver dudas como Stackoverflow. Para ilustrar esto, ingresa al sitio de Stackoverflow y coloca en la búsqueda “histograma en R”. 1.3 Cargar bases de datos En R hay algunas bases de datos que acompañan los paquetes que han sido instalados y sirven para ilustrar su funcionamiento. Las bases precargadas se pueden observar con la siguiente función: data() Verás una nueva pestaña en la sección de Fuente con el nombre de las bases y una breve descripción. Para cargar alguna de ellas basta introducir su nombre como argumento de la función data(), por ejemplo: data(CO2) 1.3.1 Cargar un archivo de Excel (xls, xslx) tidyverse es un conjunto de paquetes diseñados especialmente para la Ciencia de Datos. Algunos de los paquetes de la familia tidyverse que usaremos aquí y un poco más adelante son: * ggplot2: es un sistema para crear gráficos basado en la llamada gramática de las gráficas. * dplyr: proporciona una serie de funciones para manipulación de datos. * readr: permite leer datos rectangualres provenientes de múltiples formatos. Como más adelante usaremos otros de los paquetes de la familia tidyverse instalaremos todos en este momento: install.packages(&quot;tidyverse&quot;) Si deseas aprender cómo usar con detalle los paquetes de la familia tidyverse y elementos básicos de Ciencia de Datos, el libro de Hadley Wickham y Garrett Grolemund, R for Data Science es una magnífica opción de la que existe una versión en castellano. Utilizando el paquete readxl de la familia tidyverse es posible cargar una base de datos en el formato de la popular hoja de cálculo de Microsoft Office. A partir de aquí introduciremos una notación particular al usar una función en R, lo que te permitirá recordar a qué paquete pertenece dicha función. La notación es: “paquete :: función()”, es decir, primero se colocará el nombre del paquete y, separado por dos pares de dos puntos, en el nombre de la función que pertenece a dicho paquete (resulta claro que los paquetes están entonces integrados por múltiples funciones). Sigamos los siguientes pasos para cargar la base de datos covid_zmvm.xlsx que contiene información de los casos positivos y defunciones por COVID19 en los municipios de la Zona Metropolitana del Valle de México durante la primera ola de la pandemia, entre marzo a septiembre de 2020, así como múltiples variables sociodemográficas y económicas. Llamemos específicamente al paquete que nos interesa: library(readxl) La función para cargar un libro de Excel es read_excel() y el argumento indispensable es la ruta o directorio donde está almacenado nuestro libro, el nombre y extensión del mismo, path. Creemos pues el objeto covid_zmvm: #Al hacer uso de la notación readxl::read_excel, no es necesario usar library(readxl). Sin embargo, por cuestiones pedagógicas si lo haremos covid_zmvm &lt;- readxl::read_excel(path=&quot;base de datos\\\\covid_zmvm.xlsx&quot;) Para que puedas cargar satisfactoriamente la base, deberás sustituir la ruta por el directorio en el que está almacenado el archivo en tu equipo de cómputo. Una función útil para generar la cadena de texto de la ruta es file.choose(), del paquete base de R. Lleva a la consola el código: base::file.choose() y presiona enter, verás que aparece un cuadro de diálogo en el que deberás seleccionar el archivo deseado y luego pulsar “abrir”. El resultado aparecerá en tu consola como una cadena de texto entre comillas. Usa esa información para leer la base covid_zmvm. Revisa la ayuda de la función read_excel() para comprender todos los argumentos con los que puede operar la función y que te permitirán personalizar la carga de la base de datos en caso de que tengas múltiples hojas en el libro de Excel o desees rangos personalizados para importar. Ejercicio ¿Quién es el autor o autora del paquete readxl? Descarga la hoja de trucos del paquete desde el sitio de tidyverse y responde, ¿el paquete sirve sólo para cargar información o es posible escribir y almacenar bases de datos con él? 1.3.2 Cargar archivos separados por comas (csv) Alternativamente, es posible que la base que deseemos cargar se encuentre en un formato diferente, por ejemplo, una base de datos con valores separados por comas (coma separate values, CSV). Para cargar una base en dicho formato, usaremos el paquete readr, que forma parte del paquete utils Llama el paquete a tu entorno de trabajo: library(readr) Con la función anterior, file.choose() obtendremos la cadena de texto que indica la ruta del archivo a cargar: ruta &lt;- base::file.choose() Y, finalmente, cargamos la información con la función read.csv() covid_zmvm &lt;- utils::read.csv(ruta) 1.3.3 Cargar bases con la funcionalidad importar Otra manera de cargar archivos de Excel o de texto (formato CSV) es ubicarnos en la sección de ambiente, ventana superior derecha, y seleccionar el ícono Import Dataset. Se desplegará un menú donde habremos de elegir el tipo de archivo que se desee importar. R no sólo permite importar archivos de texto o Excel, también bases de SPSS o STATA. Siguiendo con la ilustración relativa a libros de Excel, en la ventana que se despliega, habremos de señalar la ruta exacta o directorio donde está nuestro archivo, incluyendo el nombre y extensión de éste. Al pulsar en el botón actulizar (Update) se desplegara una vista previa de la base. Si aparece de forma correcta, seleccionaremos importar (Import), en caso contrario, se deberá modificar las opciones de importación. Después de seleccionar Import Dataset/From Excel y elegir la ubicación del archivo que deseas importar, deberías ver en tu pantalla una imagen como la de la Figura 1.2: Figura 1.2: Importar datos de excel 1.4 Exploración inicial a través de gráficos Ahora que has revisado algunas de las diversas maneras de cargar tu información al entorno de trabajo en R, es momento de comenzar a analizarla. Si importaste la base de datos a través de la función readxl::read_excel() notarás que el nuestra base covid_zmvm un objeto de tipo tbl_df que es un subtipo de data.frame o arreglo de datos como el que se mencionó antes. Las siguientes instrucciones nos ayudarán a conocer los nombres de las columnas, la estructura de los datos y la dimensión de la base: base::names(covid_zmvm) #Nombres de las variables ## [1] &quot;cvemun&quot; &quot;cve_mun&quot; &quot;nom_mun&quot; &quot;cve_ent&quot; &quot;nom_ent&quot; ## [6] &quot;nom_abr&quot; &quot;nom_zm&quot; &quot;cvm&quot; &quot;ext&quot; &quot;pob20&quot; ## [11] &quot;pob20_h&quot; &quot;pob20_m&quot; &quot;positivos&quot; &quot;defuncione&quot; &quot;pos_mil&quot; ## [16] &quot;pos_hab&quot; &quot;def_hab&quot; &quot;ss&quot; &quot;ppob_sines&quot; &quot;ppob_basi&quot; ## [21] &quot;ppob_media&quot; &quot;ppob_sup&quot; &quot;ocviv&quot; &quot;occu&quot; &quot;pintegra4_&quot; ## [26] &quot;pintegra6_&quot; &quot;pintegra8_&quot; &quot;ppob_5_o_m&quot; &quot;ppob_3_o_m&quot; &quot;ppob_1&quot; ## [31] &quot;ppob_1dorm&quot; &quot;ppob_2dorm&quot; &quot;ppob_3dorm&quot; &quot;pviv_ocu5_&quot; &quot;pviv_ocu7_&quot; ## [36] &quot;pviv_ocu9_&quot; &quot;analf&quot; &quot;sbasc&quot; &quot;vhac&quot; &quot;po2sm&quot; ## [41] &quot;idh2015&quot; &quot;im&quot; &quot;gm_2020&quot; &quot;grad&quot; &quot;grad_h&quot; ## [46] &quot;grad_m&quot; &quot;poind&quot; &quot;pocom&quot; &quot;poss&quot; &quot;tmind&quot; ## [51] &quot;tmcom&quot; &quot;tmss&quot; &quot;rmind&quot; &quot;rmcom&quot; &quot;rmss&quot; ## [56] &quot;den&quot; utils::str(covid_zmvm) #Estructura de la base de datos ## tibble [76 × 56] (S3: tbl_df/tbl/data.frame) ## $ cvemun : chr [1:76] &quot;09010&quot; &quot;09012&quot; &quot;09015&quot; &quot;09017&quot; ... ## $ cve_mun : chr [1:76] &quot;010&quot; &quot;012&quot; &quot;015&quot; &quot;017&quot; ... ## $ nom_mun : chr [1:76] &quot;Álvaro Obregón&quot; &quot;Tlalpan&quot; &quot;Cuauhtémoc&quot; &quot;Venustiano Carranza&quot; ... ## $ cve_ent : chr [1:76] &quot;09&quot; &quot;09&quot; &quot;09&quot; &quot;09&quot; ... ## $ nom_ent : chr [1:76] &quot;Ciudad de México&quot; &quot;Ciudad de México&quot; &quot;Ciudad de México&quot; &quot;Ciudad de México&quot; ... ## $ nom_abr : chr [1:76] &quot;CDMX&quot; &quot;CDMX&quot; &quot;CDMX&quot; &quot;CDMX&quot; ... ## $ nom_zm : chr [1:76] &quot;Valle de México&quot; &quot;Valle de México&quot; &quot;Valle de México&quot; &quot;Valle de México&quot; ... ## $ cvm : num [1:76] 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 ... ## $ ext : num [1:76] 96.2 310.4 32.5 33.9 85.8 ... ## $ pob20 : num [1:76] 759137 699928 545884 443704 392313 ... ## $ pob20_h : num [1:76] 361007 334877 260951 210118 190190 ... ## $ pob20_m : num [1:76] 398130 365051 284933 233586 202123 ... ## $ positivos : num [1:76] 10905 11887 7289 7172 6812 ... ## $ defuncione: num [1:76] 868 542 754 585 289 713 648 397 190 391 ... ## $ pos_mil : num [1:76] 14.4 17 13.4 16.2 17.4 ... ## $ pos_hab : num [1:76] 14.4 17 13.4 16.2 17.4 ... ## $ def_hab : num [1:76] 1.143 0.774 1.381 1.318 0.737 ... ## $ ss : num [1:76] 75 71.1 71.6 71.6 72.7 ... ## $ ppob_sines: num [1:76] 0.0204 0.0195 0.0119 0.0121 0.0193 ... ## $ ppob_basi : num [1:76] 0.41 0.391 0.307 0.379 0.469 ... ## $ ppob_media: num [1:76] 0.252 0.253 0.251 0.291 0.299 ... ## $ ppob_sup : num [1:76] 0.314 0.335 0.427 0.316 0.211 ... ## $ ocviv : num [1:76] 3.45 3.44 2.75 3.26 3.67 3.21 3.2 3.74 3.6 2.81 ... ## $ occu : num [1:76] 0.81 0.81 0.72 0.82 0.93 0.77 0.7 0.91 0.81 0.66 ... ## $ pintegra4_: num [1:76] 0.648 0.644 0.48 0.609 0.699 ... ## $ pintegra6_: num [1:76] 0.252 0.226 0.142 0.219 0.262 ... ## $ pintegra8_: num [1:76] 0.1071 0.0864 0.0422 0.0798 0.0982 ... ## $ ppob_5_o_m: num [1:76] 0.726 0.73 0.888 0.821 0.792 ... ## $ ppob_3_o_m: num [1:76] 0.32 0.315 0.363 0.354 0.354 ... ## $ ppob_1 : num [1:76] 0.0369 0.0499 0.0249 0.0247 0.0513 ... ## $ ppob_1dorm: num [1:76] 0.211 0.212 0.199 0.202 0.209 ... ## $ ppob_2dorm: num [1:76] 0.566 0.547 0.8 0.696 0.614 ... ## $ ppob_3dorm: num [1:76] 0.832 0.848 0.947 0.89 0.864 ... ## $ pviv_ocu5_: num [1:76] 0.226 0.22 0.127 0.198 0.267 ... ## $ pviv_ocu7_: num [1:76] 0.0641 0.0568 0.026 0.0513 0.0701 ... ## $ pviv_ocu9_: num [1:76] 0.02334 0.01815 0.00645 0.01593 0.02167 ... ## $ analf : num [1:76] 1.574 1.606 0.953 1.085 1.673 ... ## $ sbasc : num [1:76] 19 18.2 13.5 16.9 20.3 ... ## $ vhac : num [1:76] 15.25 15.19 9.42 14.43 18.3 ... ## $ po2sm : num [1:76] 49.3 61.2 41.4 57.2 65.4 ... ## $ idh2015 : num [1:76] 0.815 0.832 0.854 0.826 0.784 0.842 0.868 0.812 0.812 0.888 ... ## $ im : num [1:76] 60.4 59.5 61.3 60.3 59.3 ... ## $ gm_2020 : chr [1:76] &quot;Muy Bajo&quot; &quot;Muy Bajo&quot; &quot;Muy Bajo&quot; &quot;Muy Bajo&quot; ... ## $ grad : num [1:76] 11.3 11.5 12.4 11.5 10.5 ... ## $ grad_h : num [1:76] 11.5 11.7 12.8 11.7 10.7 ... ## $ grad_m : num [1:76] 11.1 11.4 12.1 11.3 10.4 ... ## $ poind : num [1:76] 0.0912 0.0874 0.1334 0.0714 0.2442 ... ## $ pocom : num [1:76] 0.16 0.185 0.166 0.289 0.394 ... ## $ poss : num [1:76] 0.749 0.728 0.7 0.64 0.361 ... ## $ tmind : num [1:76] 20.32 8.9 27.57 6.57 6.42 ... ## $ tmcom : num [1:76] 5.95 3.48 4.36 2.66 2.12 ... ## $ tmss : num [1:76] 27.69 13.91 24.87 9.94 2.59 ... ## $ rmind : num [1:76] 68.2 116.7 55.9 64.9 78.3 ... ## $ rmcom : num [1:76] 50.6 31.3 45.5 31.2 23.5 ... ## $ rmss : num [1:76] 178.3 36.3 255.6 101.4 23.4 ... ## $ den : num [1:76] 7895 2255 16786 13104 4575 ... base::dim(covid_zmvm) #Dimensiones de la base ## [1] 76 56 Recuerda la notación que mencionamos antes, “paquete::función”; además, si el paquete está cargado, no es necesario especificar su nombre al usar la función; al arrancar cada sesión de R, un conjunto de paquetes se cargan de forma automática, dichos paquetes aparecen marcados con una palomita, \\(\\checkmark\\), en la pestaña Packages en la sección de archivo. Ahora bien, si deseas ver la base completa puedes llamarla a una nueva pestaña que se visualizará en la sección de fuente. Usa la siguiente instrucción: utils::View(covid_zmvm) #Que es equivalente a: View(covid_zmvm) Alternativamente, puedes llamar la base para que aparezca en la consola, lo que no siempre es recomendable si la base es muy grande. base::print(covid_zmvm) ## # A tibble: 76 × 56 ## cvemun cve_mun nom_mun cve_ent nom_ent nom_abr nom_zm cvm ext pob20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 09010 010 Álvaro Obre… 09 Ciudad… CDMX Valle… 9.01 96.2 759137 ## 2 09012 012 Tlalpan 09 Ciudad… CDMX Valle… 9.01 310. 699928 ## 3 09015 015 Cuauhtémoc 09 Ciudad… CDMX Valle… 9.01 32.5 545884 ## 4 09017 017 Venustiano … 09 Ciudad… CDMX Valle… 9.01 33.9 443704 ## 5 09011 011 Tláhuac 09 Ciudad… CDMX Valle… 9.01 85.8 392313 ## 6 09002 002 Azcapotzalco 09 Ciudad… CDMX Valle… 9.01 33.5 432205 ## 7 09003 003 Coyoacán 09 Ciudad… CDMX Valle… 9.01 53.9 614447 ## 8 09013 013 Xochimilco 09 Ciudad… CDMX Valle… 9.01 118. 442178 ## 9 09004 004 Cuajimalpa … 09 Ciudad… CDMX Valle… 9.01 71.2 217686 ## 10 09016 016 Miguel Hida… 09 Ciudad… CDMX Valle… 9.01 46.4 414470 ## # ℹ 66 more rows ## # ℹ 46 more variables: pob20_h &lt;dbl&gt;, pob20_m &lt;dbl&gt;, positivos &lt;dbl&gt;, ## # defuncione &lt;dbl&gt;, pos_mil &lt;dbl&gt;, pos_hab &lt;dbl&gt;, def_hab &lt;dbl&gt;, ss &lt;dbl&gt;, ## # ppob_sines &lt;dbl&gt;, ppob_basi &lt;dbl&gt;, ppob_media &lt;dbl&gt;, ppob_sup &lt;dbl&gt;, ## # ocviv &lt;dbl&gt;, occu &lt;dbl&gt;, pintegra4_ &lt;dbl&gt;, pintegra6_ &lt;dbl&gt;, ## # pintegra8_ &lt;dbl&gt;, ppob_5_o_m &lt;dbl&gt;, ppob_3_o_m &lt;dbl&gt;, ppob_1 &lt;dbl&gt;, ## # ppob_1dorm &lt;dbl&gt;, ppob_2dorm &lt;dbl&gt;, ppob_3dorm &lt;dbl&gt;, pviv_ocu5_ &lt;dbl&gt;, … En su lugar, podrías preferir ver en la consola sólo los últimos y primeros casos: utils::head(covid_zmvm) #Para los primeros casos ## # A tibble: 6 × 56 ## cvemun cve_mun nom_mun cve_ent nom_ent nom_abr nom_zm cvm ext pob20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 09010 010 Álvaro Obreg… 09 Ciudad… CDMX Valle… 9.01 96.2 759137 ## 2 09012 012 Tlalpan 09 Ciudad… CDMX Valle… 9.01 310. 699928 ## 3 09015 015 Cuauhtémoc 09 Ciudad… CDMX Valle… 9.01 32.5 545884 ## 4 09017 017 Venustiano C… 09 Ciudad… CDMX Valle… 9.01 33.9 443704 ## 5 09011 011 Tláhuac 09 Ciudad… CDMX Valle… 9.01 85.8 392313 ## 6 09002 002 Azcapotzalco 09 Ciudad… CDMX Valle… 9.01 33.5 432205 ## # ℹ 46 more variables: pob20_h &lt;dbl&gt;, pob20_m &lt;dbl&gt;, positivos &lt;dbl&gt;, ## # defuncione &lt;dbl&gt;, pos_mil &lt;dbl&gt;, pos_hab &lt;dbl&gt;, def_hab &lt;dbl&gt;, ss &lt;dbl&gt;, ## # ppob_sines &lt;dbl&gt;, ppob_basi &lt;dbl&gt;, ppob_media &lt;dbl&gt;, ppob_sup &lt;dbl&gt;, ## # ocviv &lt;dbl&gt;, occu &lt;dbl&gt;, pintegra4_ &lt;dbl&gt;, pintegra6_ &lt;dbl&gt;, ## # pintegra8_ &lt;dbl&gt;, ppob_5_o_m &lt;dbl&gt;, ppob_3_o_m &lt;dbl&gt;, ppob_1 &lt;dbl&gt;, ## # ppob_1dorm &lt;dbl&gt;, ppob_2dorm &lt;dbl&gt;, ppob_3dorm &lt;dbl&gt;, pviv_ocu5_ &lt;dbl&gt;, ## # pviv_ocu7_ &lt;dbl&gt;, pviv_ocu9_ &lt;dbl&gt;, analf &lt;dbl&gt;, sbasc &lt;dbl&gt;, vhac &lt;dbl&gt;, … utils::tail(covid_zmvm) #Para los últimos casos ## # A tibble: 6 × 56 ## cvemun cve_mun nom_mun cve_ent nom_ent nom_abr nom_zm cvm ext pob20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15120 120 Zumpango 15 México Mex. Valle… 9.01 224. 2.80e5 ## 2 15121 121 Cuautitlán … 15 México Mex. Valle… 9.01 110. 5.55e5 ## 3 15122 122 Valle de Ch… 15 México Mex. Valle… 9.01 46.6 3.92e5 ## 4 15125 125 Tonanitla 15 México Mex. Valle… 9.01 9.04 1.49e4 ## 5 15058 058 Nezahualcóy… 15 México Mex. Valle… 9.01 63.3 1.08e6 ## 6 09005 005 Gustavo A. … 09 Ciudad… CDMX Valle… 9.01 87.9 1.17e6 ## # ℹ 46 more variables: pob20_h &lt;dbl&gt;, pob20_m &lt;dbl&gt;, positivos &lt;dbl&gt;, ## # defuncione &lt;dbl&gt;, pos_mil &lt;dbl&gt;, pos_hab &lt;dbl&gt;, def_hab &lt;dbl&gt;, ss &lt;dbl&gt;, ## # ppob_sines &lt;dbl&gt;, ppob_basi &lt;dbl&gt;, ppob_media &lt;dbl&gt;, ppob_sup &lt;dbl&gt;, ## # ocviv &lt;dbl&gt;, occu &lt;dbl&gt;, pintegra4_ &lt;dbl&gt;, pintegra6_ &lt;dbl&gt;, ## # pintegra8_ &lt;dbl&gt;, ppob_5_o_m &lt;dbl&gt;, ppob_3_o_m &lt;dbl&gt;, ppob_1 &lt;dbl&gt;, ## # ppob_1dorm &lt;dbl&gt;, ppob_2dorm &lt;dbl&gt;, ppob_3dorm &lt;dbl&gt;, pviv_ocu5_ &lt;dbl&gt;, ## # pviv_ocu7_ &lt;dbl&gt;, pviv_ocu9_ &lt;dbl&gt;, analf &lt;dbl&gt;, sbasc &lt;dbl&gt;, vhac &lt;dbl&gt;, … ¿Qué tan grande es la base de datos? Si bien ya la función str() nos brindó información sobre la estructura de la base, puedes conocer el número de filas y columnas que componen tu arreglo de datos con las siguientes funciones: base::ncol(covid_zmvm) #Para conocer el número de columnas ## [1] 56 base::nrow(covid_zmvm) #Para conocer el número de filas ## [1] 76 Conviene que se revise el archivo en formato txt que acompaña nuestra base y que opera a modo de diccionario para que conocer el significado de cada una de las variables que componen nuestro archivo. Plantemos pues algunas preguntas y busquemos responderlas a través de un análisis gráfico: ¿Habrá alguna relación entre el número de casos positivos por cada mil habitantes, pos_hab, o muertes por cada mil habitantes, def_hab, con el nivel de estudios promedio de la población por municipio? Si esta relación existe, ¿es más intensa entre las muertes o los casos positivos? Para responder a esta pregunta podemos recurrir a la construcción de diagramas de dispersión. La elaboración de gráficas dentro de tidyverse corre a cargo del paquete ggplot2, una potente librería para la representación visual de información. Este paquete se basa en lo que se denomina “gramática de gráficas” que no es otra cosa que un conjunto de reglas coherentes con base en las cuales se elaboran los gráficos (Wickham and Grolemund 2016). La construcción de gráficas con ggplot2 se hace a partir de una suerte de “capas” o “enunciados”. Cada enunciado constituye una parte de la gráfica. Según la hoja de trucos del paquete (que puedes consultar desde la barra de menú en la opción Help/CheatSheet): ggplot2 se basa en la gramática de los gráficos, la idea de que puedes construir cada gráfico a partir de los mismos componentes: un conjunto de datos, un sistema de coordenadas y “geoms”, es decir, marcas visuales que representan puntos de datos. Así pues, se requieren tres elementos para la construcción de una gráfica con ggplot2: datos, geometría y sistema de coordenadas. Dentro de la geometría se especifican no sólo la o las variables a representar, también algunos elementos estéticos como colores. En nuestro caso, la información está contenida en la base covid_zmvm pero habrá que especificar qué tipo de gráfica deseamos y sus elementos estéticos. Veamos esto en acción: library(tidyverse) ggplot2::ggplot(data=covid_zmvm)+ ggplot2::geom_point(aes(x=grad,y=pos_hab)) Tenemos pues dos enunciados o “capas”. La primera corresponde a la función ggplot() donde colocamos (casi siempre) el primer elemento requerido: la base de datos de donde tomaremos la o las variables a graficar, en tanto, el segundo enunciado corresponde a la geometría, geom_ cuya parte después del guion bajo se modificará en función del tipo de gráfica a utilizar. Algunos (sólo algunos) tipos de gráficas y su función de geometría asociada en ggplot2 aparecen en el cuadro 1.2: Cuadro 1.2 Gráfica Geometría en ggplot2 Histograma geom_histogram() Densidad geom_density() Caja geom_boxplot() Barras geom_col() geom_bar() Dispersión geom_point() Tendencia (lineal o suavizada) geom_smooth() Por favor, revisa la hoja de trucos del paquete para que te familiarices con las otras tantas geometrías y su uso, pues cada geometría requiere diferentes argumentos y admite varias posibles configuraciones. En general, dentro de la función de geometría, geom_, será necesario especificar los elementos estéticos, aes(), entre los que se cuenta la o las variables a graficar y cómo serán representadas. Añadamos una “capa” adicional a nuestra gráfica, combinando otra geometría como una línea de ajuste (pongamos atención en el uso del signo de más): ggplot2::ggplot(data=covid_zmvm)+ ggplot2::geom_point(aes(x=grad,y=pos_hab))+ ggplot2::geom_smooth(aes(x=grad,y=pos_hab)) Puedes ver cómo, al añadir una nueva geometría, fue necesario especificar los elementos estéticos del caso, pero los tres enunciados nos permiten tener una sola gráfica. Observa cómo R en tu consola R indica que estás usando determinado método de suavizamiento, el llamado “loess” que significa locally estimated scatterplot smoothing, por tanto, es fácil deducir que debe haber otros métodos de suavizamiento. Observa las siguientes líneas de código: ggplot2::ggplot(data=covid_zmvm)+ ggplot2::geom_point(aes(x=grad,y=pos_hab))+ ggplot2::geom_smooth(aes(x=grad,y=pos_hab), method = &quot;lm&quot;) Aquí, hemos añadido un argumento adicional, method=, para indicar un suavizamiento dado por un modelo lineal, lm, de linear model. Tenemos pues que entre el número de casos positivos y el número promedio de grados cursados hay una aparente relación positiva. ¿Qué hay para el caso de las defunciones: ggplot2::ggplot(data=covid_zmvm)+ ggplot2::geom_point(aes(x=grad,y=def_hab))+ ggplot2::geom_smooth(aes(x=grad,y=def_hab), method = &quot;lm&quot;) Si bien hay un poco más de dispersión, la aparente relación positiva se mantiene. ¿Qué crees que explique esta asociación? ¿Las personas con más años estudiados son susceptibles de contagiarse más fácil de COVID19? ¿O quizá se relacionará más con las diferencias que a nivel territorial se dan entre los niveles de estudio de la población en el Valle de México? Ejercicio Elabora algunas gráficas de dispersión para responder a las siguientes preguntas: ¿Qué tipo de asociación lineal hay entre los casos positivos y defunciones por COVID19 con las características de la población según el número de dormitorios de las casas, el número de personas que habitan en cada casa y la densidad de población? ¿Encuentras algún tipo de asociación entre las variables COVID19 y las características económicas de los municipios de la Zona Metropolitana del Valle de México? Quizá lo que te interese no sólo sea la asociación entre pares de variables, sino el comportamiento individual de cada variable para conocer la forma de su distribución e identificar algunos valores extremos o atípicos (los llamados ourliers). Para ello, convendrá un tipo de gráfica que permita representar sólo una variable continúa (casos positivos o defunciones por COVID19), tal como un histograma, una gráfica de densidad o de caja. #Histograma para los casos positivos por cada mil habitantes ggplot2::ggplot(covid_zmvm)+ ggplot2::geom_histogram(aes(x=pos_hab)) #Gráfico de densidad para los casos positivos por cada mil habitantes ggplot2::ggplot(covid_zmvm)+ ggplot2::geom_density(aes(x=pos_hab)) #Gráfico de caja para los casos positivos por cada mil habitantes ggplot2::ggplot(covid_zmvm)+ ggplot2::geom_boxplot(aes(x=pos_hab)) Los tres gráficos dan cuenta que la forma de la distribución muestra un marcado sesgo positivo (a la derecha), lo que significa que hay algunos municipios o alcaldías con valores muy altos para esta variable, además de algunos valores extremadamente altos según el diagrama de caja, ¿cuáles podrán ser y qué explicaría este comportamiento atípicamente alto? Ejercicio Consulta la ayuda de cada una de las funciones de geometría e intenta: Elaborar un histograma con diferentes categorías (bins). ¿Cambia esto la forma de la distribución? Señalar de un color diferente las observaciones atípicas en el diagrama de caja ¿Hay manera de identificar a qué alcaldías o municipios pertenecen dichos valores atípicos? Además, también es posible modificar las etiquetas de los ejes: ggplot2::ggplot(data=covid_zmvm)+ ggplot2::geom_histogram(aes(x=pos_hab))+ ggplot2::labs(x=&quot;Casos positivos por cada mil habitantes&quot;, y=&quot;Frecuencia&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Nuestra base de datos contiene una variable de tipo categórico, grado de marginación (gm_2020), asociada al índice de marginación (im). Para este tipo de variable puede ser conveniente construir un gráfico de barras especialmente diseñado para variables categóricas echando mano de la geometría geom_bar(): ggplot2::ggplot(covid_zmvm)+ ggplot2:: geom_bar(aes(x=gm_2020))+ ggplot2::labs(x=&quot;Grado de marginación 2020&quot;, y=&quot;Frecuencia&quot;) #Si tienes la librería ggplot2 cargada en el entorno de trabajo, alternativamente, puedes escribir: ggplot(covid_zmvm)+ geom_bar(aes(x=gm_2020))+ labs(x=&quot;Grado de marginación 2020&quot;, y=&quot;Frecuencia&quot;) Este gráfico revela que, de acuerdo con la clasificación del Consejo Nacional de Población y Vivienda (CONAPO), la gran mayoría de los municipios y alcaldías que componen el Valle de México están catalogados como de muy bajo grado de marginación. Ejercicio Obtén una gráfica de barras donde se muestre el número de unidades administrativas (municipios o alcaldías) por de cada una de las entidades que integran la Zona Metropolitana del Valle de México. Ahora bien, ¿será posible incorporar en un plano bidimensional una tercera variable? ¿Cómo añadirías a un diagrama de dispersión, que relaciona dos variables, una tercera? Este instrumento es a veces denominado gráfico de burbujas y si lo piensas con cuidado es una solución muy ingeniosa para añadir información extra sin renunciar a la simpleza. Para hacerlo, basta con añadir el argumento size= dentro de los elementos estéticos de la función geom_point(): #Tamaño del circulo dado por los años estudiados ggplot2::ggplot(covid_zmvm)+ ggplot2::geom_point(aes(x=grad,y=pos_hab,size=def_hab)) #Transparencia del círculo dado por los años estudiados ggplot2::ggplot(covid_zmvm)+ ggplot2::geom_point(aes(x=grad,y=pos_hab,alpha=def_hab, size=def_hab), color=&quot;navyblue&quot;) #Transparencia del círculo dado por los años estudiados ggplot2::ggplot(covid_zmvm)+ ggplot2::geom_point(aes(x=grad,y=pos_hab,color=def_hab)) Ejercicio Responde lo siguiente: Prueba cambiar la variable que define el tamaño del círculo por nuestra variable categórica grado de marginación, ¿es recomendable usar una variable de este tipo en esta representación? ¿Qué papel juega el argumento alpha= en la segunda gráfica? ¿Cuál es la diferencia en colocar el argumento color dentro o fuera del grupo de elementos estéticos (aes())? ¿Notaste la diferencia en el par de gráficas anteriores? Prueba cambiar el argumento color dentro de aes() por otra variable y el color de los círculos. Finalmente, elaboremos una gráfica que nos ayude a sintetizar pares de asociaciones entre variables (diagramas de dispersión) y formas de distribución a través de un gráfico muy elegante construido con una extensión del paquete ggplot2: GGally. Para instalarlo, como es usual: install.packages(&quot;GGally&quot;) Luego, para poder usarlo, lo llamamos al entorno de trabajo: library(GGally) La gráfica que buscamos es una matriz de diagramas de dispersión y la construiremos con la función ggpairs() y sólo dos argumentos: la fuente de datos y las variables que deseamos incluir. Veamos: #Definiendo los nombres de las variables GGally::ggpairs(data=covid_zmvm, columns = c(&quot;pos_hab&quot;,&quot;def_hab&quot;,&quot;im&quot;,&quot;grad&quot;)) #Indicando el número de columna según el lugar que ocupa en la base GGally::ggpairs(data=covid_zmvm, columns = c(16,17,41,43)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. La matriz de diagramas de dispersión construida con ggpairs es, desde mi personal punto de vista, no sólo muy elegante, sino que ofrece elementos informativos en el gráfico que antes no teníamos, tales como el coeficiente de correlación lineal y su nivel de significancia. Antes de pasar a tratar otro elemento, hay que decir que RStudio permite exportar las gráficas hechas, tanto en diversos formatos de imagen (jpg, png, tiff) como en PDF. Ejercicio Elabora tu propia matriz de diagramas de dispersión con las variables que, a tu juicio, resulten más relevantes para explicar la dinámica tanto de las muertes como de los casos positivos de COVID19. 1.5 Manipulación de la información 1.5.1 Seleccionar variables Tareas relacionadas con la preparación y manipulación de datos, que en una hoja de cálculo como Excel de Office o Calc de LibreOffice, son rutinarias y muy sencillas, a veces en R pueden significar un dolor de cabeza. No obstante, dentro de los paquetes de la familia tidyverse encontramos dplyr. Este paquete brinda herramientas de manipulación de datos basadas en “gramática”, que no es otra cosa que funciones (“verbos”) que permiten seleccionar o filtrar elementos en una base de datos, o bien, agrupar y crear nuevas variables. Si deseas profundizar en el conocimiento de este potente paquete consulta la Introducción al paquete dplyr, o bien, su Viñeta de ayuda. La base de datos que cargamos contiene 55 columnas, es decir, 55 variables. Quizá para su análisis no te sean de interés todas, por lo que querrías generar una base de datos más compacta mediante la selección de algunas que son de utilidad. Para llevar a cabo dicha tarea echaremos mano de la función select(), pero antes de hacerlo, es necesario introducir el operador “tubería”, pipe (%&gt;%). Este operador sirve para indicar cada fase del proceso por el que un objeto es tratado a través de la aplicación de diferentes funciones u operaciones. Por ejemplo, queremos que la base covid_zmvm “pase” por un proceso que consiste en la selección de sólo algunas variables. Comencemos usando el “verbo” seleccionar, select(), para una sola variable, por ejemplo pos_hab: covid_zmvm %&gt;% dplyr::select(pos_hab) ## # A tibble: 76 × 1 ## pos_hab ## &lt;dbl&gt; ## 1 14.4 ## 2 17.0 ## 3 13.4 ## 4 16.2 ## 5 17.4 ## 6 17.3 ## 7 14.9 ## 8 17.4 ## 9 18.7 ## 10 13.7 ## # ℹ 66 more rows La instrucción anterior es equivalente a escribir lo siguiente: dplyr::select(covid_zmvm,pos_hab) ## # A tibble: 76 × 1 ## pos_hab ## &lt;dbl&gt; ## 1 14.4 ## 2 17.0 ## 3 13.4 ## 4 16.2 ## 5 17.4 ## 6 17.3 ## 7 14.9 ## 8 17.4 ## 9 18.7 ## 10 13.7 ## # ℹ 66 more rows Así, usar el operador tubería permite no sólo ahorrar la escritura de algunos argumentos, sino simplificar la forma en que las instrucciones dadas a R son leídas por un ser humano. Veamos cómo seleccionar ahora, por ejemplo, tres variables: la clave de municipio (cvemun), los casos positivos (pos_hab) y el grado de marginación (gm_2020): covid_zmvm %&gt;% dplyr::select(cvemun,pos_hab,gm_2020) ## # A tibble: 76 × 3 ## cvemun pos_hab gm_2020 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 09010 14.4 Muy Bajo ## 2 09012 17.0 Muy Bajo ## 3 09015 13.4 Muy Bajo ## 4 09017 16.2 Muy Bajo ## 5 09011 17.4 Muy Bajo ## 6 09002 17.3 Muy Bajo ## 7 09003 14.9 Muy Bajo ## 8 09013 17.4 Muy Bajo ## 9 09004 18.7 Muy Bajo ## 10 09016 13.7 Muy Bajo ## # ℹ 66 more rows Ahora, imagina que queremos todas las variables excepto el nombre de la Zona Metropolitana, nom_zm. Sería ocioso colocar todos los nombres de las variables menos el citado. Para hacer esto más eficiente es necesario introducir solamente un signo de menos (-) antes del nombre de la variable, para de esta sencilla forma seleccionar todas las variables menos nom_zm. covid_zmvm %&gt;% dplyr::select(-nom_zm) ## # A tibble: 76 × 55 ## cvemun cve_mun nom_mun cve_ent nom_ent nom_abr cvm ext pob20 pob20_h ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 09010 010 Álvaro Obr… 09 Ciudad… CDMX 9.01 96.2 759137 361007 ## 2 09012 012 Tlalpan 09 Ciudad… CDMX 9.01 310. 699928 334877 ## 3 09015 015 Cuauhtémoc 09 Ciudad… CDMX 9.01 32.5 545884 260951 ## 4 09017 017 Venustiano… 09 Ciudad… CDMX 9.01 33.9 443704 210118 ## 5 09011 011 Tláhuac 09 Ciudad… CDMX 9.01 85.8 392313 190190 ## 6 09002 002 Azcapotzal… 09 Ciudad… CDMX 9.01 33.5 432205 204950 ## 7 09003 003 Coyoacán 09 Ciudad… CDMX 9.01 53.9 614447 289110 ## 8 09013 013 Xochimilco 09 Ciudad… CDMX 9.01 118. 442178 215452 ## 9 09004 004 Cuajimalpa… 09 Ciudad… CDMX 9.01 71.2 217686 104149 ## 10 09016 016 Miguel Hid… 09 Ciudad… CDMX 9.01 46.4 414470 195467 ## # ℹ 66 more rows ## # ℹ 45 more variables: pob20_m &lt;dbl&gt;, positivos &lt;dbl&gt;, defuncione &lt;dbl&gt;, ## # pos_mil &lt;dbl&gt;, pos_hab &lt;dbl&gt;, def_hab &lt;dbl&gt;, ss &lt;dbl&gt;, ppob_sines &lt;dbl&gt;, ## # ppob_basi &lt;dbl&gt;, ppob_media &lt;dbl&gt;, ppob_sup &lt;dbl&gt;, ocviv &lt;dbl&gt;, occu &lt;dbl&gt;, ## # pintegra4_ &lt;dbl&gt;, pintegra6_ &lt;dbl&gt;, pintegra8_ &lt;dbl&gt;, ppob_5_o_m &lt;dbl&gt;, ## # ppob_3_o_m &lt;dbl&gt;, ppob_1 &lt;dbl&gt;, ppob_1dorm &lt;dbl&gt;, ppob_2dorm &lt;dbl&gt;, ## # ppob_3dorm &lt;dbl&gt;, pviv_ocu5_ &lt;dbl&gt;, pviv_ocu7_ &lt;dbl&gt;, pviv_ocu9_ &lt;dbl&gt;, … En lugar de usar los nombres de las variables, puedes recurrir al número de columna en la que se hallan, contando a partir del 1. Por ejemplo, nom_mun es la columna 3 y pos_hab es la columna 16, entonces podríamos escribir: covid_zmvm %&gt;% dplyr::select(3,16) ## # A tibble: 76 × 2 ## nom_mun pos_hab ## &lt;chr&gt; &lt;dbl&gt; ## 1 Álvaro Obregón 14.4 ## 2 Tlalpan 17.0 ## 3 Cuauhtémoc 13.4 ## 4 Venustiano Carranza 16.2 ## 5 Tláhuac 17.4 ## 6 Azcapotzalco 17.3 ## 7 Coyoacán 14.9 ## 8 Xochimilco 17.4 ## 9 Cuajimalpa de Morelos 18.7 ## 10 Miguel Hidalgo 13.7 ## # ℹ 66 more rows Y si es de nuestro interés seleccionar un número consecutivo de columnas, podemos servirnos de: covid_zmvm %&gt;% dplyr::select(1:10) ## # A tibble: 76 × 10 ## cvemun cve_mun nom_mun cve_ent nom_ent nom_abr nom_zm cvm ext pob20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 09010 010 Álvaro Obre… 09 Ciudad… CDMX Valle… 9.01 96.2 759137 ## 2 09012 012 Tlalpan 09 Ciudad… CDMX Valle… 9.01 310. 699928 ## 3 09015 015 Cuauhtémoc 09 Ciudad… CDMX Valle… 9.01 32.5 545884 ## 4 09017 017 Venustiano … 09 Ciudad… CDMX Valle… 9.01 33.9 443704 ## 5 09011 011 Tláhuac 09 Ciudad… CDMX Valle… 9.01 85.8 392313 ## 6 09002 002 Azcapotzalco 09 Ciudad… CDMX Valle… 9.01 33.5 432205 ## 7 09003 003 Coyoacán 09 Ciudad… CDMX Valle… 9.01 53.9 614447 ## 8 09013 013 Xochimilco 09 Ciudad… CDMX Valle… 9.01 118. 442178 ## 9 09004 004 Cuajimalpa … 09 Ciudad… CDMX Valle… 9.01 71.2 217686 ## 10 09016 016 Miguel Hida… 09 Ciudad… CDMX Valle… 9.01 46.4 414470 ## # ℹ 66 more rows Ejercicio Para finalizar lo relacionado con la selección, genera una nueva base de datos llamada covid que contenga las siguientes variables: cvemun, nom_mun, cve_ent, nom_ent, ext, pos_hab, def_hab, ocviv, occu, ppob_1, ppob_1dorm, im, gm_2020, grad, poind, pocom, poss, tmind, tmcom, tmss, rmind, rmcom, rmss, den, pob20. 1.5.2 Filtrar (seleccionar casos) Si ahora lo que te interesa es seleccionar filas o casos en lugar de columnas, hay que echar mano de otro “verbo”, es decir, de la función filtrar, filter(). Por ejemplo, imagina que deseamos seleccionar sólo los municipios del Estado de México. Estos municipios cumplen con la condición de que todos ellos tienen el valor “México” en la variable nom_ent (nombre de entidad), o bien, tener el valor “15” en el de cve_ent (clave de entidad). Para poder hacer un uso eficiente de la función de filtrado es necesario introducir también otro tipo de operadores, los llamados operadores lógicos, que aparecen en el cuadro 1.3: Cuadro 1.3 Operadores Definiciones == igual que != diferente de &lt;= menor o igual que &gt;= mayor o igual que &lt; mayor que &gt; menor que &amp; y Entonces, para usar la función filter() y tomando como base la información previa: covid_zmvm %&gt;% dplyr::filter(cve_ent==&quot;15&quot;) ## # A tibble: 59 × 56 ## cvemun cve_mun nom_mun cve_ent nom_ent nom_abr nom_zm cvm ext pob20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15002 002 Acolman 15 México Mex. Valle… 9.01 83.9 171507 ## 2 15009 009 Amecameca 15 México Mex. Valle… 9.01 189. 53441 ## 3 15010 010 Apaxco 15 México Mex. Valle… 9.01 75.7 31898 ## 4 15011 011 Atenco 15 México Mex. Valle… 9.01 84.6 75489 ## 5 15013 013 Atizapán de… 15 México Mex. Valle… 9.01 91.1 523674 ## 6 15015 015 Atlautla 15 México Mex. Valle… 9.01 162. 31900 ## 7 15016 016 Axapusco 15 México Mex. Valle… 9.01 231. 29128 ## 8 15017 017 Ayapango 15 México Mex. Valle… 9.01 36.4 10053 ## 9 15020 020 Coacalco de… 15 México Mex. Valle… 9.01 35.1 293444 ## 10 15022 022 Cocotitlán 15 México Mex. Valle… 9.01 15.0 15107 ## # ℹ 49 more rows ## # ℹ 46 more variables: pob20_h &lt;dbl&gt;, pob20_m &lt;dbl&gt;, positivos &lt;dbl&gt;, ## # defuncione &lt;dbl&gt;, pos_mil &lt;dbl&gt;, pos_hab &lt;dbl&gt;, def_hab &lt;dbl&gt;, ss &lt;dbl&gt;, ## # ppob_sines &lt;dbl&gt;, ppob_basi &lt;dbl&gt;, ppob_media &lt;dbl&gt;, ppob_sup &lt;dbl&gt;, ## # ocviv &lt;dbl&gt;, occu &lt;dbl&gt;, pintegra4_ &lt;dbl&gt;, pintegra6_ &lt;dbl&gt;, ## # pintegra8_ &lt;dbl&gt;, ppob_5_o_m &lt;dbl&gt;, ppob_3_o_m &lt;dbl&gt;, ppob_1 &lt;dbl&gt;, ## # ppob_1dorm &lt;dbl&gt;, ppob_2dorm &lt;dbl&gt;, ppob_3dorm &lt;dbl&gt;, pviv_ocu5_ &lt;dbl&gt;, … #O bien covid_zmvm %&gt;% dplyr::filter(nom_ent==&quot;México&quot;) ## # A tibble: 59 × 56 ## cvemun cve_mun nom_mun cve_ent nom_ent nom_abr nom_zm cvm ext pob20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15002 002 Acolman 15 México Mex. Valle… 9.01 83.9 171507 ## 2 15009 009 Amecameca 15 México Mex. Valle… 9.01 189. 53441 ## 3 15010 010 Apaxco 15 México Mex. Valle… 9.01 75.7 31898 ## 4 15011 011 Atenco 15 México Mex. Valle… 9.01 84.6 75489 ## 5 15013 013 Atizapán de… 15 México Mex. Valle… 9.01 91.1 523674 ## 6 15015 015 Atlautla 15 México Mex. Valle… 9.01 162. 31900 ## 7 15016 016 Axapusco 15 México Mex. Valle… 9.01 231. 29128 ## 8 15017 017 Ayapango 15 México Mex. Valle… 9.01 36.4 10053 ## 9 15020 020 Coacalco de… 15 México Mex. Valle… 9.01 35.1 293444 ## 10 15022 022 Cocotitlán 15 México Mex. Valle… 9.01 15.0 15107 ## # ℹ 49 more rows ## # ℹ 46 more variables: pob20_h &lt;dbl&gt;, pob20_m &lt;dbl&gt;, positivos &lt;dbl&gt;, ## # defuncione &lt;dbl&gt;, pos_mil &lt;dbl&gt;, pos_hab &lt;dbl&gt;, def_hab &lt;dbl&gt;, ss &lt;dbl&gt;, ## # ppob_sines &lt;dbl&gt;, ppob_basi &lt;dbl&gt;, ppob_media &lt;dbl&gt;, ppob_sup &lt;dbl&gt;, ## # ocviv &lt;dbl&gt;, occu &lt;dbl&gt;, pintegra4_ &lt;dbl&gt;, pintegra6_ &lt;dbl&gt;, ## # pintegra8_ &lt;dbl&gt;, ppob_5_o_m &lt;dbl&gt;, ppob_3_o_m &lt;dbl&gt;, ppob_1 &lt;dbl&gt;, ## # ppob_1dorm &lt;dbl&gt;, ppob_2dorm &lt;dbl&gt;, ppob_3dorm &lt;dbl&gt;, pviv_ocu5_ &lt;dbl&gt;, … Para hacer una selección de dos entidades, incluiremos dos expresiones en una misma consulta echando mano del operador lógico “o”, identificado con el signo |: covid_zmvm %&gt;% dplyr::filter(cve_ent==&quot;09&quot;|cve_ent==&quot;15&quot;) ## # A tibble: 75 × 56 ## cvemun cve_mun nom_mun cve_ent nom_ent nom_abr nom_zm cvm ext pob20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 09010 010 Álvaro Obre… 09 Ciudad… CDMX Valle… 9.01 96.2 759137 ## 2 09012 012 Tlalpan 09 Ciudad… CDMX Valle… 9.01 310. 699928 ## 3 09015 015 Cuauhtémoc 09 Ciudad… CDMX Valle… 9.01 32.5 545884 ## 4 09017 017 Venustiano … 09 Ciudad… CDMX Valle… 9.01 33.9 443704 ## 5 09011 011 Tláhuac 09 Ciudad… CDMX Valle… 9.01 85.8 392313 ## 6 09002 002 Azcapotzalco 09 Ciudad… CDMX Valle… 9.01 33.5 432205 ## 7 09003 003 Coyoacán 09 Ciudad… CDMX Valle… 9.01 53.9 614447 ## 8 09013 013 Xochimilco 09 Ciudad… CDMX Valle… 9.01 118. 442178 ## 9 09004 004 Cuajimalpa … 09 Ciudad… CDMX Valle… 9.01 71.2 217686 ## 10 09016 016 Miguel Hida… 09 Ciudad… CDMX Valle… 9.01 46.4 414470 ## # ℹ 65 more rows ## # ℹ 46 more variables: pob20_h &lt;dbl&gt;, pob20_m &lt;dbl&gt;, positivos &lt;dbl&gt;, ## # defuncione &lt;dbl&gt;, pos_mil &lt;dbl&gt;, pos_hab &lt;dbl&gt;, def_hab &lt;dbl&gt;, ss &lt;dbl&gt;, ## # ppob_sines &lt;dbl&gt;, ppob_basi &lt;dbl&gt;, ppob_media &lt;dbl&gt;, ppob_sup &lt;dbl&gt;, ## # ocviv &lt;dbl&gt;, occu &lt;dbl&gt;, pintegra4_ &lt;dbl&gt;, pintegra6_ &lt;dbl&gt;, ## # pintegra8_ &lt;dbl&gt;, ppob_5_o_m &lt;dbl&gt;, ppob_3_o_m &lt;dbl&gt;, ppob_1 &lt;dbl&gt;, ## # ppob_1dorm &lt;dbl&gt;, ppob_2dorm &lt;dbl&gt;, ppob_3dorm &lt;dbl&gt;, pviv_ocu5_ &lt;dbl&gt;, … Ejercicio Selecciona los municipios con una extensión territorial mayor a 84 \\(km^2\\). Selecciona los municipios con entre 10 y 20 casos positivos por cada mil habitantes. Selecciona los municipios del Estado de México que tengan más de 10 años promedio de estudio y menos de 10 casos positivos por cada mil habitantes. ¿Qué otros filtros relevantes podrías pensar y elaborar? 1.5.3 Ordenar Una operación útil, cuando se analiza información, tiene que ver con ordenar la base de datos con arreglo al valor de una variable. En R con dplyr de tidyverse esto se hace con la función arrange(), por ejemplo: covid_zmvm %&gt;% dplyr::arrange(cvemun) ## # A tibble: 76 × 56 ## cvemun cve_mun nom_mun cve_ent nom_ent nom_abr nom_zm cvm ext pob20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 09002 002 Azcapotzalco 09 Ciudad… CDMX Valle… 9.01 33.5 4.32e5 ## 2 09003 003 Coyoacán 09 Ciudad… CDMX Valle… 9.01 53.9 6.14e5 ## 3 09004 004 Cuajimalpa … 09 Ciudad… CDMX Valle… 9.01 71.2 2.18e5 ## 4 09005 005 Gustavo A. … 09 Ciudad… CDMX Valle… 9.01 87.9 1.17e6 ## 5 09006 006 Iztacalco 09 Ciudad… CDMX Valle… 9.01 23.1 4.05e5 ## 6 09007 007 Iztapalapa 09 Ciudad… CDMX Valle… 9.01 113. 1.84e6 ## 7 09008 008 La Magdalen… 09 Ciudad… CDMX Valle… 9.01 63.4 2.48e5 ## 8 09009 009 Milpa Alta 09 Ciudad… CDMX Valle… 9.01 298. 1.53e5 ## 9 09010 010 Álvaro Obre… 09 Ciudad… CDMX Valle… 9.01 96.2 7.59e5 ## 10 09011 011 Tláhuac 09 Ciudad… CDMX Valle… 9.01 85.8 3.92e5 ## # ℹ 66 more rows ## # ℹ 46 more variables: pob20_h &lt;dbl&gt;, pob20_m &lt;dbl&gt;, positivos &lt;dbl&gt;, ## # defuncione &lt;dbl&gt;, pos_mil &lt;dbl&gt;, pos_hab &lt;dbl&gt;, def_hab &lt;dbl&gt;, ss &lt;dbl&gt;, ## # ppob_sines &lt;dbl&gt;, ppob_basi &lt;dbl&gt;, ppob_media &lt;dbl&gt;, ppob_sup &lt;dbl&gt;, ## # ocviv &lt;dbl&gt;, occu &lt;dbl&gt;, pintegra4_ &lt;dbl&gt;, pintegra6_ &lt;dbl&gt;, ## # pintegra8_ &lt;dbl&gt;, ppob_5_o_m &lt;dbl&gt;, ppob_3_o_m &lt;dbl&gt;, ppob_1 &lt;dbl&gt;, ## # ppob_1dorm &lt;dbl&gt;, ppob_2dorm &lt;dbl&gt;, ppob_3dorm &lt;dbl&gt;, pviv_ocu5_ &lt;dbl&gt;, … El resultado de la función anterior ordena nuestra base de menor a mayor, con base en la clave municipal. Además, es posible incluir varios criterios de ordenación, por ejemplo, si tuvieras los campos año, mes y día podría ordenarse conforme al calendario. En el caso del ejemplo siguiente, primero se ordena con arreglo a la clave de entidad y luego con respecto a la clave municipal y luego por la extensión territorial por cada mil habitantes: covid_zmvm %&gt;% dplyr::arrange(cvemun,ext) ## # A tibble: 76 × 56 ## cvemun cve_mun nom_mun cve_ent nom_ent nom_abr nom_zm cvm ext pob20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 09002 002 Azcapotzalco 09 Ciudad… CDMX Valle… 9.01 33.5 4.32e5 ## 2 09003 003 Coyoacán 09 Ciudad… CDMX Valle… 9.01 53.9 6.14e5 ## 3 09004 004 Cuajimalpa … 09 Ciudad… CDMX Valle… 9.01 71.2 2.18e5 ## 4 09005 005 Gustavo A. … 09 Ciudad… CDMX Valle… 9.01 87.9 1.17e6 ## 5 09006 006 Iztacalco 09 Ciudad… CDMX Valle… 9.01 23.1 4.05e5 ## 6 09007 007 Iztapalapa 09 Ciudad… CDMX Valle… 9.01 113. 1.84e6 ## 7 09008 008 La Magdalen… 09 Ciudad… CDMX Valle… 9.01 63.4 2.48e5 ## 8 09009 009 Milpa Alta 09 Ciudad… CDMX Valle… 9.01 298. 1.53e5 ## 9 09010 010 Álvaro Obre… 09 Ciudad… CDMX Valle… 9.01 96.2 7.59e5 ## 10 09011 011 Tláhuac 09 Ciudad… CDMX Valle… 9.01 85.8 3.92e5 ## # ℹ 66 more rows ## # ℹ 46 more variables: pob20_h &lt;dbl&gt;, pob20_m &lt;dbl&gt;, positivos &lt;dbl&gt;, ## # defuncione &lt;dbl&gt;, pos_mil &lt;dbl&gt;, pos_hab &lt;dbl&gt;, def_hab &lt;dbl&gt;, ss &lt;dbl&gt;, ## # ppob_sines &lt;dbl&gt;, ppob_basi &lt;dbl&gt;, ppob_media &lt;dbl&gt;, ppob_sup &lt;dbl&gt;, ## # ocviv &lt;dbl&gt;, occu &lt;dbl&gt;, pintegra4_ &lt;dbl&gt;, pintegra6_ &lt;dbl&gt;, ## # pintegra8_ &lt;dbl&gt;, ppob_5_o_m &lt;dbl&gt;, ppob_3_o_m &lt;dbl&gt;, ppob_1 &lt;dbl&gt;, ## # ppob_1dorm &lt;dbl&gt;, ppob_2dorm &lt;dbl&gt;, ppob_3dorm &lt;dbl&gt;, pviv_ocu5_ &lt;dbl&gt;, … Si deseas ordenar de forma descendente, es decir, de mayor a menor hay que introducir una función adicional, desc(), por ejemplo: covid_zmvm %&gt;% dplyr::arrange(cvemun,desc(ext)) ## # A tibble: 76 × 56 ## cvemun cve_mun nom_mun cve_ent nom_ent nom_abr nom_zm cvm ext pob20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 09002 002 Azcapotzalco 09 Ciudad… CDMX Valle… 9.01 33.5 4.32e5 ## 2 09003 003 Coyoacán 09 Ciudad… CDMX Valle… 9.01 53.9 6.14e5 ## 3 09004 004 Cuajimalpa … 09 Ciudad… CDMX Valle… 9.01 71.2 2.18e5 ## 4 09005 005 Gustavo A. … 09 Ciudad… CDMX Valle… 9.01 87.9 1.17e6 ## 5 09006 006 Iztacalco 09 Ciudad… CDMX Valle… 9.01 23.1 4.05e5 ## 6 09007 007 Iztapalapa 09 Ciudad… CDMX Valle… 9.01 113. 1.84e6 ## 7 09008 008 La Magdalen… 09 Ciudad… CDMX Valle… 9.01 63.4 2.48e5 ## 8 09009 009 Milpa Alta 09 Ciudad… CDMX Valle… 9.01 298. 1.53e5 ## 9 09010 010 Álvaro Obre… 09 Ciudad… CDMX Valle… 9.01 96.2 7.59e5 ## 10 09011 011 Tláhuac 09 Ciudad… CDMX Valle… 9.01 85.8 3.92e5 ## # ℹ 66 more rows ## # ℹ 46 more variables: pob20_h &lt;dbl&gt;, pob20_m &lt;dbl&gt;, positivos &lt;dbl&gt;, ## # defuncione &lt;dbl&gt;, pos_mil &lt;dbl&gt;, pos_hab &lt;dbl&gt;, def_hab &lt;dbl&gt;, ss &lt;dbl&gt;, ## # ppob_sines &lt;dbl&gt;, ppob_basi &lt;dbl&gt;, ppob_media &lt;dbl&gt;, ppob_sup &lt;dbl&gt;, ## # ocviv &lt;dbl&gt;, occu &lt;dbl&gt;, pintegra4_ &lt;dbl&gt;, pintegra6_ &lt;dbl&gt;, ## # pintegra8_ &lt;dbl&gt;, ppob_5_o_m &lt;dbl&gt;, ppob_3_o_m &lt;dbl&gt;, ppob_1 &lt;dbl&gt;, ## # ppob_1dorm &lt;dbl&gt;, ppob_2dorm &lt;dbl&gt;, ppob_3dorm &lt;dbl&gt;, pviv_ocu5_ &lt;dbl&gt;, … La función anterior primero ordena los municipios con base en la clave de municipio de forma ascendente (de menor a mayor) y simultáneamente coloca de mayor a menor los municipios de acuerdo con su extensión territorial. Ejercicios Genera una nueva base de datos (un nuevo objeto) que contenga sólo las alcaldías de la Ciudad de México y las variables casos por cada mil habitantes, luego ordena esa base de datos con arreglo al número de casos positivos de forma ascendente. Has lo mismo que en el inciso anterior, pero para los municipios del Estado de México y las defunciones. 1.5.4 Crear nuevas variables Para añadir una nueva variable a partir de las existentes recurrimos a la función mutate(). Por ejemplo, calculemos la variable densidad de casos positivos, es decir, número de casos positivos dividido entre la extensión territorial, a dicha variable la llamaremos pos_den. Pero antes, generemos una nueva base de datos que sólo contenga algunas de las variables de la base original: covid_zmvm %&gt;% dplyr::select(cvemun,nom_mun,positivos,pos_hab,ext) %&gt;% dplyr::mutate(pos_den=positivos/ext) ## # A tibble: 76 × 6 ## cvemun nom_mun positivos pos_hab ext pos_den ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 09010 Álvaro Obregón 10905 14.4 96.2 113. ## 2 09012 Tlalpan 11887 17.0 310. 38.3 ## 3 09015 Cuauhtémoc 7289 13.4 32.5 224. ## 4 09017 Venustiano Carranza 7172 16.2 33.9 212. ## 5 09011 Tláhuac 6812 17.4 85.8 79.4 ## 6 09002 Azcapotzalco 7483 17.3 33.5 223. ## 7 09003 Coyoacán 9182 14.9 53.9 170. ## 8 09013 Xochimilco 7696 17.4 118. 65.1 ## 9 09004 Cuajimalpa de Morelos 4071 18.7 71.2 57.2 ## 10 09016 Miguel Hidalgo 5669 13.7 46.4 122. ## # ℹ 66 more rows Notarás como la nueva variable aparece al final de la base. Vamos a intentar unir varios de los elementos que hemos aprendido hasta este punto para que veas el potencial de uso de la tubería. Vamos a crear una nueva variable, la misma que hace un momento, luego, tomaremos la base que contiene la nueva variable y construiremos con ella un diagrama de dispersión con la nueva variable y el número de defunciones por cada mil habitantes: covid_zmvm %&gt;% dplyr::mutate(pos_den=positivos/ext) %&gt;% ggplot2::ggplot()+ ggplot2::geom_point(aes(x=pos_den,y=def_hab))+ ggplot2::geom_smooth(aes(x=pos_den,y=def_hab))+ ggplot2::labs(x=&quot;Densidad de casos positivos&quot;,y=&quot;Defunciones por cada mil habitantes&quot;) ¿Cómo explicarías la relación entre la variable creada, densidad de casos positivos, y las defunciones por cada mil habitables? Ejercicio ¿Cómo añadirías más de una nueva variable a tu base original? Construye la variable “densidad de defunciones” y grafícala en un diagrama de dispersión con la densidad de población? Usa las cañerías. Quizá lo que te interese sea sólo quedarte con algunas de las variables originales y las nuevas variables creadas a partir de la información de la variable original. Para ello es útil la función transmute(): covid_zmvm %&gt;% dplyr::transmute(cvemun,nom_mun,pos_hab,def_hab,pos_den=positivos/ext,def_den=defuncione/ext) ## # A tibble: 76 × 6 ## cvemun nom_mun pos_hab def_hab pos_den def_den ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 09010 Álvaro Obregón 14.4 1.14 113. 9.03 ## 2 09012 Tlalpan 17.0 0.774 38.3 1.75 ## 3 09015 Cuauhtémoc 13.4 1.38 224. 23.2 ## 4 09017 Venustiano Carranza 16.2 1.32 212. 17.3 ## 5 09011 Tláhuac 17.4 0.737 79.4 3.37 ## 6 09002 Azcapotzalco 17.3 1.65 223. 21.3 ## 7 09003 Coyoacán 14.9 1.05 170. 12.0 ## 8 09013 Xochimilco 17.4 0.898 65.1 3.36 ## 9 09004 Cuajimalpa de Morelos 18.7 0.873 57.2 2.67 ## 10 09016 Miguel Hidalgo 13.7 0.943 122. 8.43 ## # ℹ 66 more rows El segmento de código anterior genera una nueva base en la que sólo conservamos algunas de las variables originales y añadimos dos nuevas a partir de la información original. 1.5.5 Resúmenes de información y grupos Quizá una de las funciones más potentes y sencillas que tiene dplyr es la función de resumen (summarise()). Veamos cómo opera. Imagina que deseas un promedio del número de casos que terminaron en muerte: covid_zmvm %&gt;% dplyr::summarise(promedio_def=mean(def_hab, na.rm=TRUE)) ## # A tibble: 1 × 1 ## promedio_def ## &lt;dbl&gt; ## 1 0.683 Esto no es particularmente útil si se le compara con la función del paquete base de R, summary(), pero si la combinamos con la función de agrupamiento, group_by() la situación cambia. Imagina que queremos el promedio de casos positivos por cada mil habitantes para cada conjunto de municipios de las tres entidades que componen el Valle de México: covid_zmvm %&gt;% dplyr::group_by(cve_ent) %&gt;% summarise(promedio_def=mean(def_hab, na.rm=TRUE), promedio_pos=mean(pos_hab)) ## # A tibble: 3 × 3 ## cve_ent promedio_def promedio_pos ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 09 1.08 15.8 ## 2 13 0.790 5.88 ## 3 15 0.572 4.57 El en segmento de código anterior primero se agrupa la información con arreglo al criterio de clave de entidad, cve_ent, y luego, para cada grupo, calcula el resultado especificado: un promedio. Dentro de la función de resumen, summarise(), se puede llevar a cabo no sólo múltiples operaciones sino que estas pueden ser de diferente naturaleza, incluso por ejemplo una suma o una cuenta: covid_zmvm %&gt;% dplyr::group_by(cve_ent) %&gt;% dplyr::summarise(pob_tot=sum(pob20),im_medio=mean(im),tot_mun=n()) ## # A tibble: 3 × 4 ## cve_ent pob_tot im_medio tot_mun ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 09 9209944 60.1 16 ## 2 13 168302 59.4 1 ## 3 15 12426269 58.0 59 En el segmento de código anterior creamos tres variables de resumen: la población total, pob_tot, que es la suma de la población para cada entidad, el promedio del índice de marginación, im_medio y el número de municipios de cada entidad a través de una cuenta con la función n(). Ejercicio Construye una gráfica de barras donde aparezca el total de población para cada entidad, a partir del segmento código anterior. ¿Es posible agrupar la información con arreglo a otra variable? Construye algunas medidas de resumen con esos grupos y gráficalos. Estas y otras tantas funciones son desarrolladas con todo detalle y de forma muy amena en el citado libro de Wickham y Grolemund, R for data science, por lo que su estudio se recomienda ampliamente. En el siguiente capítulo continuamos con la exploración de la información, pero de un tipo particular de información: la información espacial. References "],["mapas-coropléticos-en-r.html", "2 Mapas coropléticos en R 2.1 Datos y datos espaciales 2.2 Los paquetes 2.3 Carga de la base de datos geográfica 2.4 Mapas coropléticos básicos 2.5 Personalización 2.6 Función para elementos de diseño de salida, tm_layout() 2.7 Mapa base interactivo 2.8 Cartograma 2.9 Centroides o coordenadas geométricas (punto medio) 2.10 Tópico adicional: Mapas de clasificación especial (elementos de programación)", " 2 Mapas coropléticos en R De forma reciente, la información georreferenciada y los Sistemas de Información Geográfica (SIG) se han vuelto instrumentos de primer orden para el análisis y la presentación de fenómenos asociados al territorO. Por ejemplo, los Tableros de la Universidad Johns Hopkins sobre la pandemia por COVID19, o los del Gobierno de México y CentroGeo han estado constantemente en los medios de comunicación. En este capítulo señalamos algunas de las características básicas de los datos y datos espaciales; además, buscamos brindar algunos elementos generales que te permitan familiarizarte con ese tipo de información y representarla gráficamente para usarla para en el analisis de fenómenos socioterritoriales. 2.1 Datos y datos espaciales Los datos son valores, números o registros originados a partir de un proceso de recolección y procesamiento. Sirven para diversos propósitos, tales como el análisis científico y la toma de decisiones. Los datos se originan a partir de mediciones de conjuntos de objetos de la realidad. En estadística, ese conjunto de objetos de los que nos interesa saber ciertas cosas es denominado población y la información de sus rasgos o características es denominada variable o atributo. Así, las variables son medidas o características de los elementos que conforman la población. Si quieres explorar un poco y ampliar tus nociones de qué es un dato, puedes visitar esta entrada de Wikipedia o esta de Economipedia. Las variables pueden ser medidas o información sobre cualidades de los objetos (por ejemplo, de una persona si está vacunada o no, si está enferma o sana) o bien, medidas o información sobre cantidades o elementos numéricos (por ejemplo, de una persona interesaría saber el número de dosis de una vacuna recibida, su edad) y las llamamos respectivamente variables cualitativas y variables cuantitativas. Un tipo particular de datos son lo que denominamos datos espaciales o datos georeferenciados. Cuando utilizamos el apelativo “espaciales” o “georeferenciados” estamos diciendo con ello que dichos datos refieren un punto específico sobre la superficie terrestre. El proceso de referenciar geográficamente un dato es complejo pues remite a problemas sobre la forma de la Tierra y de su representación bidimencional a través de proyecciones cartográficas. Recomendamos revisar este material de Antonio Vazquez Brust donde se abunda sobre este problema. Los datos espaciales se caracterizan por poseer tres componentes: localización, atributos y tiempo, aunque éste último no siempre aparece en todos los tipos de datos espaciales. Hay otro aspecto relacionado con la información espacial denominado calidad del dato geoespacial, que se refiere a que nuestro conjunto de información cumpla los requisitos necesarios para satisfacer la necesidad para la que se recabó, es decir, que sea útil para resolver la pregunta que motivó su recolección o uso. La realidad es continua y compleja, además, muchos fenómenos tienen un referente territorial, es decir, se desarrollan y ocurren en un determinado lugar. Los datos espaciales implican un esfuerzo de abstracción, es decir, de simplificación de la realidad, a esto se le suele denominar a veces modelo espacial (Olaya 2020). Este proceso de abstracción consiste en “reducir o dividir esta continuidad en entidades numéricas discretas, observables y susceptibles de medición matemática” (así, un dato espacial podría definirse como la) “observación de una variable asociada a una localización del espacio geográfico” (Chasco 2003, 17). De forma general, es posible dividir los modelos de representación espacial de la realidad en dos tipos: ráster y vectorial. Datos vectoriales: es una forma de representación de la realidad que consiste en el uso ya sea de puntos, líneas o polígonos (llamados a veces “primitivas”) para recoger ciertos aspectos de la realidad. ¿A través de qué primitiva podrías representar un camino? ¿Cuál para representar los límites de la localidad donde resides? Quizá podrías representar una vialidad a través de líneas, por un lado, y la representación de los límites administrativos de una región, a través de polígonos, por otro. Datos ráster: es un modelo de representación espacial que “se basa en una división sistemática del espacio, la cual cubre todo este, caracterizándolo como un conjunto de unidades elementales” a los que se llama pixeles (Olaya 2020: Modelos de representación) . Por ejemplo, pueden ser imágenes del territorio provenientes de instrumentos ópticos (cámaras, sensores, aunqe no se limitan a ellos) e incluso el propio usuario puede definir dicha representación.3 Una definición mucho más cuidadosa de datos ráster y vectoriales puede encontrarse en el libro de Víctor Olaya sobre sistemas de información geográfica donde trata los tipos de datos espaciales. Los ejemplos y ejercicios aquí desarrollado utilizan información espacial de tipo vectorial. Diversos formatos sirven para almacenar información de tipo vectorial, tales como Shape, GeoJSON (Geográphical JavaScript Object Notation), GeoPackage o KML (Keyhole Markup Language). No obstante, los shapefiles son aún los más comunes. Los archivos vectoriales de tipo SHP fueron originalmente desarrollados por ESRI, empresa dedicada a la cosultoría de fenómenos territoriales, aunque hoy día se usan extensamente prácticamente en cualquier ámbito de interés científico donde se involucre el territorio. Almacenar información vectorial en un archivo SHP implica usar en realidad, al menos, tres archivos diferentes: Archivo con extensión .shp: es un archivo que almacena las entidades espaciales representadas ya sea a través de puntos, líneas o polígonos. Archivo con extensión .dbf (data base file): contiene los atributos o variables de cada objeto espacial contenido en el archivo shp. Archivo con extensión .shx (index file): archivo que sirve de vínculo entre los dos previos. Estos tres archivos son los mínimos indispensables para poder trabajar con modelos vectoriales de información, sin embargo no son los únicos que encontrarás cuando descargues archivos de este tipo. 4 Los tres archivos deben llevar el mismo nombre y deben estar en el mismo directorio (carpeta) para poder funcionar correctamente, es decir, para que sean leídos por el sistema de cómputo. Para que comprendas lo anterior a cabalidad, descarga el conjunto de información espacial que usaremos aquí. Tras descomprimir la carpeta verás que contiene, entre otros, los siguientes archivos: covid_zmvm.shp covid_zmvm.shx covid_zmvm.dbf Ejercicio Revisa la carpeta descargada y descomprimida para responder a lo siguiente: ¿Cuáles son las extensiones de los otros archivos del mismo nombre? ¿Para qué sirven esos archivos? Como señalamos, la exploración y representación de la información a través de mapas es no sólo una herramienta muy útil y potente, sino que se ha popularizado en los últimos años gracias no sólo a la mayor disponibilidad de información georreferenciada, sino a la facilidad con la que ahora se puede acceder a software para la gestión y manipulación de este tipo de información. Existen diversas herramientas informáticas para el análisis de información espacial y su representación, entre ellos se cuentan las alternativas de ESRI: ArcGIS y ArcMap. No obstante, el precio de una licencia individual es considerablemente alto, convirtiéndolos en inaccesibles a las mayorías. Una alternativa cada vez más popular es el programa QGIS, una iniciativa de software libre de la Fundación OSGeo, o bien, otra posibilidad también libre es el intuitivo programa de Luc Anselin (Universidad de Chicago, Centro para las Ciencias de Datos Espaciales) y su equipo para iniciarse en el análisis espacial, GeoDa. Además, la popularidad del software R como plataforma de análisis ha hecho que múltiples entusiastas programadoras interesadas en el análisis espacial desarrollaran paquetes enfocados en el tratamiento y representación de datos espaciales para esta plataforma. Además de ser libre, entre las ventajas que tiene usar R como Sistema de Información Goegráfica y como espacio para el tratamiento de datos espaciales, es que se puede tener no sólo pleno control de la edición de los materiales cartográficos, sino que permite un entorno de trabajo integrado. En el resto de este capítulo se presenta de forma introductoria la manera en que R, a través de algunos paquetes, se puede convertir en un espacio de edición de mapas. 2.2 Los paquetes Para la elaboración de mapas en R requeriremos de los siguientes paquetes: sf: un paquete útil para manejar datos espaciales de tipo vectorial. tmap: permite crear mapas para visualizar cómo una variable se distribuye en el espacio. RColorBrewer: proporciona esquemas de colores para los mapas, basados en el trabajo de la cartógrafa estadounidense Cynthia Brewer. cartogram: permite elaborar un tipo de mapa llamado cartograma. Instala estos paquetes como es usual: install.packages(c(&quot;sf&quot;,&quot;tmap&quot;, &quot;RColorBrewer&quot;, &quot;cartogram&quot; )) Para cargar los paquetes recién instalados procedemos como es habitual: library(sf) library(tmap) library(RColorBrewer) library(cartogram) 2.3 Carga de la base de datos geográfica La base de datos a utilizar es la misma que la del capítulo anterior, es decir, información de la primera ola de la pandemia por COVID19 en la Zona Metropolitana del Valle de México, en el centro de México, y algunas variables sociodemográficas y económicas de sus unidades espaciales; no obstante, ahora haremos uso del shapefile (recuerda, al menos tres archivos: .shp, .dbf y .shx). Debemos cargar la geometría asociada a nuestra base de datos, el archivo con extensión .shp, para ello hay que usar la función st_read() del paquete sf: zmvm_cov_sf &lt;-sf::st_read(&quot;base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp&quot;) ## Reading layer `covid_zmvm&#39; from data source ## `C:\\repos\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 76 features and 57 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5 ## Projected CRS: Lambert_Conformal_Conic Nota cómo aparece en tu entorno de trabajo el objeto solicitado, zmvm_cov_sf, ¿logras ver qué tipo de objeto es? Tener esto en mente te ayudará a comprender que dependiendo del tipo de objeto con el que interactuas en R habrá determinadas funciones que podrás usar. Así como hay algunas funciones que nos permiten familiarizarnos con los objetos de tipo dataframe, las hay para el caso de los objetos de tipo sf (simple features). La función st_crs() nos ofrece información sobre el sistema de coordenadas de referencia, coordinate reference system (crs), de la capa cargada: sf::st_crs(zmvm_cov_sf) ## Coordinate Reference System: ## User input: Lambert_Conformal_Conic ## wkt: ## PROJCRS[&quot;Lambert_Conformal_Conic&quot;, ## BASEGEOGCRS[&quot;GCS_GRS_1980_IUGG_1980&quot;, ## DATUM[&quot;D_unknown&quot;, ## ELLIPSOID[&quot;GRS80&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433]]], ## CONVERSION[&quot;unnamed&quot;, ## METHOD[&quot;Lambert Conic Conformal (2SP)&quot;, ## ID[&quot;EPSG&quot;,9802]], ## PARAMETER[&quot;Latitude of false origin&quot;,12, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8821]], ## PARAMETER[&quot;Longitude of false origin&quot;,-102, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8822]], ## PARAMETER[&quot;Latitude of 1st standard parallel&quot;,17.5, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8823]], ## PARAMETER[&quot;Latitude of 2nd standard parallel&quot;,29.5, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8824]], ## PARAMETER[&quot;Easting at false origin&quot;,2500000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8826]], ## PARAMETER[&quot;Northing at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8827]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]] De la enorme cantidad de información que aparece en tu consola, nota cómo la proyección de nuestra base de datos geográfica es la Cónica Conforme de Labert, Lambert_Conformal_Conic. En su Introducción ligera a los SIG el equipo de QGIS señala que “Con la ayuda de Sistemas de Referencia de Coordenadas (SRC) cualquier punto de la tierra puede ser definido por tres números denominados coordenadas. En general, los CRS se pueden dividir en sistemas de referencia de coordenadas proyectados (también denominados Cartesianos o sistemas de referencia de coordenadas rectangulares) y sistemas de referencia de coordenadas geográficos”. Los sistemas proyectados están basados en determinada representación de los sistemas geográficos. La selección de un adecuado CRS asegura que las características deseadas se representen en el mapa de forma precisa. Existen varios elementos que diferencian los Sistema de Coordenadas Proyectadas (PCS) y los Sistema de coordenadas geográficas (GCS). Mientras que un “sistema de coordenadas geográficas es un método para describir la posición de una ubicación geográfica en la superficie de la Tierra utilizando mediciones esféricas de latitud y longitud (además de que) se trata de mediciones de los ángulos (en grados) desde el centro de la Tierra hasta un punto en la superficie de la Tierra representada como una esfera”, tal y como se señala en la ayuda del sitio de internet de la aplicación de sistemas de información geográfica (SIG), ArcMap (ESRI 2023b). En tanto, “un sistema de coordenadas proyectadas se define sobre una superficie plana de dos dimensiones. A diferencia de un sistema de coordenadas geográficas, un sistema de coordenadas proyectadas posee longitudes, ángulos y áreas constantes en las dos dimensiones. Un sistema de coordenadas proyectadas siempre está basado en un sistema de coordenadas geográficas basado en una esfera o un esferoide” (ESRI 2023a). Cuando se trabaja con datos espaciales es importante que todos ellos estén definidos en el mismo SRC, para que sean compatibles entre sí. Por ejemplo, si tengo datos de puntos que representan hospitales en un barrio, digamos el barrio Central, pero el polígono que representa a dicho barrio está en un SRC diferente al de los puntos, la representación de los hospitales en el polígono no se corresponderá con la realidad. A través de diversos programas es posible cambiar el sistema de referencia de coordenadas, es decir, reproyectar un conjunto de datos vectoriales desde un CRS a otro distinto. Es particularmente útil cambiar la proyección desde un sistema de coordenadas geográficas a un sistema de coordenadas proyectadas cuando se pretende calcular distancias. Esta operación puede ejecutarse de forma muy sencilla en programas como QGIS, o bien en R, pero ello está fuera del interés de este capítulo. Usando la función plot() que pertenece al paquete graphics, es posible elaborar una primera representación de algunas de las variables de nuestra base. Esto es sólo para darnos cuenta de que ahora tratamos con información espacial, es decir, objetos espaciales (unidades adminsitrativas: alcaldías y municipios) a los que se asocian determinados atributos o variables. graphics::plot(zmvm_cov_sf) ## Warning: plotting the first 9 out of 57 attributes; use max.plot = 57 to plot ## all 2.4 Mapas coropléticos básicos Los mapas de coropletas o mapas coropléticos se construyen en R con el paquete tmap. Este tipo de mapas, que pertenece a la categoría más general de los llamadso mapas temáticos, permiten representar la distribución espacial de una variable, es decir, cómo luce la variable representada el espacio geográfico considerado; por ejemplo, el número de casos positivos por COVID19 en alguna región de México. De nueva cuenta, te recomendamos revisar el libro de Víctor Olaya, en la sección El mapa y la comunicación cartográfica, para una exposición más amplia sobre la representación de información espacial a través de mapas. La lógica de la construcción de mapas con tmap es semejante a la de ggplot2: se usan “enunciados” para completar diferentes elementos del mapa. Así, para construir un mapa completo habrá que indicar, la mayor parte de las veces, tres elementos: i) la geometría de origen (función tm_shape()), ii) los límites internos de las formas utilizadas (función tm_borders()), y iii) la manera en que han de ser rellenados con colores los polígonos según los valores de una variable (función tm_fill()). Así, la elaboración de nuestro mapa implica un segmento de código con tres elementos: tmap::tm_shape(&quot;capa shp origen de la información&quot;)+ tmap::tm_borders(&quot;elementos de edición del borde interno&quot;)+ tmap::tm_fill(&quot;elementos de edición del relleno y representación de la variable&quot;) Resulta evidente que el segmento de código anterior arrojará un error si lo ejecutas en tu consola, pues hay que especificar los argumentos del caso, sustituyendo las expresiones entrecomilladas. Por ejemplo, para representar nuestra información usando los tres elementos para la variable pos_hab, número de casos positivos de COVID19 por cada mil habitantes, tenemos: tmap::tm_shape(shp=zmvm_cov_sf)+ tmap::tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;) Nota cómo dentro de la función de borde se encuentran operando argumentos por defecto, es decir, que sin indicar ningún argumento específico (como tipo de borde o ancho), arroja un resultado. Sobre dichos argumentos volveremos después. Ejercicio Prueba eliminando alternativamente una de las funciones de la triada anterior. ¿Qué resultado obtienes con cada combinación? Alternativamente, una manera rápida de suplir las funciones de borde y relleno es sustituirlas con la función tm_polygons() tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_polygons(col=&quot;pos_hab&quot;) La función tm_polygons() representará la variable indicada en el argumento col=. Además, tanto en esta función como en la de relleno, tm_fill(), el método de clasificación es el método o estilo estético o “bonito” en el que los cortes o categorías se dividen en números enteros siempre que sea posible y los espacia uniformemente (argumento style = \"pretty\"). Con lo anterior, tienes ya las bases y la lógica para construir mapas coropléticos en R, lo demás no son más que elementos de personalización para cada una de las funciones previas, además de la incorporación de funciones adicionales para elaborar mapas mucho más profesionales. Te recomendamos consultar, además del libro de Víctor Olaya ya comentado previamente, el espléndido material Geocomputation with R de (Lovelace, Nowosad, and Muenchow 2019), así como el material y todo lo relacionado con el paquete tmap en tmap: Thematic Maps in R (Tennekes 2018). 2.5 Personalización 2.5.1 Argumentos de personalización dentro de la función de relleno 2.5.1.1 Paleta de colores Nos concentraremos ahora en revisar cómo podemos hacer “mapas a la medida”, es decir, personalizar prácticamente cada parte del mapa, desde la paleta de colores hasta títulos y leyendas. Comencemos revisando las posibilidades de personalización dentro de la función de relleno, tm_fill(). Veremos primero cómo cambiar la paleta de colores de relleno a través del argumento palette=. Por defecto, los colores corresponden a una gama cromática de rojos, pero podemos cambiarla a una de color naranja: tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;, palette = &quot;Oranges&quot;) Ejercicio Prueba cambiar el esquema de colores por aquellos que sean de tu gusto, ¿cuáles son los colores admitidos por R dentro del argumento de paleta? Una manera alternativa de cambiar la paleta de colores consiste en fijar un color para la clase inicial y uno para la clase final, como si se tratara de colores ancla. Para ello nos servimos de un vector como valor del argumento palette=: tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;, palette = c(&quot;blue&quot;,&quot;red&quot;)) Para tener una paleta divergente insertamos otro color en el centro: tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;, palette = c(&quot;blue&quot;,&quot;white&quot;, &quot;red&quot;)) No obstante, la manera más adecuada para la selección de una paleta de colores en términos de comunicación visual es través las propuestas de especialistas, como las de la célebre cartógrafa estadounidense Cynthia Brewer. El uso de una determinada paleta de colores depende de la idea a transmitir, del tipo de datos a representar, de si el mapa será impreso o digital e incluso de a quién va dirigido. El términos generales, se puede hacer uso de tres tipos de paletas: i) secuenciales, ii) divergentes y iii) cuantitativas. Las primeras son adecuadas para ordenar datos en forma ascendente, lo que permite observar la distribución espacial de la variable y facilita la identificación de patrones de asociación, en tanto, las paletas divergentes están diseñadas para hacer énfasis en valores extremos (muy altos o muy bajos) y facilitan su identificación en el espacio, finalmente, las paletas cuantitativas buscan clasificar variables categóricas, sin algún orden de magnitud definido. Una exposición más detallada de las paletas de colores y sus usos puede ser consultada aquí. La doctora Brewer diseñó toda una serie de paletas para la representación de información espacial en función del tipo de variable a representar y del número de categorías deseadas para clasificar la información. En R, se cuenta con el paquete RColorBrewer que permite, siguiendo los principios de la doctora Brewer, elegir entre una amplia gama de posibilidades de paletas de colores, ya sea secuenciales, divergentes o cualitativas. Además, el paquete permite crear un esquema de colores personalizado indicando los “colores ancla” y el número de categorías. Ilustremos cómo nos servimos del paquete RColorBrewer para crear una paleta de seis colores, en una gama cromática del azul al verde. La función brewer.pal() es la indicada para ello: RColorBrewer::brewer.pal(6,&quot;BuGn&quot;) ## [1] &quot;#EDF8FB&quot; &quot;#CCECE6&quot; &quot;#99D8C9&quot; &quot;#66C2A4&quot; &quot;#2CA25F&quot; &quot;#006D2C&quot; Los códigos hexagesimales resultado de la función anterior poco nos dicen sobre colores, entonces, hay que visualizarlos. Para ello hemos de usar la función display.brewer.pal() RColorBrewer::display.brewer.pal(6,&quot;BuGn&quot;) Ahora, ya que tenemos una paleta que luce más estética, podemos aplicarla a nuestro mapa: tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;, palette = &quot;BuGn&quot; ) Ejercicio Solicita ayuda del paquete RColorBrewer para ver los diferentes tipos de paletas (con las que se cuenta (secuenciales, divergentes y cualitativas) e intenta hacer algunos mapas con dichas paletas. Ejecuta el siguiente segmento de código para explorar una aplicación con shiny, otro paquete más de R, que te permitirá ver todas las posibilidades de paletas creadas por la doctora Brewer. install.packages(&quot;shiny&quot;,&quot;shinyjs&quot;)#Instala Shiny y sus dependencias tmaptools::palette_explorer() #Lanza la aplicación de Shiny para mostrar las paletas Brewer #El modo estatico del libro no permite que se pueda correr este codigo, no obstante, con R en ejecución no existe ningún problema 2.5.1.2 Leyenda Para modificar el título de la leyenda, el argumento del caso es justamente title=, dentro de la función tm_fill(), a través de una cadena de texto. Por ejemplo: tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;, title = &quot;Casos positivos COVID19&quot;) Podemos también agregar elementos informativos adicionales a nuestro mapa, como un histograma, a través del argumento lógico legend.hist=: tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;, title = &quot;Casos positivos COVID19&quot;,legend.hist = TRUE) Como habrás notado, hay infinidad de elementos que es posible modificar. Particularmente para el caso de la leyenda del mapa, su formato está gobernado por el argumento legend.format= en el que es posible especificar desde las etiquetas, el formato de los números, sufijos o prefijos de las categorías, entre otros tantos elementos. Solicita ayuda sobre la función tm_fill() y observa los argumentos que la integran. Cerramos esta sección mencionando únicamente cómo cambiarla leyenda de nuestro mapa para que las etiquetas de ésta queden en castellano (para que diga “0 a 5” en vez de “0 to 5”). Esto lo hacemos añadiendo el argumento citado, legend.format=, en el que especificamos, mediante una lista, cómo debe lucir el texto que separa los valores de cada una de las categorías de nuestra leyenda legend.format = list(text.separator=\"a\"): tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;, palette = &quot;BuGn&quot;, legend.format = list(text.separator=&quot;a&quot;) ) 2.5.1.3 Métodos de clasificación en los mápas Quizá el elemento de personalización más importante en la edición de mapas tiene que ver con el método de clasificación de la variable que busca ser representada. Todas las alternativas de cómo deben construirse las categorías resultado de la clasificación deben ser especificadas dentro de la función tm_fill(). Hay diferentes métodos de clasificación y, por tanto, diferentes tipos de mapas según el método de clasificación. Podemos organizar dichos tipos de mapas de coropletas en tres grandes familias, tal como se muestra en el cuadro 2.1: Cuadro 2.1. Formas de clasificación de familias de mapas Familia Objetivo Tipos de mapa Clasificación común Conocer la distribución espacial del atributo de interés a través de diversos métodos de clasificación. - Cuantiles - Intervalos iguales - Cortes naturales - Jenks - Cortes estéticos Clasificación especial Conocer si la variable presenta algún valor atípico y representar su ubicación. - Percentiles - Desviación estándar Clasificación por categorías preexistentes Conocer la distribución espacial de una categoría predefinida. - Caja - Valores únicos Todos estos tipos de mapas pueden ser fácilmente invocados en un programa como GeoDa o QGIS, sin embargo, hasta donde sabemos, en R no todos están disponibles y a veces la construcción de alguno de ellos exige algunos fundamentos de programación. A pesar de ello, R ofrece bastantes alternativas sencillas para construir mapas de clasificación común. Ejercicio Ve a la ayuda de la función tm_fill(), identifica el argumento style=, sigue la documentación sugerida y responde: ¿Cuántos métodos de clasificación ofrece R? ¿En qué consiste el método de clasificación de k-medias? ¿En el trabajo de quiénes está basado el método de clasificación fisher?. Los mapas que es posible construir en R sin mayores complicaciones se muestran en el cuadro 2.2, así como los valores que debes indicar en el argumento del caso: Cuadro 2.2. Tipos de mapas Tipo de mapa Argumento tmap::tm_fill(style=) Cuantiles style=quantile Intervalos iguales style=equal Cortes naturales o Jenks style=jenks Cortes estéticos style=pretty Como se dijo, los mapas de clasificación común quedarán indicados en los argumentos de la función tm_fill(). Para construir un mapa de 5 cuantiles (quintiles), que es la opción por defecto para el número de categorías: tmap::tm_shape(zmvm_cov_sf) + tmap::tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;, style = &quot;quantile&quot;, title = &quot;Casos positivos&quot;)+ tmap::tm_layout(title = &quot;Mapa de cuantiles&quot;, title.position = c(&quot;center&quot;, &quot;bottom&quot;))#Más adelante se explica esta función Si deseas cambiar el número de categorías, por ejemplo a cuatro, deberás indicar explícitamente su número en el argumento n=: tmap::tm_shape(zmvm_cov_sf) + tmap:: tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;, n= 4, style = &quot;quantile&quot;, title = &quot;Casos positivos&quot;)+ tmap::tm_layout(title = &quot;Mapa de cuantiles&quot;, title.position = c(&quot;center&quot;, &quot;bottom&quot;))#Más adelante se explica esta función Ejercicios Construye: Un mapa de Jenks con 6 categorías. Un mapa de intervalos iguales con cuatro categorías. Un mapa a partir de la clasificación por clusters jerárquicos. 2.5.2 Argumentos de personalización del borde Se dijo antes que a pesar de no haber especificado argumento alguno en la función de borde, en ésta operan argumentos por defecto. Es momento de modificar dichos argumentos. Las opciones de borde se especifican dentro de tm_border() donde es posible modificar el color (col=), grosor (lwd=) y tipo de borde (lty=): tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_fill(&quot;pos_hab&quot;)+ tmap::tm_borders(col=&quot;black&quot;,lwd=2, lty = 3) Ejercicio Solicita ayuda de la función tm_fill() y explora qué otras opciones de borde existen. Haz algunos mapas para la variable def_hab cambiando el tipo de borde. 2.6 Función para elementos de diseño de salida, tm_layout() Con la combinación de las funciones previamente descritas se puede generar un mapa básico, además, dentro de ellas es posible personalizar múltiples elementos. No obstante, un diseño más adecuado y profesional es logrado a través de la función tm_layout(), que abarca, entre otras cosas, la posición de la leyenda, el título principal del mapa y tamaños de fuente. Veamos cómo opera. Para cambiar la posición de la leyenda, los argumentos deben colocarse dentro de la función, tm_layout(), a través de un vector que indique tanto la orientación vertical como la horizontal de la leyenda. Para la orientación horizontal: \"left\",\"right\" o \"center\" y para la vertical: \"top\", \"center\" o \"bottom\", tal que: tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_fill(&quot;pos_hab&quot;)+ tmap::tm_borders()+ tmap::tm_layout(legend.position = c(&quot;right&quot;, &quot;bottom&quot;)) Para posicionar la leyenda fuera del mapa, dentro de la función tm_layout() usa el argumento lógico legend.outside y si deseas especificar la posición, el argumento será legend.outside.position que tomará los valores tipo texto de \"top\", \"bottom\", \"right\" o \"left\". Por ejemplo: tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_fill(&quot;pos_hab&quot;)+ tmap::tm_borders()+ tmap::tm_layout(legend.outside = TRUE,legend.outside.position = &quot;left&quot;) Para personalizar el título dentro del mapa, en la función tm_layout() hay que agregar el argumento title= y para la posición: title.position=: tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;, title = &quot;Casos positivos COVID19&quot;)+ tmap::tm_layout(title = &quot;Casos positivos COVID19 por cada mil habitales&quot;, title.position = c(&quot;center&quot;,&quot;top&quot;)) O bien, si queremos el título afuera del mapa, con un tamaño de fuente diferente y centrado: tmap::tm_shape(zmvm_cov_sf)+ tmap:: tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;, title = &quot;Ingreso 2010&quot;)+ tmap::tm_layout(main.title = &quot;Casos positivos COVID19 por cada mil habitantes&quot;, main.title.position = &quot;center&quot;, title.size = 1.3) 2.7 Mapa base interactivo En R es posible añadir a nuestros mapas temáticos un mapa base para dar contexto a nuestra representación. Para ello, es necesario activar una suerte de “modo interactivo”, para ser más exactos, lo que activamos es el modo de visualización para poder desplazarnos sobre los mapas. Para cambiar al modo de visualización: tmap::tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing La función que permite agregar mapas base es tm_basemap(). Ejercicio ¿Cuántos tipos de mapas base es posible usar en R? Revisa la ayuda de la función y familiarízate con sus argumentos. De los argumentos para el mapa base, dos son los básicos: el servidor de donde tomaremos el mapa (consejo: ya dentro de los argumentos de la función, escribe providers$ y observarás todas las opciones disponibles) y transparencia (argumento alpha=) que toma valores de 0 a 1; el argumento alpha= altera tanto la transparencia del mapa base cuando es usado dentro de la función tm_basemap(), como la transparencia de los colores usados para representar la información si se usa dentro de la función tm_fill(). Ejecuta el siguiente segmento de código y navega sobre el mapa representado: tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;, alpha=0.7)+ tmap::tm_basemap(providers$OpenStreetMap,alpha = 0.5) Una vez que has terminado de navegar y trabajar con mapas base es necesario desactivar el modo de visualización y regresar al modo estático: tmap::tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting 2.8 Cartograma Un cartograma es un tipo de mapa que deforma la geometría de las áreas de interés en figuras cuyo tamaño depende de la magnitud de la variable representada. Para elaborar un cartograma con circulos, debemos primero generar la geometría deformada por la variable para luego rellenarla. Para tal efecto, usamos la función cartogram_dorling() que nos permitirá generar un nuevo objeto que contiene las geometrías deformadas. Luego, usaremos dicha geometría como argumento de tm_shape(). Primero creamos la nueva geometría circular: cartograma_circulos &lt;- cartogram::cartogram_dorling(zmvm_cov_sf,&quot;pos_hab&quot;) class(cartograma_circulos) ## [1] &quot;sf&quot; &quot;data.frame&quot; Hay que tener en cuenta que la función sólo admite variables no negativas. Ahora bien, este objeto recién creado será usado tal como lo hemos hecho antes: tmap::tm_shape(cartograma_circulos) + tmap::tm_borders()+ tmap:: tm_fill(&quot;pos_hab&quot;) Otro tipo de cartograma, más estético, es aquel que garantiza la contigüidad entre las unidades espaciales después de su deformación. Se procede de forma semejante a lo hecho antes, sólo que ahora usamos la función cartogram_cont(): cartograma_cont &lt;- cartogram_cont(zmvm_cov_sf,&quot;pos_hab&quot;) Ahora, usamos esta información para construir nuestro cartograma: tmap::tm_shape(cartograma_cont) + tmap::tm_fill(&quot;pos_hab&quot;) + tmap:: tm_borders() 2.9 Centroides o coordenadas geométricas (punto medio) En el blog de mappingGIS encontramos la siguiente definición de punto medio: Toda geometría vectorial (punto, línea o polígono) contiene un punto central denominado centroide. Calcular el centroide de una geometría suele ser una tarea habitual cuando se trabaja con información espacial, por ejemplo, para contar con un punto de referencia a partir del cual contar distancias lineales entre polígonos para definir vecindades, como se verá en el capítulo siguiente. Para crear una nueva capa que contenga los centroides de nuestra geometría usamos en R la función st_centroid(). La función generará una nueva serie de archivos SHP que contendrán el conjunto de variables de la base de datos y los centroides: zmvm_cntrds &lt;- st_centroid(zmvm_cov_sf) summary(zmvm_cntrds) ## cvemun cve_ent cve_mun nom_zm ## Length:76 Length:76 Length:76 Length:76 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## cve_zm nom_mun nom_ent nom_abr ## Min. :9.01 Length:76 Length:76 Length:76 ## 1st Qu.:9.01 Class :character Class :character Class :character ## Median :9.01 Mode :character Mode :character Mode :character ## Mean :9.01 ## 3rd Qu.:9.01 ## Max. :9.01 ## ext pob20 pob20_h pob20_m ## Min. : 3.17 Min. : 4862 Min. : 2338 Min. : 2524 ## 1st Qu.: 37.52 1st Qu.: 31900 1st Qu.: 15621 1st Qu.: 16302 ## Median : 76.22 Median : 160445 Median : 78574 Median : 81871 ## Mean :103.51 Mean : 286902 Mean :138458 Mean :148443 ## 3rd Qu.:157.01 3rd Qu.: 432692 3rd Qu.:206156 3rd Qu.:226858 ## Max. :434.26 Max. :1835486 Max. :887651 Max. :947835 ## positivos defuncione pos_mil lag_poshab ## Min. : 18.0 Min. : 0.0 Min. : 1.720 Min. : 2.382 ## 1st Qu.: 131.5 1st Qu.: 13.5 1st Qu.: 3.337 1st Qu.: 3.977 ## Median : 767.0 Median : 86.0 Median : 5.117 Median : 5.275 ## Mean : 2614.0 Mean : 269.7 Mean : 6.964 Mean : 6.733 ## 3rd Qu.: 3617.2 3rd Qu.: 387.0 3rd Qu.: 8.636 3rd Qu.: 8.229 ## Max. :18767.0 Max. :2078.0 Max. :22.700 Max. :16.443 ## pos_hab def_hab ss ppob_sines ## Min. : 1.720 Min. :0.0000 Min. :0.5480 Min. :0.004937 ## 1st Qu.: 3.337 1st Qu.:0.4222 1st Qu.:0.6415 1st Qu.:0.019428 ## Median : 5.117 Median :0.6528 Median :0.6763 Median :0.023845 ## Mean : 6.964 Mean :0.6735 Mean :0.6793 Mean :0.026127 ## 3rd Qu.: 8.636 3rd Qu.:0.8791 3rd Qu.:0.7266 3rd Qu.:0.031717 ## Max. :22.700 Max. :1.6497 Max. :0.7980 Max. :0.064699 ## ppob_basi ppob_media ppob_sup ocviv ## Min. :0.1410 Min. :0.1645 Min. :0.07101 Min. :2.460 ## 1st Qu.:0.4481 1st Qu.:0.2466 1st Qu.:0.13614 1st Qu.:3.530 ## Median :0.5295 Median :0.2644 Median :0.17064 Median :3.745 ## Mean :0.5041 Mean :0.2624 Mean :0.20549 Mean :3.690 ## 3rd Qu.:0.5718 3rd Qu.:0.2831 3rd Qu.:0.25936 3rd Qu.:3.900 ## Max. :0.6842 Max. :0.3235 Max. :0.67480 Max. :4.520 ## occu pintegra4_ pintegra6_ pintegra8_ ## Min. :0.5600 Min. :0.3657 Min. :0.06644 Min. :0.01745 ## 1st Qu.:0.8600 1st Qu.:0.6772 1st Qu.:0.22736 1st Qu.:0.07521 ## Median :0.9900 Median :0.7161 Median :0.26250 Median :0.08992 ## Mean :0.9692 Mean :0.7013 Mean :0.26006 Mean :0.09188 ## 3rd Qu.:1.0625 3rd Qu.:0.7474 3rd Qu.:0.29090 3rd Qu.:0.10844 ## Max. :1.2900 Max. :0.8261 Max. :0.42810 Max. :0.20046 ## ppob_5_o_m ppob_3_o_m ppob_1 ppob_1dorm ## Min. :0.6825 Min. :0.1680 Min. :0.00611 Min. :0.08567 ## 1st Qu.:0.7781 1st Qu.:0.3425 1st Qu.:0.03234 1st Qu.:0.20592 ## Median :0.8183 Median :0.3925 Median :0.04198 Median :0.22763 ## Mean :0.8133 Mean :0.3934 Mean :0.04413 Mean :0.23367 ## 3rd Qu.:0.8569 3rd Qu.:0.4406 3rd Qu.:0.05341 3rd Qu.:0.26855 ## Max. :0.9150 Max. :0.5947 Max. :0.09461 Max. :0.38234 ## ppob_2dorm ppob_3dorm pviv_ocu5_ pviv_ocu7_ ## Min. :0.4875 Min. :0.7781 Min. :0.0654 Min. :0.009724 ## 1st Qu.:0.5883 1st Qu.:0.8581 1st Qu.:0.2434 1st Qu.:0.056245 ## Median :0.6237 Median :0.8848 Median :0.2835 Median :0.069583 ## Mean :0.6307 Mean :0.8808 Mean :0.2799 Mean :0.069614 ## 3rd Qu.:0.6757 3rd Qu.:0.9042 3rd Qu.:0.3209 3rd Qu.:0.082869 ## Max. :0.8006 Max. :0.9484 Max. :0.4420 Max. :0.155107 ## pviv_ocu9_ analf sbasc vhac ## Min. :0.002354 Min. :0.3534 Min. : 5.535 Min. : 3.95 ## 1st Qu.:0.015350 1st Qu.:1.6505 1st Qu.:19.915 1st Qu.:16.40 ## Median :0.020738 Median :1.9798 Median :22.973 Median :21.19 ## Mean :0.020961 Mean :2.3555 Mean :23.160 Mean :21.01 ## 3rd Qu.:0.025183 3rd Qu.:2.8267 3rd Qu.:27.193 3rd Qu.:25.00 ## Max. :0.060174 Max. :7.4096 Max. :41.399 Max. :38.81 ## po2sm idh2015 im gm_2020 ## Min. :28.45 Min. :0.6240 Min. :53.57 Length:76 ## 1st Qu.:61.27 1st Qu.:0.7288 1st Qu.:57.36 Class :character ## Median :67.40 Median :0.7585 Median :58.46 Mode :character ## Mean :66.38 Mean :0.7629 Mean :58.47 ## 3rd Qu.:72.76 3rd Qu.:0.7945 3rd Qu.:59.65 ## Max. :86.67 Max. :0.9290 Max. :62.36 ## grad grad_h grad_m poind ## Min. : 8.080 Min. : 8.140 Min. : 8.030 Min. :0.06475 ## 1st Qu.: 9.557 1st Qu.: 9.658 1st Qu.: 9.490 1st Qu.:0.11379 ## Median :10.025 Median :10.160 Median : 9.945 Median :0.17993 ## Mean :10.252 Mean :10.376 Mean :10.139 Mean :0.19532 ## 3rd Qu.:10.840 3rd Qu.:10.967 3rd Qu.:10.730 3rd Qu.:0.26407 ## Max. :14.550 Max. :14.930 Max. :14.220 Max. :0.44291 ## pocom poss tmind tmcom ## Min. :0.1269 Min. :0.2113 Min. : 1.636 Min. : 1.380 ## 1st Qu.:0.3363 1st Qu.:0.3033 1st Qu.: 3.166 1st Qu.: 1.876 ## Median :0.4358 Median :0.3610 Median : 5.086 Median : 2.139 ## Mean :0.4085 Mean :0.3961 Mean : 9.086 Mean : 2.759 ## 3rd Qu.:0.4970 3rd Qu.:0.4176 3rd Qu.: 9.195 3rd Qu.: 3.009 ## Max. :0.6867 Max. :0.7686 Max. :48.067 Max. :10.310 ## tmss rmind rmcom rmss ## Min. : 1.670 Min. : 10.26 Min. : 4.083 Min. : 2.586 ## 1st Qu.: 2.072 1st Qu.: 30.95 1st Qu.:14.085 1st Qu.: 16.312 ## Median : 2.775 Median : 63.61 Median :21.002 Median : 28.922 ## Mean : 5.765 Mean : 67.19 Mean :24.502 Mean : 39.904 ## 3rd Qu.: 4.604 3rd Qu.: 88.88 3rd Qu.:31.630 3rd Qu.: 52.806 ## Max. :41.086 Max. :177.45 Max. :67.438 Max. :255.589 ## den geometry ## Min. : 123.2 POINT :76 ## 1st Qu.: 463.5 epsg:NA : 0 ## Median : 1830.4 +proj=lcc ...: 0 ## Mean : 4227.5 ## 3rd Qu.: 6216.9 ## Max. :17519.3 El conjunto de datos recién creado y que contiene los centroides puede ser representado agregando otra “capa” con la función tm_shape(). En el ejemplo, sólo se superponen las dos capas: la de los polígonos originales y la de los centroides: tm_shape(zmvm_cov_sf) + tm_borders() + tm_shape(zmvm_cntrds) + tm_dots() Como habrás podido notar, hemos agregado otra función que define la forma en que ha de representarse la capa de los centroides: tm_dots(). Ahí, es posible especificar la manera en que deseamos que aparezcan los centroides, por ejemplo, en color rojo y más grandes: tm_shape(zmvm_cov_sf) + tm_borders() + tm_shape(zmvm_cntrds) + tm_dots(size=0.2,col=&quot;red&quot;) Para guardar la capa que contiene los centroides en un nuevo archivo SHP, usamos las siguientes líneas de código: sf::st_write(obj=zmvm_cntrds, &quot;zmvm_cntrds&quot;, driver = &quot;ESRI Shapefile&quot;) Como siempre, se recomienda revisar la documentación de ayuda de la función st_write() para conocer todos los detalles de los argumentos. 2.10 Tópico adicional: Mapas de clasificación especial (elementos de programación) 2.10.1 Una palabra de advertencia En esta sección se construyen dos tipos de mapas de clasificación especial, es decir, destinados a hacer notar los valores atípicos. No obstante, la construcción de dichos mapas en R supone cierto conocimiento sobre programación que está fuera del alcance de estas notas. Si deseas profundizar en el aprendizaje de programación en R, el libro de Roger D. Peng, R Programming for Data Science es un excelente material, particularmente el capítulo 14 dedicado a las funciones en R (Roger D. Peng 2015). Con esta advertencia, llevamos a cabo la exposición de esta sección, esperando motivarte para que tú misma profundices en los tópicos de programación. 2.10.2 Mapa de intervalos personalizados Se señaló antes que, hasta donde tenemos conocimiento, en R no es posible construir mapas de clasificación especial con tmap a través de las opciones de estilo, no obstante, con algunos elementos de programación es posible solventar esta tarea. En esta sección nos servimos del código proporcionado por Luc Anselin y su equipo quienes, en un esfuerzo de difusión del conocimiento sobre el uso de estas herramientas, pone a nuestra disposición una basta cantidad de materiales en la página del Center for Spatial Data Science de la Universidad de Chicago (Anselin and Morrison 2018). Para construir un mapa de intervalos definidos por el usuario, hay que recurrir al argumento breaks= dentro de la función tm_fill(). Para definir los intervalos de forma adecuada, se recomienda mirar las características resumen de la variable de interés, en este caso el número de casos positivos por COVID19 por cada mil habitantes, pos_hab: summary(zmvm_cov_sf$pos_hab) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.720 3.337 5.117 6.964 8.636 22.700 Esto nos permitirá conocer los valores que toma la variable de interés y pensar la manera en que deseamos delimitar las categorías de nuestro mapa. Supongamos que deseamos 6 categorías y, dado el rango de nuestra variable (valores entre 1.720 y 22.7), podemos fijar los cortes de los intervalos en 2.0, 6.0, 12., 18.0 y 24.0; además, se requiere incluir también un mínimo y máximo, digamos 1.0 y 25.0, para las categorías. La información, tanto de los cortes como de los máximos y mínimos deber ser especifica en forma de vector: c(1.0, 2.0, 6.0, 12., 18.0, 24.0, 25.0). Debemos además especificar la paleta de colores que deseamos usar, atendiendo a lo dicho antes, usaremos una paleta con tres anclas: del amarillo, al naranja y al café, Yellow-Orange-Brown,YlOrBr: tmap::tm_shape(zmvm_cov_sf) + tmap::tm_borders()+ tmap::tm_fill(&quot;pos_hab&quot;,title=&quot;Casos positivos covid&quot;,breaks=c(1.0, 2.0, 6.0, 12., 18.0, 24.0, 25.0),palette=&quot;YlOrBr&quot;)+ tm_layout(title = &quot;Cortes personalizados&quot;, title.position = c(&quot;center&quot;,&quot;top&quot;)) 2.10.3 Mapas de valores extremos 2.10.3.1 Mapa de percentiles Un mapa de percentiles es un tipo espacial de mapa de cuantiles en el que se especifican seis categorías: 0-1%,1-10%, 10-50%,50-90%,90-99% y 99-100%; de ellas las que interesan son las categorías que agrupan el 1% de los valores más bajos (0-1%) y el 1% de los valores más altos (99-100%), es decir, las observaciones extremas. Este tipo de mapas es útil, justamente, para identificar la localización de valores atíticos en el espacio. Para poder construirlo, en R debemos llevar a cabo los siguientes pasos: Extraer la variable de nuestro arreglo de datos. Calcular los percentiles de nuestro interés, es decir, 0.0,0.01,0.1,0.5,0.9,0.99,1.0. Construir el mapa con base en los intervalos definidos a través de la función tm_fill(). Para el paso i, lo primero será construir una función que llamaremos get.var, que tendrá dos argumentos, varnom y df, el primero indicará, entre comillas, el nombre de la variable a utilizar y el segundo indicará la base de la que proviene. Esta función permite que, al extraer la variable de la base, se eliminen los “aspectos espaciales” asociadas a ella. Esta función sólo se requiere construir una vez. get.var &lt;- function(varnom,df) {#Definición de la función v &lt;- df[varnom] %&gt;% st_set_geometry(NULL) #Extracción de la variable de interés y remoción de sus características geográficas v &lt;- unname(v[,1]) #Selección de la columna del data frame extraído que contiene la variable de interés y elimina su nombre pues sólo queremos un vector. return(v) #Resultado de la función: la variable como vector sin sus características espaciales } Ahora, echando mano de la función creada extraeremos la variable de interés sin sus aspectos espaciales: pos_hab&lt;-get.var(&quot;pos_hab&quot;,zmvm_cov_sf) pos_hab ## [1] 14.364996 16.983175 13.352654 16.163929 17.363687 17.313543 14.943518 ## [8] 17.404756 18.701249 13.677709 20.559563 10.224540 14.717256 22.700331 ## [15] 11.088257 5.882283 4.938574 5.239423 2.570694 3.351482 4.267922 ## [22] 2.382445 7.896182 2.088929 5.500198 5.891309 5.601076 9.197806 ## [29] 8.113844 3.295057 3.676214 3.250036 3.583416 6.155522 2.124319 ## [36] 2.787239 3.293624 6.120050 1.760416 5.184329 2.941489 3.873824 ## [43] 2.507745 5.423064 7.354686 2.224869 3.992095 8.598203 6.385731 ## [50] 4.385253 3.702180 3.693010 5.894044 4.147922 9.766454 3.853830 ## [57] 5.546263 4.766342 8.751090 1.719690 3.024390 2.372319 2.380410 ## [64] 3.876611 4.272596 7.134726 6.920539 5.049320 4.917293 2.582625 ## [71] 3.918632 3.399002 2.473636 2.822012 6.913242 13.935302 Para el paso ii, hemos de crear un objeto en el que se guarden los percentiles de interés, un vector de 6 elementos: percentiles &lt;- c(0.0,0.01,0.1,0.5,0.9,0.99,1.0) Ahora, se calculan y guardan valores de los percentiles con base en el par de objetos creados, percentiles y pos_hab, es decir, obtendremos los valores de la variable pos_hab en los percentiles de interés: varperc &lt;- quantile(pos_hab,percentiles) varperc ## 0% 1% 10% 50% 90% 99% 100% ## 1.719690 1.750234 2.428041 5.116824 15.553724 21.094755 22.700331 Ahora, en el paso iii, la construcción del mapa de percentiles es posible uniendo todos los elementos: tmap::tm_shape(zmvm_cov_sf)+ tmap::tm_fill(&quot;pos_hab&quot;,title=&quot;Índice de marginación 2010&quot;, breaks=varperc, palette=&quot;-RdBu&quot;,labels=c(&quot;&lt; 1%&quot;, &quot;1% - %10&quot;, &quot;10% - 50%&quot;, &quot;50% - 90%&quot;,&quot;90% - 99%&quot;, &quot;&gt; 99%&quot;))+ tmap::tm_borders() + tmap::tm_layout(title = &quot;Mapa de percentiles&quot;, title.position = c(&quot;center&quot;,&quot;bottom&quot;)) 2.10.4 Construcción de una función personalizada para hacer mapas de percentiles Con el camino que hemos seguido en la sección previa, es posible ahorrarnos muchos pasos y crear nuestra propia función (que opera en el entorno de trabajo activo de la sesión) para construir mapas de percentiles. Nuestra función se llamará percentmap() y tendrá los siguientes argumentos: varnom: nombre de la variable (especificada como texto entre comillas). df: base de datos que contiene el variable. legtitle: titulo de la leyenda del mapa. mtitle: título del mapa. Para crear la función: percentmap &lt;- function(varnom,df,titulo.leyenda=NA,titulo.principal=&quot;Mapa de percentiles&quot;){ #Definición de la función y sus argumentos #Elementos preliminares percent &lt;- c(0,.01,.1,.5,.9,.99,1) #Vector que contiene los percentiles de interés para el mapa var &lt;- get.var(varnom,df) #Extracción de la variable de la base de datos con la función anterior varperc &lt;- quantile(var,percent) #Cálculo de los percentiles de la variable extraída #Especificaciones del mapa tm_shape(df) + tm_fill(varnom,title=titulo.leyenda,breaks=varperc,palette=&quot;-RdBu&quot;, labels=c(&quot;&lt; 1%&quot;, &quot;1% - %10&quot;, &quot;10% - 50%&quot;, &quot;50% - 90%&quot;,&quot;90% - 99%&quot;, &quot;&gt; 99%&quot;)) + tm_borders() + tm_layout(title = titulo.principal, title.position = c(&quot;center&quot;,&quot;bottom&quot;)) } La función que hemos creado para hacer nuestros mapas tiene cuatro argumentos: varnom,df,legtitle y mtitle. Los dos últimos tienen valores por omisión, lo que significa que no es necesario especificarlos al usar la función. Los dos primeros no tienen valor por omisión, por lo que será forzoso especificar dichos argumentos. Ahora, invocando nuestra propia función e indicando los argumentos forzosos, varnom y df tenemos que: percentmap(&quot;pos_hab&quot;,zmvm_cov_sf) Podemos, como es obvio, cambiar los dos argumentos dados por omisión: percentmap(&quot;pos_hab&quot;,zmvm_cov_sf, titulo.leyenda = &quot;Categorías&quot;, titulo.principal = &quot;El título que yo quiera&quot; ) Los capítulos 1 y 2 de este libro, constituyen lo que suele ser denominado Análisis Exploratorio de Datos, (Exploratory Data Analysis, EDA). El capítulo 1 del e-Handbook of Statistical Methods expone con detalle esta concepción en el análisis de información (Croarkin and Tobias 2014). Además, en el capítulo 7 del ya citado libro R for Data Science también explica con detalle el enfoque EDA usando R. En el siguiente capítulo continuamos con la exploración de la información, pero incorporando una estructura de relaciones en el espacio, por lo que a dicho enfoque se le conoce como Análisis Exploratorio de Datos Espaciales. En dicho capítulo será de nuestro interés particular un rasgo que suele estar presente en la información georreferenciada: la autocorrelación espacial. References "],["análisis-espacial-i-autocorrelación.html", "3 Análisis espacial I: autocorrelación 3.1 Autocorrelación espacial y definición de vecindad 3.2 Matrices de pesos espaciales en R 3.3 Variables espacialmente rezagadas 3.4 Coeficiente de correlación espacial: la I de Moran 3.5 Múltiples elementos de personalización de ésta y otras gráficas asociadas al paquete base de R pueden revisarse en la documentación de la función par(). 3.6 Índice de Moran local y mapa de clusters", " 3 Análisis espacial I: autocorrelación Observa el siguiente mapa. En él, se representa la variable índice de de desarrollo humano que calculó el Programa de Naciones Unidad para el Desarrollo (PNUD) en 2015. ## Reading layer `covid_zmvm&#39; from data source ## `C:\\repos\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 76 features and 57 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5 ## Projected CRS: Lambert_Conformal_Conic Figura 3.1. Índice de desarrollo humano para los municipios de la ZMVM, 2015 ¿Notas algún patrón? ¿Detectas cómo los municipios con un valor alto del índice de desarrollo (Benito Juárez, Alvaro Obregón, Miguel Hidalgo, Coyoacán y otros) se encuentran agrupados en la parte centro-sur de la región considerada? Dicho patrón ilustra la posible presencia de autocorrelación espacial, es decir, que los valores de un indicador en una zona estén rodeados por valores muy semejantes o nada semejantes: en ambos casos decimos que hay autocorrelación. Cuando tratamos con datos de una variable georeferenciada hay altas probabilidades de que dicha variable esté autocorrelacionada y esto hace necesario que sean necesarias ciertas técnicas para su tratamiento, es decir, para su representación y modelación. En este capítulo trataremos con el concepto clave de autocorrelación espacial y veremos a través de que instrumentos es posible medirla para, más adelante, incorporar su riqueza informativa a una explicación sustantiva de los fenómenos socioterritoriales analizados. 3.1 Autocorrelación espacial y definición de vecindad Una definición sintética de autocorrelación espacial es la que nos brinda Chasco (2003) como “la relación funcional existente entre los valores que adopta un indicador en una zona del espacio y en zonas vecinas” (Chasco 2003, 49). Por ejemplo, imagina que el barrio de la ciudad donde vives presenta un alto número de contagios por COVID19 y, además, los barrios vecinos tienen también valores altos: en este caso es probable que tengamos autocorrelación espacial positiva. La identificación de autocorrelación espacial es importante como parte del proceso de análisis del fenómeno socioterritoriales al menos por dos cuestiones, una de carácter técnica durante la modelación econométrica y otra de carácter sustantiva en relación con la explicación del fenómeno. Respecto a la razón técnica, si existe autocorrelación espacial en nuestros datos lo más probable es que la estimación de los parámetros de un modelo con de mínimos cuadrados ordinarios deje de ser válida, en la medida en que no se cumplen los supuestos que requiere dicho procedimiento, específicamente, que los errores o perturbaciones del modelo no estén correlacionados; sobre esto abundaremos en el capítulo siguiente cuando hagamos un repaso de los modelos clásicos de regresión lineal. Por otro lado, respecto a la razón sustantiva, emerge la pregunta, ¿qué pude estar ocurriendo que hace que un fenómeno aparezca, por ejemplo, agrupado en el espacio, es decir, que no se distribuya aleatoriamente en el territorio? ¿Por qué se concentra la actividad económica en determinadas ciudades o por qué algunos servicios sólo se brindan en una zona de la ciudad? Esto respecto a fenómenos económicos, pero ¿qué hay con el hecho de que una enfermedad se concentra notoriamente en algunas áreas de la ciudad y no en otras? Dicho en otras palabras, ¿qué hay detrás de la formación de un patrón en la forma en que se distribuye un fenómeno en el espacio y cómo puede ser esto explicado? A eso nos referimos cuando decimos que hay elementos sustantivos para el análisis al hallar evidencia de autocorrelación espacial. Regresemos a la definición brindada de autocorrelación y analicémosla con más cuidado: Relación funcional existente entre los valores que adopta un indicador en una zona del espacio y en zonas vecinas. La definición se integra por tres elementos clave: i) valor de un indicador, ii) relación funcional y iii) zonas vecinas. Para dar sentido a nuestra definición pensemos en una afirmación como “los casos positivos de COVID19 en la alcaldía Azcapotzalco están asociados en forma directa con los casos positivos de COVID19 en las alcaldías y municipios vecinos que integran la Zona Metropolitana del Valle de México”. Tendríamos entonces que: Indicador: casos positivos por COVID19. Relación funcional: asociación positiva o directa. Zonas vecinas: alcaldías y municipios vecinos de Azcapotzalco. El primer elemento no presenta dificultad alguna puesto que se refiere al valor de una variable en el espacio, tal y como la tenemos en la base de datos que hemos estado utilizando: casos positivos por COVID19 por cada 1 mil habitantes dentro de cuyas observaciones se encuentra Azcapotzalco; en tanto, el segundo elemento de nuestra afirmación es una mera suposición, es decir, que hay una relación positiva; por su parte, el tercer elemento “alcaldías y municipios vecinos” implica un problema: ¿de qué modo es posible establecer qué alcaldías son o no vecinas de Azcapotzalco? Hay múltiples maneras que definir si un objeto espacial tiene o no vecinos, por ejemplo, podríamos decir que aquellas alcaldías que compartan límites administrativos con la demarcación territorial de nuestro interés serán sus vecinos (vecindad por adyacencia) o también sería posible establecer que las alcaldías vecinas serán aquellas que estén a menos de 10 km de distancia del centro económico de la alcaldía (vecindad por umbral de distancia) e incluso podríamos decir que las 3 alcaldías o municipios más cercanos serán los vecinos. Ejercicio ¿Se te ocurre algún otro criterio para establecer vecindad? ¿Cómo llamarías a un criterio de vecindad donde elijas a los 3 vecinos más cercanos? ¿A partir de qué punto en el espacio será más conveniente medir la distancia, desde el centro económico de la alcaldía o municipio (por ejemplo su zona industrial o comercial) o desde la sede de la administración local? ¿La distancia más indicada usada como criterio de vecindad será una distancia lineal o una distancia por carretera? Una vez que hemos visto que que existen diferentes criterios de vecindad, debemos pensar en una manera de almacenar dicha información. La Zona Metropolitana del Valle de México tiene 76 unidades espaciales, ¿de qué modo podemos apuntar todas las posibles relaciones de vecindad entre ellas? Las personas interesadas en el análisis espacial han propuesto un ingenioso instrumento matemático para captar y sintetizar cómo un objeto se relaciona con otros, es decir, para captar la estructura espacial del área de interés dada por las relaciones de vecindad. Dicho instrumento es denominado matriz de pesos espaciales; ilustremos esta idea. Piensa en un vecindario o área de estudio compuesto sólo por seis elementos, tal como se ilustra en la figura 3.2: Figura 3.1: Vecindario regular A partir de la disposición de este hipotético vecindario nos interesa construir una matriz de pesos espaciales, el instrumento para captar la estructura espacial de dicho vecindario. De los múltiples criterios de vecindad existentes, comencemos por el de adyacencia o contigüidad. A decir de Anselin (2020) “contigüidad significa que dos unidades espaciales comparten un borde común de longitud distinta de cero. Desde el punto de vista operativo, podemos distinguir entre un criterio de contigüidad de tipo torre y de tipo reina, en analogía con los movimientos permitidos para las piezas así nombradas en un tablero de ajedrez. El criterio de la torre define a los vecinos por la existencia de un borde común entre dos unidades espaciales. El criterio de la reina es algo más amplio y define a los vecinos como unidades espaciales que comparten un borde o un vértice comunes” (Anselin 2020). De este modo, del criterio de vecindad por adyacencia tenemos dos tipos: torre y reina. Para construir nuestra matriz de pesos espaciales elijamos el criterio más amplio, de la reina. ¿Cómo podemos plasmar las relaciones de contigüidad entre los seis elementos de la figura 3.2? Pensemos en un cuadro que tiene tantas filas y columnas como objetos espaciales tiene nuestro vecindario, semejante al que aparece en la figura 3.3: Figura 3.2: Matriz ejemplo vacía Dicho cuadro será nuestra matriz de pesos espaciales y contendrá la estructura espacial del vecindario de la figura 3.2. Para un criterio de vecindad por adyacencia de tipo reina, ¿el elemento 1 y 2, son vecinos? Las unidades espaciales 1 y 2 de nuestro vecindario comparten un borde, por tanto, son vecinos y en el elemento (1,2) de nuestra matriz colocaremos un número 1; lo mismo ocurre entre las unidades espaciales 1 y 3 que, al compartir un lado, son vecinos y por tanto el elemento (1,3) de la matriz será también un 1. ¿Qué pasa entre los objetos 1 y 4? En este caso, no hay ni bordes ni vértices en común, por tanto, no hay una relación de vecindad, entonces, en el elemento (1,4) habremos de colocar un 0 que indica ausencia de vecindad. En síntesis, dado determinado criterio de vecindad, si dos objetos espaciales son vecinos, la relación de vecindad se indica mediante un número 1, en tanto, cuando no hay relación de vecindad su ausencia se indica colocando un 0. Hagamos esto para cada celda de la matriz hasta llenarla completamente y obtener algo parecido a lo que aparece en la figura 3.4. Figura 3.3: Matriz ejemplo llena Resumamos lo dicho hasta este punto. El cuadro que acabamos de llenar es conocido como matriz de pesos espaciales5 y es el instrumento que nos permite sintetizar las relaciones espaciales o estructura de vecindad que corresponde a determinado criterio. Habrá, por tanto, diversos tipos de matrices en función del criterio de vecindad elegido. Esta matriz se denotada por la letra mayúscula \\(W\\) y está integrada por los elementos \\(w_{ij}\\) que toman el valor de 1 cuando el elemento \\(j\\) y el elemento \\(i\\) son vecinos y 0 (cero) en cualquier otro caso. Este instrumento es uno de los más importantes en econometría espacial ya que permite construir los estadísticos de autocorrelación espacial y es la manera en que podemos incorporar al espacio como variable a partir de lo que denominamos “rezago espacial”, como más adelante veremos en este y en el siguiente capítulo. Las características de la matriz de pesos espaciales son: Es una matriz que en la diagonal principal contiene sólo ceros, es decir, se asume que por definición no hay interacciones dentro de un mismo elemento (lo que no necesariamente es cierto y que dependerá de la escala de análisis). Es una matriz simétrica, es decir, se asume que hay interacción de “ida y vuelta”, por lo que con un instrumento de estas características no es posible asumir efectos de interacción en un solo sentido. Es una matriz cuadrada, es decir, de dimensiones \\(n \\cdot n\\), donde \\(n\\) es el número de objetos espaciales. Acabamos de ilustrar la lógica con la que puede ser construida una matriz de pesos espaciales a partir de una retícula regular con apenas seis elementos. Veamos ahora cómo obtener matrices de pesos espaciales sirviéndonos de R, ya que desarrollar los pasos anteriores para un vecindario compuesto por 76 objetos espaciales que son las alcaldías y municipios que integran el Valle de México es tarea para una máquina, no para nosotros. 3.2 Matrices de pesos espaciales en R En R hay muchas rutas para desarrollar la misma tarea. Presentamos en este capítulo dos rutas: la primera se sirve del paquete spdep de Roger Bivand, en tanto que la segunda sigue la propuesta de Xun Li y su paquete rgeoda, una librería para llevar a cabo análisis espacial basado en la funcionalidades del software GeoDa. 3.2.1 Construcción de matrices con spdep Roger Bivand y un equipo de colaboradores desarrollaron el paquete spdep para la construcción de matrices de pesos espaciales y el análisis espacial. En este enfoque la función poly2nb() nos permite calcular estructuras espaciales dadas a partir de dos tipos de vecindad por adyacencia, una más estricta (argumento queen=FALSE) y que la otra (argumento queen=TRUE). En la documentación de la función podemos leer que: “si es VERDADERO, TRUE, un solo punto límite compartido cumple la condición de contigüidad; si es FALSO, FALSE, se requiere más de un punto compartido; ten en cuenta que más de un punto límite compartido no significa necesariamente una línea límite compartida”. Además, para cargar la base de datos espacial recurriremos al paquete rgdal. Lo dicho en el párrafo anterior es relevante en el sentido de que estos criterios no son exactamente los mismos que definimos antes (contigüidad reina y torre). Procedamos pues a la instalación de los paquetes, en caso de que aún no estén en nuestro sistema: install.packages(c(&quot;spdep&quot;, &quot;rgdal&quot;)) Primero, llamemos las librerías y carguemos la base de datos: library(rgdal) library(spdep) covid_zmvm &lt;-rgdal::readOGR(&quot;base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\repos\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp&quot;, layer: &quot;covid_zmvm&quot; ## with 76 features ## It has 57 fields Ahora, construyamos un objeto que llamaremos mTRUE, dicho objeto contendrá los elementos que definen la estructura espacial, es decir, será nuestra matriz de pesos espaciales. Esto lo haremos con la función poly2nb(): mTRUE &lt;- spdep::poly2nb(covid_zmvm) El segmento de código anterior genera un objeto de tipo nb. Verifica sus características con class() y str() . Además, nota que en la sección de ambiente de trabajo (cuadrante superior derecho, en la pestaña ambiente) aparece el objeto creado. Da clic en la imagen de la lupa para visualizarlo e intenta interpretar el resultado de la ventana. Ahora, llama al objeto y presta atención sobre los resultados que aparecen en la consola: mTRUE ## Neighbour list object: ## Number of regions: 76 ## Number of nonzero links: 384 ## Percentage nonzero weights: 6.648199 ## Average number of links: 5.052632 En la consola aparecen los siguientes elementos: Objeto de lista de vecinos: Número de regiones: 76. Corresponde al número de alcaldías y municipios que componen la Zona Metropolitana del Valle de México. Número de enlaces distintos de cero: 380. Es el número de elementos de una matriz de 76x76 que registran relación de vecindad, dicho en otras palabras, es el total de números uno de la matriz. Porcentaje de pesos distintos de cero: 6.57. Resultado de dividir 380 entre (76x76). Número promedio de vínculos: 5. Número de vecinos que en promedio tiene cada municipio o alcaldía. Ahora bien, para construir una lista de vecindad con base en un criterio más estricto, es decir, queen = FALSE, procedemos como: mFALSE&lt;- spdep::poly2nb(covid_zmvm, queen = FALSE) Ahora, llama dicho objeto y contrasta con los resultados anteriores. mFALSE ## Neighbour list object: ## Number of regions: 76 ## Number of nonzero links: 372 ## Percentage nonzero weights: 6.440443 ## Average number of links: 4.894737 Ejercicio ¿Qué objeto, mTRUE o mFALSE, tiene el mayor número de vínculos diferentes de cero? ¿Por qué crees que esto es así? Estas listas, que contienen nuestras estructuras espaciales almacenadas en los objeto de tipo nb llamados mTRUE y mFALSE, pueden representarse visualmente a través de un gráfico de conectividad que representa la estructura espacial definida por cada criterio a través de líneas que unen a los municipios considerados vecinos. Para visualizar el mapa de conectividad recurriremos a la función plot() y se superpondran dos gráficas: una sólo con los bordes o límites a nivel municipal y otra con los centroides y la estructura espacial. Aquí se muestra el mapa de conectividad resultado de la matriz mTRUE plot(covid_zmvm, border = &#39;lightgrey&#39;) plot(mTRUE, coordinates(covid_zmvm), add=TRUE, col=&#39;lightblue&#39;) Para comparar ambas estructuras espaciales, mTRUE y mFALSE podemos superponer las dos gráficas y asignar colores diferentes: plot(covid_zmvm, border = &#39;lightgrey&#39;) plot(mTRUE, coordinates(covid_zmvm), add=TRUE, col=&#39;blue&#39;) plot(mFALSE, coordinates(covid_zmvm), add=TRUE, col=&#39;lightgreen&#39;) Como puedes observar, son estructuras muy parecidas, aunque aun así es posible notar sus diferencias. 3.2.2 Construcción de matrices con rgeoda A diferencia del enfoque previo, con el paquete rgeoda es necesario cargar la base de datos espacial a través del paquete sf. Si no los has instalado: install.packages(c(&quot;rgeoda&quot;, &quot;sf&quot;)) Para cargarlos: library(sf) library(rgeoda) Ahora bien, carguemos la base de datos espacial con sf: covid_zmvm_sf &lt;- sf::st_read(&quot;base de datos/covid_zmvm shp/covid_zmvm.shp&quot;) ## Reading layer `covid_zmvm&#39; from data source ## `C:\\repos\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 76 features and 57 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5 ## Projected CRS: Lambert_Conformal_Conic Ejercicio Observa tu ambiente de trabajo (ventana superior derecha, en la pestaña “ambiente”). ¿De qué tipo es el objeto covid_zmvm_sf y de cuál covid_zmvm? ¿A qué crees que se deban dichas diferencias? A través del paquete rgeoda es posible construir, como en GeoDa, cuatro tipos de matrices de pesos: Matrices basadas contigüidad Matrices basadas en distancia Matrices de k-vecinos más cercanos Matrices de pesos por kernel De ellas, aquí construiremos sólo las tres primeras. 3.2.2.1 Matrices basadas en contigüidad En rgeoda hay dos matrices de pesos espaciales basadas en contigüidad, tal y como lo expisimos en la primera sección de este capítulo, las de tipo reina y las de tipo torre. Para construir una matriz de tipo reina recurrimos a la función queen_weight() y para una de tipo torre usamos rook_weights(), ambas funciones tienen cuatro argumentos: sf_obj=: nuestra cartografía en formato sf, tal y como la hemos ya cargado. order=: orden de contigüidad, donde si es igual a 1 se indica que sólo los objetos espaciales inmediatos serán vecinos y si es mayor que uno indicará una vecindad de orden superior (si es 2 indica que los vecinos de mis vecinos serán mis vecinos, si es tres los vecinos de mis vecinos de mis vecinos serán mis propios vecinos, y así sucesivamente). include_lower_order=: indica si los vecinos de ordenes inferiores incluidos en la estructura de vecindad. precision_threshold=: este argumento modifica la precisión de la geometría y se usará en caso de que, no habiendo observaciones aisladas, se detecte que algún objeto no tiene vecinos. Ejercicio Ve a la ayuda de la funciones y responde: i. ¿Qué argumentos son obligatorios? ii. ¿Cuáles opcionales? Así, para construir la matriz de tipo reina: queen_w &lt;- rgeoda::queen_weights(covid_zmvm_sf) En tanto, una matriz de contigüidad con el criterio de tipo torre:: rook_w &lt;- rgeoda::rook_weights(covid_zmvm_sf) Los objetos recién creados y que contienen las estructuras de vecindad pueden ser grabadas en un archivo fuera del ambiente del trabajo, para hacerlas permanentes. Para ello usamos la función save_weights(), esta función tiene cuatro argumentos obligatorios: nombre de la matriz a hacerse permanente (gda_w=), el identificador único para cada objeto espacial (id_variable=), la ruta donde se almacenará el archivo de salida y el nombre del archivo (out_path=) y el nombre de la capa de entrada (layer_name=): #Para la matriz reina rgeoda::save_weights(gda_w=queen_w, id_variable=covid_zmvm_sf[&#39;cvemun&#39;], out_path = &#39;base de datos/covid_zmvm shp/q_1.gal&#39;, layer_name = &#39;covid_zmvm_sf&#39;) ## [1] TRUE #Para la matriz toree rgeoda::save_weights(rook_w, covid_zmvm_sf[&#39;cvemun&#39;], out_path = &#39;base de datos/covid_zmvm shp/r_1.gal&#39;, layer_name = &#39;covid_zmvm_sf&#39;) ## [1] TRUE Si bien se dijo antes, en la sección dedicada a comentar qué es una matriz de pesos espaciales, que se definía la estructura espacial a través de una matriz, computacionalmente esto no es así, como seguro habrás notado al tratar de leer las matrices construidas con spdep en la sección anterior. Identifica los archivos creados con las funciones previas, en la sección de archivos en la sección inferior derecha (pestaña files) y selecciona q_1.gal. Notarás que, en efecto, el archivo no es una matriz, sino una lista. El hecho de que se usen listas y no matrices para el proceso de cómputo de las estructuras espaciales es porque las listas son más conveniente en términos de la cantidad ocupada de recursos del sistema. La razón detrás de ello es que las matrices de pesos espaciales son matrices dispersas, es decir, matrices que contienen muchos elementos que son cero. Del archivo que acabas de abrir, q_1.gal, veámos con detenimiento su contenido para comprender mejor cómo se almacena la información. Reproducimos en seguida las tres primeras líneas: 0 76 | covid_zmvm_sf | cvemun | | | 09010 | 5 | | | | 09014 | 09008 | 09016 | 09003 | 09012 | Línea 1: en la primera columna aparece el número de objetos espaciales (76), en la segunda columna el nombre del archivo del que proviene la estructura (covid_zmvm_sf) y en la tercera la clave de identificación única para cada objeto (cvemun). Línea 2: en la primera columna se indica el objeto espacial su clave (09010), mientras que en la segunda columna se indica el número de vecinos que dicho objeto tiene, en este caso, cinco. Línea 3: en esta fila aparece, para cada columna, la clave de identificación de los cinco vecinos de 09010: 09014 09008 09016 09003 09012. El resto de las líneas tiene la misma interpretación que las líneas 2 y 3: número de vecinos del objeto listado e identificación de los vecinos. Una vez que hemos comprendido la estructura de dicho archivo, pidamos un resumen del objeto queen_w: summary(queen_w) ## name value ## 1 number of observations: 76 ## 2 is symmetric: TRUE ## 3 sparsity: 0.0616343490304709 ## 4 # min neighbors: 1 ## 5 # max neighbors: 9 ## 6 # mean neighbors: 4.68421052631579 ## 7 # median neighbors: 4 ## 8 has isolates: FALSE De dicho resumen es posible apuntar que: nuestro vecindario, la Zona Metropolitana del Valle de México, tiene 76 unidades espaciales (number of observations), la matriz construida es simétrica (is symmetric: TRUE), los municipios y alcaldías tienen al menos un vecino (# min neighbors: 1), el número máximo de vecinos es de 9 (# max neighbors: 9), la media de 4.68 (# mean neighbors), la mediana de 4 (# median neighbors) y que no hay observaciones sin vecinos (has isolates: FALSE). El elemento que no hemos indicado es sparsity: que tiene un valor de 0.0616. Dicho valor corresponde a la proporción de elementos diferentes a cero en una matriz de 76x76 objetos, es decir, nuestra matriz sólo tiene 6.16% son diferentes de cero. 3.2.2.2 Matrices basadas en distancia En `rgeoda`` hay dos tipos de matrices que recurren a la distancia: la matriz de úmbral de distancia mínima y la matriz de k-vecinos más cercanos. Para el caso de la matriz de umbral, se considera que dos objetos espaciales, en el caso aquí analizado, dos municipios o alcaldías, son vecinos siempre que estén dentro de cierto umbral de distancia dado. Para el caso de la matriz de k-vecinos más cercanos, la idea es que un objeto espacial tendrá como vecinos a los k-objetos más cercanos. 3.2.2.2.1 Matriz basada en umbral distancia Para construir una matriz de pesos espaciales basada este criterio tenemos proceder en dos pasos: i) definir un umbral de distancia mínimo en el cual todas las unidades espaciales tienen al menos un vecino, ii) usar dicho umbral para hallar cada uno de los vecinos, dado ese umbral. Para definir el umbral usamos la función min_distthreshold() del paquete `rgeoda``: umbral &lt;- rgeoda::min_distthreshold(covid_zmvm_sf) umbral ## [1] 13943.63 La distancia mínima para que cada uno de los 76 municipios y alcaldías tenga un vecino es 13,943.63 metros. Sabemos que las unidades del resultado anterior son metros puesto que en el archivo con extensión .prj (el archivo que contiene la información sobre la proyección cartográfica utilizada) así se indica. Ahora bien, definido el umbral ya podemos construir la matriz: dist_w &lt;- rgeoda::distance_weights(covid_zmvm_sf, umbral) summary(dist_w) ## name value ## 1 number of observations: 76 ## 2 is symmetric: TRUE ## 3 sparsity: 0.0772160664819945 ## 4 # min neighbors: 1 ## 5 # max neighbors: 11 ## 6 # mean neighbors: 5.86842105263158 ## 7 # median neighbors: 5.5 ## 8 has isolates: FALSE 3.2.2.2.2 Matriz de k-vecinos más cercanos Cuando decimos k-vecinos más cercanos, con ello queremos decir que se identificará determinado número “k” de vecinos más cercanos: los 2 más cercanos (k=2), los 8 más cercanos (k=8), etcétera. Para ello nos servimos de la función knn_weights(). Si quiseramos una matriz con los 4 vecinos más cercanos, tenemos que: k4_w &lt;- knn_weights(covid_zmvm_sf, 4) summary(k4_w) ## name value ## 1 number of observations: 76 ## 2 is symmetric: FALSE ## 3 sparsity: 0.0526315789473684 ## 4 # min neighbors: 4 ## 5 # max neighbors: 4 ## 6 # mean neighbors: 4 ## 7 # median neighbors: 4 ## 8 has isolates: FALSE A diferencia de las matrices anteriores, ésta no es simétrica y, como te podrás dar cuenta, hace que cada objeto espacial tenga exactamente el mismo número de vecinos, en este caso, cuatro. 3.3 Variables espacialmente rezagadas La construcción de un rezago espacial, también llamada variable espacialmente rezagada, es un elemento clave para poder operacionalizar y, por tanto, medir la autocorrelación. Pero, ¿qué es un rezago espacial? Imagina que vives en un barrio de la Ciudad de México que tiene 8 barrios vecinos, mismos que calculaste con alguno de los criterios de que vimos antes y que apuntaste en una matriz de pesos espaciales. Supón ahora que tu barrio tiene 5 casos de COVID19 y quieres comparar dicho dato con el de los 8 barrios vecinos, ¿cómo lo harías? Una alternativa útil es sintetizar la información de los ocho barrios en un sólo indicador que sume y pondere los datos de los casos positivos de los vecinos. Veamos esto con más cuidado y definamos formalmente rezago espacial. Antes dijimos que cada uno de los elementos \\(w_{ij}\\) de la matriz de pesos espaciales \\(W\\) pueden tomar como valores ceros o unos. La matriz \\(W\\) puede escribirse como: \\[ W= \\begin{pmatrix} w_{11} &amp; w_{12} &amp; \\cdots &amp; w_{1n}\\\\ w_{21} &amp; w_{22} &amp; \\cdots &amp; w_{2n}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ w_{n1} &amp; w_{n2} &amp; \\dots &amp; w_{nn}\\\\ \\end{pmatrix} \\] No obstante, es posible expresar dicha matriz \\(W\\) de una forma diferente, normalizandola por filas. Normalizar una matriz de pesos espaciales por filas implica dividir cada elemento \\(w_{ij}\\) de una fila entre la suma de elementos diferentes a cero de dicha fila, por lo que los elementos de una matriz de pesos espaciales estandarizada por fila es: \\[w_{ij(s)}=\\frac {w_{ij}} {\\sum{w_{ij}}}\\] Nuestra matriz de pesos espaciales estandarizada por filas, \\(W_s\\), es pues una transformación de la matriz original que hace que todas las filas sumen en total 1: $$ w_{11(s)} + w_{12(s)} + + w_{1n(s)}= 1\\ w_{21(s)} + w_{22(s)} + + w_{2n(s)}=1\\ w_{n1(s)} + w_{n2(s)} + + w_{nn(s)}=1 $$ Regresemos a nuestra definición sobre la autocorrelación espacial: relación funcional existente entre los valores que adopta un indicador en una zona del espacio con respecto al valor de sus zonas vecinas, dicho valor es lo que llamamos rezago espacial. Así pues, un rezago espacial es definido como el promedio ponderado del valor de la variable de los vecinos (Chasco 2003, 61; Anselin 2020). Siguiendo a Anselin (2020), “el rezago espacial de \\(y\\) del objeto espacial \\(i\\) es expresado como \\(Wy_{i}\\): \\[ \\begin{aligned} Wy_i &amp;= w_{i1(s)}y_1+w_{i2(s)}y_2+...+w_{in(s)}y_n \\\\ Wy_i &amp;= \\sum_{j=1}^nw_{ij(s)}y_j \\\\ \\end{aligned} \\] Donde \\(w_{ij(s)}\\) es cada uno de los elementos de la matriz de pesos estandarizada por fila y \\(y_n\\) es el valor de la variable de interés. Así pues, el rezago espacial pondera la variable de interés a través del número de vecinos que cada unidad espacial posee. De nueva cuenta, es posible construir rezagos espaciales ya sea con el paquete spded o rgeoda. Primero ilustraremos la alternativa con spdep y en seguida con rgeoda. 3.3.1 Rezagos espaciales con spdep Para construir con spdep el rezago espacial de una variable, digamos de los casos positivos por COVID19, debemos primero estandarizar los objetos que contienen las estructuras espaciales que previamente construimos: mTRUE y mFALSE. Este proceso corre a cuenta de la función nb2listw() del paquete spdep: mTRUE.est &lt;- spdep::nb2listw(mTRUE) mTRUE.est ## Characteristics of weights list object: ## Neighbour list object: ## Number of regions: 76 ## Number of nonzero links: 384 ## Percentage nonzero weights: 6.648199 ## Average number of links: 5.052632 ## ## Weights style: W ## Weights constants summary: ## n nn S0 S1 S2 ## W 76 5776 76 35.56142 319.6623 mFALSE.est &lt;- spdep::nb2listw(mFALSE) mFALSE.est ## Characteristics of weights list object: ## Neighbour list object: ## Number of regions: 76 ## Number of nonzero links: 372 ## Percentage nonzero weights: 6.440443 ## Average number of links: 4.894737 ## ## Weights style: W ## Weights constants summary: ## n nn S0 S1 S2 ## W 76 5776 76 36.56499 319.6716 Notaras cómo en el ambiente de trabajo se ha creado un objeto nuevo de tipo listw. Ábrelo y observa su contenido. Ejercicio ¿Explica por qué se le denomina matriz (lista) estandarizada? ¿A cuanto es igual la suma de cada renglón de la lista? ¿Cómo se relaciona la forma en que aparecen enlistados los elementos con el número de vecinos que tiene cada objeto espacial? Construiremos un rezago espacial de la variable pos_hab, número de casos positivos por COVID19, con ayuda de la función lag.listw() del paquete spdep. Indicamos dos argumentos en la función: la estructura espacial dada por la matriz estandarizada, mTRUE.est, y la variable de la que deseamos el rezago espacial, pos_hab, esto es guardado en un nuevo objeto, lag_poshab, tal y como se muestra en el siguiente segmento de código: lag_poshab &lt;- spdep::lag.listw(mTRUE.est, covid_zmvm$pos_hab) Para lograr apreciar mejor el rezago espacial, construiremos una tabla de dos columnas que almacenaremos en el objeto df, la primera contendrá la variable original y la segunda el rezago espacial, luego pediremos que nos muestre los primeros registros de la tabla con la función head() en un formato estilizado a través de la función kable() del paquete knitr: #Crea un nuevo arreglo de datos donde se almacena la variable original y el rezago espacial df &lt;- base::data.frame(pos_hab = covid_zmvm$pos_hab, lag_poshab) library(knitr) #Coloca los primeros valores de ambas variables en una tabla, requiere instalación y carga del paquete knitr kable(head(df)) pos_hab lag_poshab 14.36500 15.99225 16.98318 16.81821 13.35265 14.48267 16.16393 12.22961 17.36369 12.18342 17.31354 11.97155 El valor de la segunda columna, lag_poshab, es el rezago espacial. ¿Cómo interpretamos dicho valor? Veamos con cuidado. El primer valor listado de pos_hab, 14.365, es el número de casos positivos de COVID19 por cada 1 mil habitantes y dicho valor pertenece a la alcaldía Álvaro Obregón, en tanto, el primer valor listado en la columna lag_poshab es el rezago espacial y asciende a 15.992, este valor es el promedio ponderado de casos positivos por COVID19 en los vecinos Álvaro Obregón. Para identificar a los vecinos de cada objeto espacial (municipio o alcaldía), así como el respectivo valor del rezago espacial hay que extraer dicha información utilizando una notación de dobles corchetes. Vayamos por pasos. Primero, para conocer cual es el primer elemento u objeto espacial de nuestra base usamos la notación del doble corchete sobre el objeto covid_zmvm: covid_zmvm$nom_mun[[1]] #Devuelve el nombre del municipio identificado con el número 1 ## [1] &quot;Álvaro Obregón&quot; Una vez que sabemos que dicho objeto es la alcaldía llamada Álvaro Obregón, pidamos a R que nos muestre cuáles son sus vecinos. Para ello, usando de nuevo la notación de doble corchete sobre la estructura espacial, el objeto mTRUE: mTRUE[[1]]#Devuelve los números de identificación de los municipios y alcaldías vecinas del Álvaro Obregón, la observación identificada con el número 1. ## [1] 2 7 9 10 11 15 Los números anteriores corresponden a los vecinos de Álvaro Obregón, pero, ¿cómo saber su nombre y el número de casos positivos de cada uno? El siguiente segmento de código nos dará los nombres de los vecinos y el número de casos positivos en cada uno. #Crea un objeto que contiene el nombre de las alcaldías y municipios vecinos de Álvaro Obregón Alcaldías &lt;- covid_zmvm$nom_mun[mTRUE[[1]]] #Crea un objeto que contiene el valor de la variable pos_hab (casos positivos por mil habitantes) para cada observación vecina de Álvaro Obregón Casos_covid &lt;- covid_zmvm$pos_hab[mTRUE[[1]]] # Guarda los dos objetos anteriores en un dataframe y lo muestra en una tabla db &lt;- data.frame(Alcaldías, Casos_covid) kable(db) Alcaldías Casos_covid Tlalpan 16.98318 Coyoacán 14.94352 Cuajimalpa de Morelos 18.70125 Miguel Hidalgo 13.67771 La Magdalena Contreras 20.55956 Benito Juárez 11.08826 Los valores previamente listados corresponden tanto al nombre como al número de casos positivos de los vecinos de Álvaro Obregón. Así, el rezago espacial de Álvaro Obregón es el resultado de sumar los valores de la tabla anterior y multiplicarlos por \\(\\frac {1}{6}\\), es decir, 15.99. Ejercicio ¿Por que para el caso de Álvaro Obregón hubo que multiplicar por un sexto? Obtén un cuadro con las lista de vecinos y los valores de casos positivos para el objeto espacial 35 3.3.2 Rezagos espaciales con rgeoda Con el paquete rgeoda calcular rezagos espaciales es sencillo con la función spatial_lag(), en la que hay que especificar dos argumentos: gda_w= la matriz de pesos espaciales y df= la variable sobre la que se desea el rezago espacial, que proviene del objeto de tipo sf. Por ejemplo, para la variable casos positivos, pos_hab y con una matriz de tipo reina, queen_w: lag &lt;- rgeoda::spatial_lag(queen_w, covid_zmvm_sf[&#39;pos_hab&#39;]) head(lag) ## Spatial.Lag ## 1 15.45044 ## 2 15.57109 ## 3 13.91179 ## 4 12.22961 ## 5 13.20082 ## 6 11.51118 Para mejor mirar el rezago espacial, incorporémoslo a un nuevo dataframe con la variable pos_hab, de forma emejante a como lo hicimos antes: df &lt;- base::data.frame(pos_hab = covid_zmvm_sf$pos_hab, lag) kable(head(df)) pos_hab Spatial.Lag 14.36500 15.45044 16.98318 15.57109 13.35265 13.91179 16.16393 12.22961 17.36369 13.20082 17.31354 11.51118 Ejercicio Con el paquete rgeoda, construye rezagos espaciales con las otras estructuras espaciales: torre, distancia mínima, k-vecinos. ¿Por qué en cada caso es diferente el valor rezago? 3.4 Coeficiente de correlación espacial: la I de Moran Una vez que hemos abordado y resuelto el problema de cómo definir vecindad a partir de la matriz de pesos espaciales y que hemos operacionalizado la definición de valor de la variable en los vecinos a través de la noción de rezago espacial, tenemos todos los elementos que integran la definición de autocorrelación espacial: i) relación funcional, ii) valor de una variable y iii) relación de vecindad. Ahora bien, ¿cómo medimos la autocorrelación? Es decir, cómo sabemos si, por ejemplo, la variable casos positivos por COVID19 está autocorrelacionada. Al principio de este capítulo dijimos que apreciar ciertos patrones de agrupamiento en un mapa, como el del índice de desarrollo humano, era un posible indicio de autocorrelación espacial. Es momento de formalizar dicho indicio a través de un indicador apropiado. Debemos pues construir un estadístico, un estadístico de autocorrelación espacial. El estadístico de asociación espacial más socorrido es el propuesto por Patrick Moran: la I de Moran. La I de Moran es un coeficiente de correlación lineal que “incorpora al espacio”, es decir, mide la asociación lineal entre una variable (digamos casos positivos por COVID19) y su rezago espacial (el valor promedio de los casos positivos de los vecinos). Como todo coeficiente de asociación, el valor de la I de Moran se encuentra entre -1 y 1. Si el valor de la I de Moran es positivo decimos que hay signos de concentración o patrones de aglomeración pues existe autocorrelación espacial positiva, por lo que existen unidades espaciales (alcaldías, municipios) que tienen valores altos en la variable medida que están rodeadas por otras unidades espaciales que tienen valores también altos. Decir que existe autocorrelación espacial positiva también implica afirmar que hay unidades espaciales con valores bajos rodeadas de otras que tienen también valores bajos. Por otro lado, si la I de Moran es negativa esto es evidencia de otro tipo de patrones, ya no de aglomeración sun ode dispersión o repulsión: una alcaldía o municipio que tiene valores altos está rodeada de vecinos con valores bajos y viceversa. Hasta donde sabemos, el paquete rgeoda aún no incorpora una función para construir automáticamente la I de Moran, por lo que el enfoque aquí presentado corresponde, para simplificar, al del paquete spdep. Para calcular en R el coeficiente o estadístico de Moran necesitamos recurrir a la función moran.test() del paquete spdep. Los argumentos de la función deben especificar el nombre de la variable y el tipo de estructura espacial dado por la matriz de pesos usada; adicionalmente, se puede indicar qué hacer en caso de que existan islas (objetos espaciales sin vecinos) con el argumento zero.policy. Ejercicio Solicita ayuda del paquete spdep y responde, ¿para qué sirve el argumento randomization de la función moran.test()? De nuevo, en la ayuda de la función, ¿con cuál de los argumentos es posible cambiar la hipótesis alternativa de la evaluación de autocorrelación espacial en la prueba de Moran? Construyamos el estadístico de Moran para la variable pos_hab usando la estructura espacial llamada mTRUE.est: spdep::moran.test(covid_zmvm$pos_hab, mTRUE.est) ## ## Moran I test under randomisation ## ## data: covid_zmvm$pos_hab ## weights: mTRUE.est ## ## Moran I statistic standard deviate = 8.8625, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.656334129 -0.013333333 0.005709563 Del indicador obtenido nos interesan tres elementos, como es usual: la magnitud del coeficiente, su sentido y su significancia estadística. En los resultados que aparecen en tu consola identifica cada uno de ellos y responde: Ejercicio ¿A cuánto asciende el coeficiente estimado? ¿Podría decirse que es alto o bajo? ¿La relación identificada es positiva o negativa? Un coeficiente como el obtenido, de 0.656, indica que hay una relación positiva entre los valores de los casos positivos de COVID19 y los valores de los casos positivos por COVID19 en los entornos vecinos (sentido de la asociación), además, podríamos decir que es relativamente alto en la medida en que está más próximo a uno que a cero (magnitud de la relación). Ahora, ¿qué hay con su significancia estadística? ¿Cómo podemos saber que dicho resultado,. el 0.656, no es producto de una coincidencia sino un resultado sistemático o consistente? Para ello, evaluaremos el siguiente juego de hipótesis sobre el índice de Moran: \\(Ho: I=0\\), es decir, ausencia de autocorrelación o, de forma equivalente, distribución espacial aleatoria. \\(Ha:I \\neq 0\\), es decir, presencia de autocorrelación o, de forma equivalente, distribución espacial no aleatoria. Con la información disponible evaluamos dichas hipótesis. Observa como el p-valor obtenido en extremadamente pequeño (p-value &lt; 2.2e-16); si fijamos un nivel de significancia, \\(\\alpha=0.05\\), se rechaza la hipótesis nula en favor de la hipótesis alternativa, por tanto, la variable no se distribuye de forma aleatoria en el espacio, sino que muestra indicios de autocorrelación espacial positiva relativamente alta (0.656). Ejercicio Construya una I de Moran con la estructura espacial dada por la matriz donde queen=FALSE responda: ¿La asociación espacial es positiva o negativa? ¿Consideras que es alta o baja? ¿Dirías que dicha relación es producto del azar o que existe un comportamiento sistemático? 3.4.1 Diagrama de Moran Una forma creativa de expresar gráficamente la autocorrelación es a través de un diagrama de dispersión cuyo eje \\(x\\) corresponde a la variable de interés, casos positivos de COVID19 por cada 1 mil habitantes, y el eje \\(y\\) a su rezago espacial; además, al agregar una recta de ajuste sobre los datos estandarizados lograremos que la pendiente de dicha recta corresponda exactamente al valor de la I de Moran. Una de las características del diagrama de Moran es que se descompone en cuatro cuadrantes, tal como aparece en la figura 3.5. Figura 3.4: Diagrama de dispersión de Moran Para representar el diagrama de Moran recurrimos a la función moran.plot() del paquete spdep que para una matriz de tipo reina con el argumento queen=TRUE y la base de datos de tipo SpatialPolygonsDataFrame: spdep::moran.plot(((covid_zmvm$pos_hab)-mean(covid_zmvm$pos_hab))/(sd(covid_zmvm$pos_hab)), listw = mTRUE.est, xlab=&quot;Casos positivos&quot;, ylab=&quot;Rezago espacial de los casos positivos&quot;, main=&quot;Diagrama de Moran para casos positivos&quot;, col=&quot;lightblue&quot;) En el diagrama de Moran recién construido notamos varias cosas: i) Una parte importante de las observaciones caen en el cuadrante I y III, por lo que la relación funcional que predomina entre el conjunto de puntos es positiva. ii) Que la relación funcional dominante sea positiva no implica que no haya observaciones en los cuadrantes II y IV. 3.5 Múltiples elementos de personalización de ésta y otras gráficas asociadas al paquete base de R pueden revisarse en la documentación de la función par(). Ejercicio ¿Es posible construir un diagrama de Moran usando el paquete ggplot2? De ser así, ¿cómo lo harías? 3.6 Índice de Moran local y mapa de clusters El índice de Moran que recién hemos calculado permite evaluar la existencia de un patrón espacial completo o global, es decir, para el conjunto de todas las observaciones. Por ello, no proporciona información de la ubicación de las agrupaciones o clusters de alcaldías y municipios. Por ello se dice que la I de Moran es una medida de autocorrelación espacial global: nos dice que hay patrones de concentración o dispersión pero no nos dice a qué municipios o alcaldías específicos es posible atribuir dichas fuerzas de aglomeración. Para subsanar esta situación, Anselin (1995) propuso la versión local de la I de Moran: el indicador local de asociación espacial o LISA (local indicator of spatial association). El LISA: Proporciona un estadístico para cada ubicación con un nivel de significancia y Establece una relación proporcional entre el estadístico local y el global. Es decir, nos permite identificar a que unidades espaciales (alcaldías o municipios) es posible atribuir la autocorrelación espacial de forma específica y con qué intensidad, en relación con el indicador global. La representación del LISA se hace con arreglo a dos mapas: El mapa de cluster o mapa de agrupaciones que permite clasificar las áreas (alcaldías y municipios) con presencia de autocorrelación espacial según el tipo de asociación identificada. Este mapa permite la clasificación de las áreas estadísticamente significativas en clusters o agrupamientos (alto-alto y bajo-bajo) y de áreas que se constituyen como observaciones espaciales atípicas o spatial outliers (agrupamientos bajo-alto y alto-bajo). El mapa de significancia: muestra las ubicaciones con la I de Moran local que son representativas en diferentes niveles de significancia. El Índice local de Moran toma la forma de: \\[I_i = \\frac{(x_i-\\bar{X})}{S_i^2}{\\sum_{j=1,j \\neq i}^{n}w_{ij}(x_j-\\bar{X})}\\] Donde: \\[ S_i^2= \\frac{\\sum_{j=1,j \\neq i}^{n}(x_j-\\bar{X}) ^2}{n-1} \\] Además, \\(x_i\\) es el valor de la variable de interés, \\(\\bar{X}\\) es el promedio de dicha variable, \\(w_{ij}\\) es cada uno de los elementos de la matriz de pesos espaciales y \\(n\\) es el número de objetos espaciales. Para calcular un índice de Moran local en R usamos el paquete rgeoda, que es el enfoque más sencillo de ejecutar. Echaremos mano de la función local_moran(), que requiere dos argumentos forzosos: la estructura espacial(w=) y la variable para la que se desea el LISA (df=). Primero, guardemos la variable de la que deseamos un LISA en un objeto independiente: pos_hab=covid_zmvm_sf[&quot;pos_hab&quot;] Luego, usemos dicha variable para construir el LISA: lisa_poshab &lt;- local_moran(w=queen_w, df=pos_hab) Dentro del objeto lisa_poshab hay múltiples elementos, que serán útiles más adelante. De momento, llamemos los valores del indicador local de asociación espacial a través de la función lisa_values(), es decir, los valores del índice computado para cada uno de los 76 municipios y alcaldías que componen la ZMVM: lisaval_poshab &lt;- lisa_values(lisa_poshab) Para representar en un mapa los valores del LISA, es necesario añadirlos a la base de datos original en formato sf: mapa.lisa &lt;- base::cbind(covid_zmvm_sf, lisaval_poshab) Luego, con las funciones aprendidas en el capítulo anterior, podemos presentar un mapa de quintiles que represente el valor del indicador local de asociación espacial: tmap::tm_shape(mapa.lisa) + tmap::tm_fill(col = &quot;lisaval_poshab&quot;, style = &quot;quantile&quot;, palette = &quot;Spectral&quot;, midpoint= NA, title = &quot;I de Moran local&quot;) + tmap::tm_borders() El mapa permite observar la manera en que varía la correlación espacial a nivel local para la variable pos_hab, casos positivos por COVID19 por cada 1 mil habitantes, pues proporciona un valor de correlación para cada municipio y alcaldía: es claro que la mayor parte de los valores altos del LISA se encuentran en la Ciudad de México (tonos en azul). Necesitamos otros instrumentos que nos permitan identificar si los valores de la I de Moran local son o no significativos, lo que haremos a través del mapa de cluster y su respectivo mapa de significancia. Esto nos permitirá identificar agrupaciones o núcleos de cluster significativos, así como observaciones espaciales atípicas. Dentro del objeto creado, lisa_poshab, hay múltiples elementos, mismos que pueden ser llamados por funciones específicas, entre las que se encuentran: lisa_clusters(): que los valores de clasificación de cada cluster. lisa_colors(): brinda los colores asociados a cada uno de los clusters computados. lisa_labels(): proporciona las etiquetas de los clusters computados. Así pues, el mapa los construimos en dos pasos: i) primero obtendremos algunos elementos preliminares para construir el mapa (colores, etiquetas y valores de agrupamiento) y ii) construiremos el mapa con las funciones básicas de R_ #Elementos preliminares lisa_colores &lt;- lisa_colors(lisa_poshab) lisa_etiq &lt;- c(&quot;No significativo&quot;, &quot;Alto-Alto&quot;, &quot;Bajo-Bajo&quot;, &quot;Bajo-Alto&quot;, &quot;Alto-Bajo&quot;, &quot;No definido&quot;, &quot;Aislado&quot;) lisa_clusters &lt;- lisa_clusters(lisa_poshab) Ahora, con dichos elementos auxiliares, construimos el mapa de cluster: #Mapa de cluster plot(st_geometry(covid_zmvm_sf), col=sapply(lisa_clusters, function(x){return(lisa_colores[[x+1]])}), border = &quot;#333333&quot;, lwd=0.2) title(main = &quot;Moran Local de pos_hab&quot;) legend(&#39;bottomleft&#39;, legend = lisa_etiq, fill = lisa_colores, border = &quot;#eeeeee&quot;) Junto con el mapa anterior es común presentar el mapa de significancia, un tipo de representación que indica el nivel de significancia inividual para cada una de las observaciones (municipios y alcaldías). Para ello, también diviimos el procedimiento en dos partes: #Elementos preliminares lisa_p &lt;- lisa_pvalues(lisa_poshab) p_etiq &lt;- c(&quot;No significativo&quot;, &quot;p &lt;= 0.05&quot;, &quot;p &lt;= 0.01&quot;, &quot;p &lt;= 0.001&quot;) p_colores &lt;- c(&quot;#eeeeee&quot;, &quot;#84f576&quot;, &quot;#53c53c&quot;, &quot;#348124&quot;) #Mapa de significancia plot(st_geometry(covid_zmvm_sf), col=sapply(lisa_p, function(x){ if (x &lt;= 0.001) return(p_colores[4]) else if (x &lt;= 0.01) return(p_colores[3]) else if (x &lt;= 0.05) return (p_colores[2]) else return(p_colores[1]) }), border = &quot;#333333&quot;, lwd=0.2) title(main = &quot;Mapa de significancia de los casos positivos&quot;) legend(&#39;bottomleft&#39;, legend = p_etiq, fill = p_colores, border = &quot;#eeeeee&quot;) El par de mapas anteriores permiten identificar agrupamientos de valores significativos al 5% o menos, es decid, núcleos de cluster que muestran municipios y alcaldías con valores altos de tasas positivas de COVID19 rodeados de vecinos con valores altos (agrupamiento Alto-Alto), así como agrupamientos de valores bajos (cuadrante Bajo-Bajo), además de observaciones espaciales atípicas (cuadrante Alto-bajo y Bajo-Alto). En síntesis, hasta este punto hemos visto en este capítulo: Cómo definir estructuras de relación espacial a través de diversos criterios, Cómo identificar autocorrelación espacial global a través de la I de Moran, Cómo evaluar la significancia estadística de la I de Moran, Cómo identificar agrupaciones locales a través del indicador LISA. En el capítulo 5 nos adentraremos en cómo incorporar la riqueza que proporciona el análisis espacial en un modelo econométrico. Mientras tanto, en el capítulo 4 llevaremos a cabo un repaso de elementos básicos sobre los modelos de regresión lineal clásica con mínimos cuadrados ordinarios. References "],["modelos-de-regresión-lineal.html", "4 Modelos de regresión lineal 4.1 Causalidad y correlación: el alcance del análisis de regresión 4.2 Lo “artesanal” de la econometría 4.3 Qué buscamos a través de una regresión (con mínimos cuadrados ordinarios) 4.4 Modelo de regresión lineal simple 4.5 Regresión lineal simple en R 4.6 Algunos elementos de cuidado con las regresiones lineales 4.7 Regresión lineal múltiple: una propuesta de modelo 4.8 Alcance de la regresión y en análisis causal", " 4 Modelos de regresión lineal Hasta este punto hemos hecho un recorrido que va desde los elementos más básicos sobre la exploración de información (capítulo 1), pasando por la representación de información espacial a través de mapas coropléticos (capítulo 2), hasta la exploración de patrones de asociación de una variable en el espacio, la llamada autocorrelación (capítulo 3). Antes de avanzar en el conocimiento de las herramientas que nos permitirán elaborar algunos modelos básicos de econometría espacial que incorporen los patrones espaciales que hemos identificado (capítulo 5), en este capítulo hacemos un pequeño paréntesis en el tratamiento de la información espacial para llevar a cabo un repaso elemental, desde una perspectiva práctica, del modelo clásico de regresión lineal. Esto nos permitirá que tengas en la mente dos cosas: i) qué es lo que se busca a través de modelos econométricos y ii) cuáles son los supuestos detrás del modelo clásico de regresión lineal estimado con el método de mínimos cuadrados ordinarios (MCO). 4.1 Causalidad y correlación: el alcance del análisis de regresión En términos de su origen etimológico, econometría significa medición econonómica. Pero su alcance se extiende más allá de la simple medición ya que a través de ella es posible contribuir a dar soporte empírico a la teoría económica a través del uso de herramientas estadísticas como los modelos de regresión y sus estimaciones (Gujarati and Porter 2011). Veamos esto con un poco más de detalle. En el proceso de investigación podemos plantearnos preguntas con un grado de complejidad diverso. Habrá preguntas que, al ser respondidas, nos permitan tener una descripción general del fenómeno, habrá otras que nos permitan explorar la relación entre dos fenómenos, o bien, preguntas cuya respuesta nos permita predecir el comportamiento futuro del elemento estudiado. Así pues, hay diferentes tipos de preguntas de investigación, dependiendo de su grado de complejidad. En su libro “El arte de la ciencia de datos”, Roger Peng y Elizabeth Matsui(2017), abordan sistemáticamente esta cuestión. Cuando, por ejemplo, nos preguntamos “¿el cambio en una variable provoca el cambio en otra diferente?”, este tipo de pregunta es llamada pregunta de análisis causal. Con el análisis causal se busca saber si un par de variables están asociadas, más aún, si el cambio en una variable causa cambios en la otra. Responder este tipo de preguntas exige el dominio de diversas herramientas estadísticas, cuyo abordaje está fuera del alcance de este capítulo. No obstante, la econometría a través del análisis de regresión nos brinda una oportunidad que, cuando es bien utilizada, permite aproximarnos al análisis causal. A partir de una relación causal anclada en la teoría (modelo teórico), es preciso indicar el tipo de relación específico entre las variables del caso (modelo matemático), para luego expresar dicha relación en una relación no exacta sino sujeta a variaciones individuales (modelo econométrico) (Gujarati and Porter 2011: 3-5). La regresión como instrumento analítico nos brinda elementos para acercarnos a la comprensión de la asociación entre dos variables, aunque no es posible hablar en sentido estricto descubrir relaciones de causalidad como resultado del análisis de regresión. Así, “El análisis de regresión solo puede abordar los problemas de correlación. No puede abordar el problema de la necesidad (causalidad). Por tanto, nuestras expectativas de descubrir relaciones de causa y efecto a partir de la regresión deberían ser modestas” (Montgomery, Peck., and Vining. 2012, 81:3). Entonces, ¿de qué modo el análisis de regresión se convierte potencialmente en un elemento que nos permite analizar causalidad? Lo que resultará fundamental en el proceso de investigación que recurre a esta herramienta es pues la pregunta que se plantee el investigador, preguntas que deben ser planteadas desde el conocimiento científico previamente existente. Dicho en otros términos, el análisis de causalidad que nos da la regresión, sí lo puede ofrecer la teoría existente en determinado campo de conocimiento: ¿qué dice la teoría sobre la relación entre la variable A y la variable B? ¿Qué asociaciones entre pares de variables recuerdas de tus cursos teoría económica? Quizá los siguientes ejemplos te resulten familiares: i) la asociación entre la tasa de interés y eficiencia marginal del capital en la teoría de la inversión de Keynes, ii) la relación entre la composición orgánica del capital y tasa de ganancia en la perspectiva de Marx sobre el comportamiento de largo plazo del capitalismo (la teoría de la tasa de ganancia decreciente), iii) los rendimientos decrecientes en el producto al añadir una unidad adicional de trabajo, ceteris paribus, en la teoría del productor de los neoclásicos. o bien, preguntas de otra naturaleza como “¿una educación formal de más años está asociada a un ingreso personal más elevado?”, o bien, “¿la población que vive barrios con viviendas de menores dimensiones está más expuesta al COVID19?” En el proceso de investigación científica, las preguntas planteadas deberán hacerse desde el conocimiento previamente existente, es decir, se plantean desde determinados cuerpos teóricos reconocidos y validados, o bien, a partir de los resultados de investigaciones previas. Antes del método está la pregunta de investigación que establece la línea de causalidad entre las variables postuladas en el modelo. Esta base teórica ha de ser complementada con los resultados arrojados en el análisis exploratorio. Así pues, es relevante la siguiente advertencia: “Para establecer la causalidad, la relación entre los regresores y la variable de respuesta debe tener una base fuera de los datos de la muestra; por ejemplo, la relación puede ser sugerida por consideraciones teóricas. El análisis de regresión puede ayudar a confirmar una relación de causa y efecto, pero no puede ser la única base de tal afirmación” (Montgomery, Peck., and Vining. 2012, 81:3). Los elementos anteriores, la metodología econométrica clásica, son explicados con detalle en el primer capítulo del libro Econometría Básica de Gujarati y Porter, del que existen varias ediciones y que se recomienda ampliamente consultar. 4.2 Lo “artesanal” de la econometría No cabe duda que la especificación de un buen modelo econométrico tiene algo de artesanal, es decir, serán pocas las ocasiones en las que no tendremos que proponer diferentes alternativas de modelo, probar con múltiples variables y llevar a cabo un sin número de ajustes. Conviene desde este momento eliminar la idea de que “solo hay que aplicar una técnica”: todas las técnicas requieren práctica y el desarrollo de habilidad en su uso pues no siempre enfrentaremos un mismo problema en las mismas condiciones. A este respecto puede ser apuntado que: “La construcción de un modelo de regresión es un proceso iterativo. Comienza utilizando conocimiento teórico del proceso que se está estudiando y los datos disponibles para especificar un modelo de regresión inicial. Las visualizaciones de datos gráficos suelen ser muy útiles para especificar el modelo inicial. Luego, los parámetros del modelo se estiman, normalmente por mínimos cuadrados (…). Luego debe evaluarse la adecuación del modelo. Consiste en buscar posibles errores de especificación de la fórmula del modelo, como no incluir variables importantes, o incluir variables innecesarias, o datos inusuales o inapropiados. Si el modelo es inadecuado, entonces debe proponerse nuevamente uno diferente y estimar nuevamente los parámetros. Este proceso puede repetirse varias veces hasta obtener un modelo adecuado. Finalmente, la validación del modelo debe llevarse a cabo para asegurar que el modelo producirá resultados que sean aceptables en la aplicación final” (Montgomery, Peck., and Vining. 2012, 81:3–10). La idea anterior aparece esquematizada en la figura 4.1, que es tomada de Montgomery et al. (2012: p. 10)y . Figura 4.1: Construcción de un modelo de regresión En este capítulo buscamos que recuerdes algunos de los elementos fundamentales del análisis de regresión, como sus supuestos básicos y la interpretación de los resultados que nos ofrece este instrumento. Para ilustrar dichos elementos nos serviremos de la base de datos sobre COVID19 que contiene información sobre las condiciones sociodemodráficas y económicas de los municipios en la Zona Metropolitana del Valle de México, la misma base que hemos explorado en los capítulos previos. La pregunta que nos interesa responder es de carácter preliminar y no atiende a cuerpo teórico alguno, por lo que los resultados son solo ilustrativos y tienen solo fines didácticos. La pregunta planteada es: ¿de qué manera se asocian las condiciones sociales, demográficas y económicas, tanto con los casos positivos de COVID19 por cada 1 mil habitantes, como con las muertes provocadas por este mal por cada 1 mil habitantes? Para lograr dar solución a la pregunta propondremos algunos modelos econométricos y buscaremos interpretarlos. 4.3 Qué buscamos a través de una regresión (con mínimos cuadrados ordinarios) Un modelo es una representación simplificada de la realidad. En el caso particular de un modelo econométrico y de la regresión lineal lo que se pretende encontrar la recta que mejor se ajusta a los datos observados. La técnica más usual para lograrlo son los llamados mínimos cuadrados ordinarios, MCO, justamente poque este método minimiza la suma de los residuales al cuadrado. Veamos esta idea de forma interactiva a través de una sencilla ilustración en R. Para ello, crearemos una función que tendrá un solo parámetro al cual llamaremos beta y que será el valor de la pendiente de una recta que buscaremos ajustar a un conjunto de datos. Lo que pretendemos pues es encontrar el mejor valor de beta, es decir, el valor que permita minimizar la distancia entre los puntos dados por los pares ordenados (variable \\(x\\) y \\(y\\)) y la recta, en otras palabras, buscamos un valor de beta que minimice el error. Para usar la aplicación interactiva, debes instalar los siguientes paquetes: #Instalar dos paquetes install.packages(c(&quot;manipulate&quot;, &quot;UsingR&quot;)) Ahora bien, antes de ejecutar los segmentos de código que aparecen más abajo, leélos e intenta comprender qué es lo que hace cada línea: Se usa function() justamente para crear para crear una función que llevará por nombre Gráfica (function() es una función como cualquier otra en R, semejante a la función para calcular una media, mean(), o para crear una gráfica,plot()). Dentro del paréntesis colocamos los argumentos con los que trabajará nuestra función y, entre las llaves, {}, indicamos los elementos con los que vamos a operar, incluidos los argumentos. Nuestra función tomará información de la base de datos llamada Galton, que viene con el paquete HistData, recuerda que Galton (1886) sentó las bases del análisis de regresión a través de un estudio de lo que podríamos aquí llamar informalmente “herencia genética”, en otras palabras, cómo la altura de padres e hijos adultos está asociada. Después de haber leído el código, cópialo y pégalo en tu consola6: #Para llamar las librerías que recién instalaste library(manipulate) library(UsingR) #Crear función llamada &quot;Gráfica&quot; Gráfica &lt;- function(beta){ y &lt;- Galton$child - mean(Galton$child) #Crea un vector llamado &quot;y&quot; con las desviaciones de la media de las alturas de los hijos. x &lt;- Galton$parent - mean(Galton$parent) #Crea un vector llamado &quot;x&quot; con las desviaciones de la media de las alturas de los padres. freqData &lt;- as.data.frame(table(x, y)) #Crea un arreglo de datos llamado freqData con los vectores anteriores names(freqData) &lt;- c(&quot;child&quot;, &quot;parent&quot;, &quot;freq&quot;) #Asigna nombres a las columnas del data frame anterior plot( as.numeric(as.vector(freqData$parent)), as.numeric(as.vector(freqData$child)), pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = .15 * freqData$freq, xlab = &quot;Padres&quot;, ylab = &quot;Hijos&quot; )#Crea un diagrama de dispersión con varios elementos de formato, el más importante es que el tamaño de los símbolos usados dependerá de la frecuencia. abline(0, beta, lwd = 3)#Añade una línea en función del valor elegido de beta points(0, 0, cex = 2, pch = 19) #Añade puntos mse &lt;- mean( (y - beta * x)^2 ) #Muestra el valor de la suma de los residuales al cuadrado title(paste(&quot;beta = &quot;, beta, &quot;mse = &quot;, round(mse, 3)))} #Añade títulos a la gráfica Una vez que has ejecutado el código, observa la sección de ambiente en RStudio y deberás ver la función creada, Gráfica(). Vamos a interactuar con ella y para ello ejecutaremos el siguiente segmento de código: manipulate(Gráfica(beta), beta = slider(-1.5, 1.5, step = 0.02)) Verás en la sección de gráficos el resultado del código anterior: una gráfica con círculos de diferente tamaño que representan la altura de padres y sus hijos adultos; además, identificarás una línea (nuestra recta de ajuste). Como encabezado encontraras el nombre del único argumento de nuestra función, beta. Recuerda: beta es el valor de la pendiente de esa recta. Junto al valor del argumento de nuestra función aparece la suma de los residuales al cuadrado, o Error Cuadrático Medio (Mean Squared Error, MSE). ¿Logras observar el engrane en la parte superior izquierda de la gráfica? Usa el deslizador que aparece cuando das clic en él para elegir diferentes valores para beta y seleccionar aquel que te permita obtener el menor valor de MSE: ese valor de beta es el que buscamos con los MCO. Como verás, lo que buscamos es justamente un modelo que describa nuestros datos. Hay muchos tipos de modelos para describir el comportamiento entre dos variables (curvas cuadráticas, curvas exponenciales o curvas polinomiales). Aquí buscamos expresar la relación entre nuestro par de variables de la forma más simple posible: a través de una línea recta. Te recomendamos ampliamente el material de David Dalpiaz, Applied Statistics with R, particularmente el capítulo 7 en caso de que quieras recordar con todo detalle los fundamentos estadísticos de la regresión (Dalpiaz 2022). 4.4 Modelo de regresión lineal simple Ya que tenemos una noción básica de lo que buscamos con una regresión, es decir, un modelo lineal que describa el comportamiento de nuestros datos, es momento de dar una definición más precisa: “El análisis de regresión es una técnica estadística para investigar y modelar la relación entre variables” (Montgomery, Peck., and Vining. 2012). Un modelo de regresión lineal simple es un modelo con una sola regresora (una variable \\(x\\)) cuya relación con la variable de respuesta (\\(y\\)) está dada por una línea recta: \\[y=\\beta_0+\\beta_1x+u \\] En donde \\(\\beta_0\\) y \\(\\beta_1\\) son constates desconocidas, el intercepto y la pendiente, respectivamente; en tanto, \\(u\\) es el componente de error aleatorio (que se asume con una distribución normal con media cero y varianza constante, \\(N(\\mu=0,\\sigma_u^2=\\sigma^2)\\)), que es la “falla” de nuestro modelo, es decir, la diferencia entre el valor observado de \\(y\\) y el valor estimado por nuestro modelo. ¿Cómo deben ser entendidos los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\), el intercepto y la pendiente? “La pendiente, \\(\\beta_1\\) se puede interpretar como el cambio en la media de \\(y\\) para un cambio unitario en \\(x\\)” (Montgomery, Peck., and Vining. 2012, 81:3). En tanto, “si el rango de datos en \\(x\\) incluye \\(x = 0\\), entonces la intersección \\(\\beta_0\\) es la media de la distribución de la variable de respuesta \\(y\\) cuando \\(x = 0\\) (es decir, la media de la variable \\(y\\) cuando \\(x=0\\)) Si el rango de \\(x\\) no incluye cero, entonces \\(β_0\\) no tiene una interpretación práctica”. Una regresión lineal es pues el método que nos permite evaluar la relación lineal entre variables numéricas y se le llama precisamente lineal por la linealidad de sus parámetros (las betas que, como notarás, están elevadas al exponente 1). Lo que buscamos con el análisis de regresión es encontrar los valores estimados de \\(\\beta_0\\) y \\(\\beta_1\\), es decir, \\(\\hat\\beta_0\\) y \\(\\hat\\beta_1\\), que nos permitan describir con una línea recta el par de variables consideradas y el método más usual para hallarlas son los Mínimos Cuadrados Ordinarios, MCO. Los estimadores de MCO ordinarios, tienen tres propiedades: son lineales, insesgados y de varianza mínima. 4.4.1 Supuestos del módelo clásico de regresión lineal En esta sección enlistamos los elementos sobre los que se funda la estimación de un modelo clásico de regresión lineal con mínimos cuadrados ordinarios y haremos énfasis en uno de ellos que se vincula directamente con los contenidos del capítulo siguiente: la autocorrelación. Los supuestos sobre los que se funda en modelo clásico de regresión lineal son: El modelo es líneal en los parámetros, aunque no necesariamente en sus variables. Los valores de la variable o variables explicativas son fijos, es decir, no aleatorios, o bien, independientes del término de error. La media de los términos de error, dado el valor de la variable explicativa, es cero. La varianza de los errores es constante (homoscedasticidad). Los errores o perturbaciones son independientes entre sí (no hay autocorrelación). El número de observaciones \\(n\\) debe ser mayor que el número de parámetros a estimar. La variable o variables explicativas debe tener suficiente variabilidad. Detengámonos un momento en el primer supuesto, relativo a la linealiadad de los parámetros. Trata de responder a la siguiente pregunta: Ejercicio De las siguientes expresiones, ¿cuál corresponde a un modelo líneal en los parámetros? \\(Y=\\beta_1+\\beta_2X +\\beta_3X^2\\) \\(Y=e^{\\beta_1+\\beta_2X}\\) \\(Y=\\beta_1+\\beta_2X\\) Como habrás podido notar, todas las opciones corresponden a modelos lineales en los parámetros, es decir, en las “betas”. No importa si en en el modelo las variables explicativas, las \\(X\\) aparecen elevadas a algún exponente o ellas mismas son exponentes. Respecto al resto de los supuestos, no necesariamente éstos se cumplen cuando se recurre a la econometría. Para un exámen cuidadoso de la violación de estos supuestos y qué hacer en cada caso, remitimos a la estudiante al clásico libro de Gujarati y Porter (2011), donde en la tabla 3.4 podrá encontrar una ruta que le guie en cómo atender cada caso. Aquí sólo haremos mención con un poco más de detalle sobre el supuesto v o supuesto de no autocorrelación de los errores, en la medida en que se vincula directamente con el contenido del siguiete capítulo. El supuesto v o supuesto de no autocorrelación establece que dos errores, \\(u_i\\) y \\(u_j\\), donde los subíndices \\(j\\) e \\(i\\) corresponden a perturbaciones de dos observaciones diferentes, no están asociados, es decir, que la covarianza entre ellos es igual a cero. Si se grafican los errores de un modelo en un diagrama de dispersión, lo que esperamos es que no haya ningún patrón sistemático. Reproducimos aquí la figura 3.6 de la obra de Gujarati y Porter (2011), en donde los paneles a y b dan cuenta de un comportamiento sistemático o no aleatorio en los errores, lo que es indicio de autocorrelación o dependencia entre los errores, en cambio, en el panel c no se observa ningún patrón sistemático, esto es justamente lo que se busca en los errores del modelo, que los errores se distribuyan aleatoriamente. Figura 4.2: Patrones de correlación. Fuente: Gujarati y Porter, 2011, p. 67 De forma simple, con el supuesto de no autocorrelación “se afirma que se considerará el efecto sistemático, si existe, de \\(X_t\\) sobre \\(Y_t\\), sin preocuparse por las demás influencias que podrían actuar sobre \\(Y\\) como resultado de las posibles correlaciones entre las \\(u\\)” (Gujarati and Porter 2011, 67). Como vimos en el capítulo previo, los datos espaciales suelen presentar autocorrelación como una de sus características usuales. Estimar con MCO un modelo econométrico con este tipo de datos muy probablemente violará este supuesto, por lo que se recurre a técnicas específicas para la modelación con datos espaciales, esto será objeto del capítulo 5 donde veremos algunos modelos para solventar este problema. En el resto del capítulo se presenta la manera de construir e interpretar modelos econométricos lineales en R a través de MCO. 4.5 Regresión lineal simple en R Para ilustrar cómo emplear una regresión lineal en R usaremos de nuevo la base de datos covid_zmvm, conformada por 54 variables de 76 observaciones, municipios y alcaldías, que conforman la Zona Metropolitana del Valle de México. Carguemos nuestra base de datos: library(readxl) covid_zmvm &lt;- read_excel(path=&quot;base de datos\\\\covid_zmvm.xlsx&quot;) La base recién cargada es de tamaño considerable, por lo que te recomendamos revisar el diccionario que la acompaña para que comprendas el significado las etiquetas usadas para cada variable. Para mejor comprender el tipo de variables de la base, podemos dividirlas en tres categorías: COVID19: cuatro variables sobre casos positivos y defunciones por COVID19 durante la primera ola de contagios, en términos absolutos y relativos. Sociodemográficas: 32 variables asociadas a las características de la población y sus condiciones de vivienda Estructura económica: nueve variables que describen la estructura económica y remuneraciones medias por gran sector de actividad. Para ilustrar el método de regresión con MCO y estimar los parámetros \\(\\beta_1\\) y \\(\\beta_0\\) propondremos un modelo de regresión lineal simple, es decir, compuesto solo por una variable explicativa o predictoria, \\(x\\), y una sola variable a explicar o variable de respuesta, \\(y\\). Con base en la información que integra la base, la variable que buscamos explicar con nuestro modelo será casos positivos a COVID19 por cada 1 mil habitantes, pos_hab, explicada a través del porcentaje de población con acceso a a servicios de salud, ss. El modelo a estimar tomaría la forma de: \\[poshab_i=\\beta_0+\\beta_1ss_i+u_i\\] El subíndice \\(i\\) da cuenta de cada una de las 76 unidades espaciales que conforman la Zona Metropolitana del Valle de México. Para estimar un modelo como el anterior, en R recurrimos a la función lm(), linear model, del paquete stats que se instala desde que añades R a tu equipo de cómputo. La función usada cuenta con dos argumentos data= y formula= que indican la fuente de datos y la expresión a estimar, respectivamente. El resultado lo guardaremos en un nuevo objeto que nombraremos modelo_simple: modelo_simple &lt;- stats::lm (formula=pos_hab ~ ss, data = covid_zmvm) Nota cómo la especificación del modelo tiene la forma “variable de respuesta ~ variable explicativa”. Observa la parte superior derecha de tu ambiente de trabajo, donde aparece el objeto que se ha creado, éste es de tipo lm, una lista con 12 elementos. Da clic en el icono de la lente de aumento y podrás observar que el resultado de la función lm() contiene muchos datos de interés. Ejercicio Revisa la documentación de la función lm() y responde: ¿Es posible ponderar el proceso de ajuste con base en alguna variable? De ser así, ¿cuál es el argumento que lo permite? Si alguna de las variables usadas en el modelo tiene registros vacíos, ¿a qué argumento debes recurrir para corregir esta situación? ¿Qué elemento del objeto de tipo lm contiene los coeficientes estimados? ¿Al menos cuántos componentes podría contener la lista de tipo lm? ¿Para qué sirven las funciones coef(), resid() y fitted() aplicadas a un objeto de tipo lm? Para observar los resultados de nuestra estimación basta con llamar al objeto a nuestra consola: modelo_simple ## ## Call: ## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm) ## ## Coefficients: ## (Intercept) ss ## -19.8622 0.3949 Una correcta interpretación de los resultados atraviesa por recordar cómo están medidas cada una de las variables usadas en el modelo. La variable pos_hab es el número de casos positivos por cada 1 mil habitantes; tomemos un municipio en particular, por ejemplo, Isidro Fabela, en el Estado de México, que tiene 1.71 casos por cada 1 mil habitantes. En tanto, la variable ss es la proporción de población con afiliación a servicios de salud de cada municipio o alcaldía, que para Isidro Fabela es de 78.6, es decir, la proporción de población con acceso a servicios de salud es de 78.6%. Veamos un par de gráficas para recordar cómo se comportan este par de variables: library(tidyverse) covid_zmvm %&gt;% ggplot2::ggplot()+geom_dotplot(aes(pos_hab)) covid_zmvm %&gt;% ggplot2::ggplot()+geom_dotplot(aes(poss)) El valor de \\(\\hat \\beta_1\\), el coeficiente de la variable ss, es de 0.3949, lo que significa que cuando la proporción de población con acceso a servicios de salud aumenta en 1 punto porcentual (recuerda que así lo estamos midiendo), el número de casos positivos por cada 1 mil habitantes aumenta 0.39, en promedio. Dicho en otras palabras, el parámetro estimado de la pendiente, \\(\\hat \\beta_1\\), nos indica cómo la media de los casos positivos, \\(y\\), es afectada por un cambio en \\(x\\), el porcentaje de personas con acceso a servicios de salud. En este caso, el coeficiente estimado \\(\\hat \\beta_0\\) carece de interpretación razonable. Así, el modelo estimado, usando los valores de los coeficientes obtenidos es: \\[\\hat y_i=\\hat \\beta_0+\\hat\\beta_1x_i\\] Es decir, \\[\\hat {poshab_i}=-19.8622+0.3949ss_i\\] Adicionalmente, podemos construir un diagrama de dispersión entre las variables pos_hab y ss añadiendo un ajuste líneal, es decir, la recta de nuestro modelo regresión: covid_zmvm %&gt;% ggplot()+ geom_point(aes(x=ss,y=pos_hab))+ geom_smooth(aes(x=ss,y=pos_hab),method = &quot;lm&quot;)+ labs(x=&quot;Población con acceso a servicios de salud (%)&quot;, y=&quot;Casos positivos por COVID19&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Como puedes observar, entre ambas variables existe una relación positiva que puedes identificar por la línea con pendiente positiva que representa la recta de regresión que hemos obtenido, cuya pendiente es igual al valor de \\(\\hat \\beta_1\\), es decir, 0.3949. La distancia vertical entre cada punto y la recta de ajuste de la regresión es el error del modelo, es decir \\(u_i\\) 4.5.1 Una exploración visual de los errores Un elemento de interés para el análisis en la regresión lineal estimada son los errores observados, \\(u_i\\), puesto que los supuestos sobre los que se basa el método de mínimos cuadrados ordinarios son, en buena medida, sobre dichos errores. Para lograr comprender mejor qué son los errores y su importancia analítica, conviene imaginar nuestro modelo como “Respuesta = Predicción + Error”, es decir, \\(y=\\hat y+u\\). De esta manera podemos expresar el error como: \\[u_i=y_i-\\hat {y_i}\\] Los errores observados, \\(u_i\\), y los valores estimados, \\(\\hat y_i\\), de nuestro modelo son almacenados en el elemento residuals y fitted.values, respectivamente, dentro de la lista lm. Da clic en la lente de aumento al final de la fila que contiene el objeto de tipo lm en la sección de ambiente en tu entorno de trabajo para observarlos. Para poder analizar mejor los errores del modelo, vamos a crear un nuevo objeto que contenga, además de nuestras variables de interés, pos_hab y ss, la clave municipal, cvemun, los errores observados y los valores ajustados o predichos. Para ello, primero generamos una tabla con la función data.frame() que contendrán las variables mencionadas (aquí, los dobles corchetes, [[]], son utilizados para extraer los elementos solicitados): tabla &lt;- data.frame(covid_zmvm$cvemun, covid_zmvm$ss, covid_zmvm$pos_hab, as.data.frame(modelo_simple[[&quot;fitted.values&quot;]]), as.data.frame(modelo_simple[[&quot;residuals&quot;]])) Luego, creamos una lista que contiene las etiquetas de los nombres deseados para cada variable: nombres &lt;- c(&quot;cvemun&quot;,&quot;ss&quot;, &quot;pos_hab&quot;,&quot;ajustados&quot;, &quot;errores&quot;) Y, finalmente, asignamos los nombres deseados a la tabla creada colnames(tabla)&lt;-nombres Ejercicio Con los datos de la tabla anterior: ¿Cómo esperarías que luzca un diagrama de dispersión entre los valores observados, pos_hab, y los valores ajustados, \\(\\hat {poshab}\\)? Elabora dicho diagrama. Grafica los errores, primero respecto a un índice del 1 al 76 y luego respecto a los valores ajustados, ¿puedes observar algún patrón en ellos? El supuesto de linealidad de nuestro modelo puede ser verificado a partir de una exploración visual de un diagrama de dispersión entre los errores del modelo y la variable explicativa, nuestra \\(x\\), es decir, el porcentaje de población con acceso a servicios de salud. Si nuestro modelo es verdaderamente un modelo lineal, en la gráfica no debería mostrarse patrón alguno. Veamos: tabla %&gt;% ggplot()+ geom_point(aes(x=pos_hab,y=errores)) Claramente hay un patrón, por lo que probablemente un modelo lineal como el propuesto no sería la mejor opción. ¿Recuerdas cómo luce el diagrama de dispersión entre las dos variables de nuestro modelo? Ejercicio Elabora un diagrama de dispersión entre pos_hab y ss y añade un ajuste no lineal. Entonces, parece que el modelo lineal propuesto no capta con precisión la relación entre este par de variables, por lo que un modelo no lineal podría ser una mejor opción; sin embargo, esta alternativa de modelación no es objeto de estas notas, pero en el capítulo 14 de la obra de Gujarati y Porter (2011) puedes revisar con detalle una ruta para solventar esta situación. Otro de los supuestos sobre los que se basa la estimación de modelos con MCO es que los errores deberían tener una distribución normal, lo que no se cumplirá cuando haya observaciones atípicas, los llamados outliers. Para verificar esto visualmente, es posible recurrir a un histograma de los errores, o bien, a una gráfica de normalidad: tabla %&gt;% ggplot()+ geom_histogram(aes(errores)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ¿Notas los valores extremos en el histograma anterior? Una gráfica cuantil-cuantil o gráfica QQ, contrasta la distribución de una variable con una distribución teórica: si la distribución de la variable fuera idéntica a la distribución teórica, los puntos se alinearían sobre la línea de 45°. En este caso, la distribución teórica a probar es una distribución normal: tabla %&gt;% ggplot()+ geom_qq(aes(sample=errores))+ geom_qq_line(aes(sample=errores)) Ejercicio Consulta la ayuda de las funciones geom_qq() y stat_qq_line(), ¿qué otras distribuciones es posible verificar y con qué argumentos? Finalmente, el supuesto de varianza constante de los errores también puede explorarse visualmente a través de una gráfica donde se debería lograr ver variabilidad constante de los errores alrededor del valor cero de los errores: este es el denominado supuesto de homoscedasticidad. Exploremos si los errores de nuestro modelo cumplen o no con este supuesto a través de una gráfica de dispersión: tabla %&gt;% ggplot()+ geom_point(aes(ajustados,errores)) Patrones visuales de “tipo ventilador” o “embudo” no deberían ser observados cuando los errores son homoscedásticos, lo que no es el caso. Los ejercicios gráficos hechos aquí con los errores para verificar si se cumplen o no los supuestos de un modelo de MCO no son, desde luego, la única ruta para verificar los supuestos, analizar si una regresión cumple con los supuestos necesarios requiere de práctica. En esta página encontrarás una serie de ejemplos de cuándo una regresión cumple o no con los supuestos enlistados antes y cómo lucen las gráficas mencionadas en cada caso. Sin embargo, como recordarás, existen pruebas formales que te permitirán determinar si el modelo cumple o no con los supuestos del caso. Ejercicio ¿Cuál es el nombre de la prueba más popular para evaluar normalidad y cómo se interpreta? Menciona una prueba para evaluar homoscedasticidad y cómo es interpretada. 4.5.2 Coeficiente de determinación El \\(R^2\\) o coeficiente de determinación es una medida de bondad de ajuste, es decir, un número de nos indica qué tanto nuestro modelo es capaz de explicar la variable de interés. El \\(R^2\\) es la proporción de la variabilidad de \\(y\\) que es explicada por \\(x\\). Valores más cercanos a 1 significan que una mayor parte de la variabilidad de \\(y\\) es explicada por \\(x\\). Solicita de nuevo un resumen del objeto modelo_simple e identifica la sección Multiple R-squared: summary(modelo_simple) ## ## Call: ## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.4391 -2.7744 -0.6709 1.5324 14.9969 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -19.86224 5.80867 -3.419 0.00102 ** ## ss 0.39490 0.08516 4.637 1.49e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.572 on 74 degrees of freedom ## Multiple R-squared: 0.2252, Adjusted R-squared: 0.2147 ## F-statistic: 21.5 on 1 and 74 DF, p-value: 1.488e-05 En nuestro caso, hemos obtenido un \\(R^2\\) de 0.2252, lo que puede ser interpretado de la siguiente manera: poco más de la quinta parte de la variabilidad de los casos positivos por cada 1 mil habitantes es explicada por el porcentaje de población con acceso a servicios de salud. 4.5.3 Una aproximación a la predicción A través de nuestro modelo estimado (también llamado ajustado) es posible hacer predicciones de \\(y\\), es decir, de los casos positivos por cada 1 mil habitantes, siempre que los valores de la variable predictora estén dentro del rango original. Ejercicio ¿Cuál es el rango de la variable predictora, es decir de \\(x\\)? Hagamos una predicción de los casos positivos cuando \\(x\\) toma el valor de 60%: \\[\\hat {poshab}=-19.8622+0.3949(60)\\] Con independencia de la consistencia y validez de nuestro modelo, situación que de momento no estamos evaluando, ¿cuál es el valor promedio de los casos positivos de COVID19 por cada 1 mil habitantes cuando el porcentaje de población con acceso a servicios de salud es de 60%? Bastará resolver la expresión anterior para responder a la pregunta: 3.83 casos por cada 1 mil habitantes, aproximadamente; es decir, si un municipio tiene un porcentaje de población con acceso a servicios de salud de 60%, nuestro modelo estima un total de 3.83 casos positivos de COVID19. Alternativamente, en R contamos con la función predict() para llevar a cabo, justamente, predicciones. La función requiere de dos argumentos forzosos: i) el modelo usado para la predicción (argumento object=) y ii) los valores de la regresión (argumento newdata=). Así: stats::predict(object = modelo_simple, newdata=data.frame(ss=60)) ## 1 ## 3.831836 En este caso, estamos indicando los nuevos datos de forma manual para la variable de interés, ss. 4.5.4 Significancia individual de los coeficientes obtenidos Uno de los aspectos más importantes del análisis de regresión es la interpretación de los coeficientes obtenidos. Será de particular interés para nosotros conocer, de cada coeficiente: i) el sentido de su asociación con la variable dependiente, es decir, el signo del coeficiente; ii) la magnitud de la asociación lineal, qué tan grande es dicho coeficiente en términos de las unidades de la variable y iii) su significancia estadística, es decir, si hay evidencia de que el verdadero valor del estimador se corresponde con el estimado. Los dos primeros elementos, sentido y magnitud, ya fueron comentados con anterioridad y pueden ser conocidos llamando al objeto que contiene los resultados de nuestro modelo: modelo_simple ## ## Call: ## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm) ## ## Coefficients: ## (Intercept) ss ## -19.8622 0.3949 R despliega en tu consola el valor de los dos coeficientes estimados, en este caso \\(\\hat \\beta_0\\) es negativo y \\(\\hat \\beta_1\\) es positivo. El punto iii consiste en verificar si, en términos estadísticos, nuestros coeficientes estimados son iguales o no a determinado valor, concretamente, buscamos llevar a cabo una prueba de hipótesis sobre dichos coeficientes. La prueba de hipótesis se refiere a la evaluación de si el valor de un estadístico, un valor observado u obtenido a través de un cálculo, se acerca lo suficiente a un valor hipotético. Por ejemplo, algún estudio puede sugerir que la relación entre la educación formal y el ingreso personal es de 0.25, es decir, que a medida que crece un año la educación formal el ingreso personal se incrementa en 0.25 unidades, a este valor llamémoslo \\(\\nu\\). Quiza en un estudio más reciente se encontró que \\(\\nu\\) es igual a 0.19, ¿está este último valor lo suficientemente cerca del 0.25 original para decir que no se rechaza el planteamiento original? Siguiendo a Gujarati y Porter (2011: 113), “la hipótesis planteada se conoce como hipótesis nula, y se denota con el símbolo \\(Ho\\). La hipótesis nula suele probarse frente a una hipótesis alternativa (también conocida como hipótesis mantenida) denotada con \\(Ha\\)”. Así, con relación al ajemplo anterior, la \\(Ho: \\nu=0.19\\) y la \\(Ha: \\nu \\neq 0.19\\). Sinteticemos el procedimiento de prueba de hipótesis en tres pasos y desde una perspectiva netamente práctica y simplificada, desde el ángulo de la prueba de significancia: Definir el juego de hipótesis a verificar, la hipótesis nula (Ho) y la hipótesis alternativa (Ha). Definir un nivel de significancia, conocido como \\(\\alpha\\), valor con base en el cual tomarás una decisión en torno a las hipótesis propuestas. Además, recuerda que (1-\\(\\alpha\\)) define el nivel de confianza con el que la decisión sobre el juego de hipótesis a verificar es tomada. Comparar el valor p asociado a cada coeficiente estimado con el nivel de \\(\\alpha\\) elegido y decidir con arreglo a los siguientes criterios: a. Si el valor-p &lt; \\(\\alpha\\): se rechaza Ho en favor de Ha. b. Si el valor-p &gt; \\(\\alpha\\): no se rechaza Ho. Así pues, el juego de hipótesis sobre la significancia individual de los estimadores calculados puede ser apuntado como: \\[H_o:\\hat\\beta_i=0 \\\\ H_a:\\hat\\beta_i\\not=0 \\] Ahora bien, \\(\\alpha\\) usualmente toma valores de 0.1, 0.05 y 0.01, lo que equivale a niveles de confianza de 0.9, 0.95 y 0.99, es decir, de 90%, 95% y 99%, respectivamente; en tanto, el valor-p para tomar la decisión lo tomamos del resumen del objeto modelo_simple. Para observar los valores-p de cada uno de los coeficientes estimados solicitamos un resumen del objeto modelo_simple con la función summary(): base::summary(modelo_simple) ## ## Call: ## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.4391 -2.7744 -0.6709 1.5324 14.9969 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -19.86224 5.80867 -3.419 0.00102 ** ## ss 0.39490 0.08516 4.637 1.49e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.572 on 74 degrees of freedom ## Multiple R-squared: 0.2252, Adjusted R-squared: 0.2147 ## F-statistic: 21.5 on 1 and 74 DF, p-value: 1.488e-05 Observa con cuidado los resultados que ahora se presentan como una tabla en su consola. Identifica las columnas estimate y Pr(&gt;|t|): en esta última aparece el valor-p y es el elemento con base en el cual tomaremos la decisión sobre la significancia estadística de los coeficientes obtenidos. Veamos los valores de los coeficientes estimados: el intercepto, \\(\\hat \\beta_0\\) es igual a -19.8 y el coeficiente vinculado a ss, \\(\\hat \\beta_1\\), tiene un valor de 0.394. Ahora bien, recuerda el juego de hipótesis a verificar: \\(H_o:\\hat\\beta_i=0\\) y \\(H_a:\\hat\\beta_i\\not=0\\) (paso 1), elijamos un nivel de significancia, \\(\\alpha\\), de 0.05 (paso 2) y comparemos el valor p asociado a cada coeficiente con \\(\\alpha\\) (paso 3). El siguiente cuadro sintentiza lo anterior: Variable Coeficiente Valor p \\(\\alpha\\) Decisión intercepto -19.86224 0.00102 0.05 Rechazo Ho en favor de Ha ss 0.39490 1.49e-05 0.05 Rechazo Ho en favor de Ha Así, decimos que para el caso del coeficiente estimado de ss hay evidencia suficiente para afirmar que su verdadero valor es diferente de cero, por lo que decimos que es estadísticamente significativo, o lo que es lo mismo, hay evidencia suficiente con un nivel de confianza de 95% para sostener que entre el número de casos positivos por cada 1 mil habitantes y el porcentaje de población con acceso afiliación a servicios de salud presentan una asociación lineal. 4.6 Algunos elementos de cuidado con las regresiones lineales Antes de concluir esta sección, es importante tener en mente los siguientes puntos cuando usemos regresiones: Los modelos de regresión son útiles para llevar a cabo intrapolación, es decir, para obtener valores en el rango de nuestras variables explicativas. No obstante, deben ser usados con cuidado cuando lo que se pretende es extrapolar datos, es decir, conocer los valores de \\(y\\) que no están en el rango original de \\(x\\). Como el ajuste de mínimos cuadrados depende en buena medida de los valores de \\(x\\) y con este método cada punto \\(x\\) tiene la misma ponderación, la pendiente de la regresión está mayormente influenciada por los valores más extremos de \\(x\\). Cuando se tienen valores inusuales se podría proceder a eliminarlos, o bien, recurrir a técnicas diferentes a los mínimos cuadrados ordinarios que sean menos sensibles a estos puntos. Que la regresión tuviera como resultado que dos variables están asociadas, no significa que entre ellas haya una relación causal, tal como se indicó al inicio de este capítulo. Causalidad implica asociación, pero lo opuesto no es necesariamente cierto. 4.7 Regresión lineal múltiple: una propuesta de modelo La regresión lineal múltiple es, en esencia, una extensión lógica de todos los elementos descritos antes: “Un modelo de regresión que involucra más de una variable regresora se llama modelo de regresión múltiple” (Montgomery, Peck., and Vining. 2012, 81:67). Así, por ejemplo, un modelo con dos regresoras, \\(x_1\\) y \\(x_2\\) luciría como: \\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+u\\] Esta expresión que incluye tres variables, dos independientes y una dependiente, aún es posible representarse en espacio tridimensional con los ejes \\(y\\), \\(x_1\\) y \\(x_2\\). El parámetro \\(\\beta_0\\) es el intercepto del plano de la regresión que, cuando el origen está incluido en el rango de \\(x_1\\) y \\(x_2\\), \\(\\beta_0\\) indica la media de \\(y\\) cuando \\(x_1=x_2=0\\). En tanto, \\(\\beta_1\\) es el cambio esperado \\(y\\) cuando \\(x_1\\) varia en una unidad, siempre que \\(x_2\\) permanezca constante, análogamente para \\(\\beta_2\\). ¿Qué variables de nuestra base incluirías en un modelo que incluya dos regresoras? Ejercicio ¿Recuerdas cómo construir una matriz de diagramas de dispersión a través del paquete GGally? Explora el conjunto de variables de la base covid_zmvm y construye una matriz de diagramas de dispersión solo con 4 o 5 variables que presenten el mayor nivel de correlación tanto con los casos positivos como con las defunciones por cada 1 mil habitantes, pos_hab y def_hab. Un modelo con buen nivel de ajuste debería procurar que las variables explicativas propuestas presenten una alta correlación con la variable a explicar. Como podrás haberte dado cuenta, las variables que presentan el mayor nivel de asociación lineal negativa son: ppob_basi, pocom, occu, sbasc, ppob_5_o_m. En tanto, las variables que muestran el mayor nivel de asociación lineal positiva son: poss, grad_m, ppob_sup, grad, grad_h, tmss y rmss. Otro principio recomendable a la hora de proponer un modelo, amén de su consistencia teórica, es su simpleza: ¿qué relevancia tendrá incluir en un modelo el grado promedio de escolaridad total, el de hombres y el de mujeres por separado? Bastaría incluir solo una de ellas. Por otro lado, ¿qué tan oportuno sería incluir simultáneamente variables que buscan recoger el mismo aspecto de la realidad? Esto es así porque tanto las variables grad, ppob_basi y sbasc refieren el nivel de educación formal de la población, así, bastaría usar alternativamente alguna de ellas, algo parecido pasa con occu y ppob_5_o_m que expresan las condiciones de vivienda y habitación de las personas. En tanto, parece que de los grandes sectores: industria, comercio y servicios, las variables de éste último son las que muestran un mayor nivel de asociación lineal: poss, tmss y rmss, ¿será relevante incluir todas en el modelo? Con base en lo dicho antes, propongamos un primer modelo, lo más sencillo posible: \\[poshab_i=\\beta_0+\\beta_1grad_i+\\beta_2occu_i+\\beta_3poss_i+\\beta_4pocom_i+\\epsilon_i\\] De nuevo, a través de la función lm() estimaremos este modelo y al resultado lo guardaremos en un nuevo objeto llamado modelo_multi modelo_multi &lt;- stats::lm (formula=pos_hab ~ grad+occu+poss+pocom, data = covid_zmvm) summary(modelo_multi) ## ## Call: ## stats::lm(formula = pos_hab ~ grad + occu + poss + pocom, data = covid_zmvm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.0017 -2.4621 -0.7526 1.2428 16.6659 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.43576 15.07149 0.095 0.924374 ## grad 0.09555 1.01112 0.094 0.924981 ## occu -4.25136 7.65251 -0.556 0.580264 ## poss 21.80677 5.91896 3.684 0.000445 *** ## pocom 0.07430 6.71860 0.011 0.991208 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.843 on 71 degrees of freedom ## Multiple R-squared: 0.4748, Adjusted R-squared: 0.4452 ## F-statistic: 16.04 on 4 and 71 DF, p-value: 2.112e-09 Ejercicio ¿Qué pasa con las variables propuestas en este modelo? ¿Cuáles resultaron estadísticamente significativas y por qué? Aquí hemos expuesto una versión simplificada en extremo de cómo tomar decisiones a través de las pruebas de hipótesis, puedes remitirte a cualquier libro de econometría y responder, ¿de qué modo la prueba t (t value, penúltima columna del cuadro de resumen) nos permitiría llegar a las mismas conclusiones? 4.7.1 Significancia conjunta de los coeficientes calculados Seguramente notaste que cuando se llamó el objeto modelo_multi a tu consola aparecieron otros tantos elementos ,además de los coeficientes, entre ellos uno llamado F-statistic , el estadístico F. En este contexto es utilizado para evaluar si la particular combinación de variables propuesta contribuye o no a explicar la variable de interés. En términos concretos, la prueba F y el valor p asociado a ella, permiten evaluar el siguiente juego de hipótesis: \\[ \\begin{aligned} Ho&amp;: \\hat\\beta_1=\\hat\\beta_2=...=\\hat\\beta_i=0 \\\\ Ha&amp;: al.menos.una. \\hat\\beta_i\\not=0 \\\\ \\end{aligned} \\] es decir, al menos uno de los coeficientes estimados, en su particular combinación, es diferente de cero. Así, con un valor p de casi cero (2.112e-09), resulta claro que se rechaza Ho en favor de Ha, es decir, que al menos uno de los coeficientes estimados es diferente de 0 en el modelo propuesto y que esta particular combinación de variables explicativas tiene algo interesante para decirnos. 4.8 Alcance de la regresión y en análisis causal La primera parte de este capítulo llamó la atención sobre el llamado análisis causal, por lo que dedicaremos unas cuantas palabras para cerrar la discusión con la que comenzamos. La realidad social es compleja y si bien se han extendido las técnicas experimentales, incluso en disciplinas como la economía, la mayor parte de la información con la que cotidianamente trabaja un científico social son datos observados, no experimentales. Esto implica que el uso de técnicas de análisis de información como la regresión en ciencias sociales tienen un alcance limitado en términos del análisis de causalidad; no obstante, el análisis de regresión sí nos permite estudiar rigurosamente la correlación entre variables. Que exista correlación entre dos variables significa que sus movimientos están asociados, por ejemplo, que cuando el ingreso personal es alto, se consume más en servicios culturales, pero eso no implica que una elevación del ingreso cause el incremento en el consumo de los servicios culturales. En este sentido, lo que permite anclar la correlación identificada con la técnica a una línea de causalidad es la reflexión teórica, es decir, un estudio cuidadoso de las teóricas propuestas para la explicación de un fenómeno. Por eso, como se dijo en la primera parte de este capítulo, el análisis de regresión debe ser tomado con cautela. Dejemos hasta aquí este somero repaso de los aspectos más elementales del análisis de regresión. Es primordial que por tu propia cuenta estudies los materiales sugeridos, pues este capítulo en modo alguno suple un curso formal de los tópicos básicos de econometría. En el siguiente capítulo, retomamos el hilo del tratamiento de la información espacial a través de su incorporación a los modelos econométricos. References "],["análisis-espacial-ii-modelos-econométricos-espaciales.html", "5 Análisis espacial II: modelos econométricos espaciales 5.1 Econometría espacial 5.2 Los modelos econométricos de error espacial y rezago espacial 5.3 Modelos espaciales en R: un ejemplo para el Valle de México 5.4 Reflexiones finales", " 5 Análisis espacial II: modelos econométricos espaciales En el capítulo 3 nos hemos referido a la necesidad de identificar la presencia de autocorrelación espacial en el conjunto de información utilizado. Mencionamos dos razones por las que conviene saber si la información usada presenta este rasgo: una de carácter técnico y otra que llamamos sustantiva. En términos de la segunda, la presencia de autocorrelación espacial significa que el fenómeno de interés no se distribuye de manera aleatoria en el espacio por lo que algo está pasando y debe ser investigado, ¿por qué la variable exhibe patrones en su distribución en el espacio? Por otro lado, en términos de la razón de carácter técnico, que no está desvinculada necesariamente de la razón previa, al estimar un modelo clásico de regresión lineal usando datos espaciales a través del método de Mínimos Cuadrados Ordinarios seguramente se violaría uno de sus supuestos, específicamente, aquel que tiene que ver con la independencia de los términos de error. Por tanto, por razones de la técnica utilizada y del interés de la investigadora, es necesario recurrir a un modelo econométrico espacial. Al hacerlo, se puede resolver el problema de la violación del citado supuesto y a la vez brindar elementos para buscar las causas que explican la formación de patrones en el espacio. En este capítulo nos dedicamos a mostrar dos de las alternativas de modelos econométricos espaciales, es decir, modelos econométricos que incorporan diversos tipos de interacciones que se suceden en el espacio. Es crucial que, antes de abordar este capítulo, hayas asimilado los contenidos del capítulo 3 y 4, en la medida en que lo expuesto aquí se basa en los conocimientos previos. 5.1 Econometría espacial La econometría espacial puede ser entendida como una rama de la econometría que incluye “…aquellos métodos y técnicas que, sustentados en una representación formal de la estructura de la dependencia y heterogeneidad espacial, provee el medio para llevar a cabo la adecuada especificación, estimación, prueba de hipótesis y predicción para modelos en la ciencia regional” (Anselin, 1988, citado por Quintana y Mendoza (2016: 356). Desde una perspectiva teórica, la econometría espacial se ha desarrollado para resolver dos problemas que presenta la modelación econométrica con datos espaciales, relacionados con la violación de los supuestos del modelo clásico de regresión lineal que se expusieron en el capítulo previo: la heteroscedasticiadad (varianza no constante de los errores) y la autocorrelación (la dependencia entre los términos de error). No obstante, la econometría espacial se ha ocupado fundamentalmente del segundo problema, puesto que la heteroscedasticidad ha podido ser abordada en el marco de la econometría convencional. No existe una única manera de incorporar en un modelo econométrico los efectos de interacción que se dan en el espacio y, por tanto, existe una amplia variedad de modelos econométricos espaciales, dependiendo del tipo de interacciones que en cada uno se incorpore. 5.1.1 Diversas alternativas de modelos econométricos Existen múltiples modelos econométricos espaciales que buscan resolver el problema de la autocorrelación espacial. En todos los casos se trata de captar las interacciones que se dan en el espacio entre las variables consideradas. Estas interacciones son de diferente naturaleza, las puede haber en la varible dependiente y entonces se las denomina interacciones endógenas, las puede haber en la o las variables independientes y entonces se les llama interacciones exógenas, o bien, se pueden incorporar a través del término de error de la regresión. El modelo que incorpora todas las posibles interacciones espaciales recibe el nombre de Modelo General Anidado (General Nesting Spatial Model (Elhorst 2006, 7) y puede ser escrito, en términos matriciales, como: \\[ \\begin{aligned} Y &amp;= \\rho WY +\\alpha1_N+X\\beta+WX\\theta+u \\\\ u &amp;=\\lambda Wu+\\epsilon \\\\ \\end{aligned} \\] Siguiendo a Elhorst (2006: 10), se le denomina modelo general justamente por el hecho de que incorporara todos los tipos de efectos espaciales posibles. Veamos con cuidado la expresión anterior para distinguir cada uno de sus términos y ver con claridad los diferentes tipos de efectos espaciales. El término identificado como \\(Y\\) no es otro que el vector que contiene la variable dependiente de nuestro modelo, por ejemplo, el número de casos positivos por COVID19; \\(X\\) corresponde a la matriz de variables independientes que pueden ser, por ejemplo, el acceso a los servicios de salud y la proporción promedio de ocupantes por vivienda; el término \\(\\beta\\) corresponde al conjunto de parámetros a ser estimados, además, seguro que te resulta familiar el término \\(W\\) que no es otra cosa que la estructura espacial definida a partir de la matriz de pesos espaciales; finalmente, listemos los diferentes efectos espaciales que aparecen en la expresión: Efectos de interacción endógenos, es decir, cuando la variable dependiente reaparece como independiente pero a través de su rezago espacial (\\(WY\\)), aquí el sentido y magnitud de la interacción está dado por el parámetro \\(\\rho\\), llamado coeficiente autoregresivo espacial; Efectos de interacción exógenos, es decir, cuando las variables independientes son incluidas en forma de sus rezagos espaciales (\\(WX\\)) en la que la relación es captada por \\(\\theta\\), también un parámetro o parámetros a estimar; Efectos de interacción dados por los términos de error de la regresión (\\(Wu\\)) y de ellos interesa el sentido y magnitud de \\(\\lambda\\), el coeficiente a estimar, que es llamado coeficiente de autocorrelación espacial. A partir del modelo general es posible derivar otros, más simples, si se suponen determinados valores para los coeficientes \\(\\rho\\), \\(\\theta\\) y \\(\\lambda\\). Toda la constelación de modelos econométricos espaciales aparece en la figura 5.1 (Elhorst 2006, 7). Por ejemplo, si partiendo del modelo general se asume que \\(\\lambda=0\\), se obtiene el denominado modelo espacial de Durbin; en tanto si \\(\\rho=\\theta=\\lambda=0\\), llegamos al conocido modelo clásico de regresión lineal (MCO). Observa con cuidado cada una de las diferentes alternativas de modelación econométrica espacial y trata de reflexionar sobre sus diferencias. Figura 5.1: Clasificación de modelos espaciales, Elhorst (2006) Cada uno de los modelos econométricos espaciales de la figura buscan responder diferentes preguntas en relación a cómo la variable de interés, nuestra variable dependiente, se ve afectada por las interacciones espaciales que hemos mencionado. Por ejemplo, quizá como parte de la exploración de la pandemia por COVID19 nos dimos cuenta de que el número de casos positivos por este mal está asociado con el acceso a los servicios de salud en los municipios vecinos, es decir, el rezago espacial de la población con acceso a servicios de salud, tal como se aprecia en el siguiente diagrama de dispersión: ## `geom_smooth()` using formula = &#39;y ~ x&#39; Como puedes observar, el diagrama de dispersión muestra que existe asociación lineal positiva entre el valor promedio de la población con acceso a servicios de salud en los municipios vecinos (el rezago espacial en el eje X) y el número de casos positivos por COVID19 (en el eje Y), por tanto, podría ser pertinente proponer un modelo que incluya dicho efecto espacial, es decir, un efecto espacial dado por la variable independiente (efecto espacial exógeno). Ejercicio ¿Cuál es el nombre de los modelos econométricos espaciales que incorporan dicho efecto? Una relación como la anterior quizá pueda ser modelada a través de un modelo espacial de Durbin, \\(Y=\\rho WY+\\alpha 1n+X\\beta+WX \\theta+\\epsilon\\), puesto que a la investigadora pudiera interesarle cómo los valores de las variables independientes en los espacios vecinos inciden en el fenómeno de interés. Así pues, la elección del tipo de modelo espacial depende, en lo fundamental, de dos elementos: i) la evidencia de asociación espacial entre el conjunto de variables disponibles y que se revela a través del análisis exploratorio de datos espaciales (definición de criterios de vecindad, construcción de índices de Moran) y ii) los intereses y propósitos de la investigación. 5.2 Los modelos econométricos de error espacial y rezago espacial De toda la gama de modelos econométricos espaciales que es posible construir, aquí sólo nos ocuparemos de dos de ellos, el modelo de rezago espacial y el modelo de error espacial. Para ello, seguimos la ruta metodológica propuesta por Anselin y Rey (2014) que consiste en comenzar con un modelo lineal no espacial estimado con mínimos cuadrados ordinarios (MCO) que luego es extendido con la incorporación de interacciones espaciales, ya sea incluyendo a \\(\\rho\\), el coeficiente autoregresivo espacial o a \\(\\lambda\\), el término de autocorrelación espacial, tal y como se muestra en la figura 5.2. Figura 5.2: De un modelo con MCO a un modelo espacial 5.2.1 Secuencia general para la estimación La estimación de un modelo econométrico de error o rezago espacial, siguiendo a Anselin y Rey (2014), puede ser dividida en los siguientes pasos: Estimar un modelo clásico de regresión lineal con mínimos cuadrados ordinarios. Evaluar la presencia de autocorrelación espacial en los errores del modelo. Evaluar, alternativamente, la pertinencia de un modelo de rezago o de error espacial. Estimar el modelo de rezago y de error espacial y apuntar cuál de ellos capta mejor la relación propuesta. El punto i ha sido abordado en el capítulo previo, sin embargo regresaremos a ello más adelante de forma práctica para describir la secuencia general. Sobre el punto ii nos detendremos un momento para explicarlo con más detalle. Bien podrías pensar que la evaluación de la autocorrelación ha quedado plenamente cubierta en el capítulo 3 cuando nos referimos al análisis exploratorio de datos espaciales y usamos la I de Moran; sin embargo, es necesario distinguir entre autocorrelación en la variable de interés y autocorrelación en los errores del modelo, lo primero lo hicimos en el citado capítulo, mientras que lo segundo corresponde a la secuencia para la selección del modelo econométrico espacial. Si se ha encontrado evidencia de autocorrelación en la variable a través de los instrumentos de exploración, como los diversos mapas y la I de Moran, estos elementos nos llevan a pesar que al usar dicha variable en un modelo econométrico lineal simple estimado con MCO, muy probablemente los términos de error estén correlacionados y, por tanto, se violaría dicho supuesto. Para verificarlo, ahora estimaremos la I de Moran sobre los errores del modelo: en esto justamente consiste el paso ii. Ahora bien, si hallamos evidencia de que los errores del modelo lineal están autocorrelacionados, sabremos que lo mejor es estimar un modelo econométrico espacial. Pero, ¿cuál es mejor, el modelo de error espacial o el de rezago? Para ello recurriremos a un estadístico denominado Prueba de los Multiplicadores de Lagrange o Prueba de Puntaje de Rao (Rao score test), que fue desarrollada por Anselin a finales de la década de los ochenta del siglo pasado (Anselin and Rey 2014: 105). Las pruebas nos permitirán decidir qué modelo es mejor, evaluando entre el modelo lineal y el de rezago, por un lado, y el modelo lineal y el de error, por otro. Así, cuando se evalúa entre el modelo lineal y el modelo de rezago lo que en el fondo hacemos es probar estadísticamente si \\(\\rho\\), el coeficiente autoregresivo espacial, realmente está presente en la relación propuesta. Recordemos la estructura del modelo de rezago: \\[ y=\\rho Wy+X\\beta+u \\] De modo que, formalmente, el juego de hipótesis a evaluar y que nos permitirán decidir entre el modelo lineal y el modelo de rezago es: \\[ \\begin {aligned} Ho &amp;: \\rho=0 \\\\ Ha &amp;: \\rho\\not=0 \\\\ \\end {aligned} \\] Según lo que nos indique el multiplicador de Lagrange y su valor-p asociado, a determinado nivel de significancia, rechazar la hipótesis nula significará que, entre el modelo lineal y el modelo de rezago, el mejor modelo es un modelo de rezago espacial. En esta misma línea de razonamiento, del modelo de error nos interesa saber si \\(\\lambda\\) es diferente de 0 y, por tanto, si juega algún papel relevante en la relación propuesta. Recuerda que el modelo de error luce como: \\[ \\begin{aligned} Y &amp;= \\alpha1_N+X\\beta+u \\\\ u &amp;=\\lambda Wu+\\epsilon \\\\ \\end{aligned} \\] Formalmente, las hipótesis a verificar son: \\[ \\begin {aligned} Ho &amp;: \\lambda=0 \\\\ Ha &amp;: \\lambda\\not=0 \\\\ \\end {aligned} \\] Igualmente, el estadístico usado para evaluar nuestras hipótesis es el multiplicador de Lagrange y que, dado determinado nivel de significancia, rechazar la hipótesis nula significa que, entre el modelo lineal y el modelo de error, el mejor modelo es uno de error espacial. Pero, ¿qué ocurre cuando en las pruebas anteriores no son concluyentes? Es decir, cuando tanto la prueba sobre la versión de rezago y de error han resultado ambas significativas? Sin abudar en los detalles técnicos del caso, esto podría pasar porque el estadístico del multiplicador de Lagrange sobre el modelo de rezago, \\(LM_{\\rho}\\), es sensible a la presencia autocorrelación espacial en el término de error, lo mismo que el estadístico del multiplicador de Lagrange sobre el modelo de error, \\(LM_{\\lambda}\\) que es sensible a la presencia autocorrelación espacial en el rezago. Para decirlo de forma hipersimplificada, estas pruebas podrían dar un “falso positivo”. Exclusivamente cuando en ambos casos se rechazan las respectivas hipótesis nulas, es decir que \\(\\lambda\\not=0\\) y \\(\\rho\\not=0\\), habremos de recurrir a las llamadas “versiones robustas” del multiplicador de Lagrange. Las versiones robustas de dichos indicadores implican “que el estadístico original es corregido mediante la influencia potencial de ‘otras’ alternativas” (Anselin y Rey, 2014: 105). Usar la versión robusta del multiplicador de Lagrange que evalúa entre la alternativa del modelo lineal y el modelo de rezago implica que dicha prueba determina con mayor precisión si hay presencia de autocorrelación y esta es captada a través del coeficiente autoregresivo espacial, excluyendo los efectos que podrían estar dados por el coeficiente de autocorrelación espacial; reciprocamente para la prueba en la versión robusta del multiplicador de Lagrange sobre la alternativa del modelo lineal respecto al modelo de error. Usemos una analogía para mejor comprender lo anterior. Imagina que intentas pasar por un colador un poco de harina que tiene algunos terrones porque ha estado algún tiempo en la despensa. Si los orificios del colador son demasiado grandes dejarán pasar los terrones y pensaríamos que nuestra harina estaba libre de terrones. No obstante, si usamos un colador con orificios más finos, los terrones no pasarán y sabremos que la harina, en efecto, tenía terrones. Algo semejante ocurre con los multiplicadores de Lagrange en sus versiones estándar y robustas: los orificios del colador en las pruebas robustas son más finos, permitiéndonos discriminar con mayor seguridad entre uno y otro modelo, para tener evidencia concluyente. Así, por ejemplo, si el multiplicador de Lagrange en la versión robusta que evalúa entre un modelo lineal y un modelo de rezago sostiene que el mejor modelo es el modelo de rezago, mientras que el multipilcador de Lagrange en su versión robusta que evalúa entre un modelo lineal y un modelo de rezago sostiene que el mejor modelo es el modelo lineal, la conclusión a la que arribamos, a la luz ofrecida por ambas pruebas, es que el mejor modelo es el modelo de rezago espacial. Para simplificar, basta recordar lo recomendado por Anselin (2005, 197): “lo importante a recordar (en la selección entre modelos espaciales) es considerar las versiones robustas de los estadísticos (los multiplicadores de Lagrange) sólo cuando las versiones estándar son ambas significativas. Finalmente, ya que tenemos claro qué modelo capta mejor la relación propuesta, procederemos a estimar y analizar dicho modelo. Todos los pasos que hemos tratado de describir hasta aquí, son sintetizados por Anselin y Rey (2014) en el siguiente esquema: Figura 5.3: Árbol de decisión de regresión espacial, Anselin y Rey (2014) En las siguientes sección se ilustra cómo seguir esta hoja de ruta para estimar los dos modelos que incluyen efectos espaciales: el de error y el de rezago y decidir cuál es el mejor. 5.3 Modelos espaciales en R: un ejemplo para el Valle de México En esta sección exponemos cómo, a través de algunos paquetes y funciones en R, podemos recorrer el camino propuesto en la sección anterior, que nos llevará a decidir cuál es el mejor modelo que capta las interacciones que se dan en el espacio, uno de error o de rezago. No obstante, antes e ello, aprovecharemos para hacer un brevísimo recordatorio de algunas de las herramientas mostradas en los capítulos previos, a fin de mostrar su uso de forma integrada en el contexto del planteamiento de un modelo econométrico espacial. 5.3.1 Recapitulación: la necesidad de un modelo espacial Antes de emprender la tarea de la estimación de los modelos espaciales siguiendo la secuencia anterior, llevaremos a cabo un recuento de diversos elementos que nos permitirán poner en contexto el problema de la selección del modelo espacial y, al mismo tiempo, recapitular algunos de los tópicos tratados a lo largo de este libro. Hemos estado analizando un conjunto de variables sociodemográficas y económicas con la intensión de indagar cuál es su relación con la situación de la pandemia por COVID19 durante la primera ola en una región del centro de México llamada Zona Metropolitana del Valle de México. En el primer capítulo de este material expusimos una serie de herramientas para tener una primera aproximación con el conjunto de datos. Esas herramientas, entre otros aspectos, integran un enfoque denominado Análisis Exploratorio de Datos (EDA) y tiene por objeto, antes de proponer cualquier modelo, conocer la estructura subyacente de la información e indagar en la asociación entre variables para responder y plantear algunas preguntas de carácter preliminar en torno al objeto de estudio construido. Por ejemplo, a través de un diagrama de caja del número de casos positivos es posibleidentificar observaciones a típicas, es decir, valores extremadamente altos o bajos: Los puntos en color rojo son observaciones extremadamente altas, en el rango de la información utilizada. Del mismo modo, una gráfica de la densidad de probabilidad de la variable nos indica cómo ésta se distribuye: La “cola” de esta gráfica que se extiende a la derecha (sesgo positivo) indica, como la anterior, que hay valores muy altos. Esto nos lleva a pensar en las razones de este comportamiento, ¿por qué hay unidades territoriales, municipios en este caso, que presentan tal magnitud de casos? El EDA, justamente, busca despertar nuestra curiosidad e imaginación en torno al fenómeno analizado. Otro tipo de pregunta que nos permite responder de forma preliminar el EDA es, por ejemplo, la siguiente: ¿existe asociación entre el número de casos positivos por COVID19 y el acceso a los servicios de salud? Una manera sencilla de acercarse a la respuesta de tal pregunta es a través de un diagrama de dispersión: ## `geom_smooth()` using formula = &#39;y ~ x&#39; El gráfico anterior indica que, en efecto, parece haber una relación lineal positiva entre ambas variables, ¿por qué crees que pueda existir esta relación, qué la fundamenta? Habrá pues que indagar y reflexionar sobre las posibles explicaciones que se hayan dado a una relación como la identificada. Por otro lado, el interés central de este libro ha sido brindar algunas herramientas básicas para el tratamiento de información espacial: el espacio importa, dónde ocurren los fenómenos y cómo se comportan o distribuyen en el territorialmente es un elemento clave para comprender tales fenómenos cabalmente. Por ejemplo, ¿hay algún patrón en cómo se dieron los casos positivos por COVID19 en el Valle de México? Para tener una primera idea de esto, podemos construir un mapa, como los expuestos en el capítulo 2, cuando tratamos con los mapas de coropletas: ## Reading layer `covid_zmvm&#39; from data source ## `C:\\repos\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 76 features and 57 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5 ## Projected CRS: Lambert_Conformal_Conic Es valido sostener, al menos inicialmente que, en efecto, hay una concentración de los casos positivos en la zona sur del área estudiada, la Ciudad de México, donde se muestran los colores en azul más intenso. Esta concentración identificada visualmente, nos lleva a pensar en la necesidad de evaluar formalmente si los datos sobre los casos positivos por COVID19 muestran patrones sistemáticos de agrupamiento en el territorio. Para ello, podemos recurrir a la prueba de autocorrelación espacial que vimos en el capítulo 3, la I de Moran. Como recordarás del capítulo 3, para construir el estadístico de asociación espacial de Moran hay que definir una estructura espacial, es decir, definir entre qué unidades espaciales hay una relación de vecindad, misma que es recogida en la llamada matriz de pesos espaciales. Con una relación de vecindad de tipo reina, la I de Moran y el diagrama de Moran sobre el número de casos positivos luce como: ## ## Moran I test under randomisation ## ## data: covid_zmvm$pos_hab ## weights: mTRUE.pesos ## ## Moran I statistic standard deviate = 8.8625, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.656334129 -0.013333333 0.005709563 La información que nos proporciona la I de Moran indica que los casos positivos por COVID19 en el Valle de México muestran un patrón de agrupamiento sistemático en las unidades territoriales analizadas: el coeficiente es positivo, relativamente alto y estadisticamente significativo. Así pues, a un nivel aún exploratorio, tanto el mapa como la I de Moran dan cuenta de que la variable a explicar presenta autocorrelación espacial. Hay varios elementos interesantes respecto a la evidencia anterior. Primero, si estimamos un modelo econométrico lineal buscando explicar los casos positivos, lo más probable es que se violaría el supuesto de independencia en los términos de error de la regresión que exige un modelo lineal clásico, por lo no sería el más apropiado para explicar dicho fenómeno. Segundo, y quizá lo más interesante, es que debemos reflexionar sobre el hecho de que la variable a explicar presenta patrones de asociación en el espacio: ¿por qué los casos positivos de COVID19 son más altos en algunos grupos de municipios de la ciudad que en otros? La respuesta a esta pregunta es compleja y, desde luego, requiere una reflexión cuidadosa sobre las teorías y trabajos previos de especialistas en el tema que han tratado sobre los patrones espaciales en la transmisión de una enfermedad, como el clásico trabajo de Cliff y sus colaboradores (Cliff et al. 1981) sobre la dinámica espacial de las pandemias en Islandia, aunque ciertamente se ha aceptado que las epidemias pueden y deben analizarse desde una perspectiva espacial (Rezaeian et al. 2007). El caso es que hay tanto evidencia en los datos, como en trabajos previos que nos llevan a pensar en la pertinencia de un modelo econométrico espacial que incorpore el comportamiento que hemos identificado en un nivel exploratorio: la autocorrelación espacial en los casos positivos por COVID19. Ahora entonces, cuál es el mejor modelo que podemos usar, ¿uno de rezago espacial o de error espacial? Seguiremos pues la secuencia de la sección anterior para decidir, mientras ilustramos el proceso a través de los paquetes y funciones que nos ofrece R. 5.3.2 Estimación de los modelos espaciales Ahora que tenemos claro que requerimos un modelo espacial si lo que prentedemos es explicar los casos positivos por COVID19 en el Valle de México, veamos cómo llevar a cabo esto a través de R. Los paquetes utilizados para la estimación de modelos econométricos espaciales son: spatialreg: un paquete que contiene las funciones para calcular los modelos econométricos espaciales que hemos comentado antes. spdep: es el mismo paquete que usamos en el capítulo 3 y que contiene funciones para evaluar dependencia espacial y construir matrices de pesos espaciales. Como es usual, la instalación desde la consola puede hacerse con: #Instalar dos paquetes install.packages(c(&quot;spdep&quot;, &quot;spatialreg&quot; )) Además, echaremos mano de otros tantos paquetes previamente trabajados, por lo que deberán ser llamados junto con los recién instalados: #Paquetes recién instalados library(spdep) library(spatialreg) #Otros paquetes usados library(rgdal) library(sp) La base de datos a usar en este ejercicio es la misma que hemos usando antes y que puedes descargar aquí. Para cargar nuestra base de datos: covid_zmvm &lt;-rgdal::readOGR(&quot;base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp&quot;) ## Warning: OGR support is provided by the sf and terra packages among others ## Warning: OGR support is provided by the sf and terra packages among others ## Warning: OGR support is provided by the sf and terra packages among others ## Warning: OGR support is provided by the sf and terra packages among others ## Warning: OGR support is provided by the sf and terra packages among others ## Warning: OGR support is provided by the sf and terra packages among others ## Warning: OGR support is provided by the sf and terra packages among others ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\repos\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp&quot;, layer: &quot;covid_zmvm&quot; ## with 76 features ## It has 57 fields De acuerdo a lo dicho en las secciones previas, primero debemos estimar un modelo clásico de regresión lineal con mínimos cuadrados ordinarios. Usaremos el mismo modelo del capítulo sobre regresión lineal en su versión simple: un modelo que busca explicar los casos positivos por COVID-19 por cada 1 mil habitantes (pos_hab) mediante la población con acceso a servicios de salud (ss), la especificación de dicho modelo lineal luce como: \\[ poshab_i=\\beta_0+\\beta_1ss_i+u_i \\] La variable que buscamos explicar con nuestro modelo será casos positivos a COVID19 por cada 1 mil habitantes, pos_hab, explicada a través del porcentaje de población con acceso a a servicios de salud, ss. La estimación del modelo lineal, tal y como se expuso en el capítulo 4, es: modelo_mco &lt;- stats::lm (formula=pos_hab ~ ss, data = covid_zmvm) summary(modelo_mco) ## ## Call: ## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.4391 -2.7744 -0.6709 1.5324 14.9969 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -19.862 5.809 -3.419 0.00102 ** ## ss 39.490 8.516 4.637 1.49e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.572 on 74 degrees of freedom ## Multiple R-squared: 0.2252, Adjusted R-squared: 0.2147 ## F-statistic: 21.5 on 1 and 74 DF, p-value: 1.488e-05 Ejercicio ¿Qué puedes decir sobre la significancia individual del coeficiente estimado? ¿Qué sobre la bondad de ajuste del modelo? Una vez que hemos estimado el modelo lineal a través del MCO, hemos completado el paso i de nuestra secuencia general. Debemos ahora evaluar la presencia de autocorrelación en los térinos de error del modelo, es decir, el paso ii. De nueva cuenta insistimos en la diferencia entre evaluar la autocorrelación sobre la variable de interés (lo que hicimos hace un momento con la I de Moran y su diagrama sobre la variable pos_hab) y evaluar la autocorrelación sobre los errores del modelo, lo que haremos en este momento. Verificamos la presencia de autocorrelación espacial de los errores a través de la I de Moran, lo que nos permitirá responder a la pregunta ¿los errores de nuestro modelo están correlacionados? Para ello, nos servimos de la función moran.test() del paquete spdep. Como recordarás, la función requiere dos argumentos: el vector número a evaluar (en este caso los errores del modelo MCO) y la estructura espacial. En el capítulo relativo al análisis exploratorio de datos espaciales vimos que la definición de una estructura espacial atraviesa por la construcción de una matriz de pesos espaciales, recuerda que a través de la función poly2nb() del paquete spdep construimos nuestra lista de vecinos con base en los criterios de contigüidad de tipo reina (queen=TRUE): mTRUE &lt;- spdep::poly2nb(covid_zmvm) Ahora, el objeto de tipo nb debe ser trasformado a uno que contenga los pesos espaciales, es decir, la matriz ponderada: un objeto de tipo listw: mTRUE.pesos&lt;-spdep::nb2listw(mTRUE) Ahora, alculemos la I de Moran sobre los términos de error: spdep::moran.test(modelo_mco$residuals, mTRUE.pesos) ## ## Moran I test under randomisation ## ## data: modelo_mco$residuals ## weights: mTRUE.pesos ## ## Moran I statistic standard deviate = 6.9825, p-value = 1.45e-12 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.512928287 -0.013333333 0.005680411 Ejercicio Con base en los resultados de la I de Moran, ¿dirías que existe autocorrelación espacial en los errores? Lo usual es que cuando la variable de interés presenta autocorrelación, esta característica termine por afectar los resultados de la estimación por mínimos cuadrados ordinarios, tal y como ha ocurrido en este caso en donde los errores del modelo lineal están correlacionados. Así pues, es necesario, siguiendo la ruta marcada por Anselin y Rey (2014), evaluar, alternativamente, la pertinencia de un modelo de rezago o de error espacial, es decir, el paso iii. Como dijimos antes, esto lo haremos con base en las pruebas sobre las dos alternativas propuestas (modelo de rezago o modelo de error), a través de los multiplicadores de Lagrange, primero observando sus versiones estándar y, sólo si éstas no son concluyentes, después, sus versiones robustas Las pruebas son llamadas con la función lm.LMtests() del paquete spdep que requiere de los siguientes argumentos: el modelo lineal a evaluar (model=), la estructura espacial (listw=) y los pruebas solicitadas (test=): spdep::lm.LMtests(model=modelo_mco, listw=mTRUE.pesos, test=c(&quot;LMlag&quot;,&quot;LMerr&quot;, &quot;RLMlag&quot;,&quot;RLMerr&quot;)) ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm) ## weights: mTRUE.pesos ## ## LMlag = 55.313, df = 1, p-value = 1.028e-13 ## ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm) ## weights: mTRUE.pesos ## ## LMerr = 42.733, df = 1, p-value = 6.275e-11 ## ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm) ## weights: mTRUE.pesos ## ## RLMlag = 13.358, df = 1, p-value = 0.0002573 ## ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm) ## weights: mTRUE.pesos ## ## RLMerr = 0.77786, df = 1, p-value = 0.3778 Revisa la ayuda relacionada con la función lm.LMtests() para familiarizarte con todos sus argumentos. En este caso, solicitamos las 4 pruebas de las que hicimos mención previamente: la prueba del multiplicador de Lagrange sobre la alternativa del modelo de rezago (LMlag), la prueba del multiplicador de Lagrange sobre la alternativa del modelo de error (LMerr), así como sus respectivas versiones robustas (RLMlag,RLMerr). Interpretemos los resultados. Primero, contrastemos las versiones estándar de los multiplicadores, que aparecen el la siguiente tabla: Tabla 1. Multiplicadores de Lagrange, versiones estándar Estadístico valor-p LMlag 55.313 1.028e-13 LMerr 42.733 6.275e-11 Tanto el estadístico vinculado al modelo de rezago, como el del error son estadísticamente significativos, es decir, en cada caso se rechaza la hipótesis nula de que \\(\\rho=0\\) y \\(\\lambda=0\\), respectivamente, por lo que tanto un modelo de rezago como de error son plausibles: no tenemos pues, con las versiones estándar, pruebas concluyentes de qué modelo es mejor. En este caso, hay que mirar los resultados de las versiones robustas para tomar la decisión final: Estadístico valor-p RLMlag 13.358 0.0002573 RLMerr 0.77786 0.3778 En este caso, sólo el estadístico vinculado al modelo de rezago, RLMlag, resulta significativo, es decir, se rechaza la hipótesis nula de que \\(\\rho=0\\); en tanto, para el caso del estadístico vinculado al modelo de error, RLMerr, no podemos rechazar la hipótesis nula de que \\(\\lambda=0\\). A la luz de la evidencia anterior, la mejor alternativa de modelo econométrico espacial es el modelo de rezago, por las siguientes razones: La variable a explicar mostró, a través de mapas de clasificación común y elestadístico de Moran, evidencia de autocorrelación espacial. Los términos de error del modelo lineal mostraban también autocorrelación, por lo que dicho modelo viola, al menos, uno de sus supuestos. Las versiones robustas de los multiplicadores de Lagrange mostraron que, entre un modelo MCO y uno de rezago, el mejor era el de rezago, y que entre un modelo MCO y uno de error es mejor el de MCO, pero por las razones i y ii, y por la versión robusta sobre el modelo de rezago, resulta que el mejor modelo es el de rezago espacial. Revisa de nueva cuenta la figura 5.3 para mejor entender nuestra decisión. Finalmente, los tanto el modelo de rezago como el de error serán estimados, con ello, llegamos al paso iv y habremos terminado la secuencia. 5.3.2.1 Modelo de rezago espacial El modelo de rezago espacial se invoca con al función lagsarlm() del paquete spatialreg en la que la estimación corre a cargo del método de máxima verosimilitud. En la función se especifica el modelo (formula=), la base de datos (data=) y la estructura espacial (listw=). library(spatialreg) rezago &lt;- spatialreg::lagsarlm(formula=pos_hab ~ ss, data=covid_zmvm, listw=mTRUE.pesos) summary(rezago) ## ## Call:spatialreg::lagsarlm(formula = pos_hab ~ ss, data = covid_zmvm, ## listw = mTRUE.pesos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.72501 -1.71081 -0.31629 1.12054 11.64378 ## ## Type: lag ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.8906 3.4221 -2.5980 0.009377 ## ss 14.9811 5.1378 2.9159 0.003547 ## ## Rho: 0.83629, LR test value: 62.655, p-value: 2.4425e-15 ## Asymptotic standard error: 0.056938 ## z-value: 14.688, p-value: &lt; 2.22e-16 ## Wald statistic: 215.73, p-value: &lt; 2.22e-16 ## ## Log likelihood: -191.0071 for lag model ## ML residual variance (sigma squared): 7.1381, (sigma: 2.6717) ## Number of observations: 76 ## Number of parameters estimated: 4 ## AIC: 390.01, (AIC for lm: 450.67) ## LM test for residual autocorrelation ## test value: 0.10973, p-value: 0.74045 De la salida debemos llamar la atención sobre los coeficientes de interés: \\(\\rho\\), el coeficiente autoregresivo espacial, que en este caso es positivo, significativo y alto (0.83). ¿Qué es lo que dicho valor significa? La interpretación que podría darse a este coeficiente es que cuando aumenta en una unidad el número de casos positivos en los municipios y alcaldías vecinas, el número de casos positivos por COVID19 por cada 1 mil habitantes en el municipio de interés aumentará en 0.83. Hemos pues encontrado evidencia de que hay interacciones espaciales entre el estado y dinámica de la pandemía por COVID19 en la región estudiada: lo que pasa en las áreas territoriales vecinas afecta a la unidad territorial de referencia. Esto podría tener implicaciones sobre las medidas de política sanitaria en la contención de la enfermedad y en el posible desarrollo futuro de ésta. Por supuesto, hay que decir, que esto es sólo un ejercicio ilustrativo y que una interpretación más fina atraviesa por un concienzudo y sistemático estudio del carácter espacial de la transmisión de las enfermedades, lo que el autor de estas líneas está lejos de lograr, por lo que los resultados deben interpretarse con cautela y sólo con fines didácticos. Las líneas anteriores dan cuenta del elemento sustantivo del fenómeno de interés: la pandemia tiene un fuerte componente espacial y lo hemos logrado captar a través del coeficiente autorregresivo espacial en el modelo de rezago. Pero, ¿con este modelo hemos logrado resolver el problema de la autocorrelación en los errores? Los términos de error del modelo de rezago que recién hemos estimado pueden ser visualizados, guardados y evaluados para ver si presentan autocorrelación: spdep::moran.test(rezago[[&quot;residuals&quot;]], mTRUE.pesos) ## ## Moran I test under randomisation ## ## data: rezago[[&quot;residuals&quot;]] ## weights: mTRUE.pesos ## ## Moran I statistic standard deviate = -0.030802, p-value = 0.5123 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## -0.015615681 -0.013333333 0.005490469 Ejercicio Con base en los resultados de la I de Moran sobre los términos de error del modelo de rezago, ¿dirías que persite el problema de autocorrelación espacial en los errores en el modelo de rezago? 5.3.2.2 Modelo de error espacial El modelo de error espacial es llamado con la función errorsarlm() y los argumentos son los mismos: error &lt;- spatialreg::errorsarlm(formula=pos_hab ~ ss, data=covid_zmvm, listw=mTRUE.pesos) summary(error) ## ## Call:spatialreg::errorsarlm(formula = pos_hab ~ ss, data = covid_zmvm, ## listw = mTRUE.pesos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.80220 -1.62230 -0.39269 1.11193 11.55180 ## ## Type: error ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.66237 5.03921 -0.1314 0.89543 ## ss 12.54484 6.30980 1.9882 0.04679 ## ## Lambda: 0.87101, LR test value: 58.512, p-value: 2.0206e-14 ## Asymptotic standard error: 0.050779 ## z-value: 17.153, p-value: &lt; 2.22e-16 ## Wald statistic: 294.22, p-value: &lt; 2.22e-16 ## ## Log likelihood: -193.0788 for error model ## ML residual variance (sigma squared): 7.3069, (sigma: 2.7031) ## Number of observations: 76 ## Number of parameters estimated: 4 ## AIC: NA (not available for weighted model), (AIC for lm: 450.67) Aquí, el parámetro de interés estimado es \\(\\lambda\\), el coeficiente de autocorrelación espacial, que resultó positivo, alto y estadísticamente significativo, también (0.87). En este caso, las posibilidades de interpretación sustantiva del coeficiente obtenido son mucho más limitadas, ya que los efectos espaciales están modelados por el término de error. Recordemos que los términos de error incluyen los efectos de todas aquellas variables que no están siendo incluidas en nuestro modelo. En este caso, entonces, no es posible brindar una lectura explícita de la variable o variable que determinan las interacciones espaciales. Sólo podemos contentarnos con decir que el problema de la dependencia entre los términos de error ha sido resuelto y que el componente espacial es considerablemente alto. Con la siguiente línea, se verifica que se ha resuelto el problema de la autocorrelación en los errores: spdep::moran.test(error[[&quot;residuals&quot;]], mTRUE.pesos) ## ## Moran I test under randomisation ## ## data: error[[&quot;residuals&quot;]] ## weights: mTRUE.pesos ## ## Moran I statistic standard deviate = 0.18736, p-value = 0.4257 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.0005636588 -0.0133333333 0.0055017348 Con estos resultados hemos completado la ruta más elemental para la estimación de modelos econométricos espaciales; sin embargo, no debemos olvidar que existen todas las alternativas que apuntamos en la figura 5.1 y que no fueron discutidas aquí. 5.4 Reflexiones finales Hay una serie de tópicos que han quedado fuera de estas notas, tales como los modelos que buscan resolver la heterogeneidad espacial, aquellos que atienden la violación del supuesto de homoscedasticidad en las perturbaciones, o bien, la incorporación de la dimensión temporal a nuestro análisis, a través de instrumentos como los modelos de panel, por lo que este material no es sino sólo un abordaje introductorio, una invitación a que profundices por tu propia cuenta en el manejo de las herramientas para el tratamiento de datos espaciales. Cerramos este libro recomendado una serie de materiales para cada uno de los capítulos que hemos tratado aquí. Para el Capítulo 1, donde tuvimos oportunidad de presentar los elementos básicos de R y RStudio mediante el análisis exploratorio de datos, recomendamos consultar el curso gratuito ofrecido por la Universidad Nacional Autónoma de México a través de la plataforma Coursera Introducción a Data Science: Programación Estadística con R, así como la versión en castellano del excelente libro de Wickham y Grolemund R para Ciencia de Datos. Para el Capítulo 2, centrado en la elaboración de mapas coropléticos, hay varios materiales que puedes consultad, como blogs o los servidores que contiene ejercicios y prácticas, como RPubs, aunque también existen los materiales desarrollados por los propios autores de la paquería, aunque no están en castellano. En el Capítulo 3, que trató sobre los elementos básicos del análisis exploratorio de datos espaciales, existen los materiales desarrollados por los colegas del Centro de Estudios de Desarrollo Regional y Urbano Sustentable quienes, en su canal de YouTube, desarrollan una serie de prácticas introductorias de exploración de información espacial, aunque con otro Softeare (GeoDa). Para el Capítulo 4 y Capítulo 5](https://jaime-pru.github.io/Analisis-de-datos-espaciales/an%C3%A1lisis-espacial-ii-modelos-econom%C3%A9tricos-espaciales.html) existe un sin número de materiales para el análisis de regresión, como el libro electrónico de Luis Quintana y Miguel A. Mendoza “Econometría Aplicada Utilizando R”, que puedes descargar libremente desde aquí, del que el último capítulo aborda el tema de econometría espacial. Esperamos que estas notas contribuyan a alimentar tu curiosidad sobre el tratamiento y análisis de información espacial y que sean un primer paso en tu formación como profesional. References "],["sobre-el-autor.html", "Sobre el autor", " Sobre el autor Jaime A. Prudencio Vázquez ORCIDGoogle Académico es docente universitario desde 2013 en las Área de Investigación, Métodos cuantitativos, Economía Política y Economía Regional. Cursó sus estudios de maestría y doctorado en la Universidad Nacional Autónoma de México, en el campo de Economía Urbana y Regional. Ha impartido talleres sobre análisis de datos espaciales y econometría espacial, tanto en el ámbito universitario como en el marco del Encuentro Nacional sobre Desarrollo Regional de la Asociación Mexicana de Ciencias para el Desarrollo Regional (AMECIDER). Entre 2012 y 2015 colaboró como investigador asociado en el Centro de Estudios para el Desarrollo Alternativo, A. C. Ha participado, primero como becario y luego como académico, en diversos proyectos de investigación gestionados por la Coordinación de Investigación Científica de la UNAM para instituciones como INFONAVIT, BANCOMEXT y el Consejo de Investigación y Evaluación de la Política Social (CIEPS) del Estado de México. Actualmente es profesor visitante en el Departamento de Economía, Área de Relaciones Productivas en México de la Universidad Autónoma Metropolitana Unidad Azcapotzalco con actividad docente en la Licenciatura en Economía en el Área de Concentración de Economía de la Innovación: empresas, redes y territorio. Entre sus intereses de investigación se encuentran: Productividad laboral de la manufactura y desarrollo regional; Ciudades, economías de aglomeración y productividad laboral; Disparidad regiones y su medición, temas en los que cuenta con trabajos publicados. Correo electrónico: japv@azc.uam.mx. Twitter. El autor agradece a Montserrat Romero Martínez (vmrm@azc.uam.mx) y a Alvaro Martínez Rodríguez (amr@azc.uam.mx), ayudantes en el Área de Relaciones Productivas, quienes se encargaron respectivamente de la revisión de los materiales preliminares de este libro y de su edición para su publicación en línea con Bookdown en GitHub. Ciudad de México, enero de 2022. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
