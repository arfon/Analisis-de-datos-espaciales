[{"path":"index.html","id":"prólogo","chapter":"Prólogo","heading":"Prólogo","text":"Las ciencias regionales son ya una disciplina menor, pasaron de ser una suerte de espacio para la experimentación de teorías y métodos que tenían cabida en las corrientes principales de los cuerpos científicos plenamente reconocidos y se han se han ido abriendo paso y consolidando como una disciplina totalmente reconocida. Las ciencias regionales han ido adquiriendo “una amplia orientación multidisciplinaria sobre temas regionales y urbanos, combinando y siendo un complemento la economía regional, geografía social y económica, economía urbana, ciencia del transporte, ciencia ambiental, ciencia política y teoría de la planificación” (Fischer y Nijkamp 2014). La realidad es que en los pasados 50 años las ciencias regionales han evolucionado como un campo de investigación en pleno derecho, esto tras un largo recorrido desde los inicios de la economía como ciencia, comenzando con los trabajos de Adam Smith que trataban algunos elementos relacionados con la localización de actividades comerciales, pasando por los estudios clásicos de Von Thünen, . Weber, . Lösch (todos ellos relacionados con la denominada teoría de la localización), hasta llegar la consolidación de las ciencias regionales como disciplina gracias al impulso de Walter Isaard que trata de brindar una perspectiva teórica y metodológica coherente con un fuerte soporte empírico desde una perspectiva multidisciplinaria. Las contribuciones de W. Isaard abarcan una multitud de campos tales como la ecología, el estudio de los transportes e incluso el manejo de conflictos sociales.Si bien las ciencias regionales abarcan múltiples líneas temáticas, hay dos de ellas que se constituyen en tópicos clásicos y son lo que podría ser denominado la corriente principal de las ciencias regionales: el estudio de las fuerzas de aglomeración y los determinantes de la localización de la actividad. Buena parte de las investigaciones de estos ejes buscan responder preguntas como ¿por qué las actividades económicas se distribuyen de forma homogénea en el espacio?, o bien, ¿qué hace que determinadas actividades se localicen en unos sitios y en otros?En su Handbook Regional Science, Manfred Fischer y Peter Nijkamp señalan que quizá sean dos los elementos clave que constituyen la ciencia regional: un enfoque multidisciplinario y un fuerte análisis cuantitativo. Respecto al primer elemento, si bien es cierto que la economía espacial, urbana y regional, han representado pasos alentadores hacia la aproximación del economista con otros científicos sociales, aquellos suelen mirar hacia otras disciplinas con la frecuencia que exigen las problemáticas sociales. De este modo, aún sigue siendo necesario construir alternativas de formación en la fase final de los estudios de licenciatura que busquen romper la endogamia de la profesión. Esto es en alguna medida lo que en el Área de Concentración de Economía de la Innovación: empresas, redes y territorio, que forma parte de la última fase de estudios en la licenciatura en Economía de la Universidad Autónoma Metropolitana Unidad Azcapotzalco, se ha buscado impulsar.En tanto, sobre el enfoque cuantitativo que caracteriza las ciencias regionales, es cierto que en todos los planes de estudio de economía encontramos un sólido repertorio de instrumentos de carácter cuantitativo: matemáticas, estadística y, por supuesto, econometría. obstante, las herramientas requeridas para el análisis de la realidad desde una perspectiva espacial siguen siendo escasas dentro de la formación nivel de licenciatura. Estas notas buscan ser una contribución, por mínima que esta sea, para subsanar dicha situación.Este trabajo está, por tanto, fundamentalmente dirigido los estudiantes de licenciatura interesados en el desarrollo de habilidades técnicas para el análisis cuantitativo que exigen las ciencias regionales y el enfoque espacial de la economía. Así pues, el objetivo de estas notas es guiar al estudiante, desde un enfoque fundamentalmente práctico, en el conocimiento y manejo de técnicas para la exploración, análisis y modelado de información espacial mediante el uso de R, un programa informático y lenguaje de programación enfocado en el análisis estadístico y visualización de información y RStudio un entorno de desarrollo integrado (IDE) desde donde se puede interactuar con R más eficientemente.Este libro se estructura en este momento, enero de 2022, en 5 capítulos, aunque buscamos integrar uno más antes de que finalice el año. En el capítulo 1 se presentan los elementos básicos de R y RStudio través de la utilización del enfoque del análisis exploratorio de datos, es decir, introducimos de lleno al estudiante en el uso del software para plantear y resolver preguntas relativas la estructura de la información utilizada través de diversas herramientas de visualización y manipulación de la información. En el capítulo 2 se muestra cómo elaborar diversos tipos de mapas coropléticos y la enorme flexibilidad de personalización de estilos que tiene R para tal efecto. En el capítulo 3 se presentan las herramientas para llevar cabo un análisis exploratorio de datos espaciales donde el estudiante encontrará la manera de definir las interrelaciones que se dan en el espacio través de la construcción de matrices de pesos espaciales y aprenderá sobre la autocorrelación espacial y sus implicaciones en el análisis de la información. En tanto, en el capítulo 4, se presenta un repaso muy sintético de los modelos de regresión simple enfatizando el problema de la autocorrelación que se puede presentar cuando se estima un modelo lineal con datos espaciales. Finalmente, en el capítulo 5, mostramos algunas de las diferentes alternativas de modelación econométrica espacial disponibles en R.Todos los ejercicios que se desarrollan en esta versión del libro corresponden la realidad económica y social de la Zona Metropolitana del Valle de México y están disponibles para que el estudiante y lector puedan replicar todos los resultados aquí ilustrados. Así, el lector es guiado través de la exploración de la información epidemiológica relacionada con la primera ola de la pandemia por COVID19 en relación con los factores sociodemográficos y económicos de la zona metropolitana más grande de nuestro país.Finalmente, es necesario mencionar que este material es más que un esfuerzo de recopilación, sistematización y aplicación que se ha hecho partir de infinidad de materiales que la activa comunidad de R, interesada en el análisis espacial en México y el mundo, comparte desinteresadamente través de internet. Creemos que así debería ser siempre el conocimiento: libre y abierto como el software que es usado aquí. Así pues, si algún mérito tiene este material es justamente su sentido didáctico que confiamos permita contribuir la formación de profesionales interesandos en las ciencias regionales y en la mejora de las condiciones de vida de las mayorías.","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"r-una-introducción-desde-la-exploración-de-información","chapter":"1 R: Una introducción desde la exploración de información","heading":"1 R: Una introducción desde la exploración de información","text":"","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"r-lenguaje-de-programación-y-plataforma-de-análisis","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.1 R: Lenguaje de programación y plataforma de análisis","text":"R es un lenguaje de programación orientado objetos, libre y de código abierto y además un ambiente enfocado al análisis estadístico y gráfico. Que sea libre significa, diferencia de otros programas, que cualquier persona puede usarlo, redistribuirlo o modificarlo, sin necesidad de contar con una licencia pago.R es un entorno de software libre para computación estadística y gráficos. Compila y se ejecuta en una amplia variedad de plataformas UNIX, Windows y MacOS. Fuente:  http://cran.r-project.orgUsar R en lugar de otras plataformas que requieren licencia pago es recomendable al menos por tres razones:Es libre: en contraste con otros populares programas informáticos como lo es STATA, del que una licencia sencilla ronda los 100 dólares por año.R es sólo un software, también es un lenguaje de programación; así, al aprenderlo se obtiene un resultado doble. De modo que cuando llegues manejarlo con profundidad, puedes crear tus propias librerías. sólo sirve para hacer análisis gráfico y estadístico, con el puedes hacer páginas web, ser usado como Sistema de Información Geográfica e incluso para escribir libros, como éste que ahora estas leyendo.Es un lenguaje con un amplio soporte, es decir, con multitud de guías y materiales para el autoaprendizaje.lo largo de este libro usaremos R través de RStudio, un entorno de desarrollo integrado o IDE por sus siglas en inglés. Así que, lo primero será descargar e instalar R y luego descargar e instalar RStudio.R puede ser descargado de aquí. Al seleccionar “download R” hay que elegir cualquier CRAN Mirror (del inglés Comprehensive R Archive Network que son repositorios que contienen una copia del código base de R y sus paquetes). México tiene dos, puedes elegir cualquiera.En tanto, R Studio puede ser desargado de aquí. Ve la sección “Download” en la parte superior y elije la versión gratuita. El navegador debería seleccionar la versión adecuada para tu equipo, sino es así, selecciona del listado que aparece en la parte inferior. También puedes consultar este video para seguir paso paso la instalación de ambos programas.Al abrir RStudio se mostrará una pantalla como la que aparece en la figura 1.1. La pantalla principal de RStudio está dividida en cuatro partes. En caso de que el recuadro superior izquierdo aparezca, da clic en el ícono de la hoja en blanco con el signo de más (New file, parte superior izquierda de la pantalla) para que se despliegue.\nFigura 1.1: Panel principal\nLas citadas cuatro secciones son:Fuente u origen (source): esta sección, zona superior izquierda, aparecerá una vez que des clic en New script. Aquí podrás escribir las líneas de código de tu proyecto y guardarlas para consultarlas cuando lo desees, visualizarás tus bases de datos organizadas en pestañas y varios otros elementos.Consola: localizada en la parte inferior izquierda, es donde aparece el puntero o prompt (>); este es el espacio en el que se insertarán o escribirán las líneas de código para ser ejecutadas por R y en donde se mostrarán parte de los resultados. Cuando el prompt aparezca, R está listo para recibir tus instrucciones.Archivo: en el espacio inferior derecho aparecen una serie de pestañas que muestran las carpetas y archivos de tu equipo, las gráficas realizadas y los paquetes disponibles, así como la ayuda de R y de sus paquetes.Ambiente: por último el área superior derecha, mostrará las variables creadas, bases de datos cargadas y todos los objetos activos en la sesión de R. Esta sección contiene otras dos pestañas: historial y conexiones. La primera ofrece una memoria de todas las instrucciones escritas en la consola, en tanto, la segunda le será útil cuando haga trabajo colaborativo través de, por ejemplo, GitHub.","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"códigos-objetos-paquetes","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2 Códigos, objetos, paquetes","text":"","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"código","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.1 Código","text":"Los programas informáticos están construidos partir de código, éste puede ser entendido como un lenguaje partir del cual se dan las instrucciones la computadora para que realice alguna acción. Como en todo lenguaje, un aspecto sustantivo es la sintaxis y la ortografía, es decir, el orden y la corrección en la forma en que uno se comunica con el sistema de cómputo, de modo que si se escribe alguna instrucción de forma incorrecta o incompleta (faltas de ortografía o errores de sintaxis) la instrucción se ejecutará.En este sentido, se debe señalar que R es sensible al uso de mayúsculas y minúsculas, esto quiere decir para nuestro programa es lo mismo Árbol y árbol. Además, en este programa constantemente estará usando paréntesis y comas en las instrucciones, por lo que hay que poner especial atención para olvidar colocar alguno de ellos en la sentencia escrita. Otro elemento importante son las llamadas “palabras clave”, Keywords. Éstas son palabras reservadas, es decir, palabras con un significado especial para R y que hacen referencia instrucciones que pertenecen la estructura base del programa R por lo que está recomendado su uso. Algunos ejemplos de palabras reservadas son son: , else, break. Además, el nombre de las variables debe comenzar con caracteres especiales (&,#,$,%) o números.","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"objetos-operadores-y-tipos-de-variables","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.2 Objetos, operadores y tipos de variables","text":"Si bien R es tremendamente potente, puede incluso utilizarse como simple calculadora. Los operadores matemáticos que pueden emplearse en R se muestran en la figura 1.2:\nFigura 1.2: Operadores\nDe esta forma sí colocas en la consola el número 5 seguido del signo aritmético correspondiente, por ejemplo un signo de más, +, y luego otro número, R devolverá el resultado de la operación:En R, todos los elementos con los que se interactúa son denominados objetos (por eso se dice que es un lenguaje de programación orientado objetos), los hay de distintas clases y cada clase se puede identificar por un nombre. Los objetos más comunes en R son: constantes, variables, vectores, listas, matrices y arreglos de datos (data frames). Éstos últimos los hay en diferentes versiones dependiendo de algunas de sus características.Los objetos pueden contener diferentes tipos de información como: números enteros (integer), valores numéricos (numeric), números complejos (complex) o valores lógicos (logic) y de cadena/texto (character). Distinguir entre los tipos de información que contiene cada objeto es importante para conocer las operaciones que se pueden hacer entre ellos. Por ejemplo, es posible hacer operaciones matemáticas con variables tipo character ya que tiene sentido sumar aritméticamente “casa” y “azul”, pero sí es posible concatenarlos.Para crear objetos, en R se utiliza el singo “menor que” seguido de un guion para formar una suerte de flecha que apunta la izquierda, <-, éste singo es llamado operador de asignación y precisamente asigna contenido objetos, es decir, con el operador <- creamos un objeto que tendrá determinado contenido. Ve la sección consola de tu entorno de trabajo e ingresa las siguientes líneas de instrucciones (o simplemente da click en el ícono de copiar que aparece en la esquina superior derecha del cuadro que contiene el código):Tras haber escrito estas líneas de código notarás que en la sección superior derecha de RStudio aparecen, en la pestaña de ambiente, los tres objetos creados, cada uno con un nombre y contenido diferente entre ellos: x es un objeto numérico, y es una cadena de texto o carácter y z contiene un valor lógico. Basta con escribir el nombre del objeto en la consola para ver su contenido.Ahora, vuelve crear un objeto de nombre x que contenga el resultado de la suma de 5+10:Como es posible identificar, si en una sesión de R se utiliza dos veces el mismo nombre para una variable, se sobrescribe la información asignada esa variable, es decir, sólo permanece el último valor asignado: x ya contiene la primera asignación, 3, solamente la última, 15.Hay dos funciones muy útiles para conocer el tipo de objeto que tenemos en nuestro entorno de trabajo: La función class() y la función str(). La primera indica el tipo de objeto, en tanto, la segunda nos dice el tipo de objeto y su valor. En el siguiente ejemplo puedes ver que el objeto y es de tipo carácter y su contenido es la expresión “Hello World”.Los vectores son una forma de almacenar más de un elemento en un objeto y dichos elementos tienen que ser necesariamente del mismo tipo, aunque es importante tener en cuenta que es recomendable combinar diferentes tipos de objetos porque se pueden alterar las clases de cada uno de los elementos originales. Cuando se quiere crear un vector que contiene cadenas de texto, cada cadena (palabra u oración) se debe poner entre comillas, como se ilustra continuación:El uso de vectores es útil para modificar las etiquetas de una gráfica o bien los encabezados de una tabla, ya que R tomará los valores de texto almacenados en el vector para utilizarlos en la forma deseada. Aquí se muestran otros tipos de objetos en un vector:Imagina que deseas crear un vector numérico con una secuencia numérica del 1 al 10. Para hacerlo, deberías proceder como:Es decir, fue necesario insertar en el vector cada uno de los 10 números, sino que la secuencia continua (del 1 al 10) se señaló con los dos puntos.Las matrices son un tipo de objeto que se distingue porque entre sus propiedades está el de tener dimensión (filas y columnas). Se puede generar una matriz en R con la función matrix() y se procede como:Las funciones, tanto las previamente citadas como esta, requieren que el usuario indique argumentos que son los elementos que aparecen entre paréntesis. La función matrix() requirió indicar varios: los datos que forman el contenido que tendrá la matriz,data=, el número de filas , nrow= y el número de columnas, ncol=. Recuerda: toda función requiere especificar determinados argumentos para que pueda funcionar.Alternativamente se pueden construir matrices partir de vectores. Para ello, se pueden usar las funciones cbind() y rbind(). Primero creamos dos vectores, x y y:La función cbind() permite “unir por columna” (column bind) y rbind() que “une por fila” (row bind). Así, con los vectores previamente construidos podemos tener dos matrices diferentes, esto dependerá si las unimos por fila:o columna:Son un tipo especial de vectores y son utilizados para representar información categórica, lo que permite organizarla en niveles para analizarla mejor:Por ejemplo, cuando una variable contiene información sobre el sexo de un individuo es recomendable que en lugar de designar cada caso como 1 y 2 respectivamente, se usen variables categóricas como “masculino” y “femenido”, es decir, es recomendable almacenar la información como factor.Esta estructura de datos es la más usada para realizar análisis en R. Son estructuras de datos de dos dimensiones, es decir, están compuestas por filas y columnas. Los renglones de un data frame admiten datos de distintos tipos, pero sus columnas tienen la restricción de contener datos sólo de un tipo. Para comprender mejor esto piensa en un data frame como si de una hoja de cálculo se tratara: los renglones representan casos, individuos u observaciones, mientras que las columnas representan atributos, rasgos o variables. Una columna con la variable “ingreso” deberá ser del mismo tipo para todos los casos, por ejemplo un valor numérico, en tanto, para el caso o individuo 1 (fila 1) tendremos información relativa sólo al ingreso, sino al sexo, estatura y edad, es decir, variables categóricas, numéricas y enteros, respectivamente.Cuando arribemos un poco más adelante al proceso de importación de información al entorno de trabajo se presentará cómo luce un arreglo de datos de estas características.Se dijo que R es un programa especializado en el análisis estadístico y la representación gráfica, pero R sólo se limita lo que ofrece cuando lo descarga por primera vez. Uno de los elementos que hace de R una potente herramienta es la posibilidad de ampliar su potencial través de la instalación de paquetes que expanden sus funciones básicas. Existen paquetes de R para múltiples campos disciplinares y especialidades, por ejemplo estas notas se sirven de los paquetes especializados en el análisis espacial, como se observará en los capítulos subsecuentes. De momento mencionemos sólo un par:tmap: ofrece un enfoque flexible, basado en capas y fácil de usar para crear mapas temáticos.spatialreg: paquete para la elaboración de regresiones con componentes espaciales.Para poder hacer uso de los paquetes que amplían el potencial de R es necesario descargarlos, instalarlos y, en cada sesión de trabajo, llamarlos. Para la descarga e instalación podemos ir Archivo (File) en la cinta de menú que se localiza en la parte superior de tu entorno de trabajo y seleccionar tools/install package. continuación, deberás colocar el nombre del paquete deseado y dar click en instalar. O bien, alternativamente, puedes ir la sección de Archivo, en la zona inferior derecha, seleccionar la pestaña Packages y continuación el ícono de Install. Una tercera manera de instalar paquetes es desde la consola (sección inferior izquierda), para instalar un paquete la vez:O bien, varios paquetes la vez:Tras el proceso de instalación, en tu consola, R informará sobre el resultado de la instalación y te ofrecerá algunos datos sobre la ubicación del paquete en tu equipo. Para poder hacer uso de los paquetes basta con descargarlos e instalarlos, es necesario “llamarlos” en cada sesión de trabajo de R para ser utilizados. Para ello deberás usar la función library() y como argumento el nombre del paquete:Cada uno de los paquetes y funciones de R está acompañado por materiales de referencia que explican con detalle su uso, así como los diferentes argumentos en el caso de las funciones. Para solicitar ayuda en R puedes recurrir la función help() e indicar como argumento el nombre de la función:Alternativamente, para solicitar ayuda puedes escribir dos signos de interrogación y el nombre del paquete o función:También puedes buscar ayuda de una función específica, por ejemplo, de las funciones para crear una matriz (matrix())o para calcular una media (mean()):En la cinta de menú, en la sección ayuda, Help, encontrarás una serie de materiales muy útiles para familiarizarse con los paquetes instalados. Estos materiales reciben el nombre de “hojas de trucos”, Cheatsheet. Se recomienda ampliamente revisar cada uno de ellos. Además, R es un programa con un sin número de entusiastas usuarios y con un amplio soporte técnico por lo que cuando te encuentres con una dificultad para usar algún paquete o función puedes remitite al sitio de Ayuda del R, o bien, alguno de los repositorios especializados para presentar y resolver dudas como Stackoverflow. Para ilustrar esto, ingresa al sitio de Stackoverflow y coloca en la búsqueda “histograma en R”.En R hay algunas bases de datos que acompañan los paquetes que han sido instalados y sirven para ilustrar su funcionamiento. Las bases precargadas se pueden observar con la siguiente función:Verás una nueva pestaña en la sección de Fuente con el nombre de las bases y una breve descripción. Para cargar alguna de ellas basta introducir su nombre como argumento de la función data(), por ejemplo:tidyverse es un conjunto de paquetes diseñados especialmente para la Ciencia de Datos. Algunos de los paquetes de la familia tidyverse que usaremos aquí y un poco más adelante son:\n* ggplot2: es un sistema para crear gráficos basado en la llamada gramática de las gráficas.\n* dplyr: proporciona una serie de funciones para manipulación de datos. * readr: permite leer datos rectangualres provenientes de múltiples formatos.Como más adelante usaremos otros de los paquetes de la familia tidyverse instalaremos todos en este momento:Si deseas aprender cómo usar con detalle los paquetes de la familia tidyverse y elementos básicos de Ciencia de Datos, el libro de Hadley Wickham y Garrett Grolemund, R Data Science es una magnífica opción.Utilizando el paquete readxl de la familia tidyverse es posible cargar una base de datos en el formato de la popular hoja de cálculo de Microsoft Office. partir de aquí introduciremos una notación particular al usar una función en R, lo que te permitirá recordar qué paquete pertenece dicha función. La notación es: “paquete :: función()”, es decir, primero se colocará el nombre del paquete y, separado por dos pares de dos puntos, en el nombre de la función que pertenece dicho paquete (resulta claro que los paquetes están entonces integrados por múltiples funciones).Sigamos los siguientes pasos para cargar la base de datos covid_zmvm.xlsx que contiene información de los casos positivos y defunciones por COVID19 en los municipios de la Zona Metropolitana del Valle de México durante la primera ola de la pandemia, entre marzo septiembre de 2020, así como múltiples variables sociodemográficas y económicas.Llamemos específicamente al paquete que nos interesa:La función para cargar un libro de Excel es read_excel() y el argumento indispensable es la ruta o directorio donde está almacenado nuestro libro, el nombre y extensión del mismo, path. Creemos pues el objeto covid_zmvm:Para que puedas cargar satisfactoriamente la base, deberás sustituir la ruta por el directorio en el que está almacenado el archivo en tu equipo de cómputo. Una función útil para generar la cadena de texto de la ruta es file.choose(), del paquete base de R. Lleva la consola el código:y presiona enter, verás que aparece un cuadro de diálogo en el que deberás seleccionar el archivo deseado y luego pulsar “abrir”. El resultado aparecerá en tu consola como una cadena de texto entre comillas. Usa esa información para leer la base covid_zmvm.Revisa la ayuda de la función read_excel() para comprender todos los argumentos con los que puede operar la función y que te permitirán personalizar la carga de la base de datos en caso de que tengas múltiples hojas en el libro de Excel o desees rangos personalizados para importar.Ejercicio¿Quién es el autor o autora del paquete readxl?¿Quién es el autor o autora del paquete readxl?Descarga la hoja de trucos del paquete desde el sitio de tidyverse y responde, ¿el paquete sirve sólo para cargar información o es posible escribir y almacenar bases de datos con él?Descarga la hoja de trucos del paquete desde el sitio de tidyverse y responde, ¿el paquete sirve sólo para cargar información o es posible escribir y almacenar bases de datos con él?Alternativamente, es posible que la base que deseemos cargar se encuentre en un formato diferente, por ejemplo, una base de datos con valores separados por comas (coma separate values, CSV). Para cargar una base en dicho formato, usaremos el paquete readr, que forma parte del paquete utilsLlama el paquete tu entorno de trabajo:Con la función anterior, file.choose() obtendremos la cadena de texto que indica la ruta del archivo cargar:Y, finalmente, cargamos la información con la función read.csv()Otra manera de cargar archivos de Excel o de texto (formato CSV) es ubicarnos en la sección de ambiente, ventana superior derecha, y seleccionar el ícono Import Dataset. Se desplegará un menú donde habremos de elegir el tipo de archivo que se desee importar. R sólo permite importar archivos de texto o Excel, también bases de SPSS o STATA. Siguiendo con la ilustración relativa libros de Excel, en la ventana que se despliega, habremos de señalar la ruta exacta o directorio donde está nuestro archivo, incluyendo el nombre y extensión de éste. Al pulsar en el botón actulizar (Update) se desplegara una vista previa de la base. Si aparece de forma correcta, seleccionaremos importar (Import), en caso contrario, se deberá modificar las opciones de importación. Después de seleccionar Import Dataset/Excel y elegir la ubicación del archivo que deseas importar, deberías ver en tu pantalla una imagen como la de la figura 1.3:\nFigura 1.3: Importar datos de excel\nAhora que revisado algunas de las diversas maneras de cargar tu información al entorno de trabajo en R, es momento de comenzar analizarla. Si importaste la base de datos través de la función readxl::read_excel() notarás que el nuestra base covid_zmvm un objeto de tipo tbl_df que es un subtipo de data.frame o arreglo de datos Como el que se mencionó en antes.Las siguientes instrucciones nos ayudarán conocer los nombres de las columnas, la estructura de los datos y la dimensión de la base:Recuerda la notación que mencionamos antes, “paquete::función”; además, si el paquete está cargado, es necesario especificar su nombre al usar la función; al arrancar cada sesión de R, un conjunto de paquetes se cargan de forma automática, dichos paquetes aparecen marcados con una palomita, \\(\\checkmark\\), en la pestaña Packages en la sección de archivo.Ahora bien, si deseas ver la base completa puedes llamarla una nueva pestaña que se visualizará en la sección de fuente. Usa la siguiente instrucción:Alternativamente, puedes llamar la base para que aparezca en la consola, lo que siempre es recomendable si la base es muy grande.En su lugar, podrías preferir ver en la consola sólo los últimos y primeros casos:¿Qué tan grande es la base de datos? Si bien ya la función str() nos brindó información sobre la estructura de la base, puedes conocer el número de filas y columnas que componen tu arreglo de datos con las siguientes funciones:Conviene que se revise el archivo en formato txt que acompaña nuestra base y que opera modo de diccionario para que conocer el significado de cada una de las variables que componen nuestro archivo.Plantemos pues algunas preguntas y busquemos responderlas través de un análisis gráfico:¿Habrá alguna relación entre el número de casos positivos por cada mil habitantes, pos_hab, o muertes por cada mil habitantes, def_hab, con el nivel de estudios promedio de la población por municipio?Si esta relación existe, ¿es más intensa entre las muertes o los casos positivos?Para responder esta pregunta podemos recurrir la construcción de diagramas de dispersión. La elaboración de gráficas dentro de tidyverse corre cargo del paquete ggplot2, una potente librería para la representación visual de información. Este paquete se basa en lo que se denomina “gramática de gráficas” que es otra cosa que un conjunto de reglas coherentes con base en las cuales se elaboran los gráficos (Wickham y Grolemund (2016)).La construcción de gráficas con ggplot2 se hace partir de una suerte de “capas” o “enunciados”. Cada enunciado constituye una parte de la gráfica. Según la hoja de trucos del paquete (que puedes consultar desde la barra de menú en la opción Help/CheatSheet):ggplot2 se basa en la gramática de los gráficos, la idea de que puedes construir cada gráfico partir de los mismos componentes: un conjunto de datos, un sistema de coordenadas y “geoms”, es decir, marcas visuales que representan puntos de datos.Asi pues, se requieren tres elementos para la construcción de una gráfica con ggplot2: datos, geometría y sistema de coordenadas. Dentro de la geometría se especifican sólo la o las variables representar, también algunos elementos estéticos como colores. En nuestro caso, la información está contenida en la base covid_zmvm pero habrá que especificar qué tipo de gráfica deseamos y sus elementos estéticos. Veamos esto en acción:Tenemos pues dos enunciados o “capas”. La primera corresponde la función ggplot() donde colocamos (casi siempre) el primer elemento requerido: la base de datos de donde tomaremos la o las variables graficar, en tanto, el segundo enunciado corresponde la geometría, geom_ cuya parte después del guion bajo se modificará en función del tipo de gráfica utilizar. Algunos (sólo algunos) tipos de gráficas y su función de geometría asociada en ggplot2 aparecen en el cuadro 1.1:Cuadro 1.1Por favor, revisa la hoja de trucos del paquete para que te familiarices con las otras tantas geometrías y su uso, pues cada geometría requiere diferentes argumentos y admite varias posibles configuraciones. En general, dentro de la función de geometría, geom_, será necesario especificar los elementos estéticos, aes(), entre los que se cuenta la o las variables graficar y cómo serán representadas.Añadamos una “capa” adicional nuestra gráfica, combinando otra geometría como una línea de ajuste (pongamos atención en el uso del signo de más):Puedes ver cómo, al añadir una nueva geometría, fue necesario especificar los elementos estéticos del caso, pero los tres enunciados nos permiten tener una sola gráfica. Observa cómo R en tu consola R indica que estás usando determinado método de suavizamiento, el llamado “loess” que significa locally estimated scatterplot smoothing, por tanto, es fácil deducir que debe haber otros métodos de suavizamiento. Observa las siguientes líneas de código:Aquí, hemos añadido un argumento adicional, method=, para indicar un suavizamiento dado por un modelo lineal, lm, de linear model. Tenemos pues que entre el número de casos positivos y el número promedio de grados cursados hay una aparente relación positiva. ¿Qué hay para el caso de las defunciones:Si bien hay un poco más de dispersión, la aparente relación positiva se mantiene. ¿Qué crees que explique esta asociación? ¿Las personas con más años estudiados son susceptibles de contagiarse más fácil de COVID19? ¿O quizá se relacionará más con las diferencias que nivel territorial se dan entre los niveles de estudio de la población en el Valle de México?EjercicioElabora algunas gráficas de dispersión para responder las siguientes preguntas:¿Qué tipo de asociación lineal hay entre los casos positivos y defunciones por COVID19 con las características de la población según el número de dormitorios de las casas, el número de personas que habitan en cada casa y la densidad de población?¿Qué tipo de asociación lineal hay entre los casos positivos y defunciones por COVID19 con las características de la población según el número de dormitorios de las casas, el número de personas que habitan en cada casa y la densidad de población?¿Encuentras algún tipo de asociación entre las variables COVID19 y las características económicas de los municipios de la Zona Metropolitana del Valle de México?¿Encuentras algún tipo de asociación entre las variables COVID19 y las características económicas de los municipios de la Zona Metropolitana del Valle de México?Quizá lo que te interese sólo sea la asociación entre pares de variables, sino el comportamiento individual de cada variable para conocer la forma de su distribución e identificar algunos valores extremos o atípicos (los llamados ourliers). Para ello, convendrá un tipo de gráfica que permita representar sólo una variable continúa (casos positivos o defunciones por COVID19), tal como un histograma, una gráfica de densidad o de caja.Los tres gráficos dan cuenta que la forma de la distribución muestra un marcado sesgo positivo (la derecha), lo que significa que hay algunos municipios o alcaldías con valores muy altos para esta variable, además de algunos valores extremadamente altos según el diagrama de caja, ¿cuáles podrán ser y qué explicaría este comportamiento atípicamente alto?EjercicioConsulta la ayuda de cada una de las funciones de geometría e intenta:Elaborar un histograma con diferentes categorías (bins). ¿Cambia esto la forma de la distribución?Elaborar un histograma con diferentes categorías (bins). ¿Cambia esto la forma de la distribución?Señalar de un color diferente las observaciones atípicas en el diagrama de caja ¿Hay manera de identificar qué alcaldías o municipios pertenecen dichos valores atípicos?Señalar de un color diferente las observaciones atípicas en el diagrama de caja ¿Hay manera de identificar qué alcaldías o municipios pertenecen dichos valores atípicos?Además, también es posible modificar las etiquetas de los ejes:Nuestra base de datos contiene una variable de tipo categórico, grado de marginación (gm_2020), asociada al índice de marginación (im). Para este tipo de variable puede ser conveniente construir un gráfico de barras especialmente diseñado para variables categóricas echando mano de la geometría geom_bar():Este gráfico revela que, de acuerdo con la clasificación del Consejo Nacional de Población y Vivienda (CONAPO), la gran mayoría de los municipios y alcaldías que componen el Valle de México están catalogados como de muy bajo grado de marginación.EjercicioObtén una gráfica de barras donde se muestre el número de unidades administrativas (municipios o alcaldías) por de cada una de las entidades que integran la Zona Metropolitana del Valle de México.Ahora bien, ¿será posible incorporar en un plano bidimensional una tercera variable? ¿Cómo añadirías un diagrama de dispersión, que relaciona dos variables, una tercera? Este instrumento es veces denominado gráfico de burbujas y si lo piensas con cuidado es una solución muy ingeniosa para añadir información extra sin renunciar la simpleza. Para hacerlo, basta con añadir el argumento size= dentro de los elementos estéticos de la función geom_point():EjercicioResponde lo siguiente:Prueba cambiar la variable que define el tamaño del círculo por nuestra variable categórica grado de marginación, ¿es recomendable usar una variable de este tipo en esta representación?Prueba cambiar la variable que define el tamaño del círculo por nuestra variable categórica grado de marginación, ¿es recomendable usar una variable de este tipo en esta representación?¿Qué papel juega el argumento alpha= en la segunda gráfica?¿Qué papel juega el argumento alpha= en la segunda gráfica?¿Cuál es la diferencia en colocar el argumento color dentro o fuera del grupo de elementos estéticos (aes())? ¿Notaste la diferencia en el par de gráficas anteriores? Prueba cambiar el argumento color dentro de aes() por otra variable y el color de los círculos.¿Cuál es la diferencia en colocar el argumento color dentro o fuera del grupo de elementos estéticos (aes())? ¿Notaste la diferencia en el par de gráficas anteriores? Prueba cambiar el argumento color dentro de aes() por otra variable y el color de los círculos.Finalmente, elaboremos una gráfica que nos ayude sintetizar pares de asociaciones entre variables (diagramas de dispersión) y formas de distribución través de un gráfico muy elegante construido con una extensión del paquete ggplot2: GGally. Para instalarlo, como es usual:Luego, para poder usarlo, lo llamamos al entorno de trabajo:La gráfica que buscamos es una matriz de diagramas de dispersión y la construiremos con la función ggpairs() y sólo dos argumentos: la fuente de datos y las variables que deseamos incluir. Veamos:La matriz de diagramas de dispersión construida con ggpairs es, desde mi personal punto de vista, sólo muy elegante, sino que ofrece elementos informativos en el gráfico que antes teníamos, tales como el coeficiente de correlación lineal y su nivel de significancia.Antes de pasar tratar otro elemento, hay que decir que RStudio permite exportar las gráficas hechas, tanto en diversos formatos de imagen (jpg, png, tiff) como en PDF.EjercicioElabora tu propia matriz de diagramas de dispersión con las variables que, tu juicio, resulten más relevantes para explicar la dinámica tanto de las muertes como de los casos positivos de COVID19.Tareas relacionadas con la preparación y manipulación de datos, que en una hoja de cálculo como Excel de Office o Calc de LibreOffice, son rutinarias y muy sencillas, veces en R pueden significar un dolor de cabeza. obstante, dentro de los paquetes de la familia tidyverse encontramos dplyr. Este paquete brinda herramientas de manipulación de datos basadas en “gramática”, que es otra cosa que funciones (“verbos”) que permiten seleccionar o filtrar elementos en una base de datos, o bien, agrupar y crear nuevas variables.Si deseas profundizar en el conocimiento de este potente paquete consulta la Introducción al paquete dplyr, o bien, su Viñeta de ayuda.La base de datos que cargamos contiene 55 columnas, es decir, 55 variables. Quizá para su análisis te sean de interés todas, por lo que querrías generar una base de datos más compacta mediante la selección de algunas que son de utilidad. Para llevar cabo dicha tarea echaremos mano de la función select(), pero antes de hacerlo, es necesario introducir el operador “tubería”, pipe (%>%).Este operador sirve para indicar cada fase del proceso por el que un objeto es tratado través de la aplicación de diferentes funciones u operaciones. Por ejemplo, queremos que la base covid_zmvm “pase” por un proceso que consiste en la selección de sólo algunas variables. Comencemos usando el “verbo” seleccionar, select(), para una sola variable, por ejemplo pos_hab:La instrucción anterior es equivalente escribir lo siguiente:Así, usar el operador tubería permite sólo ahorrar la escritura de algunos argumentos, sino simplificar la forma en que las instrucciones dadas R son leídas por un ser humano. Veamos cómo seleccionar ahora, por ejemplo, tres variables: la clave de municipio (cvemun), los casos positivos (pos_hab) y el grado de marginación (gm_2020):Ahora, imagina que queremos todas las variables excepto el nombre de la Zona Metropolitana, nom_zm. Sería ocioso colocar todos los nombres de las variables menos el citado. Para hacer esto más eficiente es necesario introducir solamente un signo de menos (-) antes del nombre de la variable, para de esta sencilla forma seleccionar todas las variables menos nom_zm.En lugar de usar los nombres de las variables, puedes recurrir al número de columna en la que se hallan, contando partir del 1. Por ejemplo, nom_mun es la columna 3 y pos_hab es la columna 16, entonces podríamos escribir:Y si es de nuestro interés seleccionar un número consecutivo de columnas, podemos servirnos de:EjercicioPara finalizar lo relacionado con la selección, genera una nueva base de datos llamada covid que contenga las siguientes variables: cvemun, nom_mun, cve_ent, nom_ent, ext, pos_hab, def_hab, ocviv, occu, ppob_1, ppob_1dorm, im, gm_2020, grad, poind, pocom, poss, tmind, tmcom, tmss, rmind, rmcom, rmss, den, pob20.Si ahora lo que te interesa es seleccionar filas o casos en lugar de columnas, hay que echar mano de otro “verbo”, es decir, de la función filtrar, filter(). Por ejemplo, imagina que deseamos seleccionar sólo los municipios del Estado de México. Estos municipios cumplen con la condición de que todos ellos tienen el valor “México” en la variable nom_ent (nombre de entidad), o bien, tener el valor “15” en el de cve_ent (clave de entidad). Para poder hacer un uso eficiente de la función de filtrado es necesario introducir también otro tipo de operadores, los llamados operadores lógicos, que aparecen en el cuadro 1.2:Cuadro 1.2Entonces, para usar la función filter() y tomando como base la información previa:Para hacer una selección de dos entidades, incluiremos dos expresiones en una misma consulta echando mano del operador lógico “o”, identificado con el signo |:EjercicioSelecciona los municipios con una extensión territorial mayor 84 \\(km^2\\).Selecciona los municipios con una extensión territorial mayor 84 \\(km^2\\).Selecciona los municipios con entre 10 y 20 casos positivos por cada mil habitantes.Selecciona los municipios con entre 10 y 20 casos positivos por cada mil habitantes.Selecciona los municipios del Estado de México que tengan más de 10 años promedio de estudio y menos de 10 casos positivos por cada mil habitantes.Selecciona los municipios del Estado de México que tengan más de 10 años promedio de estudio y menos de 10 casos positivos por cada mil habitantes.¿Qué otros filtros relevantes podrías pensar y elaborar?Una operación útil, cuando se analiza información, tiene que ver con ordenar la base de datos con arreglo al valor de una variable. En R con dplyr de tidyverse esto se hace con la función arrange(), por ejemplo:El resultado de la función anterior ordena nuestra base de menor mayor, con base en la clave municipal. Además, es posible incluir varios criterios de ordenación, por ejemplo, si tuvieras los campos año, mes y día podría ordenarse conforme al calendario. En el caso del ejemplo siguiente, primero se ordena con arreglo la clave de entidad y luego con respecto la clave municipal y luego por la extensión territorial por cada mil habitantes:Si deseas ordenar de forma descendente, es decir, de mayor menor hay que introducir una función adicional, desc(), por ejemplo:La función anterior primero ordena los municipios con base en la clave de municipio de forma ascendente (de menor mayor) y simultáneamente coloca de mayor menor los municipios de acuerdo con su extensión territorial.EjerciciosGenera una nueva base de datos (un nuevo objeto) que contenga sólo las alcaldías de la Ciudad de México y las variables casos por cada mil habitantes, luego ordena esa base de datos con arreglo al número de casos positivos de forma ascendente.Genera una nueva base de datos (un nuevo objeto) que contenga sólo las alcaldías de la Ciudad de México y las variables casos por cada mil habitantes, luego ordena esa base de datos con arreglo al número de casos positivos de forma ascendente.lo mismo que en el inciso anterior, pero para los municipios del Estado de México y las defunciones.lo mismo que en el inciso anterior, pero para los municipios del Estado de México y las defunciones.Para añadir una nueva variable partir de las existentes recurrimos la función mutate(). Por ejemplo, calculemos la variable densidad de casos positivos, es decir, número de casos positivos dividido entre la extensión territorial, dicha variable la llamaremos pos_den. Pero antes, generemos una nueva base de datos que sólo contenga algunas de las variables de la base original:Notarás como la nueva variable aparece al final de la base. Vamos intentar unir varios de los elementos que hemos aprendido hasta este punto para que veas el potencial de uso de la tubería. Vamos crear una nueva variable, la misma que hace un momento, luego, tomaremos la base que contiene la nueva variable y construiremos con ella un diagrama de dispersión con la nueva variable y el número de defunciones por cada mil habitantes:¿Cómo explicarías la relación entre la variable creada, densidad de casos positivos, y las defunciones por cada mil habitables?Ejercicio¿Cómo añadirías más de una nueva variable tu base original?¿Cómo añadirías más de una nueva variable tu base original?Construye la variable “densidad de defunciones” y grafícala en un diagrama de dispersión con la densidad de población? Usa las cañerías.Construye la variable “densidad de defunciones” y grafícala en un diagrama de dispersión con la densidad de población? Usa las cañerías.Quizá lo que te interese sea sólo quedarte con algunas de las variables originales y las nuevas variables creadas partir de la información de la variable original. Para ello es útil la función transmute():El segmento de código anterior genera una nueva base en la que sólo conservamos algunas de las variables originales y añadimos dos nuevas partir de la información original.Quizá una de las funciones más potentes y sencillas que tiene dplyr es la función de resumen (summarise()). Veamos cómo opera. Imagina que deseas un promedio del número de casos que terminaron en muerte:Esto es particularmente útil si se le compara con la función del paquete base de R, summary(), pero si la combinamos con la función de agrupamiento, group_by() la situación cambia. Imagina que queremos el promedio de casos positivos por cada mil habitantes para cada conjunto de municipios de las tres entidades que componen el Valle de México:El en segmento de código anterior primero se agrupa la información con arreglo al criterio de clave de entidad, cve_ent, y luego, para cada grupo, calcula el resultado especificado: un promedio. Dentro de la función de resumen, summarise(), se puede llevar cabo sólo múltiples operaciones sino que estas pueden ser de diferente naturaleza, incluso por ejemplo una suma o una cuenta:En el segmento de código anterior creamos tres variables de resumen: la población total, pob_tot, que es la suma de la población para cada entidad, el promedio del índice de marginación, im_medio y el número de municipios de cada entidad través de una cuenta con la función n().EjercicioConstruye una gráfica de barras donde aparezca el total de población para cada entidad, partir del segmento código anterior.Construye una gráfica de barras donde aparezca el total de población para cada entidad, partir del segmento código anterior.¿Es posible agrupar la información con arreglo otra variable? Construye algunas medidas de resumen con esos grupos y gráficalos.¿Es posible agrupar la información con arreglo otra variable? Construye algunas medidas de resumen con esos grupos y gráficalos.Estas y otras tantas funciones son desarrolladas con todo detalle y de forma muy amena en el citado libro de Wickham y Grolemund, R data science, por lo que su estudio se recomienda ampliamente.En el siguiente capítulo continuamos con la exploración de la información, pero de un tipo particular de información: la información espacial.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2022-01-19.book built bookdown R package.","code":"\n5+10## [1] 15\nx <- 3 #Numérico\ny <- \"Hello World\" #Carácter \nz <- FALSE # Lógico\nx <- 5+10\nclass(y)## [1] \"character\"\nstr(y)##  chr \"Hello World\"\nx <- c(\"a\",\"v\")\nx## [1] \"a\" \"v\"\nx <- c(\"a\",\"v\")\nx## [1] \"a\" \"v\"\nx <- c(TRUE,FALSE)\nx## [1]  TRUE FALSE\nx<- c(4+5i,3+2i)\nx## [1] 4+5i 3+2i\nx<-c(1:10)\nx##  [1]  1  2  3  4  5  6  7  8  9 10\nm <- matrix(data=1:6,nrow = 2,ncol = 3)\nx<-1:3\nx## [1] 1 2 3\ny<-10:12\ny## [1] 10 11 12\ncbind(x,y)##      x  y\n## [1,] 1 10\n## [2,] 2 11\n## [3,] 3 12\nrbind(x,y)##   [,1] [,2] [,3]\n## x    1    2    3\n## y   10   11   12\nx <- factor(c(\"yes\",\"no\",\"no\",\"yes\",\"no\"))\nx## [1] yes no  no  yes no \n## Levels: no yes\ninstall.packages(\"tmap\")\ninstall.packages(\"spatialreg\")\ninstall.packages(c(\"tmap\", \"spatialreg\"))\nlibrary(tmap)\nhelp(tmap)\n??ggplot\nhelp(matrix)\nhelp(mean)\ndata()\ndata(CO2)\ninstall.packages(\"tidyverse\")\nlibrary(readxl)## Warning: package 'readxl' was built under R version 4.1.2\ncovid_zmvm <- readxl::read_excel(path=\"base de datos\\\\covid_zmvm.xlsx\")\nbase::file.choose()\nlibrary(readr)## Warning: package 'readr' was built under R version 4.1.2\nruta <- base::file.choose()\ncovid_zmvm <- utils::read.csv(ruta)\nbase::names(covid_zmvm) #Nombres de las variables##  [1] \"cvemun\"     \"cve_mun\"    \"nom_mun\"    \"cve_ent\"    \"nom_ent\"   \n##  [6] \"nom_abr\"    \"nom_zm\"     \"cvm\"        \"ext\"        \"pob20\"     \n## [11] \"pob20_h\"    \"pob20_m\"    \"positivos\"  \"defuncione\" \"pos_mil\"   \n## [16] \"pos_hab\"    \"def_hab\"    \"ss\"         \"ppob_sines\" \"ppob_basi\" \n## [21] \"ppob_media\" \"ppob_sup\"   \"ocviv\"      \"occu\"       \"pintegra4_\"\n## [26] \"pintegra6_\" \"pintegra8_\" \"ppob_5_o_m\" \"ppob_3_o_m\" \"ppob_1\"    \n## [31] \"ppob_1dorm\" \"ppob_2dorm\" \"ppob_3dorm\" \"pviv_ocu5_\" \"pviv_ocu7_\"\n## [36] \"pviv_ocu9_\" \"analf\"      \"sbasc\"      \"vhac\"       \"po2sm\"     \n## [41] \"im\"         \"gm_2020\"    \"grad\"       \"grad_h\"     \"grad_m\"    \n## [46] \"poind\"      \"pocom\"      \"poss\"       \"tmind\"      \"tmcom\"     \n## [51] \"tmss\"       \"rmind\"      \"rmcom\"      \"rmss\"       \"den\"\nutils::str(covid_zmvm) #Estructura de la base de datos## tibble [76 x 55] (S3: tbl_df/tbl/data.frame)\n##  $ cvemun    : chr [1:76] \"09010\" \"09012\" \"09015\" \"09017\" ...\n##  $ cve_mun   : chr [1:76] \"010\" \"012\" \"015\" \"017\" ...\n##  $ nom_mun   : chr [1:76] \"Álvaro Obregón\" \"Tlalpan\" \"Cuauhtémoc\" \"Venustiano Carranza\" ...\n##  $ cve_ent   : chr [1:76] \"09\" \"09\" \"09\" \"09\" ...\n##  $ nom_ent   : chr [1:76] \"Ciudad de México\" \"Ciudad de México\" \"Ciudad de México\" \"Ciudad de México\" ...\n##  $ nom_abr   : chr [1:76] \"CDMX\" \"CDMX\" \"CDMX\" \"CDMX\" ...\n##  $ nom_zm    : chr [1:76] \"Valle de México\" \"Valle de México\" \"Valle de México\" \"Valle de México\" ...\n##  $ cvm       : num [1:76] 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 ...\n##  $ ext       : num [1:76] 96.2 310.4 32.5 33.9 85.8 ...\n##  $ pob20     : num [1:76] 759137 699928 545884 443704 392313 ...\n##  $ pob20_h   : num [1:76] 361007 334877 260951 210118 190190 ...\n##  $ pob20_m   : num [1:76] 398130 365051 284933 233586 202123 ...\n##  $ positivos : num [1:76] 10905 11887 7289 7172 6812 ...\n##  $ defuncione: num [1:76] 868 542 754 585 289 713 648 397 190 391 ...\n##  $ pos_mil   : num [1:76] 14.4 17 13.4 16.2 17.4 ...\n##  $ pos_hab   : num [1:76] 14.4 17 13.4 16.2 17.4 ...\n##  $ def_hab   : num [1:76] 1.143 0.774 1.381 1.318 0.737 ...\n##  $ ss        : num [1:76] 75 71.1 71.6 71.6 72.7 ...\n##  $ ppob_sines: num [1:76] 0.0204 0.0195 0.0119 0.0121 0.0193 ...\n##  $ ppob_basi : num [1:76] 0.41 0.391 0.307 0.379 0.469 ...\n##  $ ppob_media: num [1:76] 0.252 0.253 0.251 0.291 0.299 ...\n##  $ ppob_sup  : num [1:76] 0.314 0.335 0.427 0.316 0.211 ...\n##  $ ocviv     : num [1:76] 3.45 3.44 2.75 3.26 3.67 3.21 3.2 3.74 3.6 2.81 ...\n##  $ occu      : num [1:76] 0.81 0.81 0.72 0.82 0.93 0.77 0.7 0.91 0.81 0.66 ...\n##  $ pintegra4_: num [1:76] 0.648 0.644 0.48 0.609 0.699 ...\n##  $ pintegra6_: num [1:76] 0.252 0.226 0.142 0.219 0.262 ...\n##  $ pintegra8_: num [1:76] 0.1071 0.0864 0.0422 0.0798 0.0982 ...\n##  $ ppob_5_o_m: num [1:76] 0.726 0.73 0.888 0.821 0.792 ...\n##  $ ppob_3_o_m: num [1:76] 0.32 0.315 0.363 0.354 0.354 ...\n##  $ ppob_1    : num [1:76] 0.0369 0.0499 0.0249 0.0247 0.0513 ...\n##  $ ppob_1dorm: num [1:76] 0.211 0.212 0.199 0.202 0.209 ...\n##  $ ppob_2dorm: num [1:76] 0.566 0.547 0.8 0.696 0.614 ...\n##  $ ppob_3dorm: num [1:76] 0.832 0.848 0.947 0.89 0.864 ...\n##  $ pviv_ocu5_: num [1:76] 0.226 0.22 0.127 0.198 0.267 ...\n##  $ pviv_ocu7_: num [1:76] 0.0641 0.0568 0.026 0.0513 0.0701 ...\n##  $ pviv_ocu9_: num [1:76] 0.02334 0.01815 0.00645 0.01593 0.02167 ...\n##  $ analf     : num [1:76] 1.574 1.606 0.953 1.085 1.673 ...\n##  $ sbasc     : num [1:76] 19 18.2 13.5 16.9 20.3 ...\n##  $ vhac      : num [1:76] 15.25 15.19 9.42 14.43 18.3 ...\n##  $ po2sm     : num [1:76] 49.3 61.2 41.4 57.2 65.4 ...\n##  $ im        : num [1:76] 60.4 59.5 61.3 60.3 59.3 ...\n##  $ gm_2020   : chr [1:76] \"Muy Bajo\" \"Muy Bajo\" \"Muy Bajo\" \"Muy Bajo\" ...\n##  $ grad      : num [1:76] 11.3 11.5 12.4 11.5 10.5 ...\n##  $ grad_h    : num [1:76] 11.5 11.7 12.8 11.7 10.7 ...\n##  $ grad_m    : num [1:76] 11.1 11.4 12.1 11.3 10.4 ...\n##  $ poind     : num [1:76] 0.0912 0.0874 0.1334 0.0714 0.2442 ...\n##  $ pocom     : num [1:76] 0.16 0.185 0.166 0.289 0.394 ...\n##  $ poss      : num [1:76] 0.749 0.728 0.7 0.64 0.361 ...\n##  $ tmind     : num [1:76] 20.32 8.9 27.57 6.57 6.42 ...\n##  $ tmcom     : num [1:76] 5.95 3.48 4.36 2.66 2.12 ...\n##  $ tmss      : num [1:76] 27.69 13.91 24.87 9.94 2.59 ...\n##  $ rmind     : num [1:76] 68.2 116.7 55.9 64.9 78.3 ...\n##  $ rmcom     : num [1:76] 50.6 31.3 45.5 31.2 23.5 ...\n##  $ rmss      : num [1:76] 178.3 36.3 255.6 101.4 23.4 ...\n##  $ den       : num [1:76] 7895 2255 16786 13104 4575 ...\nbase::dim(covid_zmvm) #Dimensiones de la base## [1] 76 55\nutils::View(covid_zmvm)\n#Que es equivalente a:\nView(covid_zmvm)\nbase::print(covid_zmvm)## # A tibble: 76 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 759137\n##  2 09012  012     Tlalpan    09      Ciudad ~ CDMX    Valle ~  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc 09      Ciudad ~ CDMX    Valle ~  9.01  32.5 545884\n##  4 09017  017     Venustian~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.9 443704\n##  5 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 392313\n##  6 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 432205\n##  7 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 614447\n##  8 09013  013     Xochimilco 09      Ciudad ~ CDMX    Valle ~  9.01 118.  442178\n##  9 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 217686\n## 10 09016  016     Miguel Hi~ 09      Ciudad ~ CDMX    Valle ~  9.01  46.4 414470\n## # ... with 66 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\nutils::head(covid_zmvm) #Para los primeros casos## # A tibble: 6 x 55\n##   cvemun cve_mun nom_mun    cve_ent nom_ent   nom_abr nom_zm    cvm   ext  pob20\n##   <chr>  <chr>   <chr>      <chr>   <chr>     <chr>   <chr>   <dbl> <dbl>  <dbl>\n## 1 09010  010     Álvaro Ob~ 09      Ciudad d~ CDMX    Valle ~  9.01  96.2 759137\n## 2 09012  012     Tlalpan    09      Ciudad d~ CDMX    Valle ~  9.01 310.  699928\n## 3 09015  015     Cuauhtémoc 09      Ciudad d~ CDMX    Valle ~  9.01  32.5 545884\n## 4 09017  017     Venustian~ 09      Ciudad d~ CDMX    Valle ~  9.01  33.9 443704\n## 5 09011  011     Tláhuac    09      Ciudad d~ CDMX    Valle ~  9.01  85.8 392313\n## 6 09002  002     Azcapotza~ 09      Ciudad d~ CDMX    Valle ~  9.01  33.5 432205\n## # ... with 45 more variables: pob20_h <dbl>, pob20_m <dbl>, positivos <dbl>,\n## #   defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>, def_hab <dbl>, ss <dbl>,\n## #   ppob_sines <dbl>, ppob_basi <dbl>, ppob_media <dbl>, ppob_sup <dbl>,\n## #   ocviv <dbl>, occu <dbl>, pintegra4_ <dbl>, pintegra6_ <dbl>,\n## #   pintegra8_ <dbl>, ppob_5_o_m <dbl>, ppob_3_o_m <dbl>, ppob_1 <dbl>,\n## #   ppob_1dorm <dbl>, ppob_2dorm <dbl>, ppob_3dorm <dbl>, pviv_ocu5_ <dbl>,\n## #   pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, analf <dbl>, sbasc <dbl>, ...\nutils::tail(covid_zmvm) #Para los últimos casos## # A tibble: 6 x 55\n##   cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm    ext  pob20\n##   <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl>  <dbl>  <dbl>\n## 1 15120  120     Zumpango   15      México   Mex.    Valle ~  9.01 224.   2.80e5\n## 2 15121  121     Cuautitlá~ 15      México   Mex.    Valle ~  9.01 110.   5.55e5\n## 3 15122  122     Valle de ~ 15      México   Mex.    Valle ~  9.01  46.6  3.92e5\n## 4 15125  125     Tonanitla  15      México   Mex.    Valle ~  9.01   9.04 1.49e4\n## 5 15058  058     Nezahualc~ 15      México   Mex.    Valle ~  9.01  63.3  1.08e6\n## 6 09005  005     Gustavo A~ 09      Ciudad ~ CDMX    Valle ~  9.01  87.9  1.17e6\n## # ... with 45 more variables: pob20_h <dbl>, pob20_m <dbl>, positivos <dbl>,\n## #   defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>, def_hab <dbl>, ss <dbl>,\n## #   ppob_sines <dbl>, ppob_basi <dbl>, ppob_media <dbl>, ppob_sup <dbl>,\n## #   ocviv <dbl>, occu <dbl>, pintegra4_ <dbl>, pintegra6_ <dbl>,\n## #   pintegra8_ <dbl>, ppob_5_o_m <dbl>, ppob_3_o_m <dbl>, ppob_1 <dbl>,\n## #   ppob_1dorm <dbl>, ppob_2dorm <dbl>, ppob_3dorm <dbl>, pviv_ocu5_ <dbl>,\n## #   pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, analf <dbl>, sbasc <dbl>, ...\nbase::ncol(covid_zmvm) #Para conocer el número de columnas## [1] 55\nbase::nrow(covid_zmvm) #Para conocer el número de filas## [1] 76\nlibrary(tidyverse)\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab))\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab))+\n  ggplot2::geom_smooth(aes(x=grad,y=pos_hab))\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab))+\n  ggplot2::geom_smooth(aes(x=grad,y=pos_hab), method = \"lm\")\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=def_hab))+\n  ggplot2::geom_smooth(aes(x=grad,y=def_hab), method = \"lm\")\n#Histograma para los casos positivos por cada mil habitantes\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_histogram(aes(x=pos_hab))\n#Gráfico de densidad para los casos positivos por cada mil habitantes\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_density(aes(x=pos_hab))\n#Gráfico de caja para los casos positivos por cada mil habitantes\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_boxplot(aes(x=pos_hab))\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_histogram(aes(x=pos_hab))+\n  ggplot2::labs(x=\"Casos positivos por cada mil habitantes\", y=\"Frecuencia\")## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nggplot2::ggplot(covid_zmvm)+\n ggplot2:: geom_bar(aes(x=gm_2020))+\n  ggplot2::labs(x=\"Grado de marginación 2020\", y=\"Frecuencia\")\n#Si tienes la librería ggplot2 cargada en el enorno de trabajo, alternativamente, puedes escribir:\nggplot(covid_zmvm)+\n geom_bar(aes(x=gm_2020))+\n  labs(x=\"Grado de marginación 2020\", y=\"Frecuencia\")\n#Tamaño del circulo dado por los años estudiados\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab,size=def_hab))\n#Transparencia del círculo dado por los años estudiados\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab,alpha=def_hab, size=def_hab), color=\"navyblue\")\n#Transparencia del círculo dado por los años estudiados\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab,color=def_hab))\ninstall.packages(\"GGally\")\nlibrary(GGally)\n#Definiendo los nombres de las variables\nGGally::ggpairs(data=covid_zmvm, columns = c(\"pos_hab\",\"def_hab\",\"im\",\"grad\"))\n#Indicando el número de columna según el lugar que ocupa en la base\nGGally::ggpairs(data=covid_zmvm, columns = c(16,17,41,43))\ncovid_zmvm %>% dplyr::select(pos_hab)## # A tibble: 76 x 1\n##    pos_hab\n##      <dbl>\n##  1    14.4\n##  2    17.0\n##  3    13.4\n##  4    16.2\n##  5    17.4\n##  6    17.3\n##  7    14.9\n##  8    17.4\n##  9    18.7\n## 10    13.7\n## # ... with 66 more rows\ndplyr::select(covid_zmvm,pos_hab)## # A tibble: 76 x 1\n##    pos_hab\n##      <dbl>\n##  1    14.4\n##  2    17.0\n##  3    13.4\n##  4    16.2\n##  5    17.4\n##  6    17.3\n##  7    14.9\n##  8    17.4\n##  9    18.7\n## 10    13.7\n## # ... with 66 more rows\ncovid_zmvm %>% dplyr::select(cvemun,pos_hab,gm_2020)## # A tibble: 76 x 3\n##    cvemun pos_hab gm_2020 \n##    <chr>    <dbl> <chr>   \n##  1 09010     14.4 Muy Bajo\n##  2 09012     17.0 Muy Bajo\n##  3 09015     13.4 Muy Bajo\n##  4 09017     16.2 Muy Bajo\n##  5 09011     17.4 Muy Bajo\n##  6 09002     17.3 Muy Bajo\n##  7 09003     14.9 Muy Bajo\n##  8 09013     17.4 Muy Bajo\n##  9 09004     18.7 Muy Bajo\n## 10 09016     13.7 Muy Bajo\n## # ... with 66 more rows\ncovid_zmvm %>% dplyr::select(-nom_zm)## # A tibble: 76 x 54\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr   cvm   ext  pob20 pob20_h\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <dbl> <dbl>  <dbl>   <dbl>\n##  1 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX     9.01  96.2 759137  361007\n##  2 09012  012     Tlalpan    09      Ciudad ~ CDMX     9.01 310.  699928  334877\n##  3 09015  015     Cuauhtémoc 09      Ciudad ~ CDMX     9.01  32.5 545884  260951\n##  4 09017  017     Venustian~ 09      Ciudad ~ CDMX     9.01  33.9 443704  210118\n##  5 09011  011     Tláhuac    09      Ciudad ~ CDMX     9.01  85.8 392313  190190\n##  6 09002  002     Azcapotza~ 09      Ciudad ~ CDMX     9.01  33.5 432205  204950\n##  7 09003  003     Coyoacán   09      Ciudad ~ CDMX     9.01  53.9 614447  289110\n##  8 09013  013     Xochimilco 09      Ciudad ~ CDMX     9.01 118.  442178  215452\n##  9 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX     9.01  71.2 217686  104149\n## 10 09016  016     Miguel Hi~ 09      Ciudad ~ CDMX     9.01  46.4 414470  195467\n## # ... with 66 more rows, and 44 more variables: pob20_m <dbl>, positivos <dbl>,\n## #   defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>, def_hab <dbl>, ss <dbl>,\n## #   ppob_sines <dbl>, ppob_basi <dbl>, ppob_media <dbl>, ppob_sup <dbl>,\n## #   ocviv <dbl>, occu <dbl>, pintegra4_ <dbl>, pintegra6_ <dbl>,\n## #   pintegra8_ <dbl>, ppob_5_o_m <dbl>, ppob_3_o_m <dbl>, ppob_1 <dbl>,\n## #   ppob_1dorm <dbl>, ppob_2dorm <dbl>, ppob_3dorm <dbl>, pviv_ocu5_ <dbl>,\n## #   pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, analf <dbl>, sbasc <dbl>, ...\ncovid_zmvm %>% dplyr::select(3,16)## # A tibble: 76 x 2\n##    nom_mun               pos_hab\n##    <chr>                   <dbl>\n##  1 Álvaro Obregón           14.4\n##  2 Tlalpan                  17.0\n##  3 Cuauhtémoc               13.4\n##  4 Venustiano Carranza      16.2\n##  5 Tláhuac                  17.4\n##  6 Azcapotzalco             17.3\n##  7 Coyoacán                 14.9\n##  8 Xochimilco               17.4\n##  9 Cuajimalpa de Morelos    18.7\n## 10 Miguel Hidalgo           13.7\n## # ... with 66 more rows\ncovid_zmvm %>% dplyr::select(1:10)## # A tibble: 76 x 10\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 759137\n##  2 09012  012     Tlalpan    09      Ciudad ~ CDMX    Valle ~  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc 09      Ciudad ~ CDMX    Valle ~  9.01  32.5 545884\n##  4 09017  017     Venustian~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.9 443704\n##  5 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 392313\n##  6 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 432205\n##  7 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 614447\n##  8 09013  013     Xochimilco 09      Ciudad ~ CDMX    Valle ~  9.01 118.  442178\n##  9 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 217686\n## 10 09016  016     Miguel Hi~ 09      Ciudad ~ CDMX    Valle ~  9.01  46.4 414470\n## # ... with 66 more rows\ncovid_zmvm %>% dplyr::filter(cve_ent==\"15\")## # A tibble: 59 x 55\n##    cvemun cve_mun nom_mun     cve_ent nom_ent nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>       <chr>   <chr>   <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 15002  002     Acolman     15      México  Mex.    Valle ~  9.01  83.9 171507\n##  2 15009  009     Amecameca   15      México  Mex.    Valle ~  9.01 189.   53441\n##  3 15010  010     Apaxco      15      México  Mex.    Valle ~  9.01  75.7  31898\n##  4 15011  011     Atenco      15      México  Mex.    Valle ~  9.01  84.6  75489\n##  5 15013  013     Atizapán d~ 15      México  Mex.    Valle ~  9.01  91.1 523674\n##  6 15015  015     Atlautla    15      México  Mex.    Valle ~  9.01 162.   31900\n##  7 15016  016     Axapusco    15      México  Mex.    Valle ~  9.01 231.   29128\n##  8 15017  017     Ayapango    15      México  Mex.    Valle ~  9.01  36.4  10053\n##  9 15020  020     Coacalco d~ 15      México  Mex.    Valle ~  9.01  35.1 293444\n## 10 15022  022     Cocotitlán  15      México  Mex.    Valle ~  9.01  15.0  15107\n## # ... with 49 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\n#O bien\ncovid_zmvm %>% dplyr::filter(nom_ent==\"México\")## # A tibble: 59 x 55\n##    cvemun cve_mun nom_mun     cve_ent nom_ent nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>       <chr>   <chr>   <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 15002  002     Acolman     15      México  Mex.    Valle ~  9.01  83.9 171507\n##  2 15009  009     Amecameca   15      México  Mex.    Valle ~  9.01 189.   53441\n##  3 15010  010     Apaxco      15      México  Mex.    Valle ~  9.01  75.7  31898\n##  4 15011  011     Atenco      15      México  Mex.    Valle ~  9.01  84.6  75489\n##  5 15013  013     Atizapán d~ 15      México  Mex.    Valle ~  9.01  91.1 523674\n##  6 15015  015     Atlautla    15      México  Mex.    Valle ~  9.01 162.   31900\n##  7 15016  016     Axapusco    15      México  Mex.    Valle ~  9.01 231.   29128\n##  8 15017  017     Ayapango    15      México  Mex.    Valle ~  9.01  36.4  10053\n##  9 15020  020     Coacalco d~ 15      México  Mex.    Valle ~  9.01  35.1 293444\n## 10 15022  022     Cocotitlán  15      México  Mex.    Valle ~  9.01  15.0  15107\n## # ... with 49 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\ncovid_zmvm %>% dplyr::filter(cve_ent==\"09\"|cve_ent==\"15\")## # A tibble: 75 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 759137\n##  2 09012  012     Tlalpan    09      Ciudad ~ CDMX    Valle ~  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc 09      Ciudad ~ CDMX    Valle ~  9.01  32.5 545884\n##  4 09017  017     Venustian~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.9 443704\n##  5 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 392313\n##  6 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 432205\n##  7 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 614447\n##  8 09013  013     Xochimilco 09      Ciudad ~ CDMX    Valle ~  9.01 118.  442178\n##  9 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 217686\n## 10 09016  016     Miguel Hi~ 09      Ciudad ~ CDMX    Valle ~  9.01  46.4 414470\n## # ... with 65 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\ncovid_zmvm %>% dplyr::arrange(cvemun)## # A tibble: 76 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A~ 09      Ciudad ~ CDMX    Valle ~  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco  09      Ciudad ~ CDMX    Valle ~  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa 09      Ciudad ~ CDMX    Valle ~  9.01 113.  1.84e6\n##  7 09008  008     La Magdal~ 09      Ciudad ~ CDMX    Valle ~  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta 09      Ciudad ~ CDMX    Valle ~  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 3.92e5\n## # ... with 66 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\ncovid_zmvm %>% dplyr::arrange(cvemun,ext)## # A tibble: 76 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A~ 09      Ciudad ~ CDMX    Valle ~  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco  09      Ciudad ~ CDMX    Valle ~  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa 09      Ciudad ~ CDMX    Valle ~  9.01 113.  1.84e6\n##  7 09008  008     La Magdal~ 09      Ciudad ~ CDMX    Valle ~  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta 09      Ciudad ~ CDMX    Valle ~  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 3.92e5\n## # ... with 66 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\ncovid_zmvm %>% dplyr::arrange(cvemun,desc(ext))## # A tibble: 76 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A~ 09      Ciudad ~ CDMX    Valle ~  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco  09      Ciudad ~ CDMX    Valle ~  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa 09      Ciudad ~ CDMX    Valle ~  9.01 113.  1.84e6\n##  7 09008  008     La Magdal~ 09      Ciudad ~ CDMX    Valle ~  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta 09      Ciudad ~ CDMX    Valle ~  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 3.92e5\n## # ... with 66 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\ncovid_zmvm %>% dplyr::select(cvemun,nom_mun,positivos,pos_hab,ext) %>%\n  dplyr::mutate(pos_den=positivos/ext)## # A tibble: 76 x 6\n##    cvemun nom_mun               positivos pos_hab   ext pos_den\n##    <chr>  <chr>                     <dbl>   <dbl> <dbl>   <dbl>\n##  1 09010  Álvaro Obregón            10905    14.4  96.2   113. \n##  2 09012  Tlalpan                   11887    17.0 310.     38.3\n##  3 09015  Cuauhtémoc                 7289    13.4  32.5   224. \n##  4 09017  Venustiano Carranza        7172    16.2  33.9   212. \n##  5 09011  Tláhuac                    6812    17.4  85.8    79.4\n##  6 09002  Azcapotzalco               7483    17.3  33.5   223. \n##  7 09003  Coyoacán                   9182    14.9  53.9   170. \n##  8 09013  Xochimilco                 7696    17.4 118.     65.1\n##  9 09004  Cuajimalpa de Morelos      4071    18.7  71.2    57.2\n## 10 09016  Miguel Hidalgo             5669    13.7  46.4   122. \n## # ... with 66 more rows\ncovid_zmvm %>% \n  dplyr::mutate(pos_den=positivos/ext) %>% \n  ggplot2::ggplot()+\n    ggplot2::geom_point(aes(x=pos_den,y=def_hab))+\n    ggplot2::geom_smooth(aes(x=pos_den,y=def_hab))+\n    ggplot2::labs(x=\"Densidad de casos positivos\",y=\"Defunciones por cada mil habitantes\")\ncovid_zmvm %>% \n  dplyr::transmute(cvemun,nom_mun,pos_hab,def_hab,pos_den=positivos/ext,def_den=defuncione/ext)## # A tibble: 76 x 6\n##    cvemun nom_mun               pos_hab def_hab pos_den def_den\n##    <chr>  <chr>                   <dbl>   <dbl>   <dbl>   <dbl>\n##  1 09010  Álvaro Obregón           14.4   1.14    113.     9.03\n##  2 09012  Tlalpan                  17.0   0.774    38.3    1.75\n##  3 09015  Cuauhtémoc               13.4   1.38    224.    23.2 \n##  4 09017  Venustiano Carranza      16.2   1.32    212.    17.3 \n##  5 09011  Tláhuac                  17.4   0.737    79.4    3.37\n##  6 09002  Azcapotzalco             17.3   1.65    223.    21.3 \n##  7 09003  Coyoacán                 14.9   1.05    170.    12.0 \n##  8 09013  Xochimilco               17.4   0.898    65.1    3.36\n##  9 09004  Cuajimalpa de Morelos    18.7   0.873    57.2    2.67\n## 10 09016  Miguel Hidalgo           13.7   0.943   122.     8.43\n## # ... with 66 more rows\ncovid_zmvm %>%\n  dplyr::summarise(promedio_def=mean(def_hab, na.rm=TRUE))## # A tibble: 1 x 1\n##   promedio_def\n##          <dbl>\n## 1        0.683\ncovid_zmvm %>%\n  dplyr::group_by(cve_ent) %>% \n  summarise(promedio_def=mean(def_hab, na.rm=TRUE), promedio_pos=mean(pos_hab))## # A tibble: 3 x 3\n##   cve_ent promedio_def promedio_pos\n##   <chr>          <dbl>        <dbl>\n## 1 09             1.08         15.8 \n## 2 13             0.790         5.88\n## 3 15             0.572         4.57\ncovid_zmvm %>% \n  dplyr::group_by(cve_ent) %>%\n  dplyr::summarise(pob_tot=sum(pob20),im_medio=mean(im),tot_mun=n())## # A tibble: 3 x 4\n##   cve_ent  pob_tot im_medio tot_mun\n##   <chr>      <dbl>    <dbl>   <int>\n## 1 09       9209944     60.1      16\n## 2 13        168302     59.4       1\n## 3 15      12426269     58.0      59"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"vectores","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.2.1 Vectores","text":"Los vectores son una forma de almacenar más de un elemento en un objeto y dichos elementos tienen que ser necesariamente del mismo tipo, aunque es importante tener en cuenta que es recomendable combinar diferentes tipos de objetos porque se pueden alterar las clases de cada uno de los elementos originales. Cuando se quiere crear un vector que contiene cadenas de texto, cada cadena (palabra u oración) se debe poner entre comillas, como se ilustra continuación:El uso de vectores es útil para modificar las etiquetas de una gráfica o bien los encabezados de una tabla, ya que R tomará los valores de texto almacenados en el vector para utilizarlos en la forma deseada. Aquí se muestran otros tipos de objetos en un vector:Imagina que deseas crear un vector numérico con una secuencia numérica del 1 al 10. Para hacerlo, deberías proceder como:Es decir, fue necesario insertar en el vector cada uno de los 10 números, sino que la secuencia continua (del 1 al 10) se señaló con los dos puntos.","code":"\nx <- c(\"a\",\"v\")\nx## [1] \"a\" \"v\"\nx <- c(\"a\",\"v\")\nx## [1] \"a\" \"v\"\nx <- c(TRUE,FALSE)\nx## [1]  TRUE FALSE\nx<- c(4+5i,3+2i)\nx## [1] 4+5i 3+2i\nx<-c(1:10)\nx##  [1]  1  2  3  4  5  6  7  8  9 10"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"matrices","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.2.2 Matrices","text":"Las matrices son un tipo de objeto que se distingue porque entre sus propiedades está el de tener dimensión (filas y columnas). Se puede generar una matriz en R con la función matrix() y se procede como:Las funciones, tanto las previamente citadas como esta, requieren que el usuario indique argumentos que son los elementos que aparecen entre paréntesis. La función matrix() requirió indicar varios: los datos que forman el contenido que tendrá la matriz,data=, el número de filas , nrow= y el número de columnas, ncol=. Recuerda: toda función requiere especificar determinados argumentos para que pueda funcionar.Alternativamente se pueden construir matrices partir de vectores. Para ello, se pueden usar las funciones cbind() y rbind(). Primero creamos dos vectores, x y y:La función cbind() permite “unir por columna” (column bind) y rbind() que “une por fila” (row bind). Así, con los vectores previamente construidos podemos tener dos matrices diferentes, esto dependerá si las unimos por fila:o columna:","code":"\nm <- matrix(data=1:6,nrow = 2,ncol = 3)\nx<-1:3\nx## [1] 1 2 3\ny<-10:12\ny## [1] 10 11 12\ncbind(x,y)##      x  y\n## [1,] 1 10\n## [2,] 2 11\n## [3,] 3 12\nrbind(x,y)##   [,1] [,2] [,3]\n## x    1    2    3\n## y   10   11   12"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"factores","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.2.3 Factores","text":"Son un tipo especial de vectores y son utilizados para representar información categórica, lo que permite organizarla en niveles para analizarla mejor:Por ejemplo, cuando una variable contiene información sobre el sexo de un individuo es recomendable que en lugar de designar cada caso como 1 y 2 respectivamente, se usen variables categóricas como “masculino” y “femenido”, es decir, es recomendable almacenar la información como factor.","code":"\nx <- factor(c(\"yes\",\"no\",\"no\",\"yes\",\"no\"))\nx## [1] yes no  no  yes no \n## Levels: no yes"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"data-frame-o-arreglo-de-datos","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.2.4 Data frame o arreglo de datos","text":"Esta estructura de datos es la más usada para realizar análisis en R. Son estructuras de datos de dos dimensiones, es decir, están compuestas por filas y columnas. Los renglones de un data frame admiten datos de distintos tipos, pero sus columnas tienen la restricción de contener datos sólo de un tipo. Para comprender mejor esto piensa en un data frame como si de una hoja de cálculo se tratara: los renglones representan casos, individuos u observaciones, mientras que las columnas representan atributos, rasgos o variables. Una columna con la variable “ingreso” deberá ser del mismo tipo para todos los casos, por ejemplo un valor numérico, en tanto, para el caso o individuo 1 (fila 1) tendremos información relativa sólo al ingreso, sino al sexo, estatura y edad, es decir, variables categóricas, numéricas y enteros, respectivamente.Cuando arribemos un poco más adelante al proceso de importación de información al entorno de trabajo se presentará cómo luce un arreglo de datos de estas características.","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"paquetes","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.3 Paquetes","text":"Se dijo que R es un programa especializado en el análisis estadístico y la representación gráfica, pero R sólo se limita lo que ofrece cuando lo descarga por primera vez. Uno de los elementos que hace de R una potente herramienta es la posibilidad de ampliar su potencial través de la instalación de paquetes que expanden sus funciones básicas. Existen paquetes de R para múltiples campos disciplinares y especialidades, por ejemplo estas notas se sirven de los paquetes especializados en el análisis espacial, como se observará en los capítulos subsecuentes. De momento mencionemos sólo un par:tmap: ofrece un enfoque flexible, basado en capas y fácil de usar para crear mapas temáticos.spatialreg: paquete para la elaboración de regresiones con componentes espaciales.Para poder hacer uso de los paquetes que amplían el potencial de R es necesario descargarlos, instalarlos y, en cada sesión de trabajo, llamarlos. Para la descarga e instalación podemos ir Archivo (File) en la cinta de menú que se localiza en la parte superior de tu entorno de trabajo y seleccionar tools/install package. continuación, deberás colocar el nombre del paquete deseado y dar click en instalar. O bien, alternativamente, puedes ir la sección de Archivo, en la zona inferior derecha, seleccionar la pestaña Packages y continuación el ícono de Install. Una tercera manera de instalar paquetes es desde la consola (sección inferior izquierda), para instalar un paquete la vez:O bien, varios paquetes la vez:Tras el proceso de instalación, en tu consola, R informará sobre el resultado de la instalación y te ofrecerá algunos datos sobre la ubicación del paquete en tu equipo. Para poder hacer uso de los paquetes basta con descargarlos e instalarlos, es necesario “llamarlos” en cada sesión de trabajo de R para ser utilizados. Para ello deberás usar la función library() y como argumento el nombre del paquete:","code":"\ninstall.packages(\"tmap\")\ninstall.packages(\"spatialreg\")\ninstall.packages(c(\"tmap\", \"spatialreg\"))\nlibrary(tmap)"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"solicitar-ayuda-de-un-paquete-o-función-de-r","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.2.4 Solicitar ayuda de un paquete o función de R","text":"Cada uno de los paquetes y funciones de R está acompañado por materiales de referencia que explican con detalle su uso, así como los diferentes argumentos en el caso de las funciones. Para solicitar ayuda en R puedes recurrir la función help() e indicar como argumento el nombre de la función:Alternativamente, para solicitar ayuda puedes escribir dos signos de interrogación y el nombre del paquete o función:También puedes buscar ayuda de una función específica, por ejemplo, de las funciones para crear una matriz (matrix())o para calcular una media (mean()):En la cinta de menú, en la sección ayuda, Help, encontrarás una serie de materiales muy útiles para familiarizarse con los paquetes instalados. Estos materiales reciben el nombre de “hojas de trucos”, Cheatsheet. Se recomienda ampliamente revisar cada uno de ellos. Además, R es un programa con un sin número de entusiastas usuarios y con un amplio soporte técnico por lo que cuando te encuentres con una dificultad para usar algún paquete o función puedes remitite al sitio de Ayuda del R, o bien, alguno de los repositorios especializados para presentar y resolver dudas como Stackoverflow. Para ilustrar esto, ingresa al sitio de Stackoverflow y coloca en la búsqueda “histograma en R”.","code":"\nhelp(tmap)\n??ggplot\nhelp(matrix)\nhelp(mean)"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"cargar-bases-de-datos","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.3 Cargar bases de datos","text":"En R hay algunas bases de datos que acompañan los paquetes que han sido instalados y sirven para ilustrar su funcionamiento. Las bases precargadas se pueden observar con la siguiente función:Verás una nueva pestaña en la sección de Fuente con el nombre de las bases y una breve descripción. Para cargar alguna de ellas basta introducir su nombre como argumento de la función data(), por ejemplo:tidyverse es un conjunto de paquetes diseñados especialmente para la Ciencia de Datos. Algunos de los paquetes de la familia tidyverse que usaremos aquí y un poco más adelante son:\n* ggplot2: es un sistema para crear gráficos basado en la llamada gramática de las gráficas.\n* dplyr: proporciona una serie de funciones para manipulación de datos. * readr: permite leer datos rectangualres provenientes de múltiples formatos.Como más adelante usaremos otros de los paquetes de la familia tidyverse instalaremos todos en este momento:Si deseas aprender cómo usar con detalle los paquetes de la familia tidyverse y elementos básicos de Ciencia de Datos, el libro de Hadley Wickham y Garrett Grolemund, R Data Science es una magnífica opción.Utilizando el paquete readxl de la familia tidyverse es posible cargar una base de datos en el formato de la popular hoja de cálculo de Microsoft Office. partir de aquí introduciremos una notación particular al usar una función en R, lo que te permitirá recordar qué paquete pertenece dicha función. La notación es: “paquete :: función()”, es decir, primero se colocará el nombre del paquete y, separado por dos pares de dos puntos, en el nombre de la función que pertenece dicho paquete (resulta claro que los paquetes están entonces integrados por múltiples funciones).Sigamos los siguientes pasos para cargar la base de datos covid_zmvm.xlsx que contiene información de los casos positivos y defunciones por COVID19 en los municipios de la Zona Metropolitana del Valle de México durante la primera ola de la pandemia, entre marzo septiembre de 2020, así como múltiples variables sociodemográficas y económicas.Llamemos específicamente al paquete que nos interesa:La función para cargar un libro de Excel es read_excel() y el argumento indispensable es la ruta o directorio donde está almacenado nuestro libro, el nombre y extensión del mismo, path. Creemos pues el objeto covid_zmvm:Para que puedas cargar satisfactoriamente la base, deberás sustituir la ruta por el directorio en el que está almacenado el archivo en tu equipo de cómputo. Una función útil para generar la cadena de texto de la ruta es file.choose(), del paquete base de R. Lleva la consola el código:y presiona enter, verás que aparece un cuadro de diálogo en el que deberás seleccionar el archivo deseado y luego pulsar “abrir”. El resultado aparecerá en tu consola como una cadena de texto entre comillas. Usa esa información para leer la base covid_zmvm.Revisa la ayuda de la función read_excel() para comprender todos los argumentos con los que puede operar la función y que te permitirán personalizar la carga de la base de datos en caso de que tengas múltiples hojas en el libro de Excel o desees rangos personalizados para importar.Ejercicio¿Quién es el autor o autora del paquete readxl?¿Quién es el autor o autora del paquete readxl?Descarga la hoja de trucos del paquete desde el sitio de tidyverse y responde, ¿el paquete sirve sólo para cargar información o es posible escribir y almacenar bases de datos con él?Descarga la hoja de trucos del paquete desde el sitio de tidyverse y responde, ¿el paquete sirve sólo para cargar información o es posible escribir y almacenar bases de datos con él?Alternativamente, es posible que la base que deseemos cargar se encuentre en un formato diferente, por ejemplo, una base de datos con valores separados por comas (coma separate values, CSV). Para cargar una base en dicho formato, usaremos el paquete readr, que forma parte del paquete utilsLlama el paquete tu entorno de trabajo:Con la función anterior, file.choose() obtendremos la cadena de texto que indica la ruta del archivo cargar:Y, finalmente, cargamos la información con la función read.csv()Otra manera de cargar archivos de Excel o de texto (formato CSV) es ubicarnos en la sección de ambiente, ventana superior derecha, y seleccionar el ícono Import Dataset. Se desplegará un menú donde habremos de elegir el tipo de archivo que se desee importar. R sólo permite importar archivos de texto o Excel, también bases de SPSS o STATA. Siguiendo con la ilustración relativa libros de Excel, en la ventana que se despliega, habremos de señalar la ruta exacta o directorio donde está nuestro archivo, incluyendo el nombre y extensión de éste. Al pulsar en el botón actulizar (Update) se desplegara una vista previa de la base. Si aparece de forma correcta, seleccionaremos importar (Import), en caso contrario, se deberá modificar las opciones de importación. Después de seleccionar Import Dataset/Excel y elegir la ubicación del archivo que deseas importar, deberías ver en tu pantalla una imagen como la de la figura 1.3:\nFigura 1.3: Importar datos de excel\nAhora que revisado algunas de las diversas maneras de cargar tu información al entorno de trabajo en R, es momento de comenzar analizarla. Si importaste la base de datos través de la función readxl::read_excel() notarás que el nuestra base covid_zmvm un objeto de tipo tbl_df que es un subtipo de data.frame o arreglo de datos Como el que se mencionó en antes.Las siguientes instrucciones nos ayudarán conocer los nombres de las columnas, la estructura de los datos y la dimensión de la base:Recuerda la notación que mencionamos antes, “paquete::función”; además, si el paquete está cargado, es necesario especificar su nombre al usar la función; al arrancar cada sesión de R, un conjunto de paquetes se cargan de forma automática, dichos paquetes aparecen marcados con una palomita, \\(\\checkmark\\), en la pestaña Packages en la sección de archivo.Ahora bien, si deseas ver la base completa puedes llamarla una nueva pestaña que se visualizará en la sección de fuente. Usa la siguiente instrucción:Alternativamente, puedes llamar la base para que aparezca en la consola, lo que siempre es recomendable si la base es muy grande.En su lugar, podrías preferir ver en la consola sólo los últimos y primeros casos:¿Qué tan grande es la base de datos? Si bien ya la función str() nos brindó información sobre la estructura de la base, puedes conocer el número de filas y columnas que componen tu arreglo de datos con las siguientes funciones:Conviene que se revise el archivo en formato txt que acompaña nuestra base y que opera modo de diccionario para que conocer el significado de cada una de las variables que componen nuestro archivo.Plantemos pues algunas preguntas y busquemos responderlas través de un análisis gráfico:¿Habrá alguna relación entre el número de casos positivos por cada mil habitantes, pos_hab, o muertes por cada mil habitantes, def_hab, con el nivel de estudios promedio de la población por municipio?Si esta relación existe, ¿es más intensa entre las muertes o los casos positivos?Para responder esta pregunta podemos recurrir la construcción de diagramas de dispersión. La elaboración de gráficas dentro de tidyverse corre cargo del paquete ggplot2, una potente librería para la representación visual de información. Este paquete se basa en lo que se denomina “gramática de gráficas” que es otra cosa que un conjunto de reglas coherentes con base en las cuales se elaboran los gráficos (Wickham y Grolemund (2016)).La construcción de gráficas con ggplot2 se hace partir de una suerte de “capas” o “enunciados”. Cada enunciado constituye una parte de la gráfica. Según la hoja de trucos del paquete (que puedes consultar desde la barra de menú en la opción Help/CheatSheet):ggplot2 se basa en la gramática de los gráficos, la idea de que puedes construir cada gráfico partir de los mismos componentes: un conjunto de datos, un sistema de coordenadas y “geoms”, es decir, marcas visuales que representan puntos de datos.Asi pues, se requieren tres elementos para la construcción de una gráfica con ggplot2: datos, geometría y sistema de coordenadas. Dentro de la geometría se especifican sólo la o las variables representar, también algunos elementos estéticos como colores. En nuestro caso, la información está contenida en la base covid_zmvm pero habrá que especificar qué tipo de gráfica deseamos y sus elementos estéticos. Veamos esto en acción:Tenemos pues dos enunciados o “capas”. La primera corresponde la función ggplot() donde colocamos (casi siempre) el primer elemento requerido: la base de datos de donde tomaremos la o las variables graficar, en tanto, el segundo enunciado corresponde la geometría, geom_ cuya parte después del guion bajo se modificará en función del tipo de gráfica utilizar. Algunos (sólo algunos) tipos de gráficas y su función de geometría asociada en ggplot2 aparecen en el cuadro 1.1:Cuadro 1.1Por favor, revisa la hoja de trucos del paquete para que te familiarices con las otras tantas geometrías y su uso, pues cada geometría requiere diferentes argumentos y admite varias posibles configuraciones. En general, dentro de la función de geometría, geom_, será necesario especificar los elementos estéticos, aes(), entre los que se cuenta la o las variables graficar y cómo serán representadas.Añadamos una “capa” adicional nuestra gráfica, combinando otra geometría como una línea de ajuste (pongamos atención en el uso del signo de más):Puedes ver cómo, al añadir una nueva geometría, fue necesario especificar los elementos estéticos del caso, pero los tres enunciados nos permiten tener una sola gráfica. Observa cómo R en tu consola R indica que estás usando determinado método de suavizamiento, el llamado “loess” que significa locally estimated scatterplot smoothing, por tanto, es fácil deducir que debe haber otros métodos de suavizamiento. Observa las siguientes líneas de código:Aquí, hemos añadido un argumento adicional, method=, para indicar un suavizamiento dado por un modelo lineal, lm, de linear model. Tenemos pues que entre el número de casos positivos y el número promedio de grados cursados hay una aparente relación positiva. ¿Qué hay para el caso de las defunciones:Si bien hay un poco más de dispersión, la aparente relación positiva se mantiene. ¿Qué crees que explique esta asociación? ¿Las personas con más años estudiados son susceptibles de contagiarse más fácil de COVID19? ¿O quizá se relacionará más con las diferencias que nivel territorial se dan entre los niveles de estudio de la población en el Valle de México?EjercicioElabora algunas gráficas de dispersión para responder las siguientes preguntas:¿Qué tipo de asociación lineal hay entre los casos positivos y defunciones por COVID19 con las características de la población según el número de dormitorios de las casas, el número de personas que habitan en cada casa y la densidad de población?¿Qué tipo de asociación lineal hay entre los casos positivos y defunciones por COVID19 con las características de la población según el número de dormitorios de las casas, el número de personas que habitan en cada casa y la densidad de población?¿Encuentras algún tipo de asociación entre las variables COVID19 y las características económicas de los municipios de la Zona Metropolitana del Valle de México?¿Encuentras algún tipo de asociación entre las variables COVID19 y las características económicas de los municipios de la Zona Metropolitana del Valle de México?Quizá lo que te interese sólo sea la asociación entre pares de variables, sino el comportamiento individual de cada variable para conocer la forma de su distribución e identificar algunos valores extremos o atípicos (los llamados ourliers). Para ello, convendrá un tipo de gráfica que permita representar sólo una variable continúa (casos positivos o defunciones por COVID19), tal como un histograma, una gráfica de densidad o de caja.Los tres gráficos dan cuenta que la forma de la distribución muestra un marcado sesgo positivo (la derecha), lo que significa que hay algunos municipios o alcaldías con valores muy altos para esta variable, además de algunos valores extremadamente altos según el diagrama de caja, ¿cuáles podrán ser y qué explicaría este comportamiento atípicamente alto?EjercicioConsulta la ayuda de cada una de las funciones de geometría e intenta:Elaborar un histograma con diferentes categorías (bins). ¿Cambia esto la forma de la distribución?Elaborar un histograma con diferentes categorías (bins). ¿Cambia esto la forma de la distribución?Señalar de un color diferente las observaciones atípicas en el diagrama de caja ¿Hay manera de identificar qué alcaldías o municipios pertenecen dichos valores atípicos?Señalar de un color diferente las observaciones atípicas en el diagrama de caja ¿Hay manera de identificar qué alcaldías o municipios pertenecen dichos valores atípicos?Además, también es posible modificar las etiquetas de los ejes:Nuestra base de datos contiene una variable de tipo categórico, grado de marginación (gm_2020), asociada al índice de marginación (im). Para este tipo de variable puede ser conveniente construir un gráfico de barras especialmente diseñado para variables categóricas echando mano de la geometría geom_bar():Este gráfico revela que, de acuerdo con la clasificación del Consejo Nacional de Población y Vivienda (CONAPO), la gran mayoría de los municipios y alcaldías que componen el Valle de México están catalogados como de muy bajo grado de marginación.EjercicioObtén una gráfica de barras donde se muestre el número de unidades administrativas (municipios o alcaldías) por de cada una de las entidades que integran la Zona Metropolitana del Valle de México.Ahora bien, ¿será posible incorporar en un plano bidimensional una tercera variable? ¿Cómo añadirías un diagrama de dispersión, que relaciona dos variables, una tercera? Este instrumento es veces denominado gráfico de burbujas y si lo piensas con cuidado es una solución muy ingeniosa para añadir información extra sin renunciar la simpleza. Para hacerlo, basta con añadir el argumento size= dentro de los elementos estéticos de la función geom_point():EjercicioResponde lo siguiente:Prueba cambiar la variable que define el tamaño del círculo por nuestra variable categórica grado de marginación, ¿es recomendable usar una variable de este tipo en esta representación?Prueba cambiar la variable que define el tamaño del círculo por nuestra variable categórica grado de marginación, ¿es recomendable usar una variable de este tipo en esta representación?¿Qué papel juega el argumento alpha= en la segunda gráfica?¿Qué papel juega el argumento alpha= en la segunda gráfica?¿Cuál es la diferencia en colocar el argumento color dentro o fuera del grupo de elementos estéticos (aes())? ¿Notaste la diferencia en el par de gráficas anteriores? Prueba cambiar el argumento color dentro de aes() por otra variable y el color de los círculos.¿Cuál es la diferencia en colocar el argumento color dentro o fuera del grupo de elementos estéticos (aes())? ¿Notaste la diferencia en el par de gráficas anteriores? Prueba cambiar el argumento color dentro de aes() por otra variable y el color de los círculos.Finalmente, elaboremos una gráfica que nos ayude sintetizar pares de asociaciones entre variables (diagramas de dispersión) y formas de distribución través de un gráfico muy elegante construido con una extensión del paquete ggplot2: GGally. Para instalarlo, como es usual:Luego, para poder usarlo, lo llamamos al entorno de trabajo:La gráfica que buscamos es una matriz de diagramas de dispersión y la construiremos con la función ggpairs() y sólo dos argumentos: la fuente de datos y las variables que deseamos incluir. Veamos:La matriz de diagramas de dispersión construida con ggpairs es, desde mi personal punto de vista, sólo muy elegante, sino que ofrece elementos informativos en el gráfico que antes teníamos, tales como el coeficiente de correlación lineal y su nivel de significancia.Antes de pasar tratar otro elemento, hay que decir que RStudio permite exportar las gráficas hechas, tanto en diversos formatos de imagen (jpg, png, tiff) como en PDF.EjercicioElabora tu propia matriz de diagramas de dispersión con las variables que, tu juicio, resulten más relevantes para explicar la dinámica tanto de las muertes como de los casos positivos de COVID19.Tareas relacionadas con la preparación y manipulación de datos, que en una hoja de cálculo como Excel de Office o Calc de LibreOffice, son rutinarias y muy sencillas, veces en R pueden significar un dolor de cabeza. obstante, dentro de los paquetes de la familia tidyverse encontramos dplyr. Este paquete brinda herramientas de manipulación de datos basadas en “gramática”, que es otra cosa que funciones (“verbos”) que permiten seleccionar o filtrar elementos en una base de datos, o bien, agrupar y crear nuevas variables.Si deseas profundizar en el conocimiento de este potente paquete consulta la Introducción al paquete dplyr, o bien, su Viñeta de ayuda.La base de datos que cargamos contiene 55 columnas, es decir, 55 variables. Quizá para su análisis te sean de interés todas, por lo que querrías generar una base de datos más compacta mediante la selección de algunas que son de utilidad. Para llevar cabo dicha tarea echaremos mano de la función select(), pero antes de hacerlo, es necesario introducir el operador “tubería”, pipe (%>%).Este operador sirve para indicar cada fase del proceso por el que un objeto es tratado través de la aplicación de diferentes funciones u operaciones. Por ejemplo, queremos que la base covid_zmvm “pase” por un proceso que consiste en la selección de sólo algunas variables. Comencemos usando el “verbo” seleccionar, select(), para una sola variable, por ejemplo pos_hab:La instrucción anterior es equivalente escribir lo siguiente:Así, usar el operador tubería permite sólo ahorrar la escritura de algunos argumentos, sino simplificar la forma en que las instrucciones dadas R son leídas por un ser humano. Veamos cómo seleccionar ahora, por ejemplo, tres variables: la clave de municipio (cvemun), los casos positivos (pos_hab) y el grado de marginación (gm_2020):Ahora, imagina que queremos todas las variables excepto el nombre de la Zona Metropolitana, nom_zm. Sería ocioso colocar todos los nombres de las variables menos el citado. Para hacer esto más eficiente es necesario introducir solamente un signo de menos (-) antes del nombre de la variable, para de esta sencilla forma seleccionar todas las variables menos nom_zm.En lugar de usar los nombres de las variables, puedes recurrir al número de columna en la que se hallan, contando partir del 1. Por ejemplo, nom_mun es la columna 3 y pos_hab es la columna 16, entonces podríamos escribir:Y si es de nuestro interés seleccionar un número consecutivo de columnas, podemos servirnos de:EjercicioPara finalizar lo relacionado con la selección, genera una nueva base de datos llamada covid que contenga las siguientes variables: cvemun, nom_mun, cve_ent, nom_ent, ext, pos_hab, def_hab, ocviv, occu, ppob_1, ppob_1dorm, im, gm_2020, grad, poind, pocom, poss, tmind, tmcom, tmss, rmind, rmcom, rmss, den, pob20.Si ahora lo que te interesa es seleccionar filas o casos en lugar de columnas, hay que echar mano de otro “verbo”, es decir, de la función filtrar, filter(). Por ejemplo, imagina que deseamos seleccionar sólo los municipios del Estado de México. Estos municipios cumplen con la condición de que todos ellos tienen el valor “México” en la variable nom_ent (nombre de entidad), o bien, tener el valor “15” en el de cve_ent (clave de entidad). Para poder hacer un uso eficiente de la función de filtrado es necesario introducir también otro tipo de operadores, los llamados operadores lógicos, que aparecen en el cuadro 1.2:Cuadro 1.2Entonces, para usar la función filter() y tomando como base la información previa:Para hacer una selección de dos entidades, incluiremos dos expresiones en una misma consulta echando mano del operador lógico “o”, identificado con el signo |:EjercicioSelecciona los municipios con una extensión territorial mayor 84 \\(km^2\\).Selecciona los municipios con una extensión territorial mayor 84 \\(km^2\\).Selecciona los municipios con entre 10 y 20 casos positivos por cada mil habitantes.Selecciona los municipios con entre 10 y 20 casos positivos por cada mil habitantes.Selecciona los municipios del Estado de México que tengan más de 10 años promedio de estudio y menos de 10 casos positivos por cada mil habitantes.Selecciona los municipios del Estado de México que tengan más de 10 años promedio de estudio y menos de 10 casos positivos por cada mil habitantes.¿Qué otros filtros relevantes podrías pensar y elaborar?Una operación útil, cuando se analiza información, tiene que ver con ordenar la base de datos con arreglo al valor de una variable. En R con dplyr de tidyverse esto se hace con la función arrange(), por ejemplo:El resultado de la función anterior ordena nuestra base de menor mayor, con base en la clave municipal. Además, es posible incluir varios criterios de ordenación, por ejemplo, si tuvieras los campos año, mes y día podría ordenarse conforme al calendario. En el caso del ejemplo siguiente, primero se ordena con arreglo la clave de entidad y luego con respecto la clave municipal y luego por la extensión territorial por cada mil habitantes:Si deseas ordenar de forma descendente, es decir, de mayor menor hay que introducir una función adicional, desc(), por ejemplo:La función anterior primero ordena los municipios con base en la clave de municipio de forma ascendente (de menor mayor) y simultáneamente coloca de mayor menor los municipios de acuerdo con su extensión territorial.EjerciciosGenera una nueva base de datos (un nuevo objeto) que contenga sólo las alcaldías de la Ciudad de México y las variables casos por cada mil habitantes, luego ordena esa base de datos con arreglo al número de casos positivos de forma ascendente.Genera una nueva base de datos (un nuevo objeto) que contenga sólo las alcaldías de la Ciudad de México y las variables casos por cada mil habitantes, luego ordena esa base de datos con arreglo al número de casos positivos de forma ascendente.lo mismo que en el inciso anterior, pero para los municipios del Estado de México y las defunciones.lo mismo que en el inciso anterior, pero para los municipios del Estado de México y las defunciones.Para añadir una nueva variable partir de las existentes recurrimos la función mutate(). Por ejemplo, calculemos la variable densidad de casos positivos, es decir, número de casos positivos dividido entre la extensión territorial, dicha variable la llamaremos pos_den. Pero antes, generemos una nueva base de datos que sólo contenga algunas de las variables de la base original:Notarás como la nueva variable aparece al final de la base. Vamos intentar unir varios de los elementos que hemos aprendido hasta este punto para que veas el potencial de uso de la tubería. Vamos crear una nueva variable, la misma que hace un momento, luego, tomaremos la base que contiene la nueva variable y construiremos con ella un diagrama de dispersión con la nueva variable y el número de defunciones por cada mil habitantes:¿Cómo explicarías la relación entre la variable creada, densidad de casos positivos, y las defunciones por cada mil habitables?Ejercicio¿Cómo añadirías más de una nueva variable tu base original?¿Cómo añadirías más de una nueva variable tu base original?Construye la variable “densidad de defunciones” y grafícala en un diagrama de dispersión con la densidad de población? Usa las cañerías.Construye la variable “densidad de defunciones” y grafícala en un diagrama de dispersión con la densidad de población? Usa las cañerías.Quizá lo que te interese sea sólo quedarte con algunas de las variables originales y las nuevas variables creadas partir de la información de la variable original. Para ello es útil la función transmute():El segmento de código anterior genera una nueva base en la que sólo conservamos algunas de las variables originales y añadimos dos nuevas partir de la información original.Quizá una de las funciones más potentes y sencillas que tiene dplyr es la función de resumen (summarise()). Veamos cómo opera. Imagina que deseas un promedio del número de casos que terminaron en muerte:Esto es particularmente útil si se le compara con la función del paquete base de R, summary(), pero si la combinamos con la función de agrupamiento, group_by() la situación cambia. Imagina que queremos el promedio de casos positivos por cada mil habitantes para cada conjunto de municipios de las tres entidades que componen el Valle de México:El en segmento de código anterior primero se agrupa la información con arreglo al criterio de clave de entidad, cve_ent, y luego, para cada grupo, calcula el resultado especificado: un promedio. Dentro de la función de resumen, summarise(), se puede llevar cabo sólo múltiples operaciones sino que estas pueden ser de diferente naturaleza, incluso por ejemplo una suma o una cuenta:En el segmento de código anterior creamos tres variables de resumen: la población total, pob_tot, que es la suma de la población para cada entidad, el promedio del índice de marginación, im_medio y el número de municipios de cada entidad través de una cuenta con la función n().EjercicioConstruye una gráfica de barras donde aparezca el total de población para cada entidad, partir del segmento código anterior.Construye una gráfica de barras donde aparezca el total de población para cada entidad, partir del segmento código anterior.¿Es posible agrupar la información con arreglo otra variable? Construye algunas medidas de resumen con esos grupos y gráficalos.¿Es posible agrupar la información con arreglo otra variable? Construye algunas medidas de resumen con esos grupos y gráficalos.Estas y otras tantas funciones son desarrolladas con todo detalle y de forma muy amena en el citado libro de Wickham y Grolemund, R data science, por lo que su estudio se recomienda ampliamente.En el siguiente capítulo continuamos con la exploración de la información, pero de un tipo particular de información: la información espacial.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2022-01-19.book built bookdown R package.","code":"\ndata()\ndata(CO2)\ninstall.packages(\"tidyverse\")\nlibrary(readxl)## Warning: package 'readxl' was built under R version 4.1.2\ncovid_zmvm <- readxl::read_excel(path=\"base de datos\\\\covid_zmvm.xlsx\")\nbase::file.choose()\nlibrary(readr)## Warning: package 'readr' was built under R version 4.1.2\nruta <- base::file.choose()\ncovid_zmvm <- utils::read.csv(ruta)\nbase::names(covid_zmvm) #Nombres de las variables##  [1] \"cvemun\"     \"cve_mun\"    \"nom_mun\"    \"cve_ent\"    \"nom_ent\"   \n##  [6] \"nom_abr\"    \"nom_zm\"     \"cvm\"        \"ext\"        \"pob20\"     \n## [11] \"pob20_h\"    \"pob20_m\"    \"positivos\"  \"defuncione\" \"pos_mil\"   \n## [16] \"pos_hab\"    \"def_hab\"    \"ss\"         \"ppob_sines\" \"ppob_basi\" \n## [21] \"ppob_media\" \"ppob_sup\"   \"ocviv\"      \"occu\"       \"pintegra4_\"\n## [26] \"pintegra6_\" \"pintegra8_\" \"ppob_5_o_m\" \"ppob_3_o_m\" \"ppob_1\"    \n## [31] \"ppob_1dorm\" \"ppob_2dorm\" \"ppob_3dorm\" \"pviv_ocu5_\" \"pviv_ocu7_\"\n## [36] \"pviv_ocu9_\" \"analf\"      \"sbasc\"      \"vhac\"       \"po2sm\"     \n## [41] \"im\"         \"gm_2020\"    \"grad\"       \"grad_h\"     \"grad_m\"    \n## [46] \"poind\"      \"pocom\"      \"poss\"       \"tmind\"      \"tmcom\"     \n## [51] \"tmss\"       \"rmind\"      \"rmcom\"      \"rmss\"       \"den\"\nutils::str(covid_zmvm) #Estructura de la base de datos## tibble [76 x 55] (S3: tbl_df/tbl/data.frame)\n##  $ cvemun    : chr [1:76] \"09010\" \"09012\" \"09015\" \"09017\" ...\n##  $ cve_mun   : chr [1:76] \"010\" \"012\" \"015\" \"017\" ...\n##  $ nom_mun   : chr [1:76] \"Álvaro Obregón\" \"Tlalpan\" \"Cuauhtémoc\" \"Venustiano Carranza\" ...\n##  $ cve_ent   : chr [1:76] \"09\" \"09\" \"09\" \"09\" ...\n##  $ nom_ent   : chr [1:76] \"Ciudad de México\" \"Ciudad de México\" \"Ciudad de México\" \"Ciudad de México\" ...\n##  $ nom_abr   : chr [1:76] \"CDMX\" \"CDMX\" \"CDMX\" \"CDMX\" ...\n##  $ nom_zm    : chr [1:76] \"Valle de México\" \"Valle de México\" \"Valle de México\" \"Valle de México\" ...\n##  $ cvm       : num [1:76] 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 ...\n##  $ ext       : num [1:76] 96.2 310.4 32.5 33.9 85.8 ...\n##  $ pob20     : num [1:76] 759137 699928 545884 443704 392313 ...\n##  $ pob20_h   : num [1:76] 361007 334877 260951 210118 190190 ...\n##  $ pob20_m   : num [1:76] 398130 365051 284933 233586 202123 ...\n##  $ positivos : num [1:76] 10905 11887 7289 7172 6812 ...\n##  $ defuncione: num [1:76] 868 542 754 585 289 713 648 397 190 391 ...\n##  $ pos_mil   : num [1:76] 14.4 17 13.4 16.2 17.4 ...\n##  $ pos_hab   : num [1:76] 14.4 17 13.4 16.2 17.4 ...\n##  $ def_hab   : num [1:76] 1.143 0.774 1.381 1.318 0.737 ...\n##  $ ss        : num [1:76] 75 71.1 71.6 71.6 72.7 ...\n##  $ ppob_sines: num [1:76] 0.0204 0.0195 0.0119 0.0121 0.0193 ...\n##  $ ppob_basi : num [1:76] 0.41 0.391 0.307 0.379 0.469 ...\n##  $ ppob_media: num [1:76] 0.252 0.253 0.251 0.291 0.299 ...\n##  $ ppob_sup  : num [1:76] 0.314 0.335 0.427 0.316 0.211 ...\n##  $ ocviv     : num [1:76] 3.45 3.44 2.75 3.26 3.67 3.21 3.2 3.74 3.6 2.81 ...\n##  $ occu      : num [1:76] 0.81 0.81 0.72 0.82 0.93 0.77 0.7 0.91 0.81 0.66 ...\n##  $ pintegra4_: num [1:76] 0.648 0.644 0.48 0.609 0.699 ...\n##  $ pintegra6_: num [1:76] 0.252 0.226 0.142 0.219 0.262 ...\n##  $ pintegra8_: num [1:76] 0.1071 0.0864 0.0422 0.0798 0.0982 ...\n##  $ ppob_5_o_m: num [1:76] 0.726 0.73 0.888 0.821 0.792 ...\n##  $ ppob_3_o_m: num [1:76] 0.32 0.315 0.363 0.354 0.354 ...\n##  $ ppob_1    : num [1:76] 0.0369 0.0499 0.0249 0.0247 0.0513 ...\n##  $ ppob_1dorm: num [1:76] 0.211 0.212 0.199 0.202 0.209 ...\n##  $ ppob_2dorm: num [1:76] 0.566 0.547 0.8 0.696 0.614 ...\n##  $ ppob_3dorm: num [1:76] 0.832 0.848 0.947 0.89 0.864 ...\n##  $ pviv_ocu5_: num [1:76] 0.226 0.22 0.127 0.198 0.267 ...\n##  $ pviv_ocu7_: num [1:76] 0.0641 0.0568 0.026 0.0513 0.0701 ...\n##  $ pviv_ocu9_: num [1:76] 0.02334 0.01815 0.00645 0.01593 0.02167 ...\n##  $ analf     : num [1:76] 1.574 1.606 0.953 1.085 1.673 ...\n##  $ sbasc     : num [1:76] 19 18.2 13.5 16.9 20.3 ...\n##  $ vhac      : num [1:76] 15.25 15.19 9.42 14.43 18.3 ...\n##  $ po2sm     : num [1:76] 49.3 61.2 41.4 57.2 65.4 ...\n##  $ im        : num [1:76] 60.4 59.5 61.3 60.3 59.3 ...\n##  $ gm_2020   : chr [1:76] \"Muy Bajo\" \"Muy Bajo\" \"Muy Bajo\" \"Muy Bajo\" ...\n##  $ grad      : num [1:76] 11.3 11.5 12.4 11.5 10.5 ...\n##  $ grad_h    : num [1:76] 11.5 11.7 12.8 11.7 10.7 ...\n##  $ grad_m    : num [1:76] 11.1 11.4 12.1 11.3 10.4 ...\n##  $ poind     : num [1:76] 0.0912 0.0874 0.1334 0.0714 0.2442 ...\n##  $ pocom     : num [1:76] 0.16 0.185 0.166 0.289 0.394 ...\n##  $ poss      : num [1:76] 0.749 0.728 0.7 0.64 0.361 ...\n##  $ tmind     : num [1:76] 20.32 8.9 27.57 6.57 6.42 ...\n##  $ tmcom     : num [1:76] 5.95 3.48 4.36 2.66 2.12 ...\n##  $ tmss      : num [1:76] 27.69 13.91 24.87 9.94 2.59 ...\n##  $ rmind     : num [1:76] 68.2 116.7 55.9 64.9 78.3 ...\n##  $ rmcom     : num [1:76] 50.6 31.3 45.5 31.2 23.5 ...\n##  $ rmss      : num [1:76] 178.3 36.3 255.6 101.4 23.4 ...\n##  $ den       : num [1:76] 7895 2255 16786 13104 4575 ...\nbase::dim(covid_zmvm) #Dimensiones de la base## [1] 76 55\nutils::View(covid_zmvm)\n#Que es equivalente a:\nView(covid_zmvm)\nbase::print(covid_zmvm)## # A tibble: 76 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 759137\n##  2 09012  012     Tlalpan    09      Ciudad ~ CDMX    Valle ~  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc 09      Ciudad ~ CDMX    Valle ~  9.01  32.5 545884\n##  4 09017  017     Venustian~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.9 443704\n##  5 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 392313\n##  6 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 432205\n##  7 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 614447\n##  8 09013  013     Xochimilco 09      Ciudad ~ CDMX    Valle ~  9.01 118.  442178\n##  9 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 217686\n## 10 09016  016     Miguel Hi~ 09      Ciudad ~ CDMX    Valle ~  9.01  46.4 414470\n## # ... with 66 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\nutils::head(covid_zmvm) #Para los primeros casos## # A tibble: 6 x 55\n##   cvemun cve_mun nom_mun    cve_ent nom_ent   nom_abr nom_zm    cvm   ext  pob20\n##   <chr>  <chr>   <chr>      <chr>   <chr>     <chr>   <chr>   <dbl> <dbl>  <dbl>\n## 1 09010  010     Álvaro Ob~ 09      Ciudad d~ CDMX    Valle ~  9.01  96.2 759137\n## 2 09012  012     Tlalpan    09      Ciudad d~ CDMX    Valle ~  9.01 310.  699928\n## 3 09015  015     Cuauhtémoc 09      Ciudad d~ CDMX    Valle ~  9.01  32.5 545884\n## 4 09017  017     Venustian~ 09      Ciudad d~ CDMX    Valle ~  9.01  33.9 443704\n## 5 09011  011     Tláhuac    09      Ciudad d~ CDMX    Valle ~  9.01  85.8 392313\n## 6 09002  002     Azcapotza~ 09      Ciudad d~ CDMX    Valle ~  9.01  33.5 432205\n## # ... with 45 more variables: pob20_h <dbl>, pob20_m <dbl>, positivos <dbl>,\n## #   defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>, def_hab <dbl>, ss <dbl>,\n## #   ppob_sines <dbl>, ppob_basi <dbl>, ppob_media <dbl>, ppob_sup <dbl>,\n## #   ocviv <dbl>, occu <dbl>, pintegra4_ <dbl>, pintegra6_ <dbl>,\n## #   pintegra8_ <dbl>, ppob_5_o_m <dbl>, ppob_3_o_m <dbl>, ppob_1 <dbl>,\n## #   ppob_1dorm <dbl>, ppob_2dorm <dbl>, ppob_3dorm <dbl>, pviv_ocu5_ <dbl>,\n## #   pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, analf <dbl>, sbasc <dbl>, ...\nutils::tail(covid_zmvm) #Para los últimos casos## # A tibble: 6 x 55\n##   cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm    ext  pob20\n##   <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl>  <dbl>  <dbl>\n## 1 15120  120     Zumpango   15      México   Mex.    Valle ~  9.01 224.   2.80e5\n## 2 15121  121     Cuautitlá~ 15      México   Mex.    Valle ~  9.01 110.   5.55e5\n## 3 15122  122     Valle de ~ 15      México   Mex.    Valle ~  9.01  46.6  3.92e5\n## 4 15125  125     Tonanitla  15      México   Mex.    Valle ~  9.01   9.04 1.49e4\n## 5 15058  058     Nezahualc~ 15      México   Mex.    Valle ~  9.01  63.3  1.08e6\n## 6 09005  005     Gustavo A~ 09      Ciudad ~ CDMX    Valle ~  9.01  87.9  1.17e6\n## # ... with 45 more variables: pob20_h <dbl>, pob20_m <dbl>, positivos <dbl>,\n## #   defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>, def_hab <dbl>, ss <dbl>,\n## #   ppob_sines <dbl>, ppob_basi <dbl>, ppob_media <dbl>, ppob_sup <dbl>,\n## #   ocviv <dbl>, occu <dbl>, pintegra4_ <dbl>, pintegra6_ <dbl>,\n## #   pintegra8_ <dbl>, ppob_5_o_m <dbl>, ppob_3_o_m <dbl>, ppob_1 <dbl>,\n## #   ppob_1dorm <dbl>, ppob_2dorm <dbl>, ppob_3dorm <dbl>, pviv_ocu5_ <dbl>,\n## #   pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, analf <dbl>, sbasc <dbl>, ...\nbase::ncol(covid_zmvm) #Para conocer el número de columnas## [1] 55\nbase::nrow(covid_zmvm) #Para conocer el número de filas## [1] 76\nlibrary(tidyverse)\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab))\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab))+\n  ggplot2::geom_smooth(aes(x=grad,y=pos_hab))\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab))+\n  ggplot2::geom_smooth(aes(x=grad,y=pos_hab), method = \"lm\")\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=def_hab))+\n  ggplot2::geom_smooth(aes(x=grad,y=def_hab), method = \"lm\")\n#Histograma para los casos positivos por cada mil habitantes\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_histogram(aes(x=pos_hab))\n#Gráfico de densidad para los casos positivos por cada mil habitantes\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_density(aes(x=pos_hab))\n#Gráfico de caja para los casos positivos por cada mil habitantes\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_boxplot(aes(x=pos_hab))\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_histogram(aes(x=pos_hab))+\n  ggplot2::labs(x=\"Casos positivos por cada mil habitantes\", y=\"Frecuencia\")## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nggplot2::ggplot(covid_zmvm)+\n ggplot2:: geom_bar(aes(x=gm_2020))+\n  ggplot2::labs(x=\"Grado de marginación 2020\", y=\"Frecuencia\")\n#Si tienes la librería ggplot2 cargada en el enorno de trabajo, alternativamente, puedes escribir:\nggplot(covid_zmvm)+\n geom_bar(aes(x=gm_2020))+\n  labs(x=\"Grado de marginación 2020\", y=\"Frecuencia\")\n#Tamaño del circulo dado por los años estudiados\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab,size=def_hab))\n#Transparencia del círculo dado por los años estudiados\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab,alpha=def_hab, size=def_hab), color=\"navyblue\")\n#Transparencia del círculo dado por los años estudiados\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab,color=def_hab))\ninstall.packages(\"GGally\")\nlibrary(GGally)\n#Definiendo los nombres de las variables\nGGally::ggpairs(data=covid_zmvm, columns = c(\"pos_hab\",\"def_hab\",\"im\",\"grad\"))\n#Indicando el número de columna según el lugar que ocupa en la base\nGGally::ggpairs(data=covid_zmvm, columns = c(16,17,41,43))\ncovid_zmvm %>% dplyr::select(pos_hab)## # A tibble: 76 x 1\n##    pos_hab\n##      <dbl>\n##  1    14.4\n##  2    17.0\n##  3    13.4\n##  4    16.2\n##  5    17.4\n##  6    17.3\n##  7    14.9\n##  8    17.4\n##  9    18.7\n## 10    13.7\n## # ... with 66 more rows\ndplyr::select(covid_zmvm,pos_hab)## # A tibble: 76 x 1\n##    pos_hab\n##      <dbl>\n##  1    14.4\n##  2    17.0\n##  3    13.4\n##  4    16.2\n##  5    17.4\n##  6    17.3\n##  7    14.9\n##  8    17.4\n##  9    18.7\n## 10    13.7\n## # ... with 66 more rows\ncovid_zmvm %>% dplyr::select(cvemun,pos_hab,gm_2020)## # A tibble: 76 x 3\n##    cvemun pos_hab gm_2020 \n##    <chr>    <dbl> <chr>   \n##  1 09010     14.4 Muy Bajo\n##  2 09012     17.0 Muy Bajo\n##  3 09015     13.4 Muy Bajo\n##  4 09017     16.2 Muy Bajo\n##  5 09011     17.4 Muy Bajo\n##  6 09002     17.3 Muy Bajo\n##  7 09003     14.9 Muy Bajo\n##  8 09013     17.4 Muy Bajo\n##  9 09004     18.7 Muy Bajo\n## 10 09016     13.7 Muy Bajo\n## # ... with 66 more rows\ncovid_zmvm %>% dplyr::select(-nom_zm)## # A tibble: 76 x 54\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr   cvm   ext  pob20 pob20_h\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <dbl> <dbl>  <dbl>   <dbl>\n##  1 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX     9.01  96.2 759137  361007\n##  2 09012  012     Tlalpan    09      Ciudad ~ CDMX     9.01 310.  699928  334877\n##  3 09015  015     Cuauhtémoc 09      Ciudad ~ CDMX     9.01  32.5 545884  260951\n##  4 09017  017     Venustian~ 09      Ciudad ~ CDMX     9.01  33.9 443704  210118\n##  5 09011  011     Tláhuac    09      Ciudad ~ CDMX     9.01  85.8 392313  190190\n##  6 09002  002     Azcapotza~ 09      Ciudad ~ CDMX     9.01  33.5 432205  204950\n##  7 09003  003     Coyoacán   09      Ciudad ~ CDMX     9.01  53.9 614447  289110\n##  8 09013  013     Xochimilco 09      Ciudad ~ CDMX     9.01 118.  442178  215452\n##  9 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX     9.01  71.2 217686  104149\n## 10 09016  016     Miguel Hi~ 09      Ciudad ~ CDMX     9.01  46.4 414470  195467\n## # ... with 66 more rows, and 44 more variables: pob20_m <dbl>, positivos <dbl>,\n## #   defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>, def_hab <dbl>, ss <dbl>,\n## #   ppob_sines <dbl>, ppob_basi <dbl>, ppob_media <dbl>, ppob_sup <dbl>,\n## #   ocviv <dbl>, occu <dbl>, pintegra4_ <dbl>, pintegra6_ <dbl>,\n## #   pintegra8_ <dbl>, ppob_5_o_m <dbl>, ppob_3_o_m <dbl>, ppob_1 <dbl>,\n## #   ppob_1dorm <dbl>, ppob_2dorm <dbl>, ppob_3dorm <dbl>, pviv_ocu5_ <dbl>,\n## #   pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, analf <dbl>, sbasc <dbl>, ...\ncovid_zmvm %>% dplyr::select(3,16)## # A tibble: 76 x 2\n##    nom_mun               pos_hab\n##    <chr>                   <dbl>\n##  1 Álvaro Obregón           14.4\n##  2 Tlalpan                  17.0\n##  3 Cuauhtémoc               13.4\n##  4 Venustiano Carranza      16.2\n##  5 Tláhuac                  17.4\n##  6 Azcapotzalco             17.3\n##  7 Coyoacán                 14.9\n##  8 Xochimilco               17.4\n##  9 Cuajimalpa de Morelos    18.7\n## 10 Miguel Hidalgo           13.7\n## # ... with 66 more rows\ncovid_zmvm %>% dplyr::select(1:10)## # A tibble: 76 x 10\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 759137\n##  2 09012  012     Tlalpan    09      Ciudad ~ CDMX    Valle ~  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc 09      Ciudad ~ CDMX    Valle ~  9.01  32.5 545884\n##  4 09017  017     Venustian~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.9 443704\n##  5 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 392313\n##  6 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 432205\n##  7 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 614447\n##  8 09013  013     Xochimilco 09      Ciudad ~ CDMX    Valle ~  9.01 118.  442178\n##  9 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 217686\n## 10 09016  016     Miguel Hi~ 09      Ciudad ~ CDMX    Valle ~  9.01  46.4 414470\n## # ... with 66 more rows\ncovid_zmvm %>% dplyr::filter(cve_ent==\"15\")## # A tibble: 59 x 55\n##    cvemun cve_mun nom_mun     cve_ent nom_ent nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>       <chr>   <chr>   <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 15002  002     Acolman     15      México  Mex.    Valle ~  9.01  83.9 171507\n##  2 15009  009     Amecameca   15      México  Mex.    Valle ~  9.01 189.   53441\n##  3 15010  010     Apaxco      15      México  Mex.    Valle ~  9.01  75.7  31898\n##  4 15011  011     Atenco      15      México  Mex.    Valle ~  9.01  84.6  75489\n##  5 15013  013     Atizapán d~ 15      México  Mex.    Valle ~  9.01  91.1 523674\n##  6 15015  015     Atlautla    15      México  Mex.    Valle ~  9.01 162.   31900\n##  7 15016  016     Axapusco    15      México  Mex.    Valle ~  9.01 231.   29128\n##  8 15017  017     Ayapango    15      México  Mex.    Valle ~  9.01  36.4  10053\n##  9 15020  020     Coacalco d~ 15      México  Mex.    Valle ~  9.01  35.1 293444\n## 10 15022  022     Cocotitlán  15      México  Mex.    Valle ~  9.01  15.0  15107\n## # ... with 49 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\n#O bien\ncovid_zmvm %>% dplyr::filter(nom_ent==\"México\")## # A tibble: 59 x 55\n##    cvemun cve_mun nom_mun     cve_ent nom_ent nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>       <chr>   <chr>   <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 15002  002     Acolman     15      México  Mex.    Valle ~  9.01  83.9 171507\n##  2 15009  009     Amecameca   15      México  Mex.    Valle ~  9.01 189.   53441\n##  3 15010  010     Apaxco      15      México  Mex.    Valle ~  9.01  75.7  31898\n##  4 15011  011     Atenco      15      México  Mex.    Valle ~  9.01  84.6  75489\n##  5 15013  013     Atizapán d~ 15      México  Mex.    Valle ~  9.01  91.1 523674\n##  6 15015  015     Atlautla    15      México  Mex.    Valle ~  9.01 162.   31900\n##  7 15016  016     Axapusco    15      México  Mex.    Valle ~  9.01 231.   29128\n##  8 15017  017     Ayapango    15      México  Mex.    Valle ~  9.01  36.4  10053\n##  9 15020  020     Coacalco d~ 15      México  Mex.    Valle ~  9.01  35.1 293444\n## 10 15022  022     Cocotitlán  15      México  Mex.    Valle ~  9.01  15.0  15107\n## # ... with 49 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\ncovid_zmvm %>% dplyr::filter(cve_ent==\"09\"|cve_ent==\"15\")## # A tibble: 75 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 759137\n##  2 09012  012     Tlalpan    09      Ciudad ~ CDMX    Valle ~  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc 09      Ciudad ~ CDMX    Valle ~  9.01  32.5 545884\n##  4 09017  017     Venustian~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.9 443704\n##  5 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 392313\n##  6 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 432205\n##  7 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 614447\n##  8 09013  013     Xochimilco 09      Ciudad ~ CDMX    Valle ~  9.01 118.  442178\n##  9 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 217686\n## 10 09016  016     Miguel Hi~ 09      Ciudad ~ CDMX    Valle ~  9.01  46.4 414470\n## # ... with 65 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\ncovid_zmvm %>% dplyr::arrange(cvemun)## # A tibble: 76 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A~ 09      Ciudad ~ CDMX    Valle ~  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco  09      Ciudad ~ CDMX    Valle ~  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa 09      Ciudad ~ CDMX    Valle ~  9.01 113.  1.84e6\n##  7 09008  008     La Magdal~ 09      Ciudad ~ CDMX    Valle ~  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta 09      Ciudad ~ CDMX    Valle ~  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 3.92e5\n## # ... with 66 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\ncovid_zmvm %>% dplyr::arrange(cvemun,ext)## # A tibble: 76 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A~ 09      Ciudad ~ CDMX    Valle ~  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco  09      Ciudad ~ CDMX    Valle ~  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa 09      Ciudad ~ CDMX    Valle ~  9.01 113.  1.84e6\n##  7 09008  008     La Magdal~ 09      Ciudad ~ CDMX    Valle ~  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta 09      Ciudad ~ CDMX    Valle ~  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 3.92e5\n## # ... with 66 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\ncovid_zmvm %>% dplyr::arrange(cvemun,desc(ext))## # A tibble: 76 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A~ 09      Ciudad ~ CDMX    Valle ~  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco  09      Ciudad ~ CDMX    Valle ~  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa 09      Ciudad ~ CDMX    Valle ~  9.01 113.  1.84e6\n##  7 09008  008     La Magdal~ 09      Ciudad ~ CDMX    Valle ~  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta 09      Ciudad ~ CDMX    Valle ~  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 3.92e5\n## # ... with 66 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\ncovid_zmvm %>% dplyr::select(cvemun,nom_mun,positivos,pos_hab,ext) %>%\n  dplyr::mutate(pos_den=positivos/ext)## # A tibble: 76 x 6\n##    cvemun nom_mun               positivos pos_hab   ext pos_den\n##    <chr>  <chr>                     <dbl>   <dbl> <dbl>   <dbl>\n##  1 09010  Álvaro Obregón            10905    14.4  96.2   113. \n##  2 09012  Tlalpan                   11887    17.0 310.     38.3\n##  3 09015  Cuauhtémoc                 7289    13.4  32.5   224. \n##  4 09017  Venustiano Carranza        7172    16.2  33.9   212. \n##  5 09011  Tláhuac                    6812    17.4  85.8    79.4\n##  6 09002  Azcapotzalco               7483    17.3  33.5   223. \n##  7 09003  Coyoacán                   9182    14.9  53.9   170. \n##  8 09013  Xochimilco                 7696    17.4 118.     65.1\n##  9 09004  Cuajimalpa de Morelos      4071    18.7  71.2    57.2\n## 10 09016  Miguel Hidalgo             5669    13.7  46.4   122. \n## # ... with 66 more rows\ncovid_zmvm %>% \n  dplyr::mutate(pos_den=positivos/ext) %>% \n  ggplot2::ggplot()+\n    ggplot2::geom_point(aes(x=pos_den,y=def_hab))+\n    ggplot2::geom_smooth(aes(x=pos_den,y=def_hab))+\n    ggplot2::labs(x=\"Densidad de casos positivos\",y=\"Defunciones por cada mil habitantes\")\ncovid_zmvm %>% \n  dplyr::transmute(cvemun,nom_mun,pos_hab,def_hab,pos_den=positivos/ext,def_den=defuncione/ext)## # A tibble: 76 x 6\n##    cvemun nom_mun               pos_hab def_hab pos_den def_den\n##    <chr>  <chr>                   <dbl>   <dbl>   <dbl>   <dbl>\n##  1 09010  Álvaro Obregón           14.4   1.14    113.     9.03\n##  2 09012  Tlalpan                  17.0   0.774    38.3    1.75\n##  3 09015  Cuauhtémoc               13.4   1.38    224.    23.2 \n##  4 09017  Venustiano Carranza      16.2   1.32    212.    17.3 \n##  5 09011  Tláhuac                  17.4   0.737    79.4    3.37\n##  6 09002  Azcapotzalco             17.3   1.65    223.    21.3 \n##  7 09003  Coyoacán                 14.9   1.05    170.    12.0 \n##  8 09013  Xochimilco               17.4   0.898    65.1    3.36\n##  9 09004  Cuajimalpa de Morelos    18.7   0.873    57.2    2.67\n## 10 09016  Miguel Hidalgo           13.7   0.943   122.     8.43\n## # ... with 66 more rows\ncovid_zmvm %>%\n  dplyr::summarise(promedio_def=mean(def_hab, na.rm=TRUE))## # A tibble: 1 x 1\n##   promedio_def\n##          <dbl>\n## 1        0.683\ncovid_zmvm %>%\n  dplyr::group_by(cve_ent) %>% \n  summarise(promedio_def=mean(def_hab, na.rm=TRUE), promedio_pos=mean(pos_hab))## # A tibble: 3 x 3\n##   cve_ent promedio_def promedio_pos\n##   <chr>          <dbl>        <dbl>\n## 1 09             1.08         15.8 \n## 2 13             0.790         5.88\n## 3 15             0.572         4.57\ncovid_zmvm %>% \n  dplyr::group_by(cve_ent) %>%\n  dplyr::summarise(pob_tot=sum(pob20),im_medio=mean(im),tot_mun=n())## # A tibble: 3 x 4\n##   cve_ent  pob_tot im_medio tot_mun\n##   <chr>      <dbl>    <dbl>   <int>\n## 1 09       9209944     60.1      16\n## 2 13        168302     59.4       1\n## 3 15      12426269     58.0      59"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"cargar-un-archivo-de-excel-xls-xslx","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.3.1 Cargar un archivo de Excel (xls, xslx)","text":"tidyverse es un conjunto de paquetes diseñados especialmente para la Ciencia de Datos. Algunos de los paquetes de la familia tidyverse que usaremos aquí y un poco más adelante son:\n* ggplot2: es un sistema para crear gráficos basado en la llamada gramática de las gráficas.\n* dplyr: proporciona una serie de funciones para manipulación de datos. * readr: permite leer datos rectangualres provenientes de múltiples formatos.Como más adelante usaremos otros de los paquetes de la familia tidyverse instalaremos todos en este momento:Si deseas aprender cómo usar con detalle los paquetes de la familia tidyverse y elementos básicos de Ciencia de Datos, el libro de Hadley Wickham y Garrett Grolemund, R Data Science es una magnífica opción.Utilizando el paquete readxl de la familia tidyverse es posible cargar una base de datos en el formato de la popular hoja de cálculo de Microsoft Office. partir de aquí introduciremos una notación particular al usar una función en R, lo que te permitirá recordar qué paquete pertenece dicha función. La notación es: “paquete :: función()”, es decir, primero se colocará el nombre del paquete y, separado por dos pares de dos puntos, en el nombre de la función que pertenece dicho paquete (resulta claro que los paquetes están entonces integrados por múltiples funciones).Sigamos los siguientes pasos para cargar la base de datos covid_zmvm.xlsx que contiene información de los casos positivos y defunciones por COVID19 en los municipios de la Zona Metropolitana del Valle de México durante la primera ola de la pandemia, entre marzo septiembre de 2020, así como múltiples variables sociodemográficas y económicas.Llamemos específicamente al paquete que nos interesa:La función para cargar un libro de Excel es read_excel() y el argumento indispensable es la ruta o directorio donde está almacenado nuestro libro, el nombre y extensión del mismo, path. Creemos pues el objeto covid_zmvm:Para que puedas cargar satisfactoriamente la base, deberás sustituir la ruta por el directorio en el que está almacenado el archivo en tu equipo de cómputo. Una función útil para generar la cadena de texto de la ruta es file.choose(), del paquete base de R. Lleva la consola el código:y presiona enter, verás que aparece un cuadro de diálogo en el que deberás seleccionar el archivo deseado y luego pulsar “abrir”. El resultado aparecerá en tu consola como una cadena de texto entre comillas. Usa esa información para leer la base covid_zmvm.Revisa la ayuda de la función read_excel() para comprender todos los argumentos con los que puede operar la función y que te permitirán personalizar la carga de la base de datos en caso de que tengas múltiples hojas en el libro de Excel o desees rangos personalizados para importar.Ejercicio¿Quién es el autor o autora del paquete readxl?¿Quién es el autor o autora del paquete readxl?Descarga la hoja de trucos del paquete desde el sitio de tidyverse y responde, ¿el paquete sirve sólo para cargar información o es posible escribir y almacenar bases de datos con él?Descarga la hoja de trucos del paquete desde el sitio de tidyverse y responde, ¿el paquete sirve sólo para cargar información o es posible escribir y almacenar bases de datos con él?","code":"\ninstall.packages(\"tidyverse\")\nlibrary(readxl)## Warning: package 'readxl' was built under R version 4.1.2\ncovid_zmvm <- readxl::read_excel(path=\"base de datos\\\\covid_zmvm.xlsx\")\nbase::file.choose()"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"cargar-archivos-separados-por-comas-csv","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.3.2 Cargar archivos separados por comas (csv)","text":"Alternativamente, es posible que la base que deseemos cargar se encuentre en un formato diferente, por ejemplo, una base de datos con valores separados por comas (coma separate values, CSV). Para cargar una base en dicho formato, usaremos el paquete readr, que forma parte del paquete utilsLlama el paquete tu entorno de trabajo:Con la función anterior, file.choose() obtendremos la cadena de texto que indica la ruta del archivo cargar:Y, finalmente, cargamos la información con la función read.csv()","code":"\nlibrary(readr)## Warning: package 'readr' was built under R version 4.1.2\nruta <- base::file.choose()\ncovid_zmvm <- utils::read.csv(ruta)"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"cargar-bases-con-la-funcionalidad-importar","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.3.3 Cargar bases con la funcionalidad importar","text":"Otra manera de cargar archivos de Excel o de texto (formato CSV) es ubicarnos en la sección de ambiente, ventana superior derecha, y seleccionar el ícono Import Dataset. Se desplegará un menú donde habremos de elegir el tipo de archivo que se desee importar. R sólo permite importar archivos de texto o Excel, también bases de SPSS o STATA. Siguiendo con la ilustración relativa libros de Excel, en la ventana que se despliega, habremos de señalar la ruta exacta o directorio donde está nuestro archivo, incluyendo el nombre y extensión de éste. Al pulsar en el botón actulizar (Update) se desplegara una vista previa de la base. Si aparece de forma correcta, seleccionaremos importar (Import), en caso contrario, se deberá modificar las opciones de importación. Después de seleccionar Import Dataset/Excel y elegir la ubicación del archivo que deseas importar, deberías ver en tu pantalla una imagen como la de la figura 1.3:\nFigura 1.3: Importar datos de excel\n","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"exploración-inicial-a-través-de-gráficos","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.4 Exploración inicial a través de gráficos","text":"Ahora que revisado algunas de las diversas maneras de cargar tu información al entorno de trabajo en R, es momento de comenzar analizarla. Si importaste la base de datos través de la función readxl::read_excel() notarás que el nuestra base covid_zmvm un objeto de tipo tbl_df que es un subtipo de data.frame o arreglo de datos Como el que se mencionó en antes.Las siguientes instrucciones nos ayudarán conocer los nombres de las columnas, la estructura de los datos y la dimensión de la base:Recuerda la notación que mencionamos antes, “paquete::función”; además, si el paquete está cargado, es necesario especificar su nombre al usar la función; al arrancar cada sesión de R, un conjunto de paquetes se cargan de forma automática, dichos paquetes aparecen marcados con una palomita, \\(\\checkmark\\), en la pestaña Packages en la sección de archivo.Ahora bien, si deseas ver la base completa puedes llamarla una nueva pestaña que se visualizará en la sección de fuente. Usa la siguiente instrucción:Alternativamente, puedes llamar la base para que aparezca en la consola, lo que siempre es recomendable si la base es muy grande.En su lugar, podrías preferir ver en la consola sólo los últimos y primeros casos:¿Qué tan grande es la base de datos? Si bien ya la función str() nos brindó información sobre la estructura de la base, puedes conocer el número de filas y columnas que componen tu arreglo de datos con las siguientes funciones:Conviene que se revise el archivo en formato txt que acompaña nuestra base y que opera modo de diccionario para que conocer el significado de cada una de las variables que componen nuestro archivo.Plantemos pues algunas preguntas y busquemos responderlas través de un análisis gráfico:¿Habrá alguna relación entre el número de casos positivos por cada mil habitantes, pos_hab, o muertes por cada mil habitantes, def_hab, con el nivel de estudios promedio de la población por municipio?Si esta relación existe, ¿es más intensa entre las muertes o los casos positivos?Para responder esta pregunta podemos recurrir la construcción de diagramas de dispersión. La elaboración de gráficas dentro de tidyverse corre cargo del paquete ggplot2, una potente librería para la representación visual de información. Este paquete se basa en lo que se denomina “gramática de gráficas” que es otra cosa que un conjunto de reglas coherentes con base en las cuales se elaboran los gráficos (Wickham y Grolemund (2016)).La construcción de gráficas con ggplot2 se hace partir de una suerte de “capas” o “enunciados”. Cada enunciado constituye una parte de la gráfica. Según la hoja de trucos del paquete (que puedes consultar desde la barra de menú en la opción Help/CheatSheet):ggplot2 se basa en la gramática de los gráficos, la idea de que puedes construir cada gráfico partir de los mismos componentes: un conjunto de datos, un sistema de coordenadas y “geoms”, es decir, marcas visuales que representan puntos de datos.Asi pues, se requieren tres elementos para la construcción de una gráfica con ggplot2: datos, geometría y sistema de coordenadas. Dentro de la geometría se especifican sólo la o las variables representar, también algunos elementos estéticos como colores. En nuestro caso, la información está contenida en la base covid_zmvm pero habrá que especificar qué tipo de gráfica deseamos y sus elementos estéticos. Veamos esto en acción:Tenemos pues dos enunciados o “capas”. La primera corresponde la función ggplot() donde colocamos (casi siempre) el primer elemento requerido: la base de datos de donde tomaremos la o las variables graficar, en tanto, el segundo enunciado corresponde la geometría, geom_ cuya parte después del guion bajo se modificará en función del tipo de gráfica utilizar. Algunos (sólo algunos) tipos de gráficas y su función de geometría asociada en ggplot2 aparecen en el cuadro 1.1:Cuadro 1.1Por favor, revisa la hoja de trucos del paquete para que te familiarices con las otras tantas geometrías y su uso, pues cada geometría requiere diferentes argumentos y admite varias posibles configuraciones. En general, dentro de la función de geometría, geom_, será necesario especificar los elementos estéticos, aes(), entre los que se cuenta la o las variables graficar y cómo serán representadas.Añadamos una “capa” adicional nuestra gráfica, combinando otra geometría como una línea de ajuste (pongamos atención en el uso del signo de más):Puedes ver cómo, al añadir una nueva geometría, fue necesario especificar los elementos estéticos del caso, pero los tres enunciados nos permiten tener una sola gráfica. Observa cómo R en tu consola R indica que estás usando determinado método de suavizamiento, el llamado “loess” que significa locally estimated scatterplot smoothing, por tanto, es fácil deducir que debe haber otros métodos de suavizamiento. Observa las siguientes líneas de código:Aquí, hemos añadido un argumento adicional, method=, para indicar un suavizamiento dado por un modelo lineal, lm, de linear model. Tenemos pues que entre el número de casos positivos y el número promedio de grados cursados hay una aparente relación positiva. ¿Qué hay para el caso de las defunciones:Si bien hay un poco más de dispersión, la aparente relación positiva se mantiene. ¿Qué crees que explique esta asociación? ¿Las personas con más años estudiados son susceptibles de contagiarse más fácil de COVID19? ¿O quizá se relacionará más con las diferencias que nivel territorial se dan entre los niveles de estudio de la población en el Valle de México?EjercicioElabora algunas gráficas de dispersión para responder las siguientes preguntas:¿Qué tipo de asociación lineal hay entre los casos positivos y defunciones por COVID19 con las características de la población según el número de dormitorios de las casas, el número de personas que habitan en cada casa y la densidad de población?¿Qué tipo de asociación lineal hay entre los casos positivos y defunciones por COVID19 con las características de la población según el número de dormitorios de las casas, el número de personas que habitan en cada casa y la densidad de población?¿Encuentras algún tipo de asociación entre las variables COVID19 y las características económicas de los municipios de la Zona Metropolitana del Valle de México?¿Encuentras algún tipo de asociación entre las variables COVID19 y las características económicas de los municipios de la Zona Metropolitana del Valle de México?Quizá lo que te interese sólo sea la asociación entre pares de variables, sino el comportamiento individual de cada variable para conocer la forma de su distribución e identificar algunos valores extremos o atípicos (los llamados ourliers). Para ello, convendrá un tipo de gráfica que permita representar sólo una variable continúa (casos positivos o defunciones por COVID19), tal como un histograma, una gráfica de densidad o de caja.Los tres gráficos dan cuenta que la forma de la distribución muestra un marcado sesgo positivo (la derecha), lo que significa que hay algunos municipios o alcaldías con valores muy altos para esta variable, además de algunos valores extremadamente altos según el diagrama de caja, ¿cuáles podrán ser y qué explicaría este comportamiento atípicamente alto?EjercicioConsulta la ayuda de cada una de las funciones de geometría e intenta:Elaborar un histograma con diferentes categorías (bins). ¿Cambia esto la forma de la distribución?Elaborar un histograma con diferentes categorías (bins). ¿Cambia esto la forma de la distribución?Señalar de un color diferente las observaciones atípicas en el diagrama de caja ¿Hay manera de identificar qué alcaldías o municipios pertenecen dichos valores atípicos?Señalar de un color diferente las observaciones atípicas en el diagrama de caja ¿Hay manera de identificar qué alcaldías o municipios pertenecen dichos valores atípicos?Además, también es posible modificar las etiquetas de los ejes:Nuestra base de datos contiene una variable de tipo categórico, grado de marginación (gm_2020), asociada al índice de marginación (im). Para este tipo de variable puede ser conveniente construir un gráfico de barras especialmente diseñado para variables categóricas echando mano de la geometría geom_bar():Este gráfico revela que, de acuerdo con la clasificación del Consejo Nacional de Población y Vivienda (CONAPO), la gran mayoría de los municipios y alcaldías que componen el Valle de México están catalogados como de muy bajo grado de marginación.EjercicioObtén una gráfica de barras donde se muestre el número de unidades administrativas (municipios o alcaldías) por de cada una de las entidades que integran la Zona Metropolitana del Valle de México.Ahora bien, ¿será posible incorporar en un plano bidimensional una tercera variable? ¿Cómo añadirías un diagrama de dispersión, que relaciona dos variables, una tercera? Este instrumento es veces denominado gráfico de burbujas y si lo piensas con cuidado es una solución muy ingeniosa para añadir información extra sin renunciar la simpleza. Para hacerlo, basta con añadir el argumento size= dentro de los elementos estéticos de la función geom_point():EjercicioResponde lo siguiente:Prueba cambiar la variable que define el tamaño del círculo por nuestra variable categórica grado de marginación, ¿es recomendable usar una variable de este tipo en esta representación?Prueba cambiar la variable que define el tamaño del círculo por nuestra variable categórica grado de marginación, ¿es recomendable usar una variable de este tipo en esta representación?¿Qué papel juega el argumento alpha= en la segunda gráfica?¿Qué papel juega el argumento alpha= en la segunda gráfica?¿Cuál es la diferencia en colocar el argumento color dentro o fuera del grupo de elementos estéticos (aes())? ¿Notaste la diferencia en el par de gráficas anteriores? Prueba cambiar el argumento color dentro de aes() por otra variable y el color de los círculos.¿Cuál es la diferencia en colocar el argumento color dentro o fuera del grupo de elementos estéticos (aes())? ¿Notaste la diferencia en el par de gráficas anteriores? Prueba cambiar el argumento color dentro de aes() por otra variable y el color de los círculos.Finalmente, elaboremos una gráfica que nos ayude sintetizar pares de asociaciones entre variables (diagramas de dispersión) y formas de distribución través de un gráfico muy elegante construido con una extensión del paquete ggplot2: GGally. Para instalarlo, como es usual:Luego, para poder usarlo, lo llamamos al entorno de trabajo:La gráfica que buscamos es una matriz de diagramas de dispersión y la construiremos con la función ggpairs() y sólo dos argumentos: la fuente de datos y las variables que deseamos incluir. Veamos:La matriz de diagramas de dispersión construida con ggpairs es, desde mi personal punto de vista, sólo muy elegante, sino que ofrece elementos informativos en el gráfico que antes teníamos, tales como el coeficiente de correlación lineal y su nivel de significancia.Antes de pasar tratar otro elemento, hay que decir que RStudio permite exportar las gráficas hechas, tanto en diversos formatos de imagen (jpg, png, tiff) como en PDF.EjercicioElabora tu propia matriz de diagramas de dispersión con las variables que, tu juicio, resulten más relevantes para explicar la dinámica tanto de las muertes como de los casos positivos de COVID19.","code":"\nbase::names(covid_zmvm) #Nombres de las variables##  [1] \"cvemun\"     \"cve_mun\"    \"nom_mun\"    \"cve_ent\"    \"nom_ent\"   \n##  [6] \"nom_abr\"    \"nom_zm\"     \"cvm\"        \"ext\"        \"pob20\"     \n## [11] \"pob20_h\"    \"pob20_m\"    \"positivos\"  \"defuncione\" \"pos_mil\"   \n## [16] \"pos_hab\"    \"def_hab\"    \"ss\"         \"ppob_sines\" \"ppob_basi\" \n## [21] \"ppob_media\" \"ppob_sup\"   \"ocviv\"      \"occu\"       \"pintegra4_\"\n## [26] \"pintegra6_\" \"pintegra8_\" \"ppob_5_o_m\" \"ppob_3_o_m\" \"ppob_1\"    \n## [31] \"ppob_1dorm\" \"ppob_2dorm\" \"ppob_3dorm\" \"pviv_ocu5_\" \"pviv_ocu7_\"\n## [36] \"pviv_ocu9_\" \"analf\"      \"sbasc\"      \"vhac\"       \"po2sm\"     \n## [41] \"im\"         \"gm_2020\"    \"grad\"       \"grad_h\"     \"grad_m\"    \n## [46] \"poind\"      \"pocom\"      \"poss\"       \"tmind\"      \"tmcom\"     \n## [51] \"tmss\"       \"rmind\"      \"rmcom\"      \"rmss\"       \"den\"\nutils::str(covid_zmvm) #Estructura de la base de datos## tibble [76 x 55] (S3: tbl_df/tbl/data.frame)\n##  $ cvemun    : chr [1:76] \"09010\" \"09012\" \"09015\" \"09017\" ...\n##  $ cve_mun   : chr [1:76] \"010\" \"012\" \"015\" \"017\" ...\n##  $ nom_mun   : chr [1:76] \"Álvaro Obregón\" \"Tlalpan\" \"Cuauhtémoc\" \"Venustiano Carranza\" ...\n##  $ cve_ent   : chr [1:76] \"09\" \"09\" \"09\" \"09\" ...\n##  $ nom_ent   : chr [1:76] \"Ciudad de México\" \"Ciudad de México\" \"Ciudad de México\" \"Ciudad de México\" ...\n##  $ nom_abr   : chr [1:76] \"CDMX\" \"CDMX\" \"CDMX\" \"CDMX\" ...\n##  $ nom_zm    : chr [1:76] \"Valle de México\" \"Valle de México\" \"Valle de México\" \"Valle de México\" ...\n##  $ cvm       : num [1:76] 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 9.01 ...\n##  $ ext       : num [1:76] 96.2 310.4 32.5 33.9 85.8 ...\n##  $ pob20     : num [1:76] 759137 699928 545884 443704 392313 ...\n##  $ pob20_h   : num [1:76] 361007 334877 260951 210118 190190 ...\n##  $ pob20_m   : num [1:76] 398130 365051 284933 233586 202123 ...\n##  $ positivos : num [1:76] 10905 11887 7289 7172 6812 ...\n##  $ defuncione: num [1:76] 868 542 754 585 289 713 648 397 190 391 ...\n##  $ pos_mil   : num [1:76] 14.4 17 13.4 16.2 17.4 ...\n##  $ pos_hab   : num [1:76] 14.4 17 13.4 16.2 17.4 ...\n##  $ def_hab   : num [1:76] 1.143 0.774 1.381 1.318 0.737 ...\n##  $ ss        : num [1:76] 75 71.1 71.6 71.6 72.7 ...\n##  $ ppob_sines: num [1:76] 0.0204 0.0195 0.0119 0.0121 0.0193 ...\n##  $ ppob_basi : num [1:76] 0.41 0.391 0.307 0.379 0.469 ...\n##  $ ppob_media: num [1:76] 0.252 0.253 0.251 0.291 0.299 ...\n##  $ ppob_sup  : num [1:76] 0.314 0.335 0.427 0.316 0.211 ...\n##  $ ocviv     : num [1:76] 3.45 3.44 2.75 3.26 3.67 3.21 3.2 3.74 3.6 2.81 ...\n##  $ occu      : num [1:76] 0.81 0.81 0.72 0.82 0.93 0.77 0.7 0.91 0.81 0.66 ...\n##  $ pintegra4_: num [1:76] 0.648 0.644 0.48 0.609 0.699 ...\n##  $ pintegra6_: num [1:76] 0.252 0.226 0.142 0.219 0.262 ...\n##  $ pintegra8_: num [1:76] 0.1071 0.0864 0.0422 0.0798 0.0982 ...\n##  $ ppob_5_o_m: num [1:76] 0.726 0.73 0.888 0.821 0.792 ...\n##  $ ppob_3_o_m: num [1:76] 0.32 0.315 0.363 0.354 0.354 ...\n##  $ ppob_1    : num [1:76] 0.0369 0.0499 0.0249 0.0247 0.0513 ...\n##  $ ppob_1dorm: num [1:76] 0.211 0.212 0.199 0.202 0.209 ...\n##  $ ppob_2dorm: num [1:76] 0.566 0.547 0.8 0.696 0.614 ...\n##  $ ppob_3dorm: num [1:76] 0.832 0.848 0.947 0.89 0.864 ...\n##  $ pviv_ocu5_: num [1:76] 0.226 0.22 0.127 0.198 0.267 ...\n##  $ pviv_ocu7_: num [1:76] 0.0641 0.0568 0.026 0.0513 0.0701 ...\n##  $ pviv_ocu9_: num [1:76] 0.02334 0.01815 0.00645 0.01593 0.02167 ...\n##  $ analf     : num [1:76] 1.574 1.606 0.953 1.085 1.673 ...\n##  $ sbasc     : num [1:76] 19 18.2 13.5 16.9 20.3 ...\n##  $ vhac      : num [1:76] 15.25 15.19 9.42 14.43 18.3 ...\n##  $ po2sm     : num [1:76] 49.3 61.2 41.4 57.2 65.4 ...\n##  $ im        : num [1:76] 60.4 59.5 61.3 60.3 59.3 ...\n##  $ gm_2020   : chr [1:76] \"Muy Bajo\" \"Muy Bajo\" \"Muy Bajo\" \"Muy Bajo\" ...\n##  $ grad      : num [1:76] 11.3 11.5 12.4 11.5 10.5 ...\n##  $ grad_h    : num [1:76] 11.5 11.7 12.8 11.7 10.7 ...\n##  $ grad_m    : num [1:76] 11.1 11.4 12.1 11.3 10.4 ...\n##  $ poind     : num [1:76] 0.0912 0.0874 0.1334 0.0714 0.2442 ...\n##  $ pocom     : num [1:76] 0.16 0.185 0.166 0.289 0.394 ...\n##  $ poss      : num [1:76] 0.749 0.728 0.7 0.64 0.361 ...\n##  $ tmind     : num [1:76] 20.32 8.9 27.57 6.57 6.42 ...\n##  $ tmcom     : num [1:76] 5.95 3.48 4.36 2.66 2.12 ...\n##  $ tmss      : num [1:76] 27.69 13.91 24.87 9.94 2.59 ...\n##  $ rmind     : num [1:76] 68.2 116.7 55.9 64.9 78.3 ...\n##  $ rmcom     : num [1:76] 50.6 31.3 45.5 31.2 23.5 ...\n##  $ rmss      : num [1:76] 178.3 36.3 255.6 101.4 23.4 ...\n##  $ den       : num [1:76] 7895 2255 16786 13104 4575 ...\nbase::dim(covid_zmvm) #Dimensiones de la base## [1] 76 55\nutils::View(covid_zmvm)\n#Que es equivalente a:\nView(covid_zmvm)\nbase::print(covid_zmvm)## # A tibble: 76 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 759137\n##  2 09012  012     Tlalpan    09      Ciudad ~ CDMX    Valle ~  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc 09      Ciudad ~ CDMX    Valle ~  9.01  32.5 545884\n##  4 09017  017     Venustian~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.9 443704\n##  5 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 392313\n##  6 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 432205\n##  7 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 614447\n##  8 09013  013     Xochimilco 09      Ciudad ~ CDMX    Valle ~  9.01 118.  442178\n##  9 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 217686\n## 10 09016  016     Miguel Hi~ 09      Ciudad ~ CDMX    Valle ~  9.01  46.4 414470\n## # ... with 66 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\nutils::head(covid_zmvm) #Para los primeros casos## # A tibble: 6 x 55\n##   cvemun cve_mun nom_mun    cve_ent nom_ent   nom_abr nom_zm    cvm   ext  pob20\n##   <chr>  <chr>   <chr>      <chr>   <chr>     <chr>   <chr>   <dbl> <dbl>  <dbl>\n## 1 09010  010     Álvaro Ob~ 09      Ciudad d~ CDMX    Valle ~  9.01  96.2 759137\n## 2 09012  012     Tlalpan    09      Ciudad d~ CDMX    Valle ~  9.01 310.  699928\n## 3 09015  015     Cuauhtémoc 09      Ciudad d~ CDMX    Valle ~  9.01  32.5 545884\n## 4 09017  017     Venustian~ 09      Ciudad d~ CDMX    Valle ~  9.01  33.9 443704\n## 5 09011  011     Tláhuac    09      Ciudad d~ CDMX    Valle ~  9.01  85.8 392313\n## 6 09002  002     Azcapotza~ 09      Ciudad d~ CDMX    Valle ~  9.01  33.5 432205\n## # ... with 45 more variables: pob20_h <dbl>, pob20_m <dbl>, positivos <dbl>,\n## #   defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>, def_hab <dbl>, ss <dbl>,\n## #   ppob_sines <dbl>, ppob_basi <dbl>, ppob_media <dbl>, ppob_sup <dbl>,\n## #   ocviv <dbl>, occu <dbl>, pintegra4_ <dbl>, pintegra6_ <dbl>,\n## #   pintegra8_ <dbl>, ppob_5_o_m <dbl>, ppob_3_o_m <dbl>, ppob_1 <dbl>,\n## #   ppob_1dorm <dbl>, ppob_2dorm <dbl>, ppob_3dorm <dbl>, pviv_ocu5_ <dbl>,\n## #   pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, analf <dbl>, sbasc <dbl>, ...\nutils::tail(covid_zmvm) #Para los últimos casos## # A tibble: 6 x 55\n##   cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm    ext  pob20\n##   <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl>  <dbl>  <dbl>\n## 1 15120  120     Zumpango   15      México   Mex.    Valle ~  9.01 224.   2.80e5\n## 2 15121  121     Cuautitlá~ 15      México   Mex.    Valle ~  9.01 110.   5.55e5\n## 3 15122  122     Valle de ~ 15      México   Mex.    Valle ~  9.01  46.6  3.92e5\n## 4 15125  125     Tonanitla  15      México   Mex.    Valle ~  9.01   9.04 1.49e4\n## 5 15058  058     Nezahualc~ 15      México   Mex.    Valle ~  9.01  63.3  1.08e6\n## 6 09005  005     Gustavo A~ 09      Ciudad ~ CDMX    Valle ~  9.01  87.9  1.17e6\n## # ... with 45 more variables: pob20_h <dbl>, pob20_m <dbl>, positivos <dbl>,\n## #   defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>, def_hab <dbl>, ss <dbl>,\n## #   ppob_sines <dbl>, ppob_basi <dbl>, ppob_media <dbl>, ppob_sup <dbl>,\n## #   ocviv <dbl>, occu <dbl>, pintegra4_ <dbl>, pintegra6_ <dbl>,\n## #   pintegra8_ <dbl>, ppob_5_o_m <dbl>, ppob_3_o_m <dbl>, ppob_1 <dbl>,\n## #   ppob_1dorm <dbl>, ppob_2dorm <dbl>, ppob_3dorm <dbl>, pviv_ocu5_ <dbl>,\n## #   pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, analf <dbl>, sbasc <dbl>, ...\nbase::ncol(covid_zmvm) #Para conocer el número de columnas## [1] 55\nbase::nrow(covid_zmvm) #Para conocer el número de filas## [1] 76\nlibrary(tidyverse)\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab))\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab))+\n  ggplot2::geom_smooth(aes(x=grad,y=pos_hab))\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab))+\n  ggplot2::geom_smooth(aes(x=grad,y=pos_hab), method = \"lm\")\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=def_hab))+\n  ggplot2::geom_smooth(aes(x=grad,y=def_hab), method = \"lm\")\n#Histograma para los casos positivos por cada mil habitantes\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_histogram(aes(x=pos_hab))\n#Gráfico de densidad para los casos positivos por cada mil habitantes\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_density(aes(x=pos_hab))\n#Gráfico de caja para los casos positivos por cada mil habitantes\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_boxplot(aes(x=pos_hab))\nggplot2::ggplot(data=covid_zmvm)+\n  ggplot2::geom_histogram(aes(x=pos_hab))+\n  ggplot2::labs(x=\"Casos positivos por cada mil habitantes\", y=\"Frecuencia\")## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nggplot2::ggplot(covid_zmvm)+\n ggplot2:: geom_bar(aes(x=gm_2020))+\n  ggplot2::labs(x=\"Grado de marginación 2020\", y=\"Frecuencia\")\n#Si tienes la librería ggplot2 cargada en el enorno de trabajo, alternativamente, puedes escribir:\nggplot(covid_zmvm)+\n geom_bar(aes(x=gm_2020))+\n  labs(x=\"Grado de marginación 2020\", y=\"Frecuencia\")\n#Tamaño del circulo dado por los años estudiados\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab,size=def_hab))\n#Transparencia del círculo dado por los años estudiados\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab,alpha=def_hab, size=def_hab), color=\"navyblue\")\n#Transparencia del círculo dado por los años estudiados\nggplot2::ggplot(covid_zmvm)+\n  ggplot2::geom_point(aes(x=grad,y=pos_hab,color=def_hab))\ninstall.packages(\"GGally\")\nlibrary(GGally)\n#Definiendo los nombres de las variables\nGGally::ggpairs(data=covid_zmvm, columns = c(\"pos_hab\",\"def_hab\",\"im\",\"grad\"))\n#Indicando el número de columna según el lugar que ocupa en la base\nGGally::ggpairs(data=covid_zmvm, columns = c(16,17,41,43))"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"manipulación-de-la-información","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.5 Manipulación de la información","text":"","code":""},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"seleccionar-variables","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.5.1 Seleccionar variables","text":"Tareas relacionadas con la preparación y manipulación de datos, que en una hoja de cálculo como Excel de Office o Calc de LibreOffice, son rutinarias y muy sencillas, veces en R pueden significar un dolor de cabeza. obstante, dentro de los paquetes de la familia tidyverse encontramos dplyr. Este paquete brinda herramientas de manipulación de datos basadas en “gramática”, que es otra cosa que funciones (“verbos”) que permiten seleccionar o filtrar elementos en una base de datos, o bien, agrupar y crear nuevas variables.Si deseas profundizar en el conocimiento de este potente paquete consulta la Introducción al paquete dplyr, o bien, su Viñeta de ayuda.La base de datos que cargamos contiene 55 columnas, es decir, 55 variables. Quizá para su análisis te sean de interés todas, por lo que querrías generar una base de datos más compacta mediante la selección de algunas que son de utilidad. Para llevar cabo dicha tarea echaremos mano de la función select(), pero antes de hacerlo, es necesario introducir el operador “tubería”, pipe (%>%).Este operador sirve para indicar cada fase del proceso por el que un objeto es tratado través de la aplicación de diferentes funciones u operaciones. Por ejemplo, queremos que la base covid_zmvm “pase” por un proceso que consiste en la selección de sólo algunas variables. Comencemos usando el “verbo” seleccionar, select(), para una sola variable, por ejemplo pos_hab:La instrucción anterior es equivalente escribir lo siguiente:Así, usar el operador tubería permite sólo ahorrar la escritura de algunos argumentos, sino simplificar la forma en que las instrucciones dadas R son leídas por un ser humano. Veamos cómo seleccionar ahora, por ejemplo, tres variables: la clave de municipio (cvemun), los casos positivos (pos_hab) y el grado de marginación (gm_2020):Ahora, imagina que queremos todas las variables excepto el nombre de la Zona Metropolitana, nom_zm. Sería ocioso colocar todos los nombres de las variables menos el citado. Para hacer esto más eficiente es necesario introducir solamente un signo de menos (-) antes del nombre de la variable, para de esta sencilla forma seleccionar todas las variables menos nom_zm.En lugar de usar los nombres de las variables, puedes recurrir al número de columna en la que se hallan, contando partir del 1. Por ejemplo, nom_mun es la columna 3 y pos_hab es la columna 16, entonces podríamos escribir:Y si es de nuestro interés seleccionar un número consecutivo de columnas, podemos servirnos de:EjercicioPara finalizar lo relacionado con la selección, genera una nueva base de datos llamada covid que contenga las siguientes variables: cvemun, nom_mun, cve_ent, nom_ent, ext, pos_hab, def_hab, ocviv, occu, ppob_1, ppob_1dorm, im, gm_2020, grad, poind, pocom, poss, tmind, tmcom, tmss, rmind, rmcom, rmss, den, pob20.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2022-01-19.book built bookdown R package.","code":"\ncovid_zmvm %>% dplyr::select(pos_hab)## # A tibble: 76 x 1\n##    pos_hab\n##      <dbl>\n##  1    14.4\n##  2    17.0\n##  3    13.4\n##  4    16.2\n##  5    17.4\n##  6    17.3\n##  7    14.9\n##  8    17.4\n##  9    18.7\n## 10    13.7\n## # ... with 66 more rows\ndplyr::select(covid_zmvm,pos_hab)## # A tibble: 76 x 1\n##    pos_hab\n##      <dbl>\n##  1    14.4\n##  2    17.0\n##  3    13.4\n##  4    16.2\n##  5    17.4\n##  6    17.3\n##  7    14.9\n##  8    17.4\n##  9    18.7\n## 10    13.7\n## # ... with 66 more rows\ncovid_zmvm %>% dplyr::select(cvemun,pos_hab,gm_2020)## # A tibble: 76 x 3\n##    cvemun pos_hab gm_2020 \n##    <chr>    <dbl> <chr>   \n##  1 09010     14.4 Muy Bajo\n##  2 09012     17.0 Muy Bajo\n##  3 09015     13.4 Muy Bajo\n##  4 09017     16.2 Muy Bajo\n##  5 09011     17.4 Muy Bajo\n##  6 09002     17.3 Muy Bajo\n##  7 09003     14.9 Muy Bajo\n##  8 09013     17.4 Muy Bajo\n##  9 09004     18.7 Muy Bajo\n## 10 09016     13.7 Muy Bajo\n## # ... with 66 more rows\ncovid_zmvm %>% dplyr::select(-nom_zm)## # A tibble: 76 x 54\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr   cvm   ext  pob20 pob20_h\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <dbl> <dbl>  <dbl>   <dbl>\n##  1 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX     9.01  96.2 759137  361007\n##  2 09012  012     Tlalpan    09      Ciudad ~ CDMX     9.01 310.  699928  334877\n##  3 09015  015     Cuauhtémoc 09      Ciudad ~ CDMX     9.01  32.5 545884  260951\n##  4 09017  017     Venustian~ 09      Ciudad ~ CDMX     9.01  33.9 443704  210118\n##  5 09011  011     Tláhuac    09      Ciudad ~ CDMX     9.01  85.8 392313  190190\n##  6 09002  002     Azcapotza~ 09      Ciudad ~ CDMX     9.01  33.5 432205  204950\n##  7 09003  003     Coyoacán   09      Ciudad ~ CDMX     9.01  53.9 614447  289110\n##  8 09013  013     Xochimilco 09      Ciudad ~ CDMX     9.01 118.  442178  215452\n##  9 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX     9.01  71.2 217686  104149\n## 10 09016  016     Miguel Hi~ 09      Ciudad ~ CDMX     9.01  46.4 414470  195467\n## # ... with 66 more rows, and 44 more variables: pob20_m <dbl>, positivos <dbl>,\n## #   defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>, def_hab <dbl>, ss <dbl>,\n## #   ppob_sines <dbl>, ppob_basi <dbl>, ppob_media <dbl>, ppob_sup <dbl>,\n## #   ocviv <dbl>, occu <dbl>, pintegra4_ <dbl>, pintegra6_ <dbl>,\n## #   pintegra8_ <dbl>, ppob_5_o_m <dbl>, ppob_3_o_m <dbl>, ppob_1 <dbl>,\n## #   ppob_1dorm <dbl>, ppob_2dorm <dbl>, ppob_3dorm <dbl>, pviv_ocu5_ <dbl>,\n## #   pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, analf <dbl>, sbasc <dbl>, ...\ncovid_zmvm %>% dplyr::select(3,16)## # A tibble: 76 x 2\n##    nom_mun               pos_hab\n##    <chr>                   <dbl>\n##  1 Álvaro Obregón           14.4\n##  2 Tlalpan                  17.0\n##  3 Cuauhtémoc               13.4\n##  4 Venustiano Carranza      16.2\n##  5 Tláhuac                  17.4\n##  6 Azcapotzalco             17.3\n##  7 Coyoacán                 14.9\n##  8 Xochimilco               17.4\n##  9 Cuajimalpa de Morelos    18.7\n## 10 Miguel Hidalgo           13.7\n## # ... with 66 more rows\ncovid_zmvm %>% dplyr::select(1:10)## # A tibble: 76 x 10\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 759137\n##  2 09012  012     Tlalpan    09      Ciudad ~ CDMX    Valle ~  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc 09      Ciudad ~ CDMX    Valle ~  9.01  32.5 545884\n##  4 09017  017     Venustian~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.9 443704\n##  5 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 392313\n##  6 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 432205\n##  7 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 614447\n##  8 09013  013     Xochimilco 09      Ciudad ~ CDMX    Valle ~  9.01 118.  442178\n##  9 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 217686\n## 10 09016  016     Miguel Hi~ 09      Ciudad ~ CDMX    Valle ~  9.01  46.4 414470\n## # ... with 66 more rows"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"filtrar-seleccionar-casos","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.5.2 Filtrar (seleccionar casos)","text":"Si ahora lo que te interesa es seleccionar filas o casos en lugar de columnas, hay que echar mano de otro “verbo”, es decir, de la función filtrar, filter(). Por ejemplo, imagina que deseamos seleccionar sólo los municipios del Estado de México. Estos municipios cumplen con la condición de que todos ellos tienen el valor “México” en la variable nom_ent (nombre de entidad), o bien, tener el valor “15” en el de cve_ent (clave de entidad). Para poder hacer un uso eficiente de la función de filtrado es necesario introducir también otro tipo de operadores, los llamados operadores lógicos, que aparecen en el cuadro 1.2:Cuadro 1.2Entonces, para usar la función filter() y tomando como base la información previa:Para hacer una selección de dos entidades, incluiremos dos expresiones en una misma consulta echando mano del operador lógico “o”, identificado con el signo |:EjercicioSelecciona los municipios con una extensión territorial mayor 84 \\(km^2\\).Selecciona los municipios con una extensión territorial mayor 84 \\(km^2\\).Selecciona los municipios con entre 10 y 20 casos positivos por cada mil habitantes.Selecciona los municipios con entre 10 y 20 casos positivos por cada mil habitantes.Selecciona los municipios del Estado de México que tengan más de 10 años promedio de estudio y menos de 10 casos positivos por cada mil habitantes.Selecciona los municipios del Estado de México que tengan más de 10 años promedio de estudio y menos de 10 casos positivos por cada mil habitantes.¿Qué otros filtros relevantes podrías pensar y elaborar?","code":"\ncovid_zmvm %>% dplyr::filter(cve_ent==\"15\")## # A tibble: 59 x 55\n##    cvemun cve_mun nom_mun     cve_ent nom_ent nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>       <chr>   <chr>   <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 15002  002     Acolman     15      México  Mex.    Valle ~  9.01  83.9 171507\n##  2 15009  009     Amecameca   15      México  Mex.    Valle ~  9.01 189.   53441\n##  3 15010  010     Apaxco      15      México  Mex.    Valle ~  9.01  75.7  31898\n##  4 15011  011     Atenco      15      México  Mex.    Valle ~  9.01  84.6  75489\n##  5 15013  013     Atizapán d~ 15      México  Mex.    Valle ~  9.01  91.1 523674\n##  6 15015  015     Atlautla    15      México  Mex.    Valle ~  9.01 162.   31900\n##  7 15016  016     Axapusco    15      México  Mex.    Valle ~  9.01 231.   29128\n##  8 15017  017     Ayapango    15      México  Mex.    Valle ~  9.01  36.4  10053\n##  9 15020  020     Coacalco d~ 15      México  Mex.    Valle ~  9.01  35.1 293444\n## 10 15022  022     Cocotitlán  15      México  Mex.    Valle ~  9.01  15.0  15107\n## # ... with 49 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\n#O bien\ncovid_zmvm %>% dplyr::filter(nom_ent==\"México\")## # A tibble: 59 x 55\n##    cvemun cve_mun nom_mun     cve_ent nom_ent nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>       <chr>   <chr>   <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 15002  002     Acolman     15      México  Mex.    Valle ~  9.01  83.9 171507\n##  2 15009  009     Amecameca   15      México  Mex.    Valle ~  9.01 189.   53441\n##  3 15010  010     Apaxco      15      México  Mex.    Valle ~  9.01  75.7  31898\n##  4 15011  011     Atenco      15      México  Mex.    Valle ~  9.01  84.6  75489\n##  5 15013  013     Atizapán d~ 15      México  Mex.    Valle ~  9.01  91.1 523674\n##  6 15015  015     Atlautla    15      México  Mex.    Valle ~  9.01 162.   31900\n##  7 15016  016     Axapusco    15      México  Mex.    Valle ~  9.01 231.   29128\n##  8 15017  017     Ayapango    15      México  Mex.    Valle ~  9.01  36.4  10053\n##  9 15020  020     Coacalco d~ 15      México  Mex.    Valle ~  9.01  35.1 293444\n## 10 15022  022     Cocotitlán  15      México  Mex.    Valle ~  9.01  15.0  15107\n## # ... with 49 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\ncovid_zmvm %>% dplyr::filter(cve_ent==\"09\"|cve_ent==\"15\")## # A tibble: 75 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 759137\n##  2 09012  012     Tlalpan    09      Ciudad ~ CDMX    Valle ~  9.01 310.  699928\n##  3 09015  015     Cuauhtémoc 09      Ciudad ~ CDMX    Valle ~  9.01  32.5 545884\n##  4 09017  017     Venustian~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.9 443704\n##  5 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 392313\n##  6 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 432205\n##  7 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 614447\n##  8 09013  013     Xochimilco 09      Ciudad ~ CDMX    Valle ~  9.01 118.  442178\n##  9 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 217686\n## 10 09016  016     Miguel Hi~ 09      Ciudad ~ CDMX    Valle ~  9.01  46.4 414470\n## # ... with 65 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ..."},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"ordenar","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.5.3 Ordenar","text":"Una operación útil, cuando se analiza información, tiene que ver con ordenar la base de datos con arreglo al valor de una variable. En R con dplyr de tidyverse esto se hace con la función arrange(), por ejemplo:El resultado de la función anterior ordena nuestra base de menor mayor, con base en la clave municipal. Además, es posible incluir varios criterios de ordenación, por ejemplo, si tuvieras los campos año, mes y día podría ordenarse conforme al calendario. En el caso del ejemplo siguiente, primero se ordena con arreglo la clave de entidad y luego con respecto la clave municipal y luego por la extensión territorial por cada mil habitantes:Si deseas ordenar de forma descendente, es decir, de mayor menor hay que introducir una función adicional, desc(), por ejemplo:La función anterior primero ordena los municipios con base en la clave de municipio de forma ascendente (de menor mayor) y simultáneamente coloca de mayor menor los municipios de acuerdo con su extensión territorial.EjerciciosGenera una nueva base de datos (un nuevo objeto) que contenga sólo las alcaldías de la Ciudad de México y las variables casos por cada mil habitantes, luego ordena esa base de datos con arreglo al número de casos positivos de forma ascendente.Genera una nueva base de datos (un nuevo objeto) que contenga sólo las alcaldías de la Ciudad de México y las variables casos por cada mil habitantes, luego ordena esa base de datos con arreglo al número de casos positivos de forma ascendente.lo mismo que en el inciso anterior, pero para los municipios del Estado de México y las defunciones.lo mismo que en el inciso anterior, pero para los municipios del Estado de México y las defunciones.","code":"\ncovid_zmvm %>% dplyr::arrange(cvemun)## # A tibble: 76 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A~ 09      Ciudad ~ CDMX    Valle ~  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco  09      Ciudad ~ CDMX    Valle ~  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa 09      Ciudad ~ CDMX    Valle ~  9.01 113.  1.84e6\n##  7 09008  008     La Magdal~ 09      Ciudad ~ CDMX    Valle ~  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta 09      Ciudad ~ CDMX    Valle ~  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 3.92e5\n## # ... with 66 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\ncovid_zmvm %>% dplyr::arrange(cvemun,ext)## # A tibble: 76 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A~ 09      Ciudad ~ CDMX    Valle ~  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco  09      Ciudad ~ CDMX    Valle ~  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa 09      Ciudad ~ CDMX    Valle ~  9.01 113.  1.84e6\n##  7 09008  008     La Magdal~ 09      Ciudad ~ CDMX    Valle ~  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta 09      Ciudad ~ CDMX    Valle ~  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 3.92e5\n## # ... with 66 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ...\ncovid_zmvm %>% dplyr::arrange(cvemun,desc(ext))## # A tibble: 76 x 55\n##    cvemun cve_mun nom_mun    cve_ent nom_ent  nom_abr nom_zm    cvm   ext  pob20\n##    <chr>  <chr>   <chr>      <chr>   <chr>    <chr>   <chr>   <dbl> <dbl>  <dbl>\n##  1 09002  002     Azcapotza~ 09      Ciudad ~ CDMX    Valle ~  9.01  33.5 4.32e5\n##  2 09003  003     Coyoacán   09      Ciudad ~ CDMX    Valle ~  9.01  53.9 6.14e5\n##  3 09004  004     Cuajimalp~ 09      Ciudad ~ CDMX    Valle ~  9.01  71.2 2.18e5\n##  4 09005  005     Gustavo A~ 09      Ciudad ~ CDMX    Valle ~  9.01  87.9 1.17e6\n##  5 09006  006     Iztacalco  09      Ciudad ~ CDMX    Valle ~  9.01  23.1 4.05e5\n##  6 09007  007     Iztapalapa 09      Ciudad ~ CDMX    Valle ~  9.01 113.  1.84e6\n##  7 09008  008     La Magdal~ 09      Ciudad ~ CDMX    Valle ~  9.01  63.4 2.48e5\n##  8 09009  009     Milpa Alta 09      Ciudad ~ CDMX    Valle ~  9.01 298.  1.53e5\n##  9 09010  010     Álvaro Ob~ 09      Ciudad ~ CDMX    Valle ~  9.01  96.2 7.59e5\n## 10 09011  011     Tláhuac    09      Ciudad ~ CDMX    Valle ~  9.01  85.8 3.92e5\n## # ... with 66 more rows, and 45 more variables: pob20_h <dbl>, pob20_m <dbl>,\n## #   positivos <dbl>, defuncione <dbl>, pos_mil <dbl>, pos_hab <dbl>,\n## #   def_hab <dbl>, ss <dbl>, ppob_sines <dbl>, ppob_basi <dbl>,\n## #   ppob_media <dbl>, ppob_sup <dbl>, ocviv <dbl>, occu <dbl>,\n## #   pintegra4_ <dbl>, pintegra6_ <dbl>, pintegra8_ <dbl>, ppob_5_o_m <dbl>,\n## #   ppob_3_o_m <dbl>, ppob_1 <dbl>, ppob_1dorm <dbl>, ppob_2dorm <dbl>,\n## #   ppob_3dorm <dbl>, pviv_ocu5_ <dbl>, pviv_ocu7_ <dbl>, pviv_ocu9_ <dbl>, ..."},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"crear-nuevas-variables","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.5.4 Crear nuevas variables","text":"Para añadir una nueva variable partir de las existentes recurrimos la función mutate(). Por ejemplo, calculemos la variable densidad de casos positivos, es decir, número de casos positivos dividido entre la extensión territorial, dicha variable la llamaremos pos_den. Pero antes, generemos una nueva base de datos que sólo contenga algunas de las variables de la base original:Notarás como la nueva variable aparece al final de la base. Vamos intentar unir varios de los elementos que hemos aprendido hasta este punto para que veas el potencial de uso de la tubería. Vamos crear una nueva variable, la misma que hace un momento, luego, tomaremos la base que contiene la nueva variable y construiremos con ella un diagrama de dispersión con la nueva variable y el número de defunciones por cada mil habitantes:¿Cómo explicarías la relación entre la variable creada, densidad de casos positivos, y las defunciones por cada mil habitables?Ejercicio¿Cómo añadirías más de una nueva variable tu base original?¿Cómo añadirías más de una nueva variable tu base original?Construye la variable “densidad de defunciones” y grafícala en un diagrama de dispersión con la densidad de población? Usa las cañerías.Construye la variable “densidad de defunciones” y grafícala en un diagrama de dispersión con la densidad de población? Usa las cañerías.Quizá lo que te interese sea sólo quedarte con algunas de las variables originales y las nuevas variables creadas partir de la información de la variable original. Para ello es útil la función transmute():El segmento de código anterior genera una nueva base en la que sólo conservamos algunas de las variables originales y añadimos dos nuevas partir de la información original.","code":"\ncovid_zmvm %>% dplyr::select(cvemun,nom_mun,positivos,pos_hab,ext) %>%\n  dplyr::mutate(pos_den=positivos/ext)## # A tibble: 76 x 6\n##    cvemun nom_mun               positivos pos_hab   ext pos_den\n##    <chr>  <chr>                     <dbl>   <dbl> <dbl>   <dbl>\n##  1 09010  Álvaro Obregón            10905    14.4  96.2   113. \n##  2 09012  Tlalpan                   11887    17.0 310.     38.3\n##  3 09015  Cuauhtémoc                 7289    13.4  32.5   224. \n##  4 09017  Venustiano Carranza        7172    16.2  33.9   212. \n##  5 09011  Tláhuac                    6812    17.4  85.8    79.4\n##  6 09002  Azcapotzalco               7483    17.3  33.5   223. \n##  7 09003  Coyoacán                   9182    14.9  53.9   170. \n##  8 09013  Xochimilco                 7696    17.4 118.     65.1\n##  9 09004  Cuajimalpa de Morelos      4071    18.7  71.2    57.2\n## 10 09016  Miguel Hidalgo             5669    13.7  46.4   122. \n## # ... with 66 more rows\ncovid_zmvm %>% \n  dplyr::mutate(pos_den=positivos/ext) %>% \n  ggplot2::ggplot()+\n    ggplot2::geom_point(aes(x=pos_den,y=def_hab))+\n    ggplot2::geom_smooth(aes(x=pos_den,y=def_hab))+\n    ggplot2::labs(x=\"Densidad de casos positivos\",y=\"Defunciones por cada mil habitantes\")\ncovid_zmvm %>% \n  dplyr::transmute(cvemun,nom_mun,pos_hab,def_hab,pos_den=positivos/ext,def_den=defuncione/ext)## # A tibble: 76 x 6\n##    cvemun nom_mun               pos_hab def_hab pos_den def_den\n##    <chr>  <chr>                   <dbl>   <dbl>   <dbl>   <dbl>\n##  1 09010  Álvaro Obregón           14.4   1.14    113.     9.03\n##  2 09012  Tlalpan                  17.0   0.774    38.3    1.75\n##  3 09015  Cuauhtémoc               13.4   1.38    224.    23.2 \n##  4 09017  Venustiano Carranza      16.2   1.32    212.    17.3 \n##  5 09011  Tláhuac                  17.4   0.737    79.4    3.37\n##  6 09002  Azcapotzalco             17.3   1.65    223.    21.3 \n##  7 09003  Coyoacán                 14.9   1.05    170.    12.0 \n##  8 09013  Xochimilco               17.4   0.898    65.1    3.36\n##  9 09004  Cuajimalpa de Morelos    18.7   0.873    57.2    2.67\n## 10 09016  Miguel Hidalgo           13.7   0.943   122.     8.43\n## # ... with 66 more rows"},{"path":"r-una-introducción-desde-la-exploración-de-información.html","id":"resúmenes-de-información-y-grupos","chapter":"1 R: Una introducción desde la exploración de información","heading":"1.5.5 Resúmenes de información y grupos","text":"Quizá una de las funciones más potentes y sencillas que tiene dplyr es la función de resumen (summarise()). Veamos cómo opera. Imagina que deseas un promedio del número de casos que terminaron en muerte:Esto es particularmente útil si se le compara con la función del paquete base de R, summary(), pero si la combinamos con la función de agrupamiento, group_by() la situación cambia. Imagina que queremos el promedio de casos positivos por cada mil habitantes para cada conjunto de municipios de las tres entidades que componen el Valle de México:El en segmento de código anterior primero se agrupa la información con arreglo al criterio de clave de entidad, cve_ent, y luego, para cada grupo, calcula el resultado especificado: un promedio. Dentro de la función de resumen, summarise(), se puede llevar cabo sólo múltiples operaciones sino que estas pueden ser de diferente naturaleza, incluso por ejemplo una suma o una cuenta:En el segmento de código anterior creamos tres variables de resumen: la población total, pob_tot, que es la suma de la población para cada entidad, el promedio del índice de marginación, im_medio y el número de municipios de cada entidad través de una cuenta con la función n().EjercicioConstruye una gráfica de barras donde aparezca el total de población para cada entidad, partir del segmento código anterior.Construye una gráfica de barras donde aparezca el total de población para cada entidad, partir del segmento código anterior.¿Es posible agrupar la información con arreglo otra variable? Construye algunas medidas de resumen con esos grupos y gráficalos.¿Es posible agrupar la información con arreglo otra variable? Construye algunas medidas de resumen con esos grupos y gráficalos.Estas y otras tantas funciones son desarrolladas con todo detalle y de forma muy amena en el citado libro de Wickham y Grolemund, R data science, por lo que su estudio se recomienda ampliamente.En el siguiente capítulo continuamos con la exploración de la información, pero de un tipo particular de información: la información espacial.","code":"\ncovid_zmvm %>%\n  dplyr::summarise(promedio_def=mean(def_hab, na.rm=TRUE))## # A tibble: 1 x 1\n##   promedio_def\n##          <dbl>\n## 1        0.683\ncovid_zmvm %>%\n  dplyr::group_by(cve_ent) %>% \n  summarise(promedio_def=mean(def_hab, na.rm=TRUE), promedio_pos=mean(pos_hab))## # A tibble: 3 x 3\n##   cve_ent promedio_def promedio_pos\n##   <chr>          <dbl>        <dbl>\n## 1 09             1.08         15.8 \n## 2 13             0.790         5.88\n## 3 15             0.572         4.57\ncovid_zmvm %>% \n  dplyr::group_by(cve_ent) %>%\n  dplyr::summarise(pob_tot=sum(pob20),im_medio=mean(im),tot_mun=n())## # A tibble: 3 x 4\n##   cve_ent  pob_tot im_medio tot_mun\n##   <chr>      <dbl>    <dbl>   <int>\n## 1 09       9209944     60.1      16\n## 2 13        168302     59.4       1\n## 3 15      12426269     58.0      59"},{"path":"mapas-coropléticos-en-r.html","id":"mapas-coropléticos-en-r","chapter":"2 Mapas coropléticos en R","heading":"2 Mapas coropléticos en R","text":"","code":""},{"path":"mapas-coropléticos-en-r.html","id":"datos-y-datos-espaciales","chapter":"2 Mapas coropléticos en R","heading":"2.1 Datos y datos espaciales","text":"De forma reciente, la información georreferenciada y los Sistemas de Información Geográfica (SIG) se han vuelto un instrumento de primer orden para el análisis y la presentación de fenómenos asociados al espacio territorial. Por ejemplo, los Tableros de la Universidad Johns Hopkins sobre la pandemia por COVID19, o los del Gobierno de México y CentroGeo han estado constantemente en los medios de comunicación.En este capítulo apuntamos algunas de las características básicas de los datos y los los datos espaciales y buscamos brindar algunos elementos generales que te permitan familiarizarte con ese tipo de información y usarla para analizar fenómenos socioterritoriales.Los datos son valores, números o registros originados partir de un proceso de recolección y procesamiento. Sirven para diversos propósitos, tales como el análisis científico y la toma de decisiones. Los datos se originan partir de mediciones de conjuntos de objetos de la realidad. En estadística, ese conjunto de objetos de los que nos interesa saber ciertas cosas es denominado población y la información de sus rasgos o características es denominada variable o atributo. Así, las variables son medidas o características de los elementos que conforman la población.Las variables pueden ser medidas o información sobre cualidades de los objetos (hombre o mujer, viejo o joven, enfermo o sano, por ejemplo) o bien, medidas o información sobre cantidades o elementos numéricos y las llamamos respectivamente variables cualitativas y variables cuantitativas. Te recomendamos revisar este breve material para comprender más fondo qué son los datos.Un tipo particular de datos son lo que denominamos datos espaciales. La realidad es continua y compleja. Muchos fenómenos tienen un referente territorial, es decir, se desarrollan y ocurren en un determinado lugar. Los datos espaciales implican un esfuerzo de abstracción, es decir, de simplificación de la realidad. Este proceso de abstracción consiste en “reducir o dividir esta continuidad en entidades numéricas discretas, observables y susceptibles de medición matemática” (así, un dato espacial podría definirse como la) “observación de una variable asociada una localización del espacio geográfico” (Chasco 2003, 17).Esxiten dos tipos de datos espaciales:Datos vectoriales: es una forma de representación de la realidad que consiste en el uso de puntos, líneas y polígonos para recoger una parte de la realidad.Datos ráster: son imágenes del territorio provenientes de instrumentos ópticos (cámaras montadas en drones, helicópteros o avionetas, o incluso sensores en satélites) que almacenan determinados valores en una retícula de cuadros o pixeles que forman la imagen.1Los datos espaciales se caracterizan por poseer tres componentes: localización, atributos y tiempo. Hay otro aspecto relacionado con la información espacial denominado calidad del dato geoespacial, que se refiere que nuestro conjunto de información cumpla los requisitos necesarios para satisfacer la necesidad para la que se recabó, es decir, que sea útil para resolver la pregunta que motivó su recolección o uso.El material aquí desarrollado se sirve de información espacial de tipo vectorial. Diversos formatos sirven para almacenar información de tipo vectorial, tales como Shape, GeoJSON, GeoPackage o KML (Keyhole Markup Language). obstante, los shapefiles son aún los más comunes.Los archivos vectoriales de tipo SHP fueron desarrollados por ESRI, empresa dedicada la cosultoría de fenómenos territoriales. Almacenar información vectorial en un archivo SHP implica usar en realidad tres archivos diferentes:Archivo con extensión .shp: es un archivo que almacena las entidades espaciales representadas ya sea través de puntos, líneas o polígonos.Archivo con extensión .dbf (data base file): contiene los atributos o variables de cada objeto espacial contenido en el archivo shp.Archivo con extensión .shx (index file): archivo que sirve de vínculo entre los dos previos.Los tres archivos2 llevan todos el mismo nombre y deben estar en el mismo directorio (carpeta) para poder funcionar correctamente, es decir, para que sean leídos por el sistema de cómputo. Para que comprendas esto cabalidad, descarga el conjunto de información espacial que usaremos aquí. Tras descomprimir la carpeta verás que contiene, al menos, los siguientes archivos:covid_zmvm.shpcovid_zmvm.shxcovid_zmvm.dbfEjercicioRevisa la carpeta descargada y descomprimida para responder lo siguiente:¿Cuáles son las extensiones de los otros archivos del mismo nombre?¿Cuáles son las extensiones de los otros archivos del mismo nombre?¿Para qué sirven esos archivos?¿Para qué sirven esos archivos?Como señalamos, la exploración y representación de la información través de mapas es sólo una herramienta muy útil y potente, sino que se ha popularizado en los últimos años gracias sólo la mayor disponibilidad de información georreferenciada, sino la facilidad con la que ahora se puede acceder software para la gestión y manipulación de este tipo de información.Existen diversas herramientas para el análisis de información espacial y su representación, entre ellos se cuentan las alternativas de ESRI: ArcGIS y ArcMap. obstante, el precio de una licencia individual es considerablemente alto, convirtiéndolos en inaccesibles las mayorías. Una alternativa cada vez más popular es el programa QGIS, una iniciativa de software libre de la Fundación OSGeo, o bien, otra posibilidad es el intuitivo programa de Luc Anselin (Universidad de Chicago, Centro para las Ciencias de Datos Espaciales) y su equipo para iniciarse en el análisis espacial, GeoDa.Además, la popularidad de R como plataforma de análisis ha hecho que múltiples entusiastas programadores interesados en el análisis espacial desarrollaran paquetes enfocados en el tratamiento y representación de información georreferenciada para esta plataforma. Además de ser libre, entre las ventajas que tiene usar R como Sistema de Información Goegráfica y como espacio para el tratamiento de datos espaciales, es que se puede tener sólo un pleno control de la edición de los materiales cartográficos, sino que permite un entorno de trabajo integrado. En el resto del capítulo se presenta de forma introductoria la manera en que R, través de algunos paquetes, se puede convertir en un espacio de edición de mapas.","code":""},{"path":"mapas-coropléticos-en-r.html","id":"los-paquetes","chapter":"2 Mapas coropléticos en R","heading":"2.2 Los paquetes","text":"Para la elaboración de mapas temáticos en R requeriremos de los siguientes paquetes:sf: según se lee en la documentación de este paquete, ofrece “soporte para funciones simples, una forma estandarizada de codificar datos vectoriales espaciales. Se une GDAL para leer y escribir datos, GEOS para operaciones geométricas y PROJ para conversiones de proyección y transformaciones de datum”.tmap: este paquete permite crear mapas temáticos, es decir, “mapas geográficos en los que se visualizan distribuciones de datos espaciales”. Este paquete ofrece un “enfoque flexible, basado en capas y fácil de usar”, según se indica en la ayuda del paquete.RColorBrewer: este paquete proporciona esquemas de color para mapas y gráficos diversos que fueron diseñados por la célebre cartógrafa estadounidense Cynthia Brewer.cartogram: permite elaborar un tipo de mapa denominado cartograma.Instale estos paquetes como es usual:Para cargar los paquetes recién instaldos procedemos como es habitual.","code":"\ninstall.packages(c(\"sf\",\"tmap\", \"RColorBrewer\", \"cartogram\" ))\nlibrary(sf)\nlibrary(tmap)\nlibrary(RColorBrewer)\nlibrary(cartogram)"},{"path":"mapas-coropléticos-en-r.html","id":"carga-de-la-base-de-datos-geográfica","chapter":"2 Mapas coropléticos en R","heading":"2.3 Carga de la base de datos geográfica","text":"La base de datos utilizar es la misma que la del capítulo anterior, es decir, información de la primera ola de la pandemia por COVID19 en la Zona Metropolitana del Valle de México y algunas variables sociodemográficas y económicas; obstante, ahora haremos uso del shapefile (recuerde, tres archivos: .shp, .dbf y .shx). Debemos cargar la geometría asociada nuestra base de datos, el archivo con extensión .shp, para ello hay que usar la función st_read() del paquete sf:Nota cómo aparece en tu entorno de trabajo el objeto solicitado, zmvm_cov_sf, ¿logras ver qué tipo de objeto es? Tener esto en mente te ayudará comprender que dependiendo del tipo de objeto con el que interactuas en R habrá determinadas funciones que podrás usar. Así como hay algunas funciones que nos permiten familiarizarnos con los objetos de tipo dataframe, las hay para el caso de los objetos de tipo sf (simple features). La función st_crs() nos ofrece información sobre el sistema de coordenadas de referencia, coordinate reference system (crs), de la capa cargada:De la mañana de información, nota cómo la proyección de nuestra base de datos geográfica es la Cónica Conforme de Labert, Lambert_Conformal_Conic. Revisa esta entrada del Blog de Relaciones Productivas en México para aprender más sobre sistemas de coordenadas de referencia.Usando la función graphics::plot() es posible elaborar una primera representación de algunas de las variables de nuestra base, sólo para darnos cuenta de que ahora tratamos con información espcial, es decir, objetos espaciales (alcaldías y municipios) los que se asocian determinados atributos o variables.","code":"\nzmvm_cov_sf <-sf::st_read(\"base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp\")## Reading layer `covid_zmvm' from data source \n##   `C:\\Users\\Jarvis\\Desktop\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 76 features and 55 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5\n## Projected CRS: Lambert_Conformal_Conic\nst_crs(zmvm_cov_sf)## Coordinate Reference System:\n##   User input: Lambert_Conformal_Conic \n##   wkt:\n## PROJCRS[\"Lambert_Conformal_Conic\",\n##     BASEGEOGCRS[\"GCS_GRS 1980(IUGG, 1980)\",\n##         DATUM[\"D_unknown\",\n##             ELLIPSOID[\"GRS80\",6378137,298.257222101,\n##                 LENGTHUNIT[\"metre\",1,\n##                     ID[\"EPSG\",9001]]]],\n##         PRIMEM[\"Greenwich\",0,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n##     CONVERSION[\"unnamed\",\n##         METHOD[\"Lambert Conic Conformal (2SP)\",\n##             ID[\"EPSG\",9802]],\n##         PARAMETER[\"Latitude of false origin\",12,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433],\n##             ID[\"EPSG\",8821]],\n##         PARAMETER[\"Longitude of false origin\",-102,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433],\n##             ID[\"EPSG\",8822]],\n##         PARAMETER[\"Latitude of 1st standard parallel\",17.5,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433],\n##             ID[\"EPSG\",8823]],\n##         PARAMETER[\"Latitude of 2nd standard parallel\",29.5,\n##             ANGLEUNIT[\"Degree\",0.0174532925199433],\n##             ID[\"EPSG\",8824]],\n##         PARAMETER[\"Easting at false origin\",2500000,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8826]],\n##         PARAMETER[\"Northing at false origin\",0,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8827]]],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"metre\",1,\n##                 ID[\"EPSG\",9001]]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"metre\",1,\n##                 ID[\"EPSG\",9001]]]]\nplot(zmvm_cov_sf)## Warning: plotting the first 9 out of 55 attributes; use max.plot = 55 to plot\n## all"},{"path":"mapas-coropléticos-en-r.html","id":"mapas-coropléticos-básicos","chapter":"2 Mapas coropléticos en R","heading":"2.4 Mapas coropléticos básicos","text":"Los mapas temáticos en R se construyen con el paquete tmap. La lógica de la construcción de mapas con tmap es semejante la de ggplot2: se usan “capas” o enunciados para completar diferentes elementos del mapa. Así, para construir un mapa completo habrá que indicar, la mayor parte de las veces, tres elementos: la geometría de origen, función tm_shape(), los límites internos de las formas utilizadas, función tm_borders(), y la manera en que han de ser rellenados los polígonos, función tm_fill().Así, la elaboración de nuestro mapa implica un segmento de código con tres elementos:Resulta evidente que el segmento de código anterior arrojará un error en la consola, pues hay que especificar los argumentos del caso. Así, para representar nuestra información usando los tres elementos para la variable pos_hab, número de casos positivos de COVID19 por cada mil habitantes:Nota cómo dentro de la función de borde se encuentran operando argumentos por defecto, es decir, que sin indicar ningún argumento específico (como tipo de borde o ancho), arroja un resultado. Sobre dichos argumentos volveremos después.EjercicioPrueba eliminando alternativamente una de las funciones de la triada anterior. ¿Qué resultado obtienes con cada combinación?Alternativamente, una manera rápida de suplir las funciones de borde y relleno es sustituirlas con la función tm_polygons()La función tm_polygons() representará la variable indicada en el argumento col=. Además, tanto en esta función como en la de relleno, tm_fill(), el método de clasificación es el método o estilo estético o “bonito” en el que los cortes o categorías se dividen en números enteros siempre que sea posible y los espacia uniformemente (argumento style = \"pretty\").Con los elementos anteriores tienes ya las bases y la lógica para construir mapas temáticos en R, lo demás son más que elementos de personalización para cada una de las funciones previas, además de la incorporación de funciones adicionales para elaborar mapas mucho más profesionales.Te recomendamos consultar el espléndido material Geocomputation R de Lovelace, Nowosad, y Muenchow (2019) y todo lo relacionado con el paquete tmap en tmap: Thematic Maps R(Tennekes 2018)","code":"\ntmap::tm_shape(\"capa shp origen de la información\")+\n  tmap::tm_borders(\"elementos de edición del borde interno\")+\n  tmap::tm_fill(\"elementos de edición del relleno y representación de la variable\")\ntmap::tm_shape(shp=zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_polygons(col=\"pos_hab\")"},{"path":"mapas-coropléticos-en-r.html","id":"personalización","chapter":"2 Mapas coropléticos en R","heading":"2.5 Personalización","text":"","code":""},{"path":"mapas-coropléticos-en-r.html","id":"argumentos-de-personalización-dentro-de-la-función-de-relleno","chapter":"2 Mapas coropléticos en R","heading":"2.5.1 Argumentos de personalización dentro de la función de relleno","text":"","code":""},{"path":"mapas-coropléticos-en-r.html","id":"paleta-de-colores","chapter":"2 Mapas coropléticos en R","heading":"2.5.1.1 Paleta de colores","text":"Nos concentraremos ahora en revisar cómo podemos hacer “mapas la medida”, es decir, personalizar prácticamente cada parte del mapa, desde la gama cromática hasta títulos y leyendas. Comencemos revisando las posibilidades de personalización dentro de la función de relleno, tm_fill(). Veremos primero cómo cambiar la paleta de colores de relleno través del argumento palette=. Por defecto, los colores corresponden una gama cromática de rojos, pero podemos cambiarla una de color naranja:EjercicioPrueba cambiar el esquema de colores por aquellos que sean de tu gusto, ¿cuáles son los colores admitidos por R dentro del argumento de paleta?Una manera alternativa de cambiar la paleta de colores consiste en fijar un color para la clase inicial y uno para la clase final, manera de anclas, es decir. Para ello nos servimos de un vector:O para tener una escala divergente, insertamos otro color en el centro:obstante, la manera más profesional de visualizar la información través de una gama de colores adecuada es través de las propuestas de especialistas, como la cartógrafa estadounidense Cynthia Brewer, quien diseñó toda una serie de propuestas para la representación de información espacial en función del tipo de variable representar y del número de categorías deseadas.En R, se cuenta con el paquete RColorBrewer que permite, siguiendo los principios de la célebre cartógrafa, elegir entre una amplia gama de posibilidades de paletas de colores. El paquete permite crear un esquema de colores personalizado indicando los “colores ancla” y el número de categorías.Primero usaremos la función brewer.pal() para obtener una gama de colores que van del azul al verde, clasificados en seis categorías. El resultado aparece en código hexagesimal:Los códigos hexagesimales poco nos dicen sobre colores, entonces, para visualizarlos hemos de usar la función display.brewer.pal()Ahora, ya que tenemos una paleta que luce más estética, podemos aplicarla nuestro mapa:EjercicioSolicita ayuda del paquete RColorBrewer para ver los diferentes tipos de paletas (con las que se (secuenciales, divergentes y cualitativas) e intenta hacer algunos mapas con dichas paletas.Solicita ayuda del paquete RColorBrewer para ver los diferentes tipos de paletas (con las que se (secuenciales, divergentes y cualitativas) e intenta hacer algunos mapas con dichas paletas.Ejecuta el siguiente segmento de código para explorar una aplicación con shiny, otro paquete más de R, que te permitirá ver todas las posibilidades de paletas creadas por Brewer.Ejecuta el siguiente segmento de código para explorar una aplicación con shiny, otro paquete más de R, que te permitirá ver todas las posibilidades de paletas creadas por Brewer.","code":"\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = \"Oranges\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = c(\"blue\",\"red\"))\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = c(\"blue\",\"white\", \"red\"))\nRColorBrewer::brewer.pal(6,\"BuGn\")## [1] \"#EDF8FB\" \"#CCECE6\" \"#99D8C9\" \"#66C2A4\" \"#2CA25F\" \"#006D2C\"\nRColorBrewer::display.brewer.pal(6,\"BuGn\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", palette = \"BuGn\" )\ninstall.packages(\"shiny\",\"shinyjs\")#Instala Shiny y sus dependencias\ntmaptools::palette_explorer() #Lanza la aplicación de Shiny para mostrar las paletas Brewer\n#El modo estatico del libro no permite que se pueda correr este codigo, no obstante, con R en ejecución no existe ningún problema"},{"path":"mapas-coropléticos-en-r.html","id":"leyenda","chapter":"2 Mapas coropléticos en R","heading":"2.5.1.2 Leyenda","text":"Para modificar la forma en que aparece la leyenda, el argumento del caso es title=. Por ejemplo:Podemos también agregar elementos informativos adicionales nuestro mapa, como un histograma, través del argumento lógico legend.hist=:","code":"\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Casos positivos COVID19\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Casos positivos COVID19\",legend.hist = TRUE)"},{"path":"mapas-coropléticos-en-r.html","id":"mapas-de-clasificación","chapter":"2 Mapas coropléticos en R","heading":"2.5.1.3 Mapas de clasificación","text":"Quizá el elemento de personalización más importante tiene que ver con el método de clasificación de la variable que busca ser representada en el mapa. Todas las alternativas de cómo deben construirse las categorías resultado de la clasificación deben ser especificadas dentro de la función tm_fill().Hay diferentes métodos de clasificación y, por tanto, diferentes tipos de mapas. Podemos organizar dichos tipos de mapas de coropletas en tres grandes familias, tal como se muestra en el cuadro 2.1:Cuadro 2.1. Formas de clasificación de familias de mapasCuantilesCuantilesIntervalos igualesIntervalos igualesCortes naturales o JenksCortes naturales o JenksCortes estéticosCortes estéticosPercentilesPercentilesDesviación estándarDesviación estándarCajaCajaValores únicosTodos estos tipos de mapas pueden ser fácilmente invocados en un programa como GeoDa o QGIS, sin embargo, en R todos están disponibles y veces la construcción de alguno de ellos exige algunos fundamentos de programación. pesar de ello, R ofrece bastantes alternativas sencillas para construir mapas de clasificación común.EjercicioVe la ayuda de la función tm_fill(), identifica el argumento style=, sigue la documentación sugerida y responde:¿Cuántos métodos de clasificación ofrece R?¿Cuántos métodos de clasificación ofrece R?¿En qué consiste el método de clasificación de k-medias?¿En qué consiste el método de clasificación de k-medias?¿En el trabajo de quiénes está basado el método de clasificación fisher?.¿En el trabajo de quiénes está basado el método de clasificación fisher?.Los mapas que es posible construir en R sin mayores complicaciones se muestran en el cuadro 2.2, así como los valores que debe indicar en el argumento del caso:Cuadro 2.2. Tipos de mapasstyle=quantilestyle=equalstyle=jenksstyle=prettyComo se dijo, los mapas de clasificación común quedarán indicados en los argumentos de la función tm_fill(). Para construir un mapa de 5 cuantiles (quintiles), que es la opción por defecto para el número de categorías:Si deseas cambiar el número de categorías, por ejemplo cuatro, deberás indicar explícitamente su número en el el argumento n=:EjerciciosConstruye:Un mapa de Jenks con 6 categorías.Un mapa de Jenks con 6 categorías.Un mapa de intervalos iguales con cuatro categorías.Un mapa de intervalos iguales con cuatro categorías.Un mapa partir de la clasificación por clusters jerárquicos.Un mapa partir de la clasificación por clusters jerárquicos.Se dijo antes que pesar de haber especificado argumento alguno en la función de borde, en ésta operan argumentos por defecto. Es momento de modificar dichos argumentos. Las opciones de borde se especifican dentro de tm_border() donde es posible modificar el color (col), grosor (lwd) y tipo de borde (lty):EjercicioSolicita ayuda de la función tm_fill() y explora de qué otro modo puedes colocar los bordes. Haga algunos mapas para la variable def_hab cambiando el tipo de borde.Con la combinación de las funciones previamente descritas se puede generar un mapa básico, además, dentro de ellas es posible personalizar múltiples elementos. obstante, un diseño más adecuado y profesional es logrado través de la función tm_layout(), que abarca, entre otras cosas, la posición de la leyenda, el título principal del mapa y tamaños de fuente. Veamos cómo opera.Para cambiar la posición de la leyenda, los argumentos deben colocarse dentro de la función, tm_layout(), través de un vector que indique tanto la orientación vertical como la horizontal de la leyenda. Para la orientación horizontal: \"left\",\"right\" o \"center\" y para la vertical: \"top\", \"center\" o \"bottom\", tal que:Para posicionar la leyenda fuera del mapa, dentro de la función tm_layout() usa el argumento lógico legend.outside y si se deseas especificar la posición, el argumento será legend.outside.position que tomará los valores tipo texto de \"top\", \"bottom\", \"right\" o \"left\". Por ejemplo:Para personalizar el título dentro del mapa, en la función tm_layout() hay que agregar el argumento title= y para la posición: title.position=:O bien, si queremos el título afuera del mapa, con un tamaño de fuente diferente y centrado:En R es posible añadir nuestros mapas temáticos un mapa base para dar contexto nuestra representación. Para ello, es necesario activar una suerte de “modo interactivo”, para ser más exactos, lo que activamos es el modo de visualización para poder desplazarnos sobre los mapas. Para cambiar el modo de visualización:La función que permite agregar mapas base es tm_basemap().Ejercicio¿Cuántos tipos de mapas base es posible usar en R? Revisa la ayuda de la función y familiarízate con sus argumentos.De los argumentos para el mapa base, dos son los básicos: el servidor de donde tomaremos el mapa (consejo: ya dentro de los argumentos de la función, escribe providers$ y observarás todas las opciones disponibles) y transparencia (argumento alpha=) que toma valores de 0 1, este argumento altera tanto la transparencia del mapa base cuando es usado dentro de la función tm_basemap(), como la transparencia de la información representada si se usa dentro de la función tm_fill():Una vez que terminado de trabajar con mapas base es necesario desactivar el modo de visualización y regresar al modo estático:Un cartograma deforma la geometría de las áreas de interés en figuras cuyo tamaño depende de la magnitud de la variable que se desee representar. Para elaborar un cartograma con circulos, debemos primero generar la geometría deformada por la variable para luego rellenarla. Para tal efecto, usamos la función cartogram_dorling() que nos permitirá generar un nuevo objeto que contiene las geometrías deformadas. Luego, usaremos dicha geometría como argumento de tm_shape(). Veamos.Primero creamos la nueva geometría circular:Hay que tener en cuenta que la función sólo admite variables negativas. Ahora bien, este objeto recién creado será usado tal como lo hemos hecho antes:Otro tipo de cartograma, más estético, es aquel que garantiza la contigüidad entre las unidades espaciales. Se procede de forma semejante lo hecho antes, sólo que ahora usamos la función cartogram_cont():Ahora, usamos esta información para construir nuestro cartograma:En el blog de mappingGIS encontramos la siguiente definición de punto medio:Toda geometría vectorial (punto, línea o polígono) contiene un punto central denominado centroide.Calcular el centroide de una geometría suele ser una tarea habitual cuando se trabaja con información espacial. Para crear una nueva capa que contenga los centroides de nuestra geometría usamos en R la función st_centroid(). La función generará una nueva serie de archivos SHP que contendrán el conjunto de variables de la base de datos y los centroides:El conjunto de datos recién creado y que contiene los centroides puede ser representado agregando otra “capa” con la función tm_shape(). En el ejemplo, sólo se superponen las dos capas: la de los polígonos originales y la de los centroides:Como pudo notarlo, hemos agregado otra función que define la forma en que ha de representarse la capa de los centroides: tm_dots(). Ahí, es posible especificar la manera en que deseamos que aparezcan los centroides, por ejemplo, en color rojo y más grandes:Para guardar la capa que contiene los centroides en un nuevo archivo SHP, usamos las siguientes líneas de código:Como siempre, se recomienda revisar la documentación de ayuda de la función st_write() para conocer todos los detalles de los argumentos.En esta sección se construyen dos tipos de mapas de clasificación especial, es decir, destinados hacer notar los valores atípicos. obstante, la construcción de dichos mapas en R supone cierto conocimiento sobre programación que está fuera del alcance de estas notas. Si deseas profundizar en el aprendizaje de programación en R, el libro de Roger D. Peng, R Programming Data Science es un excelente material, particularmente el capítulo 14 dedicado las funciones en R Peng (2015).Con esta advertencia, llevamos cabo la exposición de esta sección, esperando motivarte para que tu mismo profundices en los tópicos de programación.Se señaló antes que, hasta donde tenemos conocimiento, en R es posible construir mapas de clasificación especial con tmap través de las opciones de estilo, obstante, con algunos elementos de programación es posible solventar esta tarea. En esta sección nos servimos del código proporcionado por Luc Anselin y su equipo quienes, en un esfuerzo de difusión del conocimiento sobre el uso de estas herramientas, pone nuestra disposición una basta cantidad de materiales en la página del Center Spatial Data Science de la Universidad de Chicago. Anselin y Morrison (2018)Para construir un mapa de intervalos definidos por el usuario, hay que recurrir al argumento breaks= dentro de la función tm_fill(). Para definir los intervalos de forma adecuada, se recomienda mirar las características resumen de la variable de interés, en este caso el número de casos positivos por COVID19 por cada mil habitantes, pos_hab:Esto nos permitirá conocer los valores que toma la variable de interés y pensar la manera en que deseamos delimitar las categorías de nuestro mapa. Supongamos que deseamos 6 categorías y, dado el rango de nuestra variable (valores entre 1.720 y 22.7), podemos fijar los cortes de los intervalos en 2.0, 6.0, 12., 18.0 y 24.0; además, se requiere incluir también un mínimo y máximo, digamos 1.0 y 25.0, para las categorías.La información, tanto de los cortes como de los máximos y mínimos deber ser especifica en forma de vector: c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0). Debemos además especificar la paleta de colores que deseamos usar, atendiendo lo dicho antes, usaremos una paleta con tres anclas: del amarillo, al naranja y al café:YlOrBr:Un mapa de percentiles es un tipo espacial de mapa de cuantiles en el que se especifican seis categorías: 0-1%,1-10%, 10-50%,50-90%,90-99% y 99-100%. Para poder construirlo, en R debemos llevar cabo los siguientes pasos:Extraer la variable de nuestro arreglo de datos.Calcular los percentiles de nuestro interés, es decir, 0,0.01,0.1,0.5,0.9,0.99,1.0.Construir el mapa con base en los intervalos definidos través de la función tm_fill().Para el paso , lo primero será construir una función que llamaremos get.var, que tendrá dos argumentos, varnom y df, el primero indicará, entre comillas, el nombre de la variable utilizar y el segundo indicará la base de la que proviene. Esta función permite que, al extraer la variable de la base, se eliminen los “aspectos espaciales” asociadas ella. Esta función sólo se requiere construir una vez.Ahora, echando mano de la función creada extraeremos la variable de interés sin sus aspectos espaciales:Para el paso ii, hemos de crear un objeto en el que se guarden los percentiles de interés, un vector de 6 elementos:Ahora, se calculan y guardan valores de los percentiles con base en el par de objetos creados, percentiles y pos_hab, es decir, obtendremos los valores de la variable pos_hab en los percentiles de interés:Ahora, en el paso iii, la construcción del mapa de percentiles es posible uniendo todos los elementos:Con el camino que hemos seguido, es posible ahorrarnos muchos pasos y crear nuestra propia función (que opera en el entorno de trabajo activo de la sesión) para construir mapas de percentiles. Nuestra función se llamará percentmap(). Nuestra función tendrá los siguientes argumentos:varnom: nombre de la variable (cadena, entre comillas).df: base de datos que contiene el variable.legtitle: titulo de la leyenda del mapa.mtitle: título del mapa.Para crear la función:La función que hemos creado para hacer nuestros mapas tiene cuatro argumentos: varnom,df,legtitle y mtitle. Los dos últimos tienen valores por omisión, lo que significa que es necesario especificarlos al usar la función. Los dos primeros tienen valor por omisión, por lo que será forzoso especificar dichos argumentos.Ahora, invocando nuestra propia función e indicando los argumentos forzosos, varnom y df tenemos que:Podemos, como es obvio, cambiar los dos argumentos dados por omisión:Los capítulos 1 y 2 de este libro, constituyen lo que suele ser denominado Análisis Exploratorio de Datos, (Exploratory Data Analysis, EDA). El capítulo 1 del e-Handbook Statistical Methods expone con detalle esta concepción en el análisis de información Croarkin y Tobias (2014). Además, capítulo 7 del ya citado libro R Data Science también explica con detalle el enfoque EDA usando R.En el siguiente capítulo continuamos con la exploración de la información, pero incorporando una estructura de relaciones en el espacio, por lo que dicho enfoque se le conoce como Análisis Exploratorio de Datos Espaciales. En dicho capítulo será de nuestro interés particular un rasgo que suele estar presente en la información georreferenciada: la autocorrelación espacial.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2022-01-19.book built bookdown R package.","code":"\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap::tm_borders()+\n  tmap::tm_layout(title = \"Mapa de cuantiles\", title.position = c(\"center\", \"bottom\"))+\n  tmap::tm_fill(\"pos_hab\", style = \"quantile\", title = \"Casos positivos\")\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap:: tm_borders()+\n  tmap::tm_layout(title = \"Mapa de cuantiles\", title.position = c(\"center\", \"bottom\"))+\n  tmap::tm_fill(\"pos_hab\", n= 4, style = \"quantile\", title = \"Casos positivos\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders(col=\"black\",lwd=2, lty = 3)\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders()+\n  tmap::tm_layout(legend.position = c(\"right\", \"bottom\"))\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders()+\n  tmap::tm_layout(legend.outside = TRUE,legend.outside.position = \"left\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Casos positivos COVID19\")+\n  tmap::tm_layout(title = \"Casos positivos COVID19 por cada mil habitales\", title.position = c(\"center\",\"top\"))\ntmap::tm_shape(zmvm_cov_sf)+\n tmap:: tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Ingreso 2010\")+\n  tmap::tm_layout(main.title = \"Casos positivos COVID19 por cada mil habitantes\", main.title.position = \"center\", title.size = 1.3)\ntmap::tmap_mode(\"view\")## tmap mode set to interactive viewing\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", alpha=0.7)+\n  tmap::tm_basemap(providers$OpenStreetMap,alpha = 0.5)\ntmap::tmap_mode(\"plot\")## tmap mode set to plotting\ncartograma.circulos <- cartogram::cartogram_dorling(zmvm_cov_sf,\"pos_hab\")\nclass(cartograma.circulos)## [1] \"sf\"         \"data.frame\"\ntmap::tm_shape(cartograma.circulos) +\n  tmap::tm_borders()+\n  tmap:: tm_fill(\"pos_hab\")\ncartograma.cont <- cartogram_cont(zmvm_cov_sf,\"pos_hab\")\ntmap::tm_shape(cartograma.cont) +\n  tmap::tm_fill(\"pos_hab\") +\n tmap:: tm_borders()\nzmvm_cntrds <- st_centroid(zmvm_cov_sf)\nsummary(zmvm_cntrds)##     cvemun            cve_ent            cve_mun             nom_zm         \n##  Length:76          Length:76          Length:76          Length:76         \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n##                                                                             \n##      cve_zm       nom_mun            nom_ent            nom_abr         \n##  Min.   :9.01   Length:76          Length:76          Length:76         \n##  1st Qu.:9.01   Class :character   Class :character   Class :character  \n##  Median :9.01   Mode  :character   Mode  :character   Mode  :character  \n##  Mean   :9.01                                                           \n##  3rd Qu.:9.01                                                           \n##  Max.   :9.01                                                           \n##                                                                         \n##       ext             pob20            pob20_h          pob20_m      \n##  Min.   :  3.17   Min.   :   4862   Min.   :  2338   Min.   :  2524  \n##  1st Qu.: 37.52   1st Qu.:  31900   1st Qu.: 15621   1st Qu.: 16302  \n##  Median : 76.22   Median : 160445   Median : 78574   Median : 81871  \n##  Mean   :103.51   Mean   : 286902   Mean   :138458   Mean   :148443  \n##  3rd Qu.:157.01   3rd Qu.: 432692   3rd Qu.:206156   3rd Qu.:226858  \n##  Max.   :434.26   Max.   :1835486   Max.   :887651   Max.   :947835  \n##                                                                      \n##    positivos         defuncione        pos_mil          pos_hab      \n##  Min.   :   18.0   Min.   :   1.0   Min.   : 1.720   Min.   : 1.720  \n##  1st Qu.:  131.5   1st Qu.:  14.5   1st Qu.: 3.337   1st Qu.: 3.337  \n##  Median :  767.0   Median :  99.0   Median : 5.117   Median : 5.117  \n##  Mean   : 2614.0   Mean   : 273.3   Mean   : 6.964   Mean   : 6.964  \n##  3rd Qu.: 3617.2   3rd Qu.: 387.0   3rd Qu.: 8.636   3rd Qu.: 8.636  \n##  Max.   :18767.0   Max.   :2078.0   Max.   :22.700   Max.   :22.700  \n##                    NA's   :1                                         \n##     def_hab              ss           ppob_sines         ppob_basi     \n##  Min.   :0.09947   Min.   :0.5480   Min.   :0.004937   Min.   :0.1410  \n##  1st Qu.:0.42801   1st Qu.:0.6415   1st Qu.:0.019428   1st Qu.:0.4481  \n##  Median :0.65722   Median :0.6763   Median :0.023845   Median :0.5295  \n##  Mean   :0.68252   Mean   :0.6793   Mean   :0.026127   Mean   :0.5041  \n##  3rd Qu.:0.88532   3rd Qu.:0.7266   3rd Qu.:0.031717   3rd Qu.:0.5718  \n##  Max.   :1.64968   Max.   :0.7980   Max.   :0.064699   Max.   :0.6842  \n##  NA's   :1                                                             \n##    ppob_media        ppob_sup           ocviv            occu       \n##  Min.   :0.1645   Min.   :0.07101   Min.   :2.460   Min.   :0.5600  \n##  1st Qu.:0.2466   1st Qu.:0.13614   1st Qu.:3.530   1st Qu.:0.8600  \n##  Median :0.2644   Median :0.17064   Median :3.745   Median :0.9900  \n##  Mean   :0.2624   Mean   :0.20549   Mean   :3.690   Mean   :0.9692  \n##  3rd Qu.:0.2831   3rd Qu.:0.25936   3rd Qu.:3.900   3rd Qu.:1.0625  \n##  Max.   :0.3235   Max.   :0.67480   Max.   :4.520   Max.   :1.2900  \n##                                                                     \n##    pintegra4_       pintegra6_        pintegra8_        ppob_5_o_m    \n##  Min.   :0.3657   Min.   :0.06644   Min.   :0.01745   Min.   :0.6825  \n##  1st Qu.:0.6772   1st Qu.:0.22736   1st Qu.:0.07521   1st Qu.:0.7781  \n##  Median :0.7161   Median :0.26250   Median :0.08992   Median :0.8183  \n##  Mean   :0.7013   Mean   :0.26006   Mean   :0.09188   Mean   :0.8133  \n##  3rd Qu.:0.7474   3rd Qu.:0.29090   3rd Qu.:0.10844   3rd Qu.:0.8569  \n##  Max.   :0.8261   Max.   :0.42810   Max.   :0.20046   Max.   :0.9150  \n##                                                                       \n##    ppob_3_o_m         ppob_1          ppob_1dorm        ppob_2dorm    \n##  Min.   :0.1680   Min.   :0.00611   Min.   :0.08567   Min.   :0.4875  \n##  1st Qu.:0.3425   1st Qu.:0.03234   1st Qu.:0.20592   1st Qu.:0.5883  \n##  Median :0.3925   Median :0.04198   Median :0.22763   Median :0.6237  \n##  Mean   :0.3934   Mean   :0.04413   Mean   :0.23367   Mean   :0.6307  \n##  3rd Qu.:0.4406   3rd Qu.:0.05341   3rd Qu.:0.26855   3rd Qu.:0.6757  \n##  Max.   :0.5947   Max.   :0.09461   Max.   :0.38234   Max.   :0.8006  \n##                                                                       \n##    ppob_3dorm       pviv_ocu5_       pviv_ocu7_         pviv_ocu9_      \n##  Min.   :0.7781   Min.   :0.0654   Min.   :0.009724   Min.   :0.002354  \n##  1st Qu.:0.8581   1st Qu.:0.2434   1st Qu.:0.056245   1st Qu.:0.015350  \n##  Median :0.8848   Median :0.2835   Median :0.069583   Median :0.020738  \n##  Mean   :0.8808   Mean   :0.2799   Mean   :0.069614   Mean   :0.020961  \n##  3rd Qu.:0.9042   3rd Qu.:0.3209   3rd Qu.:0.082869   3rd Qu.:0.025183  \n##  Max.   :0.9484   Max.   :0.4420   Max.   :0.155107   Max.   :0.060174  \n##                                                                         \n##      analf            sbasc             vhac           po2sm      \n##  Min.   :0.3534   Min.   : 5.535   Min.   : 3.95   Min.   :28.45  \n##  1st Qu.:1.6505   1st Qu.:19.915   1st Qu.:16.40   1st Qu.:61.27  \n##  Median :1.9798   Median :22.973   Median :21.19   Median :67.40  \n##  Mean   :2.3555   Mean   :23.160   Mean   :21.01   Mean   :66.38  \n##  3rd Qu.:2.8267   3rd Qu.:27.193   3rd Qu.:25.00   3rd Qu.:72.76  \n##  Max.   :7.4096   Max.   :41.399   Max.   :38.81   Max.   :86.67  \n##                                                                   \n##        im          gm_2020               grad            grad_h      \n##  Min.   :53.57   Length:76          Min.   : 8.080   Min.   : 8.140  \n##  1st Qu.:57.36   Class :character   1st Qu.: 9.557   1st Qu.: 9.658  \n##  Median :58.46   Mode  :character   Median :10.025   Median :10.160  \n##  Mean   :58.47                      Mean   :10.252   Mean   :10.376  \n##  3rd Qu.:59.65                      3rd Qu.:10.840   3rd Qu.:10.967  \n##  Max.   :62.36                      Max.   :14.550   Max.   :14.930  \n##                                                                      \n##      grad_m           poind             pocom             poss       \n##  Min.   : 8.030   Min.   :0.06475   Min.   :0.1269   Min.   :0.2113  \n##  1st Qu.: 9.490   1st Qu.:0.11379   1st Qu.:0.3363   1st Qu.:0.3033  \n##  Median : 9.945   Median :0.17993   Median :0.4358   Median :0.3610  \n##  Mean   :10.139   Mean   :0.19532   Mean   :0.4085   Mean   :0.3961  \n##  3rd Qu.:10.730   3rd Qu.:0.26407   3rd Qu.:0.4970   3rd Qu.:0.4176  \n##  Max.   :14.220   Max.   :0.44291   Max.   :0.6867   Max.   :0.7686  \n##                                                                      \n##      tmind            tmcom             tmss            rmind       \n##  Min.   : 1.636   Min.   : 1.380   Min.   : 1.670   Min.   : 10.26  \n##  1st Qu.: 3.166   1st Qu.: 1.876   1st Qu.: 2.072   1st Qu.: 30.95  \n##  Median : 5.086   Median : 2.139   Median : 2.775   Median : 63.61  \n##  Mean   : 9.086   Mean   : 2.759   Mean   : 5.765   Mean   : 67.19  \n##  3rd Qu.: 9.195   3rd Qu.: 3.009   3rd Qu.: 4.604   3rd Qu.: 88.88  \n##  Max.   :48.067   Max.   :10.310   Max.   :41.086   Max.   :177.45  \n##                                                                     \n##      rmcom             rmss              den                   geometry \n##  Min.   : 4.083   Min.   :  2.586   Min.   :  123.2   POINT        :76  \n##  1st Qu.:14.085   1st Qu.: 16.312   1st Qu.:  463.5   epsg:NA      : 0  \n##  Median :21.002   Median : 28.922   Median : 1830.4   +proj=lcc ...: 0  \n##  Mean   :24.502   Mean   : 39.904   Mean   : 4227.5                     \n##  3rd Qu.:31.630   3rd Qu.: 52.806   3rd Qu.: 6216.9                     \n##  Max.   :67.438   Max.   :255.589   Max.   :17519.3                     \n## \ntm_shape(zmvm_cov_sf) +\n  tm_borders() +\n  tm_shape(zmvm_cntrds) +\n  tm_dots()\ntm_shape(zmvm_cov_sf) +\n  tm_borders() +\n  tm_shape(zmvm_cntrds) +\n  tm_dots(size=0.2,col=\"red\")\nst_write(obj=zmvm_cntrds, \"zmvm_cntrds\", driver = \"ESRI Shapefile\")\nsummary(zmvm_cov_sf$pos_hab)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   1.720   3.337   5.117   6.964   8.636  22.700\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\",title=\"Casos positivos covid\",breaks=c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0),palette=\"YlOrBr\")+\n  tm_layout(title = \"Cortes personalizados\", title.position = c(\"center\",\"top\"))\nget.var <- function(varnom,df) {#Definición de la función\n  v <- df[varnom] %>% st_set_geometry(NULL) #Extracción de la variable de interés y remoción de sus características geográficas\n  v <- unname(v[,1]) #Selección de la columna del data frame extraído que contiene la variable de interés y elimina su nombre pues sólo queremos un vector.\n  return(v) #Resultado de la función: la variable como vector sin sus características espaciales\n}\npos_hab<-get.var(\"pos_hab\",zmvm_cov_sf)\npos_hab##  [1] 14.364996 16.983175 13.352654 16.163929 17.363687 17.313543 14.943518\n##  [8] 17.404756 18.701249 13.677709 20.559563 10.224540 14.717256 22.700331\n## [15] 11.088257  5.882283  4.938574  5.239423  2.570694  3.351482  4.267922\n## [22]  2.382445  7.896182  2.088929  5.500198  5.891309  5.601076  9.197806\n## [29]  8.113844  3.295057  3.676214  3.250036  3.583416  6.155522  2.124319\n## [36]  2.787239  3.293624  6.120050  1.760416  5.184329  2.941489  3.873824\n## [43]  2.507745  5.423064  7.354686  2.224869  3.992095  8.598203  6.385731\n## [50]  4.385253  3.702180  3.693010  5.894044  4.147922  9.766454  3.853830\n## [57]  5.546263  4.766342  8.751090  1.719690  3.024390  2.372319  2.380410\n## [64]  3.876611  4.272596  7.134726  6.920539  5.049320  4.917293  2.582625\n## [71]  3.918632  3.399002  2.473636  2.822012  6.913242 13.935302\npercentiles <- c(0,.01,.1,.5,.9,.99,1)\nvarperc <- quantile(pos_hab,percentiles)\nvarperc##        0%        1%       10%       50%       90%       99%      100% \n##  1.719690  1.750234  2.428041  5.116824 15.553724 21.094755 22.700331\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\",title=\"Índice de marginación 2010\", breaks=varperc, palette=\"-RdBu\",labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))+\n  tmap::tm_borders() +\n  tmap::tm_layout(title = \"Mapa de percentiles\", title.position = c(\"center\",\"bottom\"))\n  percentmap <- function(varnom,df,titulo.leyenda=NA,titulo.principal=\"Mapa de percentiles\"){ #Definición de la función y sus argumentos\n  #Elementos preliminares\n  percent <- c(0,.01,.1,.5,.9,.99,1) #Vector que contiene los percentiles de interés para el mapa\n  var <- get.var(varnom,df) #Extracción de la variable de la base de datos con la función anterior\n  varperc <- quantile(var,percent) #Cálculo de los percentiles de la variable extraída\n  #Especificaciones del mapa\n  tm_shape(df) +\n     tm_fill(varnom,title=titulo.leyenda,breaks=varperc,palette=\"-RdBu\", labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(title = titulo.principal, title.position = c(\"center\",\"bottom\"))\n}\npercentmap(\"pos_hab\",zmvm_cov_sf)\npercentmap(\"pos_hab\",zmvm_cov_sf, titulo.leyenda = \"Categorías\", titulo.principal = \"El título que yo quiera\" )"},{"path":"mapas-coropléticos-en-r.html","id":"argumentos-de-personalización-del-borde","chapter":"2 Mapas coropléticos en R","heading":"2.5.2 Argumentos de personalización del borde","text":"Se dijo antes que pesar de haber especificado argumento alguno en la función de borde, en ésta operan argumentos por defecto. Es momento de modificar dichos argumentos. Las opciones de borde se especifican dentro de tm_border() donde es posible modificar el color (col), grosor (lwd) y tipo de borde (lty):EjercicioSolicita ayuda de la función tm_fill() y explora de qué otro modo puedes colocar los bordes. Haga algunos mapas para la variable def_hab cambiando el tipo de borde.","code":"\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders(col=\"black\",lwd=2, lty = 3)"},{"path":"mapas-coropléticos-en-r.html","id":"función-para-elementos-de-diseño-de-salida-tm_layout","chapter":"2 Mapas coropléticos en R","heading":"2.6 Función para elementos de diseño de salida, tm_layout()","text":"Con la combinación de las funciones previamente descritas se puede generar un mapa básico, además, dentro de ellas es posible personalizar múltiples elementos. obstante, un diseño más adecuado y profesional es logrado través de la función tm_layout(), que abarca, entre otras cosas, la posición de la leyenda, el título principal del mapa y tamaños de fuente. Veamos cómo opera.Para cambiar la posición de la leyenda, los argumentos deben colocarse dentro de la función, tm_layout(), través de un vector que indique tanto la orientación vertical como la horizontal de la leyenda. Para la orientación horizontal: \"left\",\"right\" o \"center\" y para la vertical: \"top\", \"center\" o \"bottom\", tal que:Para posicionar la leyenda fuera del mapa, dentro de la función tm_layout() usa el argumento lógico legend.outside y si se deseas especificar la posición, el argumento será legend.outside.position que tomará los valores tipo texto de \"top\", \"bottom\", \"right\" o \"left\". Por ejemplo:Para personalizar el título dentro del mapa, en la función tm_layout() hay que agregar el argumento title= y para la posición: title.position=:O bien, si queremos el título afuera del mapa, con un tamaño de fuente diferente y centrado:","code":"\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders()+\n  tmap::tm_layout(legend.position = c(\"right\", \"bottom\"))\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\")+\n  tmap::tm_borders()+\n  tmap::tm_layout(legend.outside = TRUE,legend.outside.position = \"left\")\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Casos positivos COVID19\")+\n  tmap::tm_layout(title = \"Casos positivos COVID19 por cada mil habitales\", title.position = c(\"center\",\"top\"))\ntmap::tm_shape(zmvm_cov_sf)+\n tmap:: tm_borders()+\n  tmap::tm_fill(\"pos_hab\", title = \"Ingreso 2010\")+\n  tmap::tm_layout(main.title = \"Casos positivos COVID19 por cada mil habitantes\", main.title.position = \"center\", title.size = 1.3)"},{"path":"mapas-coropléticos-en-r.html","id":"mapa-base-interactivo","chapter":"2 Mapas coropléticos en R","heading":"2.7 Mapa base interactivo","text":"En R es posible añadir nuestros mapas temáticos un mapa base para dar contexto nuestra representación. Para ello, es necesario activar una suerte de “modo interactivo”, para ser más exactos, lo que activamos es el modo de visualización para poder desplazarnos sobre los mapas. Para cambiar el modo de visualización:La función que permite agregar mapas base es tm_basemap().Ejercicio¿Cuántos tipos de mapas base es posible usar en R? Revisa la ayuda de la función y familiarízate con sus argumentos.De los argumentos para el mapa base, dos son los básicos: el servidor de donde tomaremos el mapa (consejo: ya dentro de los argumentos de la función, escribe providers$ y observarás todas las opciones disponibles) y transparencia (argumento alpha=) que toma valores de 0 1, este argumento altera tanto la transparencia del mapa base cuando es usado dentro de la función tm_basemap(), como la transparencia de la información representada si se usa dentro de la función tm_fill():Una vez que terminado de trabajar con mapas base es necesario desactivar el modo de visualización y regresar al modo estático:","code":"\ntmap::tmap_mode(\"view\")## tmap mode set to interactive viewing\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\", alpha=0.7)+\n  tmap::tm_basemap(providers$OpenStreetMap,alpha = 0.5)\ntmap::tmap_mode(\"plot\")## tmap mode set to plotting"},{"path":"mapas-coropléticos-en-r.html","id":"cartograma","chapter":"2 Mapas coropléticos en R","heading":"2.8 Cartograma","text":"Un cartograma deforma la geometría de las áreas de interés en figuras cuyo tamaño depende de la magnitud de la variable que se desee representar. Para elaborar un cartograma con circulos, debemos primero generar la geometría deformada por la variable para luego rellenarla. Para tal efecto, usamos la función cartogram_dorling() que nos permitirá generar un nuevo objeto que contiene las geometrías deformadas. Luego, usaremos dicha geometría como argumento de tm_shape(). Veamos.Primero creamos la nueva geometría circular:Hay que tener en cuenta que la función sólo admite variables negativas. Ahora bien, este objeto recién creado será usado tal como lo hemos hecho antes:Otro tipo de cartograma, más estético, es aquel que garantiza la contigüidad entre las unidades espaciales. Se procede de forma semejante lo hecho antes, sólo que ahora usamos la función cartogram_cont():Ahora, usamos esta información para construir nuestro cartograma:","code":"\ncartograma.circulos <- cartogram::cartogram_dorling(zmvm_cov_sf,\"pos_hab\")\nclass(cartograma.circulos)## [1] \"sf\"         \"data.frame\"\ntmap::tm_shape(cartograma.circulos) +\n  tmap::tm_borders()+\n  tmap:: tm_fill(\"pos_hab\")\ncartograma.cont <- cartogram_cont(zmvm_cov_sf,\"pos_hab\")\ntmap::tm_shape(cartograma.cont) +\n  tmap::tm_fill(\"pos_hab\") +\n tmap:: tm_borders()"},{"path":"mapas-coropléticos-en-r.html","id":"centroides-o-coordenadas-geométricas-punto-medio","chapter":"2 Mapas coropléticos en R","heading":"2.9 Centroides o coordenadas geométricas (punto medio)","text":"En el blog de mappingGIS encontramos la siguiente definición de punto medio:Toda geometría vectorial (punto, línea o polígono) contiene un punto central denominado centroide.Calcular el centroide de una geometría suele ser una tarea habitual cuando se trabaja con información espacial. Para crear una nueva capa que contenga los centroides de nuestra geometría usamos en R la función st_centroid(). La función generará una nueva serie de archivos SHP que contendrán el conjunto de variables de la base de datos y los centroides:El conjunto de datos recién creado y que contiene los centroides puede ser representado agregando otra “capa” con la función tm_shape(). En el ejemplo, sólo se superponen las dos capas: la de los polígonos originales y la de los centroides:Como pudo notarlo, hemos agregado otra función que define la forma en que ha de representarse la capa de los centroides: tm_dots(). Ahí, es posible especificar la manera en que deseamos que aparezcan los centroides, por ejemplo, en color rojo y más grandes:Para guardar la capa que contiene los centroides en un nuevo archivo SHP, usamos las siguientes líneas de código:Como siempre, se recomienda revisar la documentación de ayuda de la función st_write() para conocer todos los detalles de los argumentos.","code":"\nzmvm_cntrds <- st_centroid(zmvm_cov_sf)\nsummary(zmvm_cntrds)##     cvemun            cve_ent            cve_mun             nom_zm         \n##  Length:76          Length:76          Length:76          Length:76         \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n##                                                                             \n##      cve_zm       nom_mun            nom_ent            nom_abr         \n##  Min.   :9.01   Length:76          Length:76          Length:76         \n##  1st Qu.:9.01   Class :character   Class :character   Class :character  \n##  Median :9.01   Mode  :character   Mode  :character   Mode  :character  \n##  Mean   :9.01                                                           \n##  3rd Qu.:9.01                                                           \n##  Max.   :9.01                                                           \n##                                                                         \n##       ext             pob20            pob20_h          pob20_m      \n##  Min.   :  3.17   Min.   :   4862   Min.   :  2338   Min.   :  2524  \n##  1st Qu.: 37.52   1st Qu.:  31900   1st Qu.: 15621   1st Qu.: 16302  \n##  Median : 76.22   Median : 160445   Median : 78574   Median : 81871  \n##  Mean   :103.51   Mean   : 286902   Mean   :138458   Mean   :148443  \n##  3rd Qu.:157.01   3rd Qu.: 432692   3rd Qu.:206156   3rd Qu.:226858  \n##  Max.   :434.26   Max.   :1835486   Max.   :887651   Max.   :947835  \n##                                                                      \n##    positivos         defuncione        pos_mil          pos_hab      \n##  Min.   :   18.0   Min.   :   1.0   Min.   : 1.720   Min.   : 1.720  \n##  1st Qu.:  131.5   1st Qu.:  14.5   1st Qu.: 3.337   1st Qu.: 3.337  \n##  Median :  767.0   Median :  99.0   Median : 5.117   Median : 5.117  \n##  Mean   : 2614.0   Mean   : 273.3   Mean   : 6.964   Mean   : 6.964  \n##  3rd Qu.: 3617.2   3rd Qu.: 387.0   3rd Qu.: 8.636   3rd Qu.: 8.636  \n##  Max.   :18767.0   Max.   :2078.0   Max.   :22.700   Max.   :22.700  \n##                    NA's   :1                                         \n##     def_hab              ss           ppob_sines         ppob_basi     \n##  Min.   :0.09947   Min.   :0.5480   Min.   :0.004937   Min.   :0.1410  \n##  1st Qu.:0.42801   1st Qu.:0.6415   1st Qu.:0.019428   1st Qu.:0.4481  \n##  Median :0.65722   Median :0.6763   Median :0.023845   Median :0.5295  \n##  Mean   :0.68252   Mean   :0.6793   Mean   :0.026127   Mean   :0.5041  \n##  3rd Qu.:0.88532   3rd Qu.:0.7266   3rd Qu.:0.031717   3rd Qu.:0.5718  \n##  Max.   :1.64968   Max.   :0.7980   Max.   :0.064699   Max.   :0.6842  \n##  NA's   :1                                                             \n##    ppob_media        ppob_sup           ocviv            occu       \n##  Min.   :0.1645   Min.   :0.07101   Min.   :2.460   Min.   :0.5600  \n##  1st Qu.:0.2466   1st Qu.:0.13614   1st Qu.:3.530   1st Qu.:0.8600  \n##  Median :0.2644   Median :0.17064   Median :3.745   Median :0.9900  \n##  Mean   :0.2624   Mean   :0.20549   Mean   :3.690   Mean   :0.9692  \n##  3rd Qu.:0.2831   3rd Qu.:0.25936   3rd Qu.:3.900   3rd Qu.:1.0625  \n##  Max.   :0.3235   Max.   :0.67480   Max.   :4.520   Max.   :1.2900  \n##                                                                     \n##    pintegra4_       pintegra6_        pintegra8_        ppob_5_o_m    \n##  Min.   :0.3657   Min.   :0.06644   Min.   :0.01745   Min.   :0.6825  \n##  1st Qu.:0.6772   1st Qu.:0.22736   1st Qu.:0.07521   1st Qu.:0.7781  \n##  Median :0.7161   Median :0.26250   Median :0.08992   Median :0.8183  \n##  Mean   :0.7013   Mean   :0.26006   Mean   :0.09188   Mean   :0.8133  \n##  3rd Qu.:0.7474   3rd Qu.:0.29090   3rd Qu.:0.10844   3rd Qu.:0.8569  \n##  Max.   :0.8261   Max.   :0.42810   Max.   :0.20046   Max.   :0.9150  \n##                                                                       \n##    ppob_3_o_m         ppob_1          ppob_1dorm        ppob_2dorm    \n##  Min.   :0.1680   Min.   :0.00611   Min.   :0.08567   Min.   :0.4875  \n##  1st Qu.:0.3425   1st Qu.:0.03234   1st Qu.:0.20592   1st Qu.:0.5883  \n##  Median :0.3925   Median :0.04198   Median :0.22763   Median :0.6237  \n##  Mean   :0.3934   Mean   :0.04413   Mean   :0.23367   Mean   :0.6307  \n##  3rd Qu.:0.4406   3rd Qu.:0.05341   3rd Qu.:0.26855   3rd Qu.:0.6757  \n##  Max.   :0.5947   Max.   :0.09461   Max.   :0.38234   Max.   :0.8006  \n##                                                                       \n##    ppob_3dorm       pviv_ocu5_       pviv_ocu7_         pviv_ocu9_      \n##  Min.   :0.7781   Min.   :0.0654   Min.   :0.009724   Min.   :0.002354  \n##  1st Qu.:0.8581   1st Qu.:0.2434   1st Qu.:0.056245   1st Qu.:0.015350  \n##  Median :0.8848   Median :0.2835   Median :0.069583   Median :0.020738  \n##  Mean   :0.8808   Mean   :0.2799   Mean   :0.069614   Mean   :0.020961  \n##  3rd Qu.:0.9042   3rd Qu.:0.3209   3rd Qu.:0.082869   3rd Qu.:0.025183  \n##  Max.   :0.9484   Max.   :0.4420   Max.   :0.155107   Max.   :0.060174  \n##                                                                         \n##      analf            sbasc             vhac           po2sm      \n##  Min.   :0.3534   Min.   : 5.535   Min.   : 3.95   Min.   :28.45  \n##  1st Qu.:1.6505   1st Qu.:19.915   1st Qu.:16.40   1st Qu.:61.27  \n##  Median :1.9798   Median :22.973   Median :21.19   Median :67.40  \n##  Mean   :2.3555   Mean   :23.160   Mean   :21.01   Mean   :66.38  \n##  3rd Qu.:2.8267   3rd Qu.:27.193   3rd Qu.:25.00   3rd Qu.:72.76  \n##  Max.   :7.4096   Max.   :41.399   Max.   :38.81   Max.   :86.67  \n##                                                                   \n##        im          gm_2020               grad            grad_h      \n##  Min.   :53.57   Length:76          Min.   : 8.080   Min.   : 8.140  \n##  1st Qu.:57.36   Class :character   1st Qu.: 9.557   1st Qu.: 9.658  \n##  Median :58.46   Mode  :character   Median :10.025   Median :10.160  \n##  Mean   :58.47                      Mean   :10.252   Mean   :10.376  \n##  3rd Qu.:59.65                      3rd Qu.:10.840   3rd Qu.:10.967  \n##  Max.   :62.36                      Max.   :14.550   Max.   :14.930  \n##                                                                      \n##      grad_m           poind             pocom             poss       \n##  Min.   : 8.030   Min.   :0.06475   Min.   :0.1269   Min.   :0.2113  \n##  1st Qu.: 9.490   1st Qu.:0.11379   1st Qu.:0.3363   1st Qu.:0.3033  \n##  Median : 9.945   Median :0.17993   Median :0.4358   Median :0.3610  \n##  Mean   :10.139   Mean   :0.19532   Mean   :0.4085   Mean   :0.3961  \n##  3rd Qu.:10.730   3rd Qu.:0.26407   3rd Qu.:0.4970   3rd Qu.:0.4176  \n##  Max.   :14.220   Max.   :0.44291   Max.   :0.6867   Max.   :0.7686  \n##                                                                      \n##      tmind            tmcom             tmss            rmind       \n##  Min.   : 1.636   Min.   : 1.380   Min.   : 1.670   Min.   : 10.26  \n##  1st Qu.: 3.166   1st Qu.: 1.876   1st Qu.: 2.072   1st Qu.: 30.95  \n##  Median : 5.086   Median : 2.139   Median : 2.775   Median : 63.61  \n##  Mean   : 9.086   Mean   : 2.759   Mean   : 5.765   Mean   : 67.19  \n##  3rd Qu.: 9.195   3rd Qu.: 3.009   3rd Qu.: 4.604   3rd Qu.: 88.88  \n##  Max.   :48.067   Max.   :10.310   Max.   :41.086   Max.   :177.45  \n##                                                                     \n##      rmcom             rmss              den                   geometry \n##  Min.   : 4.083   Min.   :  2.586   Min.   :  123.2   POINT        :76  \n##  1st Qu.:14.085   1st Qu.: 16.312   1st Qu.:  463.5   epsg:NA      : 0  \n##  Median :21.002   Median : 28.922   Median : 1830.4   +proj=lcc ...: 0  \n##  Mean   :24.502   Mean   : 39.904   Mean   : 4227.5                     \n##  3rd Qu.:31.630   3rd Qu.: 52.806   3rd Qu.: 6216.9                     \n##  Max.   :67.438   Max.   :255.589   Max.   :17519.3                     \n## \ntm_shape(zmvm_cov_sf) +\n  tm_borders() +\n  tm_shape(zmvm_cntrds) +\n  tm_dots()\ntm_shape(zmvm_cov_sf) +\n  tm_borders() +\n  tm_shape(zmvm_cntrds) +\n  tm_dots(size=0.2,col=\"red\")\nst_write(obj=zmvm_cntrds, \"zmvm_cntrds\", driver = \"ESRI Shapefile\")"},{"path":"mapas-coropléticos-en-r.html","id":"tópico-adicional-mapas-de-clasificación-especial-elementos-de-programación","chapter":"2 Mapas coropléticos en R","heading":"2.10 Tópico adicional: Mapas de clasificación especial (elementos de programación)","text":"En esta sección se construyen dos tipos de mapas de clasificación especial, es decir, destinados hacer notar los valores atípicos. obstante, la construcción de dichos mapas en R supone cierto conocimiento sobre programación que está fuera del alcance de estas notas. Si deseas profundizar en el aprendizaje de programación en R, el libro de Roger D. Peng, R Programming Data Science es un excelente material, particularmente el capítulo 14 dedicado las funciones en R Peng (2015).Con esta advertencia, llevamos cabo la exposición de esta sección, esperando motivarte para que tu mismo profundices en los tópicos de programación.Se señaló antes que, hasta donde tenemos conocimiento, en R es posible construir mapas de clasificación especial con tmap través de las opciones de estilo, obstante, con algunos elementos de programación es posible solventar esta tarea. En esta sección nos servimos del código proporcionado por Luc Anselin y su equipo quienes, en un esfuerzo de difusión del conocimiento sobre el uso de estas herramientas, pone nuestra disposición una basta cantidad de materiales en la página del Center Spatial Data Science de la Universidad de Chicago. Anselin y Morrison (2018)Para construir un mapa de intervalos definidos por el usuario, hay que recurrir al argumento breaks= dentro de la función tm_fill(). Para definir los intervalos de forma adecuada, se recomienda mirar las características resumen de la variable de interés, en este caso el número de casos positivos por COVID19 por cada mil habitantes, pos_hab:Esto nos permitirá conocer los valores que toma la variable de interés y pensar la manera en que deseamos delimitar las categorías de nuestro mapa. Supongamos que deseamos 6 categorías y, dado el rango de nuestra variable (valores entre 1.720 y 22.7), podemos fijar los cortes de los intervalos en 2.0, 6.0, 12., 18.0 y 24.0; además, se requiere incluir también un mínimo y máximo, digamos 1.0 y 25.0, para las categorías.La información, tanto de los cortes como de los máximos y mínimos deber ser especifica en forma de vector: c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0). Debemos además especificar la paleta de colores que deseamos usar, atendiendo lo dicho antes, usaremos una paleta con tres anclas: del amarillo, al naranja y al café:YlOrBr:Un mapa de percentiles es un tipo espacial de mapa de cuantiles en el que se especifican seis categorías: 0-1%,1-10%, 10-50%,50-90%,90-99% y 99-100%. Para poder construirlo, en R debemos llevar cabo los siguientes pasos:Extraer la variable de nuestro arreglo de datos.Calcular los percentiles de nuestro interés, es decir, 0,0.01,0.1,0.5,0.9,0.99,1.0.Construir el mapa con base en los intervalos definidos través de la función tm_fill().Para el paso , lo primero será construir una función que llamaremos get.var, que tendrá dos argumentos, varnom y df, el primero indicará, entre comillas, el nombre de la variable utilizar y el segundo indicará la base de la que proviene. Esta función permite que, al extraer la variable de la base, se eliminen los “aspectos espaciales” asociadas ella. Esta función sólo se requiere construir una vez.Ahora, echando mano de la función creada extraeremos la variable de interés sin sus aspectos espaciales:Para el paso ii, hemos de crear un objeto en el que se guarden los percentiles de interés, un vector de 6 elementos:Ahora, se calculan y guardan valores de los percentiles con base en el par de objetos creados, percentiles y pos_hab, es decir, obtendremos los valores de la variable pos_hab en los percentiles de interés:Ahora, en el paso iii, la construcción del mapa de percentiles es posible uniendo todos los elementos:Con el camino que hemos seguido, es posible ahorrarnos muchos pasos y crear nuestra propia función (que opera en el entorno de trabajo activo de la sesión) para construir mapas de percentiles. Nuestra función se llamará percentmap(). Nuestra función tendrá los siguientes argumentos:varnom: nombre de la variable (cadena, entre comillas).df: base de datos que contiene el variable.legtitle: titulo de la leyenda del mapa.mtitle: título del mapa.Para crear la función:La función que hemos creado para hacer nuestros mapas tiene cuatro argumentos: varnom,df,legtitle y mtitle. Los dos últimos tienen valores por omisión, lo que significa que es necesario especificarlos al usar la función. Los dos primeros tienen valor por omisión, por lo que será forzoso especificar dichos argumentos.Ahora, invocando nuestra propia función e indicando los argumentos forzosos, varnom y df tenemos que:Podemos, como es obvio, cambiar los dos argumentos dados por omisión:Los capítulos 1 y 2 de este libro, constituyen lo que suele ser denominado Análisis Exploratorio de Datos, (Exploratory Data Analysis, EDA). El capítulo 1 del e-Handbook Statistical Methods expone con detalle esta concepción en el análisis de información Croarkin y Tobias (2014). Además, capítulo 7 del ya citado libro R Data Science también explica con detalle el enfoque EDA usando R.En el siguiente capítulo continuamos con la exploración de la información, pero incorporando una estructura de relaciones en el espacio, por lo que dicho enfoque se le conoce como Análisis Exploratorio de Datos Espaciales. En dicho capítulo será de nuestro interés particular un rasgo que suele estar presente en la información georreferenciada: la autocorrelación espacial.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2022-01-19.book built bookdown R package.","code":"\nsummary(zmvm_cov_sf$pos_hab)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   1.720   3.337   5.117   6.964   8.636  22.700\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\",title=\"Casos positivos covid\",breaks=c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0),palette=\"YlOrBr\")+\n  tm_layout(title = \"Cortes personalizados\", title.position = c(\"center\",\"top\"))\nget.var <- function(varnom,df) {#Definición de la función\n  v <- df[varnom] %>% st_set_geometry(NULL) #Extracción de la variable de interés y remoción de sus características geográficas\n  v <- unname(v[,1]) #Selección de la columna del data frame extraído que contiene la variable de interés y elimina su nombre pues sólo queremos un vector.\n  return(v) #Resultado de la función: la variable como vector sin sus características espaciales\n}\npos_hab<-get.var(\"pos_hab\",zmvm_cov_sf)\npos_hab##  [1] 14.364996 16.983175 13.352654 16.163929 17.363687 17.313543 14.943518\n##  [8] 17.404756 18.701249 13.677709 20.559563 10.224540 14.717256 22.700331\n## [15] 11.088257  5.882283  4.938574  5.239423  2.570694  3.351482  4.267922\n## [22]  2.382445  7.896182  2.088929  5.500198  5.891309  5.601076  9.197806\n## [29]  8.113844  3.295057  3.676214  3.250036  3.583416  6.155522  2.124319\n## [36]  2.787239  3.293624  6.120050  1.760416  5.184329  2.941489  3.873824\n## [43]  2.507745  5.423064  7.354686  2.224869  3.992095  8.598203  6.385731\n## [50]  4.385253  3.702180  3.693010  5.894044  4.147922  9.766454  3.853830\n## [57]  5.546263  4.766342  8.751090  1.719690  3.024390  2.372319  2.380410\n## [64]  3.876611  4.272596  7.134726  6.920539  5.049320  4.917293  2.582625\n## [71]  3.918632  3.399002  2.473636  2.822012  6.913242 13.935302\npercentiles <- c(0,.01,.1,.5,.9,.99,1)\nvarperc <- quantile(pos_hab,percentiles)\nvarperc##        0%        1%       10%       50%       90%       99%      100% \n##  1.719690  1.750234  2.428041  5.116824 15.553724 21.094755 22.700331\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\",title=\"Índice de marginación 2010\", breaks=varperc, palette=\"-RdBu\",labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))+\n  tmap::tm_borders() +\n  tmap::tm_layout(title = \"Mapa de percentiles\", title.position = c(\"center\",\"bottom\"))\n  percentmap <- function(varnom,df,titulo.leyenda=NA,titulo.principal=\"Mapa de percentiles\"){ #Definición de la función y sus argumentos\n  #Elementos preliminares\n  percent <- c(0,.01,.1,.5,.9,.99,1) #Vector que contiene los percentiles de interés para el mapa\n  var <- get.var(varnom,df) #Extracción de la variable de la base de datos con la función anterior\n  varperc <- quantile(var,percent) #Cálculo de los percentiles de la variable extraída\n  #Especificaciones del mapa\n  tm_shape(df) +\n     tm_fill(varnom,title=titulo.leyenda,breaks=varperc,palette=\"-RdBu\", labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(title = titulo.principal, title.position = c(\"center\",\"bottom\"))\n}\npercentmap(\"pos_hab\",zmvm_cov_sf)\npercentmap(\"pos_hab\",zmvm_cov_sf, titulo.leyenda = \"Categorías\", titulo.principal = \"El título que yo quiera\" )"},{"path":"mapas-coropléticos-en-r.html","id":"una-palabra-de-advertencia","chapter":"2 Mapas coropléticos en R","heading":"2.10.1 Una palabra de advertencia","text":"En esta sección se construyen dos tipos de mapas de clasificación especial, es decir, destinados hacer notar los valores atípicos. obstante, la construcción de dichos mapas en R supone cierto conocimiento sobre programación que está fuera del alcance de estas notas. Si deseas profundizar en el aprendizaje de programación en R, el libro de Roger D. Peng, R Programming Data Science es un excelente material, particularmente el capítulo 14 dedicado las funciones en R Peng (2015).Con esta advertencia, llevamos cabo la exposición de esta sección, esperando motivarte para que tu mismo profundices en los tópicos de programación.","code":""},{"path":"mapas-coropléticos-en-r.html","id":"mapa-de-intervalos-personalizados","chapter":"2 Mapas coropléticos en R","heading":"2.10.2 Mapa de intervalos personalizados","text":"Se señaló antes que, hasta donde tenemos conocimiento, en R es posible construir mapas de clasificación especial con tmap través de las opciones de estilo, obstante, con algunos elementos de programación es posible solventar esta tarea. En esta sección nos servimos del código proporcionado por Luc Anselin y su equipo quienes, en un esfuerzo de difusión del conocimiento sobre el uso de estas herramientas, pone nuestra disposición una basta cantidad de materiales en la página del Center Spatial Data Science de la Universidad de Chicago. Anselin y Morrison (2018)Para construir un mapa de intervalos definidos por el usuario, hay que recurrir al argumento breaks= dentro de la función tm_fill(). Para definir los intervalos de forma adecuada, se recomienda mirar las características resumen de la variable de interés, en este caso el número de casos positivos por COVID19 por cada mil habitantes, pos_hab:Esto nos permitirá conocer los valores que toma la variable de interés y pensar la manera en que deseamos delimitar las categorías de nuestro mapa. Supongamos que deseamos 6 categorías y, dado el rango de nuestra variable (valores entre 1.720 y 22.7), podemos fijar los cortes de los intervalos en 2.0, 6.0, 12., 18.0 y 24.0; además, se requiere incluir también un mínimo y máximo, digamos 1.0 y 25.0, para las categorías.La información, tanto de los cortes como de los máximos y mínimos deber ser especifica en forma de vector: c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0). Debemos además especificar la paleta de colores que deseamos usar, atendiendo lo dicho antes, usaremos una paleta con tres anclas: del amarillo, al naranja y al café:YlOrBr:","code":"\nsummary(zmvm_cov_sf$pos_hab)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   1.720   3.337   5.117   6.964   8.636  22.700\ntmap::tm_shape(zmvm_cov_sf) +\n  tmap::tm_borders()+\n  tmap::tm_fill(\"pos_hab\",title=\"Casos positivos covid\",breaks=c(1.0, 2.0, 6.0, 12., 18.0,  24.0, 25.0),palette=\"YlOrBr\")+\n  tm_layout(title = \"Cortes personalizados\", title.position = c(\"center\",\"top\"))"},{"path":"mapas-coropléticos-en-r.html","id":"mapas-de-valores-extremos","chapter":"2 Mapas coropléticos en R","heading":"2.10.3 Mapas de valores extremos","text":"","code":""},{"path":"mapas-coropléticos-en-r.html","id":"mapa-de-percentiles","chapter":"2 Mapas coropléticos en R","heading":"2.10.3.1 Mapa de percentiles","text":"Un mapa de percentiles es un tipo espacial de mapa de cuantiles en el que se especifican seis categorías: 0-1%,1-10%, 10-50%,50-90%,90-99% y 99-100%. Para poder construirlo, en R debemos llevar cabo los siguientes pasos:Extraer la variable de nuestro arreglo de datos.Calcular los percentiles de nuestro interés, es decir, 0,0.01,0.1,0.5,0.9,0.99,1.0.Construir el mapa con base en los intervalos definidos través de la función tm_fill().Para el paso , lo primero será construir una función que llamaremos get.var, que tendrá dos argumentos, varnom y df, el primero indicará, entre comillas, el nombre de la variable utilizar y el segundo indicará la base de la que proviene. Esta función permite que, al extraer la variable de la base, se eliminen los “aspectos espaciales” asociadas ella. Esta función sólo se requiere construir una vez.Ahora, echando mano de la función creada extraeremos la variable de interés sin sus aspectos espaciales:Para el paso ii, hemos de crear un objeto en el que se guarden los percentiles de interés, un vector de 6 elementos:Ahora, se calculan y guardan valores de los percentiles con base en el par de objetos creados, percentiles y pos_hab, es decir, obtendremos los valores de la variable pos_hab en los percentiles de interés:Ahora, en el paso iii, la construcción del mapa de percentiles es posible uniendo todos los elementos:","code":"\nget.var <- function(varnom,df) {#Definición de la función\n  v <- df[varnom] %>% st_set_geometry(NULL) #Extracción de la variable de interés y remoción de sus características geográficas\n  v <- unname(v[,1]) #Selección de la columna del data frame extraído que contiene la variable de interés y elimina su nombre pues sólo queremos un vector.\n  return(v) #Resultado de la función: la variable como vector sin sus características espaciales\n}\npos_hab<-get.var(\"pos_hab\",zmvm_cov_sf)\npos_hab##  [1] 14.364996 16.983175 13.352654 16.163929 17.363687 17.313543 14.943518\n##  [8] 17.404756 18.701249 13.677709 20.559563 10.224540 14.717256 22.700331\n## [15] 11.088257  5.882283  4.938574  5.239423  2.570694  3.351482  4.267922\n## [22]  2.382445  7.896182  2.088929  5.500198  5.891309  5.601076  9.197806\n## [29]  8.113844  3.295057  3.676214  3.250036  3.583416  6.155522  2.124319\n## [36]  2.787239  3.293624  6.120050  1.760416  5.184329  2.941489  3.873824\n## [43]  2.507745  5.423064  7.354686  2.224869  3.992095  8.598203  6.385731\n## [50]  4.385253  3.702180  3.693010  5.894044  4.147922  9.766454  3.853830\n## [57]  5.546263  4.766342  8.751090  1.719690  3.024390  2.372319  2.380410\n## [64]  3.876611  4.272596  7.134726  6.920539  5.049320  4.917293  2.582625\n## [71]  3.918632  3.399002  2.473636  2.822012  6.913242 13.935302\npercentiles <- c(0,.01,.1,.5,.9,.99,1)\nvarperc <- quantile(pos_hab,percentiles)\nvarperc##        0%        1%       10%       50%       90%       99%      100% \n##  1.719690  1.750234  2.428041  5.116824 15.553724 21.094755 22.700331\ntmap::tm_shape(zmvm_cov_sf)+\n  tmap::tm_fill(\"pos_hab\",title=\"Índice de marginación 2010\", breaks=varperc, palette=\"-RdBu\",labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))+\n  tmap::tm_borders() +\n  tmap::tm_layout(title = \"Mapa de percentiles\", title.position = c(\"center\",\"bottom\"))"},{"path":"mapas-coropléticos-en-r.html","id":"construcción-de-una-función-personalizada-para-hacer-mapas-de-percentiles","chapter":"2 Mapas coropléticos en R","heading":"2.10.4 Construcción de una función personalizada para hacer mapas de percentiles","text":"Con el camino que hemos seguido, es posible ahorrarnos muchos pasos y crear nuestra propia función (que opera en el entorno de trabajo activo de la sesión) para construir mapas de percentiles. Nuestra función se llamará percentmap(). Nuestra función tendrá los siguientes argumentos:varnom: nombre de la variable (cadena, entre comillas).df: base de datos que contiene el variable.legtitle: titulo de la leyenda del mapa.mtitle: título del mapa.Para crear la función:La función que hemos creado para hacer nuestros mapas tiene cuatro argumentos: varnom,df,legtitle y mtitle. Los dos últimos tienen valores por omisión, lo que significa que es necesario especificarlos al usar la función. Los dos primeros tienen valor por omisión, por lo que será forzoso especificar dichos argumentos.Ahora, invocando nuestra propia función e indicando los argumentos forzosos, varnom y df tenemos que:Podemos, como es obvio, cambiar los dos argumentos dados por omisión:Los capítulos 1 y 2 de este libro, constituyen lo que suele ser denominado Análisis Exploratorio de Datos, (Exploratory Data Analysis, EDA). El capítulo 1 del e-Handbook Statistical Methods expone con detalle esta concepción en el análisis de información Croarkin y Tobias (2014). Además, capítulo 7 del ya citado libro R Data Science también explica con detalle el enfoque EDA usando R.En el siguiente capítulo continuamos con la exploración de la información, pero incorporando una estructura de relaciones en el espacio, por lo que dicho enfoque se le conoce como Análisis Exploratorio de Datos Espaciales. En dicho capítulo será de nuestro interés particular un rasgo que suele estar presente en la información georreferenciada: la autocorrelación espacial.","code":"\n  percentmap <- function(varnom,df,titulo.leyenda=NA,titulo.principal=\"Mapa de percentiles\"){ #Definición de la función y sus argumentos\n  #Elementos preliminares\n  percent <- c(0,.01,.1,.5,.9,.99,1) #Vector que contiene los percentiles de interés para el mapa\n  var <- get.var(varnom,df) #Extracción de la variable de la base de datos con la función anterior\n  varperc <- quantile(var,percent) #Cálculo de los percentiles de la variable extraída\n  #Especificaciones del mapa\n  tm_shape(df) +\n     tm_fill(varnom,title=titulo.leyenda,breaks=varperc,palette=\"-RdBu\", labels=c(\"< 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(title = titulo.principal, title.position = c(\"center\",\"bottom\"))\n}\npercentmap(\"pos_hab\",zmvm_cov_sf)\npercentmap(\"pos_hab\",zmvm_cov_sf, titulo.leyenda = \"Categorías\", titulo.principal = \"El título que yo quiera\" )"},{"path":"análisis-espacial-i-autocorrelación.html","id":"análisis-espacial-i-autocorrelación","chapter":"3 Análisis espacial I: autocorrelación","heading":"3 Análisis espacial I: autocorrelación","text":"","code":""},{"path":"análisis-espacial-i-autocorrelación.html","id":"autocorrelación-y-dependencia-espacial","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.1 Autocorrelación y dependencia espacial","text":"La identificación de autocorrelación espacial es importante como parte del proceso de análisis del fenómeno socioterritoriales al menos por dos cuestiones, una de carácter técnico y otra de carácter sustantivo. Respecto los elementos técnicos es posible sostener que, si existe autocorrelación espacial en nuestros datos, lo más probable es que los métodos de estimación de los parámetros del modelo propuesto través de mínimos cuadrados ordinarios dejen de ser válidos, en la medida en que se cumplen los supuestos que requiere dicho procedimiento, específicamente, que los errores o perturbaciones del modelo estén correlacionados. Por otro lado, cuando nos referimos elementos sustantivos estamos queriendo expresar que nuestro fenómeno de interés, aquél que buscamos captar través del modelo propuesto, en realidad estaría dejando de lado un comportamiento sistemático de asociación nivel territorial.Sobre este segundo punto detengámonos un momento. ¿Qué puede estar ocurriendo que hace que los fenómenos se se distribuyan aleatoriamente en el espacio? Por ejemplo, ¿por qué se concentra la actividad económica en determinadas ciudades o por qué algunos servicios sólo se brindan en una zona de la ciudad? Esto respecto fenómenos económicos, pero ¿qué hay con el hecho de que una enfermedad se concentra notoriamente en algunas áreas de la ciudad y en otras? Dicho en otras palabras, ¿qué hay detrás de la formación de un patrón en la forma en que se distribuye un fenómeno en el espacio y cómo puede ser explicado? eso nos referimos cuando decimos que hay elementos sustantivos para el análisis al hallar evidencia de autocorrelación espacial.Un clásico ejemplo de lo provechoso que es el análisis espacial que indaga sobre patrones de relación espacial sustantivos es el caso de estudio de John Snow que en 1854 investigó las muertes por cólera en el barrio de Soho, en Londres. En esos años se sabía con exactitud el mecanismo de transmisión del cólera, se creía que éste se adquiría por la ingesta de agua o alimentos contaminados (tal y cómo después se verificó gracias los trabajos de Snow) o por respirar aire contaminado. En el material preparado por el Center Spatial Data Science de la Universidad de Chicago, podrás encontrar el modo en que el análisis espacial puede ayudar plantear y resolver las preguntas adecuadas: Asking right questions spatial anaysis: case John Snow.Una definición sintética de autocorrelación espacial es la que nos brinda Chasco (2003) como “la relación funcional existente entre los valores que adopta un indicador en una zona del espacio y en zonas vecinas” (Chasco 2003, 49).La definición se integra por tres elementos clave: ) valor de un indicador, ii) relación funcional y iii) zonas vecinas. Para dar sentido nuestra definición pensemos en una afirmación como “los casos positivos de COVID19 en la alcaldía Azcapotzalco están asociados en forma directa con los casos positivos de COVID19 en las alcaldías y municipios vecinos que integran la Zona Metropolitana del Valle de México”. Tendríamos entonces que:Indicador: casos positivos por COVID19.Relación funcional: asociación positiva o directa.Zonas vecinas: alcaldías y municipios vecinos de Azcapotzalco.El primer elemento presenta demasiadas dificultades ya que en la base de datos que hemos estado utilizando tenemos los casos positivos por COVID19 por cada 1 mil habitantes; en tanto, el segundo elemento de nuestra afirmación es una mera suposición, es decir, que hay una relación positiva; por su parte, el tercer elemento “alcaldías y municipios vecinos” implica un problema: ¿de qué modo es posible establecer que alcaldías son o vecinas de Azcapotzalco?Hay múltiples maneras que definir si un objeto espacial tiene o vecinos, por ejemplo, podríamos decir que aquellas alcaldías que compartan límites administrativos con la demarcación territorial de nuestro interés serán sus vecinos (vecindad por adyacencia) o también sería posible establecer que las alcaldías vecinas serán aquellas que estén menos de 10 km de distancia del centro económico de la alcaldía (vecindad por umbral de distancia).Ejercicio¿Se te ocurre algún otro criterio para establecer vecindad?¿Se te ocurre algún otro criterio para establecer vecindad?¿Cómo llamarías un criterio de vecindad donde elijas los 3 vecinos más cercanos?¿Cómo llamarías un criterio de vecindad donde elijas los 3 vecinos más cercanos?¿partir de qué punto será más conveniente medir la distancia, desde el centro económico de la alcaldía o municipio (por ejemplo su zona industrial o comercial) o desde la sede de la administración local?¿partir de qué punto será más conveniente medir la distancia, desde el centro económico de la alcaldía o municipio (por ejemplo su zona industrial o comercial) o desde la sede de la administración local?¿La distancia más indicada usada como criterio de vecindad será una distancia lineal o una distancia por carretera?¿La distancia más indicada usada como criterio de vecindad será una distancia lineal o una distancia por carretera?Los interesados en el análisis espacial han propuesto un ingenioso instrumento matemático para captar y sintetizar cómo un objeto se relaciona con otros en el espacio, es decir, para captar la estructura espacial del área de interés. Piensa en un vecindario o área de estudio compuesto sólo por seis elementos, tal como se ilustra en la figura 3.1:\nFigura 3.1: Vecindario regular\npartir de la disposición de este hipotético vecindario nos interesa construir un instrumento para captar su estructura espacial través de un criterio de adyacencia o contigüidad. decir de Anselin (2020) “contigüidad significa que dos unidades espaciales comparten un borde común de longitud distinta de cero. Desde el punto de vista operativo, podemos distinguir aún más entre un criterio de contigüidad de tipo torre y de tipo reina, en analogía con los movimientos permitidos para las piezas así nombradas en un tablero de ajedrez. El criterio de la torre define los vecinos por la existencia de un borde común entre dos unidades espaciales. El criterio de la reina es algo más amplio y define los vecinos como unidades espaciales que comparten un borde o un vértice comunes” Anselin (2020).Ahora bien, ¿cómo podemos plasmar las relaciones de contigüidad de forma sintética. Pensemos en un cuadro que tiene tantas filas y columnas como objetos espaciales tiene nuestro vecindario, semejante al que aparece en la figura 3.2.\nFigura 3.2: Matriz ejemplo vacía\nColoquemos el número uno si el elemento 1 y el 2 comparten un lado, en caso contrario, coloquemos un cero. Hagamos esto para cada celda de este cuadro hasta que lo llenemos y obtengamos algo parecido lo que aparece en la figura 3.3.\nFigura 3.3: Matriz ejemplo llena\nEl cuadro que acabamos de llenar es conocido como matriz de pesos espaciales3. Habrá, por tanto, diversos tipos de matrices en función del criterio de vecindad elegido y del propio fenómeno analizado. obstante, la más usual es una matriz de contigüidad binaria de \\(n\\) x \\(n\\), donde \\(n\\) es el número de objetos espaciales.Este tipo de matriz, denotado por la letra mayúscula \\(W\\), contiene como elementos \\(w_{ij}\\) : el número 1 cuando el elemento \\(j\\) y el elemento \\(\\) sean vecinos, o bien, 0 (cero) en cualquier otro caso. Este instrumento es uno de los más importantes en econometría espacial ya que permite construir los estadísticos de autocorrelación espacial y es la manera en que podemos incorporar al espacio como variable partir de lo que denominamos “rezago espacial”. De la imagen anterior es posible apuntar algunas de las características de esta matriz:Es una matriz que en la diagonal principal contiene sólo ceros, es decir, se asume que por definición hay interacción espacial dentro de un mismo elemento (lo que necesariamente es cierto y que dependerá de la dimensión espacial de análisis).Es una matriz simétrica, es decir, se asume que hay interacción de “ida y vuelta”, por lo que con un instrumento de estas características es posible asumir efectos de interacción en un solo sentido.Aquí acabamos de ilustrar la lógica con la que puede ser construida una matriz de pesos espaciales partir de una retícula regular con apenas seis elementos. Veamos ahora cómo obtener matrices de pesos espaciales sirviéndonos de R, ya que desarrollar los pasos anteriores para un vecindario compuesto por 76 elementos como los que integran el Valle de México es tarea para una máquina, para nosotros.","code":""},{"path":"análisis-espacial-i-autocorrelación.html","id":"matrices-de-pesos-espaciales-en-r","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.2 Matrices de pesos espaciales en R","text":"","code":""},{"path":"análisis-espacial-i-autocorrelación.html","id":"los-paquetes-1","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.2.1 Los paquetes","text":"En R hay muchas rutas para desarrollar la misma tarea. Lo que presentamos en este capítulo es una de múltiples maneras que pueden encontrarse para la construcción de matrices de pesos espaciales en R. En el apéndice, encontrarás otra alternativa, un tanto más elaborada. La manera en que desarrollamos en esta sección la tarea de construir las matrices de pesos espaciales se sirve de los siguientes paquetes:rgdal: según se lee en la documentación, “proporciona enlaces la biblioteca de abstracción de datos ‘geoespaciales’”spdep: contiene todo un arsenal de funciones para el análisis espacial, que permiten crear matrices de pesos espaciales, así como evaluar autocorrelación espacial.Como es usual, se deben descargar e instalar los paquetes:Para cargarlos:","code":"\ninstall.packages(c(\"rgdal\",\"spdep\"))\nlibrary(spdep)\nlibrary(rgdal)"},{"path":"análisis-espacial-i-autocorrelación.html","id":"matrices-de-contigüidad-el-argumento-queen-de-la-función-poly2nb","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.2.2 Matrices de contigüidad: el argumento queen de la función poly2nb()","text":"Primero, carguemos la base de datos espacial. En este caso, recurrimos la función readOGR que forma parte del paquete rgdal para leer los archivos de tipo SHP.Observa el objeto en tu ambiente de trabajo. Notaras que es un objeto de tipo Spatial polygons data frame y al hacer clic en la imagen de la lente de aumento se desplegará la base de datos como es usual, sino que estará almacenada y agrupada de otra manera:data: aquí están almacenados los datos de nuestro archivo .dbf.polygons: la geometría del área estudiada se almacena aquí y se identifica cada polígono (municipio o alcaldía) con un número del 1 al 76.proj4string: recoge la información relativa al tipo de proyección cartográfica usada en nuestra base.Cuando en la computadora queremos definir una estructura espacial lo hacemos con un tipo de objeto que denominamos lista y con matrices, es decir, el objeto con el que interactuaremos en R será uno de tipo matrix, sino una especie de lista. El hecho de que se usen listas y matrices para el proceso de cómputo es porque resulta más conveniente en términos de la cantidad ocupada de recursos del sistema puesto que las matrices, como las comentadas en la sección anterior, son matrices dispersas, es decir, contienen muchos elementos que son cero.La función poly2nb() del paquete spdep nos permite calcular dos tipos de vecindad por adyacencia través del argumento lógico queen. En la documentación de la función podemos leer que: “si es VERDADERO, TRUE, un solo punto límite compartido cumple la condición de contigüidad; si es FALSO, FALSE, se requiere más de un punto compartido; ten en cuenta que más de un punto límite compartido significa necesariamente una línea límite compartida”.Lo dicho en el párrafo anterior es relevante en el sentido de que los estos criterios, los usados por R en la construcción de matrices, son exactamente los mismos que definimos antes (contigüidad reina y torre). Aquí la diferencia entre asignar como verdadero o falso el argumento queen=, puede ser interpretado como un criterio menos estricto o más estricto, respectivamente.Vamos construir un objeto que llamaremos mTRUE, dicho objeto contendrá los elementos que definen la vecindad:El segmento de código anterior genera un objeto de tipo nb. Verifica sus características con class() y str() . Además, nota que en la sección de ambiente de trabajo (cuadrante superior derecho, en la pestaña ambiente) aparece el objeto creado. Da clic en la imagen de la lupa para visualizarlo e intenta interpretar el resultado de la ventana.Ahora, llama el objeto y presta atención sobre los resultados que ofrece:En la consola aparecen los siguientes elementos:Objeto de lista de vecinos:Número de regiones: 76. Corresponde al número de alcaldías y municipios que componen la Zona Metropolitana del Valle de México.Número de enlaces distintos de cero: 380. Es el número de elementos de una matriz de 76x76 diferentes de cero.Porcentaje de pesos distintos de cero: 6.57. Resultado de dividir 380 entre (76x76).Número promedio de vecinos o vínculos: 5. Número de vecinos que en promedio tiene cada municipio o alcaldía.Ahora bien, para construir una lista de vecindad con base en un criterio más estricto, es decir, queen = FALSE, procedemos como:Ahora, llama dicho objeto y contrasta con los resultados anteriores.Ejercicio¿Qué objeto, mTRUE o mFALSE, tiene el mayor número de vínculos diferentes de cero?¿Qué objeto, mTRUE o mFALSE, tiene el mayor número de vínculos diferentes de cero?¿Por qué crees que esto es así?¿Por qué crees que esto es así?Es posible observar que, con base en este segundo criterio de vecindad, se identificaron un menor número de vecinos (número de vínculos diferente de cero).Estas listas, que contienen nuestras estructuras espaciales en los objeto de tipo nb llamados mTRUE y mFALSE, pueden representarse visualmente través de un gráfico de conectividad que representa la estructura espacial definida por cada criterio través de líneas que unen los municipios considerados vecinos.Para visualizar el mapa de conectividad recurrimos la función plot() y superpondremos dos gráficas: una sólo con los bordes o límites nivel municipal y otra con los centroides y la estructura espacial. Aquí se muestra el mapa de conectividad resultado de la matriz mTRUEPara comparar ambas formas podemos superponer las dos gráficas y asignar colores diferentes:Como puedes observar, son estructuras muy parecidas, aunque es posible notar sus diferencias. Es ampliamente recomendable que revises el apéndice de este capítulo pues en él se trata un enfoque diferente para la construcción de matrices de pesos espaciales que, si bien es más complejo, te puede servir para avanzar en tu conocimiento sobre R.","code":"\ncovid_zmvm <-rgdal::readOGR(\"base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp\")## Warning in OGRSpatialRef(dsn, layer, morphFromESRI = morphFromESRI, dumpSRS =\n## dumpSRS, : Discarded datum D_unknown in Proj4 definition: +proj=lcc +lat_0=12\n## +lon_0=-102 +lat_1=17.5 +lat_2=29.5 +x_0=2500000 +y_0=0 +ellps=GRS80 +units=m\n## +no_defs## OGR data source with driver: ESRI Shapefile \n## Source: \"C:\\Users\\Jarvis\\Desktop\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp\", layer: \"covid_zmvm\"\n## with 76 features\n## It has 55 fields\nmTRUE <- spdep::poly2nb(covid_zmvm)\nmTRUE## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 384 \n## Percentage nonzero weights: 6.648199 \n## Average number of links: 5.052632\nmFALSE<- spdep::poly2nb(covid_zmvm, queen = FALSE)\nmFALSE## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 372 \n## Percentage nonzero weights: 6.440443 \n## Average number of links: 4.894737\nplot(covid_zmvm, border = 'lightgrey')\nplot(mTRUE, coordinates(covid_zmvm), add=TRUE, col='lightblue')\nplot(covid_zmvm, border = 'lightgrey')\nplot(mTRUE, coordinates(covid_zmvm), add=TRUE, col='blue')\nplot(mFALSE, coordinates(covid_zmvm), add=TRUE, col='lightgreen')"},{"path":"análisis-espacial-i-autocorrelación.html","id":"estadísticos-de-correlación-espacial","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.3 Estadísticos de correlación espacial","text":"","code":""},{"path":"análisis-espacial-i-autocorrelación.html","id":"la-matriz-de-pesos-espaciales-estandarizada","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.3.1 La matriz de pesos espaciales estandarizada","text":"Antes dijimos que cada uno de los elementos \\(w_{ij}\\) de la matriz \\(W\\) pueden tomar como valores ceros o unos. La matriz \\(W\\) puede escribirse como:\\[\nW=\n\\begin{pmatrix}\nw_{11} & w_{12} & \\cdots  & w_{1n}\\\\\nw_{21} & w_{22} & \\cdots  & w_{2n}\\\\\n\\vdots & \\vdots & \\ddots  & \\vdots \\\\\nw_{n1} & w_{n2} & \\dots  & w_{nn}\\\\\n\\end{pmatrix}\n\\]obstante, es posible expresar dicha matriz \\(W\\) de una forma diferente, normalizandola por filas. Normalizar una matriz de pesos espaciales por filas implica dividir cada elemento \\(w_{ij}\\) de una fila entre la suma de elementos diferentes cero de dicha fila, es decir:\\[w_{ij(s)}=\\frac {w_{ij}} {\\sum{w_{ij}}}\\]\nLa matriz de pesos espaciales estandarizada es la que usaremos en lo sucesivo para construir lo que denominaremos rezago espacial y para la construcción de estadísticos de correlación espacial, un poco más adelante. El proceso de estandarización de las matrices que recién creamos, los objetos mTRUE y mFALSE, corre cuenta de la función nb2listw() del paquete spdep:Notaras cómo en el ambiente de trabajo se ha creado un objeto nuevo de tipo listw. Ábrelo y observa su contenido.Ejercicio¿Logras comprender por qué se le denomina matriz (lista) estandarizada?¿Logras comprender por qué se le denomina matriz (lista) estandarizada?¿Logras identificar cómo la suma de cada renglón de la lista es igual uno? ¿Cómo se relaciona la forma en que aparecen enlistados los elementos con el número de vecinos que tiene cada objeto espacial?¿Logras identificar cómo la suma de cada renglón de la lista es igual uno? ¿Cómo se relaciona la forma en que aparecen enlistados los elementos con el número de vecinos que tiene cada objeto espacial?","code":"\nmTRUE.pesos <- spdep::nb2listw(mTRUE)\nmTRUE.pesos## Characteristics of weights list object:\n## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 384 \n## Percentage nonzero weights: 6.648199 \n## Average number of links: 5.052632 \n## \n## Weights style: W \n## Weights constants summary:\n##    n   nn S0       S1       S2\n## W 76 5776 76 35.56142 319.6623\nmFALSE.pesos <- spdep::nb2listw(mFALSE)\nmFALSE.pesos## Characteristics of weights list object:\n## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 372 \n## Percentage nonzero weights: 6.440443 \n## Average number of links: 4.894737 \n## \n## Weights style: W \n## Weights constants summary:\n##    n   nn S0       S1       S2\n## W 76 5776 76 36.56499 319.6716"},{"path":"análisis-espacial-i-autocorrelación.html","id":"el-rezago-o-retardo-espacial","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.3.2 El rezago o retardo espacial","text":"Regresemos nuestra definición sobre la autocorrelación espacial: relación funcional existente entre los valores que adopta un indicador en una zona del espacio y en zonas vecinas. Ya hemos visto cómo podemos operacionalizar el concepto de vecindad, por lo que podemos ahora concentrémonos en los otros elementos. Comencemos con aquel que define el valor de una variable en las zonas vecinas, lo que llamaremos variable espacialmente rezagada. Una variable espacialmente rezagada es otra cosa que un promedio ponderado del valor de una variable en un sitio y sus locaciones vecinas (Chasco 2003, 61; Anselin 2020). Siguiendo (Anselin 2020), “el rezago espacial de \\(y\\) del objeto espacial \\(\\) es expresado como \\(Wy_{}\\):\\[\n\\begin{aligned}\nWy_i &=  w_{i1}y_1+w_{i2}y_2+...+w_{}y_n \\\\\nWy_i &=  \\sum_{j=1}^nw_{ij}y_j \\\\\n\\end{aligned}\n\\]Recuerda que \\(w_{ij}\\) es otra cosa que cada uno de los elementos de la matriz de pesos estandarizada por fila y que define la estructura de asociación en el espacio.Construiremos un rezago espacial de la variable pos_hab con ayuda de la función lag.listw() del paquete spdep. Indicamos dos argumentos en la función: la estructura espacial dada por la matriz estandarizada, mTRUE.pesos, y la variable de la que deseamos el rezago espacial, pos_hab, esto es guardado en un nuevo objeto, lag_poshab, tal y como se muestra en el siguiente segmento de código:Para lograr apreciar mejor nuestro rezago espacial, construiremos una tabla de dos columnas que almacenaremos en el objeto df, la primera contendrá la variable original y la segunda el rezago espacial, luego pediremos que nos muestre los primeros registros de la tabla con la función head() en un formato estilizado través de la función kable() del paquete knitr:El valor de la segunda columna, lag_poshab, es el promedio ponderado de los casos positivos de los vecinos de cada uno de los 76 municipios y alcaldías de nuestra base. Para inspeccionar esto, observemos tanto los vecinos de una unidad espacial como sus respectivos valores de pos_hab.Para identificar los vecinos de cada observación hay que extraer el dato utilizando dobles corchetes desde el objeto que contiene la estructura espacial ponderada, mTRUE. Para nuestra observación identificada con el número 1 tenemos:Ejercicio¿Cómo interpretas el resultado anterior?¿Cómo interpretas el resultado anterior?¿Qué son los números que se desplegaron?¿Qué son los números que se desplegaron?Ahora bien, ¿cuál es el valor de los casos positivos por cada 100 mil habitantes para cada una de las observaciones recién obtenidas? Para llamarlas la consola podemos escribir:Los valores previamente listados corresponden tanto al nombre como al número de casos positivos de los vecinos de Álvaro Obregón. Así, el primer valor de la tabla es el resultado de (16.98 + 14.94 + 18.70 +13.67 + 20.55 + 11.08)/6, es decir, 15.99.","code":"\nlag_poshab <- lag.listw(mTRUE.pesos, covid_zmvm$pos_hab)\n#Crea un nuevo arreglo de datos donde se almacena la variable original y el rezago espacial\ndf <- data.frame(pos_hab = covid_zmvm$pos_hab, lag_poshab)\n\nlibrary(knitr)\n\n#Coloca los primeros valores de ambas variables en una tabla, requiere instalación y carga del paquete  knitr\nkable(head(df))\n#Devuelve el nombre del municipio identificado con el número 1\ncovid_zmvm$nom_mun[[1]]## [1] \"Ã\\201lvaro ObregÃ³n\"\n#Devuelve los números de identificación de los municipios y alcaldías vecinas del Álvaro Obregón, la observación identificada con el número 1.\nmTRUE[[1]]## [1]  2  7  9 10 11 15\n#Devuelve el nombre de las alcaldías y municipios vecinos de Álvaro Obregón\ncovid_zmvm$nom_mun[mTRUE[[1]]]## [1] \"Tlalpan\"                \"CoyoacÃ¡n\"              \"Cuajimalpa de Morelos\" \n## [4] \"Miguel Hidalgo\"         \"La Magdalena Contreras\" \"Benito JuÃ¡rez\"\n#Devuelve el valor de pos_hab para cada observación vecina de Álvaro Obregón\ncovid_zmvm$pos_hab[mTRUE[[1]]]## [1] 16.98318 14.94352 18.70125 13.67771 20.55956 11.08826"},{"path":"análisis-espacial-i-autocorrelación.html","id":"coeficiente-de-correlación-espacial-la-i-de-moran","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.3.3 Coeficiente de correlación espacial: la I de Moran","text":"Sin duda, el estadístico de asociación espacial más común es la de Moran, que es otra cosa que un coeficiente de correlación lineal que “incorpora al espacio”, es decir, mide la asociación lineal entre la variable de interés y su rezago espacial.Para calcular el coeficiente o estadístico de Moran necesitamos recurrir la función moran.test() del paquete spdep. Los argumentos de la función deben especificar el nombre de la variable y el tipo de estructura espacial dado por la matriz de pesos usada; adicionalmente, se puede indicar qué hacer en caso de que existan islas (objetos espaciales sin vecinos) con el argumento zero.policy.Ejercicio¿Para qué sirve el argumento randomization de la función moran.test()?¿Para qué sirve el argumento randomization de la función moran.test()?¿De qué modo es posible cambiar la hipótesis alternativa de la evaluación de autocorrelación espacial en la prueba de Moran?¿De qué modo es posible cambiar la hipótesis alternativa de la evaluación de autocorrelación espacial en la prueba de Moran?Construyamos el estadístico de Moran para la variable pos_hab usando la estructura espacial llamada mTRUE.pesos:Del indicador obtenido nos interesan tres elementos, como es usual: la magnitud del coeficiente, su sentido y su significancia estadística. En los resultados que aparecen en tu consola identifica cada uno de ellos:Ejercicio¿cuánto asciende el coeficiente estimado? ¿Podría decirse que es alto o bajo?¿cuánto asciende el coeficiente estimado? ¿Podría decirse que es alto o bajo?¿La relación identificada es positiva o negativa?¿La relación identificada es positiva o negativa?Un coeficiente como el obtenido, de 0.657, indica que hay una relación positiva entre los valores de los casos positivos de COVID19 y los valores de los casos positivos por COVID19 en los entornos vecinos, además, podríamos decir que es relativamente alto en la medida en que el coeficiente puede tomar valores entre -1 y 1 y el que hemos obtenido está más cercano 1. ¿Qué hay con su significancia estadística? ¿cómo podemos saber que dicho resultado es producto del azar? Para evaluar su significancia estadística hay que observar el p-value, y evaluemos su significancia estadística través del siguiente juego de hipótesis:\\(Ho:\\) distribución espacial aleatoria\\(Ha:\\) distribución espacial aleatoriaCon la información disponible y fijando un nivel de significancia, \\(\\alpha\\), en 0.05 se rechaza la hipótesis nula en favor de la hipótesis alternativa, por tanto, la variable se distribuye de forma aleatoria en el espacio, sino que muestra indicios de asociación espacial positiva y relativamente alta (0.657).EjercicioConstruya una de Moran con la estructura espacial dada por la matriz donde queen=FALSE responda:¿La asociación espacial es positiva o negativa?¿La asociación espacial es positiva o negativa?¿Consideras que es alta o baja?¿Consideras que es alta o baja?¿Dirías que dicha relación es producto del azar o que existe un comportamiento sistemático?¿Dirías que dicha relación es producto del azar o que existe un comportamiento sistemático?","code":"\nspdep::moran.test(covid_zmvm$pos_hab, mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  covid_zmvm$pos_hab  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = 8.8625, p-value < 2.2e-16\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##       0.656334129      -0.013333333       0.005709563"},{"path":"análisis-espacial-i-autocorrelación.html","id":"diagrama-de-dispersión-de-moran","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.3.4 Diagrama de dispersión de Moran","text":"Una forma creativa de expresar gráficamente la autocorrelación de la variable de interés es través de un diagrama de dispersión cuyo eje \\(x\\) corresponde la variable de interés, casos positivos de COVID19 por cada 1 mil habitantes, y en el eje \\(y\\) su rezago espacial; además, al agregar una recta de ajuste sobre los datos estandarizados lograremos que la pendiente de dicha recta corresponda exactamente al valor de la de Moran. Una de las características del diagrama de Moran es que se descompone en cuatro cuadrantes, tal como aparece en la figura 3.4.\nFigura 3.4: Diagrama de dispersión de Moran\nPara representar el diagrama de dispersión de Moran o simplemente diagrama de Moran recurrimos la función moran.plot() del paquete spdep que para una matriz de tipo reina con el argumento queen=TRUE:","code":"\nspdep::moran.plot(((covid_zmvm$pos_hab)-mean(covid_zmvm$pos_hab))/(sd(covid_zmvm$pos_hab)),\n                  listw = mTRUE.pesos, \n                  xlab=\"Casos positivos\",\n                  ylab=\"Rezago espacial de los casos positivos\",\n                  main=\"Diagrama de Moran para casos positivos\",\n                  col=\"lightblue\")"},{"path":"análisis-espacial-i-autocorrelación.html","id":"múltiples-elementos-de-personalización-de-ésta-y-otras-gráficas-asociadas-al-paquete-base-de-r-pueden-revisarse-en-la-documentación-de-la-función-par.","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.4 Múltiples elementos de personalización de ésta y otras gráficas asociadas al paquete base de R pueden revisarse en la documentación de la función par().","text":"Ejercicio¿Es posible construir un diagrama de Moran usando el paquete ggplot2? De ser así, ¿cómo lo harías de ser así?","code":""},{"path":"análisis-espacial-i-autocorrelación.html","id":"índice-de-moran-local-y-mapa-de-clusters","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.5 Índice de Moran local y mapa de clusters","text":"El índice de Moran que recién hemos descrito permite evaluar la existencia de un patrón espacial completo, por lo que proporciona información de la ubicación de las agrupaciones, es decir, es una medida de autocorrelación espacial global. Para subsanar esta situación, (Anselin 1995) propuso la versión local de la de Moran: LISA (local indicator spatial association).Este indicador:Proporciona un estadístico para cada ubicación con un nivel de significancia.Establece una relación proporcional entre el estadístico local y el global.La representación del LISA en un mapa, conocido como mapa de Cluster (que permite clasificar las áreas significativas según el tipo de asociación identificada) en compañía con un mapa que indique la significancia estadística de cada valor (muestra las ubicaciones con la de Moran local que son representativas en diferentes niveles de \\(\\alpha\\)) permite la clasificación de las áreas estadísticamente significativas en clusters o agrupamientos (alto-alto y bajo-bajo) y de áreas que se constituyen como observaciones espaciales atípicas o spatial outliers (agrupamientos bajo-alto y alto-bajo).El Índice local de Moran toma la forma de:\\(I_i = \\frac{(x_i-\\bar{x})}{{∑_{k=1}^{n}(x_k-\\bar{x})^2}/(n-1)}{∑_{j=1}^{n}w_{ij}(x_j-\\bar{x})}\\)Para calcular un índice de Moran local en R usamos la función localmoran(). Creamos un objeto denominado local que echa mano de la matriz mTRUE.pesos:El objeto creado, local es de tipo “localmoran”, es otra cosa que una tabla que contiene múltiples variables cuyo contenido corresponde, según indica la propia ayuda de la función, :Ii: estadístico de Moran local.E.Ii: valor esperado del estadístico de Moran local.Var.Ii: varianza del estadístico local de Moran.Z.Ii: desviación estándar del estadístico de Moran local.Pr (): valor p del estadístico de Moran local.Nos interesa la primera columna, Ii, el índice de Moran local y la columna Pr que contiene la probabilidad asociada, por lo que uniremos los resultados del objeto local la tabla original que contiene nuestra base de datos. Para eso hay que usar la función cbind() (combinar por columna) en un nuevo objeto que llamaremos mapa.moran:Primero, llevemos los resultados del Moran Local un mapa de quintiles aplicando las funciones aprendidas en tmap:El mapa permite observar la manera en que varía la correlación espacial nivel local. Pero necesitamos otro instrumento que nos permita identificar si los valores de la de Moran local son o significativos en esas áreas y de esta manera identificar agrupaciones o núcleos de cluster significativos, así como observaciones espaciales atípicas, es decir, un mapa de significancia. Esto lo hacemos siguiendo la propuesta de (Lovelace y Cheshire 2014):El mapa anterior permite identificar agrupamientos de valores significativos al 10%, núcleos de cluster que muestran municipios y alcaldías con valores altos de tasas positivas de COVID19 rodeados de vecinos con valores altos (agrupamiento Alto-Alto), así como agrupamientos de valores bajos (cuadrante Bajo-Bajo), además de observaciones espaciales atípicas (cuadrante Alto-bajo).En síntesis, hasta este punto hemos visto en este capítulo:Cómo definir estructuras de relación espacial través de dos criterios,Cómo identificar autocorrelación espacial global través de la de Moran,Cómo evaluar la significancia estadística de la de Moran,Cómo identificar agrupaciones locales través del indicador LISA.En el capítulo 5 nos adentraremos en cómo incorporar la riqueza que proporciona el análisis espacial en un modelo econométrico. Mientras tanto, en el capítulo 4 llevaremos cabo un repaso de elementos básicos sobre los modelos de regresión lineal clásica con mínimos cuadrados ordinarios.","code":"\nlocal <- spdep::localmoran(covid_zmvm$pos_hab, mTRUE.pesos)\nmapa.moran <- base::cbind(covid_zmvm, local)\ntmap::tm_shape(mapa.moran) +\n  tmap::tm_fill(col = \"Ii\", style = \"quantile\",\n                palette = \"Spectral\", midpoint= NA,\n                title = \"I de Moran local\") +\n  tmap::tm_borders()\n# crea un vector numérico de la misma longitud del tamaño de filas de la base \"local\"\ncuadrantes <- vector(mode=\"numeric\",length=nrow(local))\n\n# Centra la variable de interés (CRIME) alrededor de su media\nm.COVID <- covid_zmvm$pos_hab - mean(covid_zmvm$pos_hab)\n\n# Centra el Índice de Moran local alrededor de su media (columna 1 de la base \"local\")\nm.local <- local[,1] - mean(local[,1])    \n\n# Fija el umbral de significancia\nsignif <- 0.05 \n\n# Identificación de los cuadrantes de interés\ncuadrantes[m.COVID >0 & m.local>0] <- 4  \ncuadrantes[m.COVID <0 & m.local<0] <- 1      \ncuadrantes[m.COVID <0 & m.local>0] <- 2\ncuadrantes[m.COVID >0 & m.local<0] <- 3\ncuadrantes[local[,5]>signif] <- 0   \n\n#Para graficarlo\ncortes <- c(0,1,2,3,4)\ncolores <- c(\"white\",\"blue\",rgb(0,0,1,alpha=0.4),rgb(1,0,0,alpha=0.4),\"red\")\nplot(mapa.moran, border=\"lightgray\",col=colores[findInterval(cuadrantes,cortes,all.inside=FALSE)])\nbox()\nlegend(\"bottomleft\",legend=c(\"No signficativo\",\"Bajo-Bajo\",\"Bajo-alto\",\"Alto-Bbajo\",\"Alto-Alto\"),\n       fill=colores,bty=\"n\")"},{"path":"análisis-espacial-i-autocorrelación.html","id":"apéndice-un-enfoque-adicional-para-construir-matrices-de-pesos-espaciales","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.6 Apéndice: Un enfoque adicional para construir matrices de pesos espaciales","text":"Aquí presentamos el enfoque utilizado por el Center Spatial Data Science de la Universidad de Chicago para la definición de estructuras espaciales. Es una ruta más compleja que la descrita en las secciones previas y, por tanto sino más rica pues permite definir matrices de adyacencia con criterios específicos (torre y reina), así como matrices de umbrales de distancia (k-vecinos más cercanos), tal como se hace en GeoDa. Los paquetes utilizados son:sf: paquete para el “soporte de funciones simples, una forma estandarizada de codificar datos vectoriales espaciales”, es decir, permite que R lea el tipo de datos que vamos usar aquí, datos en formato SHP.spdep: el mismo que describimos en el enfoque anterior.purrr: es un paquete que potencia las funciones de programación funcional de R, implica cierto conocimiento de cómo automatizar algunas tareas en esta plataforma.ggplot2: el popular paquete para hacer bellas gráficas.knitr: permite generar reportes estilizados en R.Si aún los instalado, hazlo. Si ya los instaste, llámalos al entorno de trabajo:Para construir las matrices de pesos espaciales con este segundo enfoque, es necesario cargar de nueva cuenta la base de datos, pero ahora través de la función st_read() del paquete sf. Para ello, recurrimos :Note el tipo de objeto creado (sf) al cargar la base con esta función que difiere del tipo de objeto usado en el primer método (spatial polygon dataframe).La construcción de las matrices de pesos espaciales en R con este segundo enfoque se hará través de los paquetes sf y spdep, que permitirá crear una matriz de tipo torre y reina.Para hacerlo, es necesario seguir dos pasos. En el primero se construirá una función que recoja la relación de vecindad, esta función la llamaremos func_torre y dentro de dicha función definiremos la relación de vecindad que nos interesa través de otra función, la función st_relate() del paquete sf que “calcula relaciones entre pares de geometrías o las empareja con un patrón dado. Esta función también tiene un parámetro para un patrón específico” (Anselin y Morrison 2018) y la que nos interesa es el patrón “compartir lados”. Posteriormente, en el segundo paso, aplicamos la función creada nuestra geometría para crear un objeto de tipo sgbp:Observa la estructura y clase que tiene el objeto recién creado, torre.sgbp: es un objeto de tipo sgbp. Para poder interactuar más adelante con este objeto, es necesario modificar su clase. Se trata, pues, de transformar el objeto una clase nb, es decir, una neighbor list (lista de vecinos). Llevaremos cabo este procedimiento creando una función para el caso, .nb.sgbp. Esta función nos permitirá además lidiar con los polígonos que tienen vecinos.Ahora, través de la función recién creada trasformaremos el objeto torre.sgbp un objeto de clase nb.Finalmente, construimos la matriz (lista) ponderada través de la función nb2listw(), verifique por su cuenta la estructura del objeto creado:Procedemos de forma semejante para la construcción de una matriz tipo torre, través de un par de funciones. Primero, se establece la función que recoge los patrones de interés, “compartir lados y vértices”:Luego, con esta función, creamos un objeto de tipo sgbp que contendrá los elementos informativos de vecindad tipo reina:continuación convertimos el objeto creado de sgbp nb con la función elaborada en la secuencia de la matriz tipo torre:Es turno de la matriz ponderada:Es posible contar con matrices de órdenes superiores, para ello, recurrimos la función nblag() con la que creamos un objeto del que, en el segundo paso, extraeremos la información de interés. Nota que el orden de contigüidad está establecido en el segundo argumento:Verifica la estructura con str(). partir del objeto realizado, reina.2o, se creará otro de tipo nb que contendrá la información relativa los datos de vecindad de tipo reina de orden 2, través de una selección del segundo elemento de dicho objeto:Para poder tener los dos tipos de estructura espacial (reina de primer y de segundo orden) en un solo objeto, es necesario crear un objeto partir de la función nblag_cumul() que arrojará como resultado un elemento de tipo nb que contendrá tanto la estructura de dependencia de primer como la de segundo orden:La visualización de la estructura espacial en este segundo enfoque es un poco más complicada, pues para poder mostrarla en un mapa de conectividad es necesario contar con los centroides geométricos como objetos independientes, es decir, como un par de coordenadas en un objeto independiente. Éstos los extraeremos del atributo “Geometry” que aparece en nuestra base de datos de tipo sf (covid_zmvm_sf). Para extraerlos usaremos la función map_dbl() del paquete purrr y st_centroid() del paquete sf, como se muestra enseguida:Ahora, esos elementos los guardamos en un nuevo objeto con la función “combina por columna”, es decir, cbind() (column bind):Para poder visualizar la estructura espacial de cada tipo de vecindad debemos graficar los centroides, la geometría de interés y las líneas que unen los centroides ,las cuales indican la vecindad. Para el caso de la matriz tipo torre:Para el caso de la matriz de tipo reina de primer orden:Para la de segundo orden:En este segundo enfoque expuesto, el de Anselin y su equipo, es posible construir otro tipo de matrices partir de umbrales de distancia: matriz de k-vecinos más cercanos. Para ello, usaremos de nueva cuenta las coordenadas con los centroides como punto de referencia para medir la distancia (el objeto coords). Luego, través de dos funciones knearneigh y knn2nb del paquete spdep construiremos la matriz, transformándola enseguida en un objeto de tipo nb. Luego, la convertiremos propiamente en un objeto tipo lista con los pesos:Para visualizar la estructura espacial:\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2022-01-19.book built bookdown R package.","code":"\nlibrary(sf)\nlibrary(spdep)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(knitr)\ncovid_zmvm_sf <-sf::st_read(\"base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp\")## Reading layer `covid_zmvm' from data source \n##   `C:\\Users\\Jarvis\\Desktop\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 76 features and 55 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2745632 ymin: 774927.1 xmax: 2855437 ymax: 899488.5\n## Projected CRS: Lambert_Conformal_Conic\nfunc_torre = function(a, b = a) st_relate(a, b, pattern = \"F***1****\")#Define la función y sus parámetros\ntorre.sgbp <- func_torre(covid_zmvm_sf) # Crea un objeto a partir de la función anterior. Requiere tener cargado el paquete sf\nas.nb.sgbp <- function(x, ...) {#Define la función con 1 argumento, un objeto de tipo `sgbp`\n  attrs <- attributes(x) #Guarda los atributos del objeto de tipo sgbp\n  x <- lapply(x, function(i) { if(length(i) == 0L) 0L else i } ) #Función que coloca un 0 a cada elemento sin vecinos.\n  attributes(x) <- attrs #aplica los atributos recién modificados al argumento x.\n  class(x) <- \"nb\" # Cambia el tipo de objeto de `sgbp` a `nb`\n  x #Devuelve el objeto\n}\ntorre.nb <- as.nb.sgbp(torre.sgbp)\nsummary(torre.nb)## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 196 \n## Percentage nonzero weights: 3.393352 \n## Average number of links: 2.578947 \n## 18 regions with no links:\n## 4 5 6 9 10 11 14 15 16 30 34 37 48 53 64 67 73 76\n## Link number distribution:\n## \n##  0  1  2  3  4  5  6  7  8 \n## 18 13  8 11 12  7  1  3  3 \n## 13 least connected regions:\n## 1 2 3 7 13 19 23 32 35 38 49 51 75 with 1 link\n## 3 most connected regions:\n## 28 46 71 with 8 links\nclass(torre.nb)## [1] \"nb\"\nlength(torre.nb)## [1] 76\ntorre.pesos <- spdep::nb2listw(torre.nb, zero.policy = TRUE)\nfunc_reina <- function(a, b = a) st_relate(a, b, pattern = \"F***T****\")\nreina.sgbp <- func_reina(covid_zmvm_sf)\nreina.nb <- as.nb.sgbp(reina.sgbp)\nsummary(reina.nb)## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 206 \n## Percentage nonzero weights: 3.566482 \n## Average number of links: 2.710526 \n## 18 regions with no links:\n## 4 5 6 9 10 11 14 15 16 30 34 37 48 53 64 67 73 76\n## Link number distribution:\n## \n##  0  1  2  3  4  5  6  7  8 \n## 18 13  7 12  7  9  3  4  3 \n## 13 least connected regions:\n## 1 2 3 7 13 19 23 32 35 38 49 51 75 with 1 link\n## 3 most connected regions:\n## 28 46 71 with 8 links\nreina.pesos <- spdep::nb2listw(reina.nb, zero.policy = TRUE)\nreina.2o <- spdep::nblag(reina.nb, 2)\nreina.2o.nb <- reina.2o[[2]]\nreina.ambos <- spdep::nblag_cumul(reina.2o)\nlongitud <- purrr::map_dbl(covid_zmvm_sf$geometry, ~st_centroid(.x)[[1]])\nlatitud <- purrr::map_dbl(covid_zmvm_sf$geometry, ~st_centroid(.x)[[2]])\ncoords <- cbind(longitud, latitud)\nplot(torre.nb, coords, lwd=.2, col=\"blue\", cex = .5)\nplot(reina.nb, coords, lwd=.2, col=\"red\", cex = .5)\nplot(reina.2o.nb, coords, lwd=.2, col=\"blue\", cex = .5)\nk6nb <- knn2nb(knearneigh(coords, k = 6))\nk6.pesos <- nb2listw(k6nb)\nplot(k6nb, coords, lwd=.2, col=\"blue\", cex = .5)"},{"path":"análisis-espacial-i-autocorrelación.html","id":"matriz-tipo-torre-rook","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.6.1 Matriz tipo torre (rook)","text":"La construcción de las matrices de pesos espaciales en R con este segundo enfoque se hará través de los paquetes sf y spdep, que permitirá crear una matriz de tipo torre y reina.Para hacerlo, es necesario seguir dos pasos. En el primero se construirá una función que recoja la relación de vecindad, esta función la llamaremos func_torre y dentro de dicha función definiremos la relación de vecindad que nos interesa través de otra función, la función st_relate() del paquete sf que “calcula relaciones entre pares de geometrías o las empareja con un patrón dado. Esta función también tiene un parámetro para un patrón específico” (Anselin y Morrison 2018) y la que nos interesa es el patrón “compartir lados”. Posteriormente, en el segundo paso, aplicamos la función creada nuestra geometría para crear un objeto de tipo sgbp:Observa la estructura y clase que tiene el objeto recién creado, torre.sgbp: es un objeto de tipo sgbp. Para poder interactuar más adelante con este objeto, es necesario modificar su clase. Se trata, pues, de transformar el objeto una clase nb, es decir, una neighbor list (lista de vecinos). Llevaremos cabo este procedimiento creando una función para el caso, .nb.sgbp. Esta función nos permitirá además lidiar con los polígonos que tienen vecinos.Ahora, través de la función recién creada trasformaremos el objeto torre.sgbp un objeto de clase nb.Finalmente, construimos la matriz (lista) ponderada través de la función nb2listw(), verifique por su cuenta la estructura del objeto creado:\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2022-01-19.book built bookdown R package.","code":"\nfunc_torre = function(a, b = a) st_relate(a, b, pattern = \"F***1****\")#Define la función y sus parámetros\ntorre.sgbp <- func_torre(covid_zmvm_sf) # Crea un objeto a partir de la función anterior. Requiere tener cargado el paquete sf\nas.nb.sgbp <- function(x, ...) {#Define la función con 1 argumento, un objeto de tipo `sgbp`\n  attrs <- attributes(x) #Guarda los atributos del objeto de tipo sgbp\n  x <- lapply(x, function(i) { if(length(i) == 0L) 0L else i } ) #Función que coloca un 0 a cada elemento sin vecinos.\n  attributes(x) <- attrs #aplica los atributos recién modificados al argumento x.\n  class(x) <- \"nb\" # Cambia el tipo de objeto de `sgbp` a `nb`\n  x #Devuelve el objeto\n}\ntorre.nb <- as.nb.sgbp(torre.sgbp)\nsummary(torre.nb)## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 196 \n## Percentage nonzero weights: 3.393352 \n## Average number of links: 2.578947 \n## 18 regions with no links:\n## 4 5 6 9 10 11 14 15 16 30 34 37 48 53 64 67 73 76\n## Link number distribution:\n## \n##  0  1  2  3  4  5  6  7  8 \n## 18 13  8 11 12  7  1  3  3 \n## 13 least connected regions:\n## 1 2 3 7 13 19 23 32 35 38 49 51 75 with 1 link\n## 3 most connected regions:\n## 28 46 71 with 8 links\nclass(torre.nb)## [1] \"nb\"\nlength(torre.nb)## [1] 76\ntorre.pesos <- spdep::nb2listw(torre.nb, zero.policy = TRUE)"},{"path":"análisis-espacial-i-autocorrelación.html","id":"matriz-tipo-reyna-queen","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.6.2 Matriz tipo reyna (queen)","text":"Procedemos de forma semejante para la construcción de una matriz tipo torre, través de un par de funciones. Primero, se establece la función que recoge los patrones de interés, “compartir lados y vértices”:Luego, con esta función, creamos un objeto de tipo sgbp que contendrá los elementos informativos de vecindad tipo reina:continuación convertimos el objeto creado de sgbp nb con la función elaborada en la secuencia de la matriz tipo torre:Es turno de la matriz ponderada:","code":"\nfunc_reina <- function(a, b = a) st_relate(a, b, pattern = \"F***T****\")\nreina.sgbp <- func_reina(covid_zmvm_sf)\nreina.nb <- as.nb.sgbp(reina.sgbp)\nsummary(reina.nb)## Neighbour list object:\n## Number of regions: 76 \n## Number of nonzero links: 206 \n## Percentage nonzero weights: 3.566482 \n## Average number of links: 2.710526 \n## 18 regions with no links:\n## 4 5 6 9 10 11 14 15 16 30 34 37 48 53 64 67 73 76\n## Link number distribution:\n## \n##  0  1  2  3  4  5  6  7  8 \n## 18 13  7 12  7  9  3  4  3 \n## 13 least connected regions:\n## 1 2 3 7 13 19 23 32 35 38 49 51 75 with 1 link\n## 3 most connected regions:\n## 28 46 71 with 8 links\nreina.pesos <- spdep::nb2listw(reina.nb, zero.policy = TRUE)"},{"path":"análisis-espacial-i-autocorrelación.html","id":"matrices-de-contigüidad-de-orden-superior","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.6.3 Matrices de contigüidad de orden superior","text":"Es posible contar con matrices de órdenes superiores, para ello, recurrimos la función nblag() con la que creamos un objeto del que, en el segundo paso, extraeremos la información de interés. Nota que el orden de contigüidad está establecido en el segundo argumento:Verifica la estructura con str(). partir del objeto realizado, reina.2o, se creará otro de tipo nb que contendrá la información relativa los datos de vecindad de tipo reina de orden 2, través de una selección del segundo elemento de dicho objeto:Para poder tener los dos tipos de estructura espacial (reina de primer y de segundo orden) en un solo objeto, es necesario crear un objeto partir de la función nblag_cumul() que arrojará como resultado un elemento de tipo nb que contendrá tanto la estructura de dependencia de primer como la de segundo orden:","code":"\nreina.2o <- spdep::nblag(reina.nb, 2)\nreina.2o.nb <- reina.2o[[2]]\nreina.ambos <- spdep::nblag_cumul(reina.2o)"},{"path":"análisis-espacial-i-autocorrelación.html","id":"visualización-de-la-estructura-espacial","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.7 Visualización de la estructura espacial","text":"La visualización de la estructura espacial en este segundo enfoque es un poco más complicada, pues para poder mostrarla en un mapa de conectividad es necesario contar con los centroides geométricos como objetos independientes, es decir, como un par de coordenadas en un objeto independiente. Éstos los extraeremos del atributo “Geometry” que aparece en nuestra base de datos de tipo sf (covid_zmvm_sf). Para extraerlos usaremos la función map_dbl() del paquete purrr y st_centroid() del paquete sf, como se muestra enseguida:Ahora, esos elementos los guardamos en un nuevo objeto con la función “combina por columna”, es decir, cbind() (column bind):Para poder visualizar la estructura espacial de cada tipo de vecindad debemos graficar los centroides, la geometría de interés y las líneas que unen los centroides ,las cuales indican la vecindad. Para el caso de la matriz tipo torre:Para el caso de la matriz de tipo reina de primer orden:Para la de segundo orden:","code":"\nlongitud <- purrr::map_dbl(covid_zmvm_sf$geometry, ~st_centroid(.x)[[1]])\nlatitud <- purrr::map_dbl(covid_zmvm_sf$geometry, ~st_centroid(.x)[[2]])\ncoords <- cbind(longitud, latitud)\nplot(torre.nb, coords, lwd=.2, col=\"blue\", cex = .5)\nplot(reina.nb, coords, lwd=.2, col=\"red\", cex = .5)\nplot(reina.2o.nb, coords, lwd=.2, col=\"blue\", cex = .5)"},{"path":"análisis-espacial-i-autocorrelación.html","id":"matrices-de-distancia-k-vecinos-más-cercanos","chapter":"3 Análisis espacial I: autocorrelación","heading":"3.8 Matrices de distancia: k-vecinos más cercanos","text":"En este segundo enfoque expuesto, el de Anselin y su equipo, es posible construir otro tipo de matrices partir de umbrales de distancia: matriz de k-vecinos más cercanos. Para ello, usaremos de nueva cuenta las coordenadas con los centroides como punto de referencia para medir la distancia (el objeto coords). Luego, través de dos funciones knearneigh y knn2nb del paquete spdep construiremos la matriz, transformándola enseguida en un objeto de tipo nb. Luego, la convertiremos propiamente en un objeto tipo lista con los pesos:Para visualizar la estructura espacial:","code":"\nk6nb <- knn2nb(knearneigh(coords, k = 6))\nk6.pesos <- nb2listw(k6nb)\nplot(k6nb, coords, lwd=.2, col=\"blue\", cex = .5)"},{"path":"modelos-de-regresión-lineal.html","id":"modelos-de-regresión-lineal","chapter":"4 Modelos de regresión lineal","heading":"4 Modelos de regresión lineal","text":"","code":""},{"path":"modelos-de-regresión-lineal.html","id":"causalidad-y-correlación-el-alcance-del-análisis-de-regresión","chapter":"4 Modelos de regresión lineal","heading":"4.1 Causalidad y correlación: el alcance del análisis de regresión","text":"En el proceso de investigación podemos plantearnos preguntas con un grado de complejidad diverso. Habrá preguntas que, al ser respondidas, nos permitan tener una descripción general del fenómeno, habrá otras que nos permitan explorar la relación entre dos fenómenos, o bien, preguntas cuya respuesta nos permita predecir el comportamiento futuro del elemento estudiado. Así pues, hay diferentes tipos de preguntas, dependiendo de su grado de complejidad.Entre los tipos de preguntas planteadas se halla uno que es de particular interés: el análisis causal. El objetivo del análisis causal es “ver qué sucede con una variable cuando manipulamos otra variable -mirar la causa y el efecto de una relación”(University 2020). Con el análisis causal se busca saber si un par de variables están asociadas, más aún, si si el cambio en una variable causa cambios en la otra. Responder este tipo de preguntas exige el dominio de diversas herramientas estadísticas, cuyo abordaje está completamente fuera del alcance de este capítulo. obstante, el anális de regresión nos brinda una oportunidad que, cuando es bien utilizada, nos permite aproximarnos al análisis causal.La regresión como instrumento analítico nos brinda los elementos para acercarnos la comprensión de la asociación entre dos variables, aunque es posible hablar en sentido estricto de causalidad como resultado del análisis efectuado. Así, “El análisis de regresión solo puede abordar los problemas de correlación. puede abordar el problema de la necesidad (causalidad). Por tanto, nuestras expectativas de descubrir relaciones de causa y efecto partir de la regresión deberían ser modestas (C., ., y Geoffrey 2012, 81:3).Entonces, ¿de qué modo el análisis de regresión se convierte potencialmente en un elemento que nos permita analizar causalidad? Lo que resultará fundamental en el proceso de investigación en donde se incorpore esta herramienta es la pregunta que se plantee el investigador. Y lo fundamental es plantear nuestras preguntas desde el conocimiento científico previamente existente, ¿qué dice la teoría sobre la relación entre la variable y la variable B?¿Qué asociaciones entre pares de variables recuerdas de tus cursos teoría económica? Quizá los siguientes ejemplos te resulten familiares: ) la asociación entre la tasa de interés y eficiencia marginal del capital en la teoría de la inversión de Keynes, ii) la relación entre la composición orgánica del capital y tasa de ganancia en la perspectiva de Marx sobre el comportamiento de largo plazo del capitalismo (la teoría de la tasa de ganancia decreciente), iii) los rendimientos decrecientes en el producto al añadir una unidad adicional de trabajo, ceteris paribus, en la teoría del productor de los neoclásicos.En el proceso de investigación científica, las preguntas planteadas deberán hacerse desde el conocimiento previamente existente, es decir, se plantean desde determinados cuerpos teóricos reconocidos y validados, o bien, partir de los resultados de investigaciones previas. Antes del método está la pregunta de investigación que establece la línea de causalidad entre las variables postuladas en el modelo. Esta base teórica ha de ser complementada con los resultados arrojados en el análisis exploratorio.Así pues, es relevante la siguiente advertencia:“Para establecer la causalidad, la relación entre los regresores y la variable de respuesta debe tener una base fuera de los datos de la muestra; por ejemplo, la relación puede ser sugerida por consideraciones teóricas. El análisis de regresión puede ayudar confirmar una relación de causa y efecto, pero puede ser la única base de tal afirmación” (C., ., y Geoffrey 2012, 81:3).","code":""},{"path":"modelos-de-regresión-lineal.html","id":"lo-artesanal-de-la-econometría","chapter":"4 Modelos de regresión lineal","heading":"4.2 Lo “artesanal” de la econometría","text":"cabe duda que la especificación de un buen modelo econométrico tiene algo de artesanal, es decir, serán pocas las ocasiones en las que tendremos que proponer diferentes alternativas de modelo, probar con múltiples variables y llevar cabo un sin número de ajustes. Conviene desde un momento inicial eliminar la idea de que “sólo hay que aplicar una técnica”: todas las técnicas requieren práctica y repetición pues siempre enfrentaremos un mismo problema en las mismas condiciones. este respecto puede ser apuntado que:“La construcción de un modelo de regresión es un proceso iterativo. Comienza utilizando conocimiento teórico del proceso que se está estudiando y los datos disponibles para especificar un modelo de regresión inicial. Las visualizaciones de datos gráficos suelen ser muy útiles para especificar el modelo inicial. Luego, los parámetros del modelo se estiman, normalmente por mínimos cuadrados (…). Luego debe evaluarse la adecuación del modelo. Consiste en buscar posibles errores de especificación de la formula del modelo, como incluir variables importantes, o incluir variables innecesarias, o datos inusuales o inapropiados. Si el modelo es inadecuado, entonces debe proponerse nuevamente uno diferente y estimar nuevamente los parámetros. Este proceso puede repetirse varias veces hasta obtener un modelo adecuado. Finalmente, la validación del modelo debe llevarse cabo para asegurar que el modelo producirá resultados que sean aceptables en la aplicación final” (C., ., y Geoffrey 2012, 81:3-10).\nFigura 4.1: Construcción de un modelo de regresión\nEn este capítulo buscamos que recuerdes algunos de los elementos fundamentales del análisis de regresión, como sus supuestos básicos y la interpretación de los resultados que nos ofrece este instrumento. Para ilustrar dichos elementos nos serviremos de la base de datos sobre COVID19 que contiene información sobre las condiciones sociodemodráficas y económicas de los municipios en la Zona Metropolitana del Valle de México, la misma base que hemos explorado en los capítulos previos. La pregunta que nos interesa responder es de carácter preliminar y atiende cuerpo teórico alguno, por lo que los resultados son sólo ilustrativos y tienen sólo fines didácticos. La pregunta planteada es: ¿de qué manera se asocian las condiciones sociales, demográficas y económicas, tanto con los casos positivos de COVID19 por cada 1 mil habitantes, como con las muertes provocadas por este mal por cada 1 mil habitantes? Para lograr dar solución la pregunta propondremos algunos modelos econométricos y buscaremos interpretarlos.","code":""},{"path":"modelos-de-regresión-lineal.html","id":"qué-buscamos-a-través-de-una-regresión-con-mínimos-cuadrados-ordinarios","chapter":"4 Modelos de regresión lineal","heading":"4.3 Qué buscamos a través de una regresión (con mínimos cuadrados ordinarios)","text":"Una regresión lineal pretende encontrar la recta que mejor se ajusta los datos observados. La técnica más usual para lograr esto es través de mínimos cuadrados ordinarios, MCO. Los MCO minimizan la suma de los residuales al cuadrado. Veamos este principio de forma interactiva. Para ello, crearemos una función en R que tendrá un sólo parámetro al cual llamaremos beta,el cual será el valor de la pendiente de la recta. Lo que pretendemos es encontrar el mejor valor de beta, es decir, el valor que permita minimizar la distancia entre los puntos dados por los pares ordenados (variable \\(x\\) y \\(y\\)) y la recta, en otras palabras, buscamos un valor de beta que minimice los errores.Para usar la aplicación interactiva, debes instalar los siguientes paquetes:Ahora bien, antes de ejecutar los siguientes segmentos de código, leélos e intenta comprender qué es lo que hace cada línea:Se usa function() justamente para crear para crear una función que llevará por nombre Gráfica (function() es una función como cualquier otra en R, semejante la función para calcular una media, mean(), o para crear una gráfica,plot()).Dentro del paréntesis colocamos los argumentos con los que trabajará nuestra función y, entre las llaves, {}, indicamos los elementos con los que vamos operar, incluidos los argumentos.Nuestra función tomará información de la base de datos llamada Galton, que viene con el paquete HistData, recuerda que Galton (1886) sentó las bases del análisis de regresión través de un estudio de lo que podríamos aquí llamar informalmente “herencia genética”, en otras palabras, cómo la altura de padres e hijos adultos está asociada.Después de haber leído el código, cópialo y pégalo en tu consola4:Una vez que ejecutado el código, observa la sección de ambiente en RStudio donde deberás ver la función creada, Gráfica(). Vamos interactuar con ella y para ello ejecutaremos el siguiente segmento de código:Verás en la sección de gráficos el resultado del código anterior: una gráfica con círculos de diferente tamaño que representan la altura de padres y sus hijos adultos; además, identificarás una línea (nuestra recta de ajuste). Como encabezado encontraras el nombre del único argumento de nuestra función, beta. Recuerda: beta es el valor de la pendiente de esa recta. Junto al valor del argumento de nuestra función aparece la suma de los residuales al cuadrado, o Error Cuadrático Medio (Mean Squared Error, MSE). ¿Logras observar el engrane en la parte superior izquierda de la gráfica? Usa el deslizador que aparece cuando das clic en él para elegir diferentes valores para beta y seleccionar aquel que te permita obtener el menor valor de MSE: ese valor de beta es el que buscamos con los MCO.Como verás, lo que buscamos es justamente un modelo que describa nuestros datos. Hay muchos tipos de modelos para describir el comportamiento entre dos variables (curvas cuadráticas, curvas exponenciales o curvas polinomiales). Aquí buscamos expresar la relación entre nuestro par de variables de la forma más simple posible: través de una línea recta. Te recomendamos ámpliamente el material de David Dalpiaz, Applied Statistics R, particularmente el capítulo 7 en caso de que quieras recordar con todo detalle los fundamentos estadísticos de la regresión (Dalpiaz, s. f.), así como el material de (C., ., y Geoffrey 2012).","code":"\n#Instalar dos paquetes\ninstall.packages(\"manipulate\", \"UsingR\")#Para llamar las librerías que recien instalaste\nlibrary(manipulate)\nlibrary(UsingR)\n\n#Crear función llamada \"Gráfica\"\nGráfica <- function(beta){\n  y <- Galton$child - mean(Galton$child) #Crea un vector llamado \"y\" con las desviaciones de la media de las alturas de los hijos.\n  x <- Galton$parent - mean(Galton$parent) #Crea un vector llamado \"x\" con las desviaciones de la media de las alturas de los padres.\n  freqData <- as.data.frame(table(x, y)) #Crea un arreglo de datos llamado freqData con los vectores anteriores\n  names(freqData) <- c(\"child\", \"parent\", \"freq\") #Asigna nombres a las columnas del data frame anterior\n  plot(                 \n    as.numeric(as.vector(freqData$parent)),\n    as.numeric(as.vector(freqData$child)),\n    pch = 21, col = \"black\", bg = \"lightblue\",\n    cex = .15 * freqData$freq,\n    xlab = \"Padres\",\n    ylab = \"Hijos\"\n  )#Crea un diagrama de dispersión con varios elementos de formato, el más importante es que el tamaño de los símbolos usados dependerá de la frecuencia. \n  abline(0, beta, lwd = 3)#Añade una línea en función del valor elegido de beta\n  points(0, 0, cex = 2, pch = 19) #Añade puntos\n  mse <- mean( (y - beta * x)^2 ) #Muestra el valor de la suma de los residuales al cuadrado \n  title(paste(\"beta = \", beta, \"mse = \", round(mse, 3)))} #Añade títulos a la gráficamanipulate(Gráfica(beta), beta = slider(-1.5, 1.5, step = 0.02))"},{"path":"modelos-de-regresión-lineal.html","id":"modelo-de-regresión-lineal-simple","chapter":"4 Modelos de regresión lineal","heading":"4.4 Modelo de regresión lineal simple","text":"Ya que tenemos una noción básica de lo que buscamos con una regresión, es decir, un modelo lineal que describa el comportamiento de nuestros datos, es momento de dar una definición más precisa: “El análisis de regresión es una técnica estadística para investigar y modelar la relación entre variables” (C., ., y Geoffrey 2012). Un modelo de regresión lineal simple es un modelo con una sola regresora (una variable \\(x\\)) cuya relación con la variable de respuesta (\\(y\\)) está dada por una línea recta:\\[y=\\beta_0+\\beta_1x+u \\]En donde \\(\\beta_0\\) y \\(\\beta_1\\) son constates desconocidas, el intercepto y la pendiente, respectivamente; en tanto, \\(u\\) es el componente de error aleatorio (que se asume con una distribución normal con media cero y varianza constante, \\(N(\\mu=0,\\sigma_u^2=\\sigma^2)\\)), que es la “falla” de nuestro modelo, es decir, la diferencia entre el valor observado de \\(y\\) y el valor estimado por nuestro modelo.¿Cómo deben ser entendidos los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\), el intercepto y la pendiente? “La pendiente, \\(\\beta_1\\) se puede interpretar como el cambio en la media de \\(y\\) para un cambio unitario en \\(x\\)” (C., ., y Geoffrey 2012, 81:3). En tanto, “si el rango de datos en \\(x\\) incluye \\(x = 0\\), entonces la intersección \\(\\beta_0\\) es la media de la distribución de la variable de respuesta \\(y\\) cuando \\(x = 0\\) (…es decir, la media de la variable \\(y\\) cuando \\(x=0\\)…) Si el rango de \\(x\\) incluye cero, entonces \\(β_0\\) tiene una interpretación práctica”.Una regresión lineal es pues el método que nos permite evaluar la relación lineal entre variables numéricas y se le llama precisamente lineal por la linealidad de sus parámetros (las betas). Lo que buscamos con el análisis de regresión es encontrar los valores estimados de \\(\\beta_0\\) y \\(\\beta_1\\), es decir, \\(\\hat\\beta_0\\) y \\(\\hat\\beta_1\\), que nos permitan describir con una línea recta el par de variables consideradas y el método más usual son los Mínimos Cuadrados Ordinarios (MCO.Los estimadores obtenidos por MCO tienen las siguientes propiedades (C., ., y Geoffrey (2012),: 19):Son combinaciones lineales de las observaciones de \\(y_i\\).Son estimadores insesgados, es decir, \\(E(\\hat\\beta_1)=\\beta_1\\).Su varianza es constante.En esta sección enlistamos los elementos sobre los que se funda la estimación de un modelo clásico de regresión lineal con mínimos cuadrados y haremos énfasis en uno de ellos que se vincula directamente con los contenidos del capítulo siguiente: la autocorrelación. Para una explicación más detallada de estos supuestos en los que se basa la estimación con MCO, conviene que revises el capítulo 3 de la clásica obra de Gujarati y Porter (Gujarati y Porter 2011).El modelo es líneal en los parámetros, aunque necesariamente en sus variables.Los valores de la variable o variables explicativas son fijos, es decir, aleatorios, o bien, independientes del término de error.La media de los términos de error, dado el valor de la variable explicativa, es cero.La varianza de los errores es constante (homoscedasticidad).Los errores o perturbaciones son independientes entre sí (hay autocorrelación).El número de observaciones \\(n\\) debe ser mayor que el número de parámetros estimar.La variable o variables explicativas debe tener suficiente variabilidad.El supuesto v o supuesto de autocorrelación establece que dos errores, \\(u_i\\) y \\(u_j\\), donde los subíndices \\(j\\) e \\(\\) se refieren que corresponden perturbaciones de dos observaciones diferentes, están asociados, es decir, que la covarianza entre ellos es igual cero. Si se graficasen los errores en un diagrama de dispersión, lo que esperamos es que haya ningún patrón sistemático. Reproducimos aquí la figura 3.6 de la obra de (Gujarati y Porter 2011), en donde los paneles y b dan cuenta de un comportamiento sistemático o aleatorio en los errores, lo que es indicio de autocorrelación o dependencia entre los errores, en cambio, en el panel c se observa ningún patrón sistemático, esto es justamente lo que se busca en los errores del modelo, que los errores se distribuyan aleatoriamente.\nFigura 4.2: Patrones de correlación. Fuente: Gujarati y Porter, 2011, p. 67\nDe forma simple, el supuesto de autocorrelación “se afirma que se considerará el efecto sistemático, si existe, de \\(X_t\\) sobre \\(Y_t\\), sin preocuparse por las demás influencias que podrían actuar sobre \\(Y\\) como resultado de las posibles correlaciones entre las \\(u\\)” (Gujarati y Porter 2011, 67).El tipo de datos que son objeto de este libro, los datos espaciales, suelen presentar entre sus características autocorrelación espacial, por tanto, recurrir una estimación con MCO violará este supuesto, por lo que tendremos que recurrir técnicas específicas para la modelación con datos espaciales, esto será objeto del capítulo 5 donde veremos algunos modelos para solventar este problema. En tanto, veamos cómo construir un modelo de regresión lineal con mínimos cuadros ordinarios en R y cómo interpretar nuestros resultados.","code":""},{"path":"modelos-de-regresión-lineal.html","id":"supuestos-del-módelo-clásico-de-regresión-lineal","chapter":"4 Modelos de regresión lineal","heading":"4.4.1 Supuestos del módelo clásico de regresión lineal","text":"En esta sección enlistamos los elementos sobre los que se funda la estimación de un modelo clásico de regresión lineal con mínimos cuadrados y haremos énfasis en uno de ellos que se vincula directamente con los contenidos del capítulo siguiente: la autocorrelación. Para una explicación más detallada de estos supuestos en los que se basa la estimación con MCO, conviene que revises el capítulo 3 de la clásica obra de Gujarati y Porter (Gujarati y Porter 2011).El modelo es líneal en los parámetros, aunque necesariamente en sus variables.Los valores de la variable o variables explicativas son fijos, es decir, aleatorios, o bien, independientes del término de error.La media de los términos de error, dado el valor de la variable explicativa, es cero.La varianza de los errores es constante (homoscedasticidad).Los errores o perturbaciones son independientes entre sí (hay autocorrelación).El número de observaciones \\(n\\) debe ser mayor que el número de parámetros estimar.La variable o variables explicativas debe tener suficiente variabilidad.El supuesto v o supuesto de autocorrelación establece que dos errores, \\(u_i\\) y \\(u_j\\), donde los subíndices \\(j\\) e \\(\\) se refieren que corresponden perturbaciones de dos observaciones diferentes, están asociados, es decir, que la covarianza entre ellos es igual cero. Si se graficasen los errores en un diagrama de dispersión, lo que esperamos es que haya ningún patrón sistemático. Reproducimos aquí la figura 3.6 de la obra de (Gujarati y Porter 2011), en donde los paneles y b dan cuenta de un comportamiento sistemático o aleatorio en los errores, lo que es indicio de autocorrelación o dependencia entre los errores, en cambio, en el panel c se observa ningún patrón sistemático, esto es justamente lo que se busca en los errores del modelo, que los errores se distribuyan aleatoriamente.\nFigura 4.2: Patrones de correlación. Fuente: Gujarati y Porter, 2011, p. 67\nDe forma simple, el supuesto de autocorrelación “se afirma que se considerará el efecto sistemático, si existe, de \\(X_t\\) sobre \\(Y_t\\), sin preocuparse por las demás influencias que podrían actuar sobre \\(Y\\) como resultado de las posibles correlaciones entre las \\(u\\)” (Gujarati y Porter 2011, 67).El tipo de datos que son objeto de este libro, los datos espaciales, suelen presentar entre sus características autocorrelación espacial, por tanto, recurrir una estimación con MCO violará este supuesto, por lo que tendremos que recurrir técnicas específicas para la modelación con datos espaciales, esto será objeto del capítulo 5 donde veremos algunos modelos para solventar este problema. En tanto, veamos cómo construir un modelo de regresión lineal con mínimos cuadros ordinarios en R y cómo interpretar nuestros resultados.","code":""},{"path":"modelos-de-regresión-lineal.html","id":"regresión-lineal-simple-en-r","chapter":"4 Modelos de regresión lineal","heading":"4.5 Regresión lineal simple en R","text":"Para ilustrar cómo emplear una regresión lineal en R usaremos de nuevo la base de datos covid_zmvm, conformada por 54 variables de 76 observaciones, municipios y alcaldías, que conforman la Zona Metropolitana del Valle de México. Carguemos nuestra base:La base recién cargada es de tamaño considerable, por lo que te recomendamos revisar el diccionario que la acompaña para que comprendas el significado las etiquetas usadas para cada variable. Será más sencillo que comprendas cómo está integrada la base si agrupamos en tres categorías las variables relevantes para elaborar nuestro modelo:COVID19: cuatro variables sobre casos positivos y defunciones por COVID19 durante la primera ola de contagios, en términos absolutos y relativos.Sociodemográficas: 32 variables asociadas las características de la población y sus condiciones de viviendaEstructura económica: nueve variables que describen la estructura económica y remuneraciones medias por gran sector de actividad.Para ilustrar el método de regresión con MCO y estimar los parámetros \\(\\beta_1\\) y \\(\\beta_0\\) propondremos un modelo de regresión lineal simple, es decir, compuesto sólo por la variable de respuesta (nuestra \\(y\\)). De modo que la variable que buscaremos explicar con nuestro modelo son los casos positivos COVID19 por cada 1 mil habitantes, pos_hab) través de sólo un predictor (nuestra \\(x\\), en este caso el porcentaje de población con acceso servicios de salud, ss). El modelo estimar será entonces:\\[poshab_i=\\beta_0+\\beta_1ss_i+u_i\\]\nEl subíndice \\(\\) da cuenta de cada una de las 76 unidades espaciales que conforman la Zona Metropolitana del Valle de México. Para estimar un modelo como el anterior, en R recurrimos la función lm(), linear model, del paquete stats que se instala desde que añades R tu equipo de cómputo. La función usada cuenta con dos argumentos data= y formula= que indican la fuente de datos y la expresión estimar, respectivamente. El resultado lo guardaremos en un nuevo objeto que nombraremos modelo_simple:Nota cómo la especificación del modelo tiene la forma “variable de respuesta ~ variable explicativa”. Observa la parte superior derecha de tu ambiente de trabajo, donde aparece el objeto que se ha creado, éste es de tipo lm, es una lista con 12 elementos. Da clic en el icono de la lente de aumento y podrás observar que el resultado de la función lm() contiene muchos datos de interés.EjercicioRevisa la documentación de la función lm() y responde:¿Es posible ponderar el proceso de ajuste con base en alguna variable? De ser así, ¿cuál es el argumento que lo permite?¿Es posible ponderar el proceso de ajuste con base en alguna variable? De ser así, ¿cuál es el argumento que lo permite?Si alguna de las variables usadas en el modelo tiene registros vacíos, ¿qué argumento debo recurrir para corregir esta situación?Si alguna de las variables usadas en el modelo tiene registros vacíos, ¿qué argumento debo recurrir para corregir esta situación?¿Qué elemento del objeto de tipo lm contiene los coeficientes estimados?¿Qué elemento del objeto de tipo lm contiene los coeficientes estimados?¿Al menos cuántos componentes podría contener la lista de tipo lm?¿Al menos cuántos componentes podría contener la lista de tipo lm?¿Para qué sirven las funciones coef(), resid() y fitted() aplicadas un objeto de tipo lm?¿Para qué sirven las funciones coef(), resid() y fitted() aplicadas un objeto de tipo lm?Para observar los resultados de nuestra estimación basta con llamar al objeto nuestra consola:Una correcta interpretación de los resultados atraviesa por recordar cómo están medidas cada una de las variables, en este caso la variable pos_hab es el número de casos positivos por cada 1 mil habitantes. Tomemos un municipio en particular, por ejemplo, Isidro Fabela, en el Estado de México que tiene 1.71 casos por cada 1 mil habitantes. En tanto, la variable ss es la proporción de población con afiliación servicios de salud de cada municipio o alcaldía, que para Isidro Fabela es de 78.6, es decir, la proporción de población con acceso servicios de salud es de 78.6%. Veamos un par de gráficas para recordar cómo se comportan este par de variables:El valor de \\(\\beta_1\\), el coeficiente de la variable ss, es de 0.3949, lo que significa que cuando la proporción de población con acceso servicios de salud aumenta en 1 punto porcentual (recuerda que así lo estamos midiendo), el número de casos positivos por cada 1 mil habitantes aumenta 0.39, en promedio. Dicho en otras palabras, el parámetro estimado de la pendiente, \\(\\hat \\beta_1\\), nos indica como la media de los casos positivos, \\(y\\), es afectada por un cambio en \\(x\\), el porcentaje de personas con acceso servicios de salud. En este caso, el coeficiente estimado \\(\\hat \\beta_0\\) carece de interpretación razonable. Así, el modelo estimado, usando los valores de los coeficientes obtenidos es:\\[\\hat y=\\hat \\beta_0+\\hat\\beta_1x\\]\nEs decir,\\[\\hat {poshab}=-19.8622+0.3949ss\\]Un elemento de interés para el análisis en la regresión lineal estimada son los errores observados, \\(u_i\\). Para lograr comprender mejor este elemento conviene imaginar nuestro modelo como “Respuesta = Predicción + Error”, es decir, \\(y=\\hat y+u\\). De esta manera podemos expresar el error como:\\[u_i=y_i-\\hat {y_i}\\]\nLos errores observados, \\(u_i\\), y los valores estimados, \\(\\hat y_i\\), producto de nuestro modelo estimado para cada uno de los 76 municipios y alcaldías del Valle de México son almacenados en el elemento residuals y fitted.values, respectivamente, dentro de la lista lm, da clic en la lente de aumento al final de la fila que contiene el objeto de tipo lm en la sección de ambiente en tu entorno de trabajo. Para poder analizar mejor los errores del modelo, vamos crear un nuevo objeto que contenga, además de nuestras variables de interés (pos_hab y ss), la clave municipal (cvemun), los errores observados y los valores ajustados o predichos:Ejercicio¿Cómo esperarías que luzca un diagrama de dispersión entre los valores observados y ajustados de la variable de respuesta? Elabora dicho diagrama.¿Cómo esperarías que luzca un diagrama de dispersión entre los valores observados y ajustados de la variable de respuesta? Elabora dicho diagrama.Grafica los errores, primero respecto un índice del 1 al 76 y luego respecto los valores ajustados ¿puedes observar algún patrón en ellos?Grafica los errores, primero respecto un índice del 1 al 76 y luego respecto los valores ajustados ¿puedes observar algún patrón en ellos?El supuesto de linealidad de nuestro modelo puede ser verificado partir de una exploración visual de un diagrama de dispersión entre los errores observados y la predictora. Si nuestro modelo es verdaderamente un modelo lineal, en la gráfica debería mostrarse patrón alguno. Veamos:Claramente hay un patrón, por lo que probablemente un modelo lineal como el propuesto sería la mejor opción. ¿Recuerdas cómo luce un diagrama de dispersión entre las dos variables de nuestro modelo?EjercicioElabora un diagrama de dispersión entre pos_hab y ss y añade un ajuste lineal.Entonces, parece que el modelo lineal propuesto capta con precisión la relación entre este par de variables, por lo que un modelo lineal podría ser una mejor opción; sin embargo, esta alternativa de modelación es objeto de estas notas, pero en el capítulo 14 de la obra de (Gujarati y Porter 2011) puedes revisar con detalle una ruta para solventar esta situación.Otro de los supuestos mencionados es que los errores deberían tener una distribución normal, lo que se cumplirá cuando haya observaciones inusuales. Para verificar esto visualmente, es posible recurrir un histograma de los errores, o bien, una gráfica de normalidad:¿Notas los valores extremos en el histograma anterior? Una gráfica cuantil-cuantil o gráfica QQ, contrasta la distribución de una variable con una distribución teórica: si la distribución de la variable fuera idéntica la distribución teórica, los puntos se alinearían sobre la línea de 45°. En este caso, la distribución teórica probar es una distribución normal:EjercicioConsulta la ayuda de las funciones geom_qq() y stat_qq_line(), ¿qué otras distribuciones es posible verificar y con qué argumento se hace?Finalmente, el supuesto de varianza constante de los errores también puede explorarse visualmente través de una gráfica donde se debería lograr ver variabilidad constante de los errores alrededor de la recta de ajuste: este es el denominado supuesto de homoscedasticidad. Exploremos si los errores de nuestro modelo cumplen o con este supuesto través de una gráfica de dispersión:Patrones visuales de “tipo ventilador” o “embudo” deberían ser observados cuando los errores son homoscedásticos, lo que es el caso. Los ejercicios gráficos hechos aquí con los errores para verificar si se cumplen o los supuestos de un modelo de MCO son desde luego la única ruta para verificar los supuestos. El análisis de si una regresión cumple con los supuestos necesarios requiere de práctica. En esta página encontrarás una serie de ejemplos de cuando una regresión cumple o con los supuestos enlistados antes y como lucen las gráficas mencionadas en cada caso. Sin embargo, como recordarás, existen pruebas formales que te permitirán determinar si el modelo cumple o con los supuestos del caso.Ejercicio¿Cuál es el nombre de la prueba más popular para evaluar normalidad y cómo se interpreta?¿Cuál es el nombre de la prueba más popular para evaluar normalidad y cómo se interpreta?Menciona dos pruebas para evaluar homoscedasticidad y cómo son interpretadas.Menciona dos pruebas para evaluar homoscedasticidad y cómo son interpretadas.Además, los errores del modelo nos sirven para calcular la varianza, \\(\\sigma^2\\) del modelo estadístico propuesto. Para ello, recuerda que:\\[\n\\begin{aligned}\ns_e ^2 &=\\frac{1}{n-2}\\sum_{=1}^n(y_i-\\hat y _i)^2 \\\\\n  s_e ^2 &=\\frac{1}{n-2}\\sum_{=1}^n(e_i)^2 \\\\\n  \\end{aligned}\n\\]Este dato por sí mismo ofrece mucha información de la variabilidad de nuestro modelo, pero, si obtenemos su raíz cuadrada. La cifra estará ya en las mismas unidades que las usadas en el modelo y tendremos la desviación estándar, también conocida como error estándar residual.Ejercicio¿cuánto asciende el error estándar residual, residual standard error, de nuestro modelo?¿cuánto asciende el error estándar residual, residual standard error, de nuestro modelo?¿Cómo interpretarías dicha cifra?¿Cómo interpretarías dicha cifra?¿Un modelo con error estándar residual pequeño es preferible uno que tenga un error estándar residual alto?¿Un modelo con error estándar residual pequeño es preferible uno que tenga un error estándar residual alto?El \\(R^2\\) o coeficiente de determinación es una medida de bondad de ajuste, es decir, un número de nos indica qué tanto nuestro modelo explica la variable de interés. El \\(R^2\\) es la proporción de la variabilidad de \\(y\\) que es explicada por \\(x\\). Los valores más cercanos 1 significan que una mayor parte de la variabilidad de \\(y\\) es explicada por \\(x\\). En nuestro caso, hemos obtenido un \\(R^2\\) de 0.22, lo que puede ser interpretado de la siguiente manera: poco más de la quinta parte de la variabilidad de los casos positivos por cada 1 mil habitantes es explicada por el porcentaje de población con acceso servicios de salud.través de nuestro modelo estimado (también llamado ajustado) es posible hacer predicciones de \\(y\\), es decir, de los casos positivos por cada 1 mil habitantes, siempre que los valores de la variable predictora estén dentro del rango original.Ejercicio¿Cuál es el rango de la variable predictora, es decir de \\(x\\)?Hagamos una predicción de los casos positivos cuando \\(x\\) toma el valor de 60%:\\[\\hat {poshab}=-19.8622+0.3949(60)\\]Con independencia de la consistencia y validez de nuestro modelo, situación que de momento estamos evaluando, ¿cuál es el valor promedio de los casos positivos de COVID19 por cada 100 mil habitantes cuando el porcentaje de población con acceso servicios de salud es de 60%? Bastará resolver la expresión anterior para saber que el valor estimado promedio de los casos activos por cada 1 mil habitantes cuando el porcentaje de población con acceso servicios de salud es de 60%, es decir, 3.83 casos por cada 1 mil habitantes, aproximadamente.Alternativamente, en R contamos con la función predict() para llevar cabo, justamente, predicciones. La función requiere de dos argumentos forzosos: ) el modelo usado para la predicción (argumento object=) y ii) los valores de la regresión (argumento newdata=). Así:En este caso, estamos indicando los nuevos datos de forma manual para la variable de interés, ss.Uno de los aspectos más importantes del análisis de regresión es la interpretación de los coeficientes obtenidos. Será de particular interés para nosotros tener la información para cada coeficiente: ) conocer el sentido de su asociación con la variable dependiente, es decir, el signo del coeficiente; ii) identificar la magnitud de la asociación lineal, qué tan grande es dicho coeficiente en el contexto de las unidades de la variable y iii) su significancia estadística, si hay evidencia de que el verdadero valor del estimador es correspondiente con el estimado.Los dos primeros elementos, sentido y magnitud, ya fueron comentados con anterioridad y pueden ser conocidos llamando al objeto que contiene los resultados de nuestro modelo:R despliega en su consola el valor de los dos coeficientes estimados, en este caso \\(\\hat \\beta_0\\) es negativo y \\(\\hat \\beta_1\\) es positivo. El punto iii consiste en verificar si, en términos estadísticos, nuestros coeficientes estimados son iguales o determinado valor, concretamente, buscamos llevar cabo una prueba de hipótesis sobre dichos coeficientes.Sinteticemos el procedimiento de prueba de hipótesis desde una perspectiva netamente práctica y simplificada:Definir el juego de hipótesis verificar, la hipótesis nula (Ho) y la hipótesis alternativa (Ha).Definir un nivel de significancia, conocido como \\(\\alpha\\), valor con base en el cual tomarás una decisión en torno las hipótesis propuestas. Además, recuerda que (1-\\(\\alpha\\)) define el nivel de confianza con el que la decisión sobre el juego de hipótesis verificar es tomada.Comparar el valor p asociado cada coeficiente estimado con el nivel de \\(\\alpha\\) elegido y decidir con arreglo los siguientes criterios:\n. Si el valor-p < \\(\\alpha\\): se rechaza Ho en favor de Ha.\nb. Si el valor-p > \\(\\alpha\\): se rechaza Ho.El juego de hipótesis verificar sobre cada uno de los coeficientes estimados es:\\[H_o:\\hat\\beta_i=0 \\\\\nH_a:\\hat\\beta_i\\=0 \\]Ahora bien, \\(\\alpha\\) usualmente toma valores de 0.1, 0.05 y 0.01, lo que equivale niveles de confianza de 0.9, 0.95 y 0.99, es decir, de 90%, 95% y 99%, respectivamente. ¿Pero de dónde obtendremos el valor-p para tomar la decisión? La salida de la regresión viene acompañada de él y con base en él llevaremos cabo la comparación. Para observar los valores-p de cada uno de los coeficientes estimados solicitamos un resumen del objeto modelo_mco con la función summary():Observa con cuidado los resultados que ahora se presentan como una tabla en su consola. Identifica las columnas estimate y Pr(>|t|): en esta última aparece el valor-p y es el elemento con base en el cual tomaremos la decisión sobre la significancia estadística de los coeficientes obtenidos.Veamos los valores de los coeficientes estimados: el intercepto, \\(\\hat \\beta_0\\) es igual -19.8 y el coeficiente vinculado ss, \\(\\hat \\beta_1\\), tiene un valor de 0.394. Ahora bien, recuerda el juego de hipótesis verificar: \\(H_o:\\hat\\beta_i=0\\) y \\(H_a:\\hat\\beta_i\\=0\\) (paso 1), elijamos un nivel de significancia, \\(\\alpha\\), de 0.05 (paso 2) y comparemos el valor p asociado cada coeficiente con \\(\\alpha\\) (paso 3). El siguiente cuadro sintentiza lo anterior:Cuadro 4.1Así, decimos que para el caso del coeficiente estimado de ss hay evidencia suficiente para afirmar que su verdadero valor es diferente de cero, por lo que decimos que es estadísticamente significativo, o lo que es lo mismo, hay evidencia suficiente con un nivel de confianza de 95% para sostener que entre el número de casos positivos por cada 1 mil habitantes y el porcentaje de población con acceso afiliación servicios de salud presentan una asociación lineal.Antes de concluir esta sección, es importante tener en mente los siguientes puntos cuando usemos regresiones:Los modelos de regresión son útiles para llevar cabo intrapolación, es decir, para obtener valores en el rango de nuestras variables explicativas. obstante, deben ser usados con cuidado cuando lo que se pretende es extrapolar datos, es decir, conocer los valores de \\(y\\) que están en el rango original de \\(X\\).Como el ajuste de mínimos cuadrados depende en buena medida de los valores de \\(x\\) y con este método cada punto \\(x\\) tiene la misma ponderación, la pendiente de la regresión está mayormente influenciada por los valores más extremos de \\(x\\). Cuando se tienen valores inusuales se podría proceder eliminarlos, o bien, recurrir técnicas diferentes los mínimos cuadrados ordinarios que sean menos sensibles estos puntos.Que la regresión tuviera como resultado que dos variables están asociadas, significa que entre ellas haya una relación causal. Causalidad implica asociación, pero lo opuesto es necesariamente cierto.La regresión lineal múltiple es, en esencia, una extensión lógica de todos los elementos descritos antes. En palabras de (C., ., y Geoffrey 2012, 81:67), “Un modelo de regresión que involucra más de una variable regresora se llama modelo de regresión múltiple”. Así, por ejemplo, un modelo con dos regresoras, \\(x_1\\) y \\(x_2\\) luciría como:\\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\epsilon\\]Esta expresión que incluye tres variables aún es posible representarse en espacio tridimensional con los ejes \\(y\\), \\(x_1\\) y \\(x_2\\) y correspondería múltiples planos en el espacio. El parámetro \\(\\beta_0\\) es el intercepto del plano de la regresión que, cuando el origen está incluido en el rango de \\(x_1\\) y \\(x_2\\), \\(\\beta_0\\) indica la media de \\(y\\) cuando \\(x_1=x_2=0\\). En tanto, \\(\\beta_1\\) es el cambio esperado \\(y\\) cuando \\(x_1\\) varia en una unidad, siempre que \\(x_2\\) permanezca constante, análogamente para \\(\\beta_2\\). ¿Qué variables de nuestra base incluirías en un modelo que incluya dos regresoras?Ejercicio¿Recuerdas cómo construir una matriz de diagramas de dispersión través del paquete GGally? Explora el conjunto de variables de la base covid_zmvm y construye una matriz de diagramas de dispersión sólo con 4 o 5 variables que presenten el mayor nivel de correlación tanto con los casos positivos como con las defunciones por cada 1 mil habitantes, pos_hab y def_hab.Un modelo con buen nivel de ajuste debería procurar que las variables explicativas propuestas presenten una alta correlación con la variable explicar. Como podrás haberte dado cuenta, las variables que presentan el mayor nivel de asociación lineal negativa son: ppob_basi, pocom, occu, sbasc, ppob_5_o_m. En tanto, las variables que muestran el mayor nivel de asociación lineal positiva son: poss, grad_m, ppob_sup, grad, grad_h, tmss y rmss. obstante, otro principio recomendable la hora de proponer un modelo, amén de su consistencia teórica, es su simpleza: ¿qué relevancia tendrá incluir el grado promedio de escolaridad total, además del de hombres y mujeres por separado? Bastaría incluir sólo una de ellas. Por otro lado, ¿qué tan oportuno sería incluir simultáneamente variables que buscan recoger el mismo aspecto de la realidad? Tanto las variables grad, ppob_basi y sbasc refieren el nivel de educación formal de la población, así, bastaría usar alternativamente alguna de ellas, algo parecido pasa con occu y ppob_5_o_m que expresan las condiciones de vivienda y habitación de las personas. En tanto, parece que de los grandes sectores: industria, comercio y servicios, las variables de éste último son las que muestran un mayor nivel de asociación lineal: poss, tmss y rmss, ¿será relevante incluir todas en el modelo? .Con base en lo dicho antes, propongamos un primer modelo lo más sencillo posible:\\[poshab_i=\\beta_0+\\beta_1grad_i+\\beta_2occu_i+\\beta_3poss_i+\\beta_4pocom_i+\\epsilon_i\\]\nDe nuevo, través de la función lm() estimaremos este modelo y al resultado lo guardaremos en un nuevo objeto llamado modelo_multiEjercicio¿Qué pasa con las variables propuestas en este modelo? ¿Cuáles resultaron estadísticamente significativas y por qué?¿Qué pasa con las variables propuestas en este modelo? ¿Cuáles resultaron estadísticamente significativas y por qué?Aquí hemos expuesto una versión simplificada en extremo de cómo tomar decisiones través de las pruebas de hipótesis, puedes remitirte cualquier libro de econometría y responder, ¿de qué modo la prueba t (t value, penúltima columna del cuadro de resumen) nos permitiría llegar las mismas conclusiones?Aquí hemos expuesto una versión simplificada en extremo de cómo tomar decisiones través de las pruebas de hipótesis, puedes remitirte cualquier libro de econometría y responder, ¿de qué modo la prueba t (t value, penúltima columna del cuadro de resumen) nos permitiría llegar las mismas conclusiones?Seguramente notaste que cuando se llamó el objeto modelo_multi tu consola aparecieron otros tantos elementos ,además de los coeficientes, entre ellos uno llamado F-statistic , el estadístico F. En este contexto es utilizado para evaluar si la particular combinación de variables propuesta contribuye o explicar la variable de interés. En términos concretos, la prueba F y el valor p asociado ella, permiten evaluar el siguiente juego de hipótesis:\\[\n\\begin{aligned}\nHo&: \\hat\\beta_1=\\hat\\beta_2=...=\\hat\\beta_i=0 \\\\\nHa&: al.menos.una. \\hat\\beta_i\\=0 \\\\\n\\end{aligned}\n\\]es decir, al menos uno de los coeficientes estimados, en su particular combinación, es diferente de cero. Así, con un valor p de casi cero (2.112e-09), resulta claro que se rechaza Ho en favor de Ha, es decir, que al menos uno de los coeficientes estimados es diferente de 0 en el modelo propuesto y que esta particular combinación de variables explicativas tiene algo interesante para decirnos.Dejemos hasta aquí este somero repaso de los aspectos más elementales del análisis de regresión. Es primordial que por tu propia cuenta estudies los materiales sugeridos, pues este capítulo en modo alguno suple un curso formal de los tópicos básicos de econometría. En el siguiente capítulo, retomamos el hilo del tratamiento de la información espacial través de su incorporación los modelos econométricos.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2022-01-19.book built bookdown R package.","code":"\nlibrary(readxl)\ncovid_zmvm <- read_excel(path=\"base de datos\\\\covid_zmvm.xlsx\")\nmodelo_simple <- stats::lm (formula=pos_hab ~ ss, data = covid_zmvm)\nmodelo_simple## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Coefficients:\n## (Intercept)           ss  \n##    -19.8622       0.3949\nlibrary(tidyverse)\n\ncovid_zmvm %>% \n  ggplot2::ggplot()+geom_dotplot(aes(pos_hab))\ncovid_zmvm %>% \n  ggplot2::ggplot()+geom_dotplot(aes(poss))\n#Generamos la tabla con la función data.frame(), aquí, los dobles corchetes, [[]], nos utilizados para extraer los elementos solicitados.\ntabla <- data.frame(covid_zmvm$cvemun, covid_zmvm$ss, covid_zmvm$pos_hab, as.data.frame(modelo_simple[[\"fitted.values\"]]),\n                    as.data.frame(modelo_simple[[\"residuals\"]]))\n\n#Creamos una lista que contiene los nombres deseados para cada variable\nnombres <- c(\"cvemun\",\"ss\", \"pos_hab\",\"ajustados\", \"errores\") \n\n#Asignamos los nombres deseados a la tabla creada\ncolnames(tabla)<-nombres\ntabla %>% \n  ggplot()+geom_point(aes(x=pos_hab,y=errores))\ntabla %>%\n  ggplot()+geom_histogram(aes(errores))## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\ntabla %>% \n  ggplot()+geom_qq(aes(sample=errores))+\n  geom_qq_line(aes(sample=errores))\ntabla %>%\n  ggplot()+geom_point(aes(ajustados,errores))\nstats::predict(object = modelo_simple, newdata=data.frame(ss=60))##        1 \n## 3.831836\nmodelo_simple## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Coefficients:\n## (Intercept)           ss  \n##    -19.8622       0.3949\nbase::summary(modelo_simple)## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.4391 -2.7744 -0.6709  1.5324 14.9969 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -19.86224    5.80867  -3.419  0.00102 ** \n## ss            0.39490    0.08516   4.637 1.49e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.572 on 74 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2147 \n## F-statistic:  21.5 on 1 and 74 DF,  p-value: 1.488e-05\nmodelo_multi <- stats::lm (formula=pos_hab ~ grad+occu+poss+pocom, data = covid_zmvm)"},{"path":"modelos-de-regresión-lineal.html","id":"una-exploración-visual-de-los-errores","chapter":"4 Modelos de regresión lineal","heading":"4.5.1 Una exploración visual de los errores","text":"Un elemento de interés para el análisis en la regresión lineal estimada son los errores observados, \\(u_i\\). Para lograr comprender mejor este elemento conviene imaginar nuestro modelo como “Respuesta = Predicción + Error”, es decir, \\(y=\\hat y+u\\). De esta manera podemos expresar el error como:\\[u_i=y_i-\\hat {y_i}\\]\nLos errores observados, \\(u_i\\), y los valores estimados, \\(\\hat y_i\\), producto de nuestro modelo estimado para cada uno de los 76 municipios y alcaldías del Valle de México son almacenados en el elemento residuals y fitted.values, respectivamente, dentro de la lista lm, da clic en la lente de aumento al final de la fila que contiene el objeto de tipo lm en la sección de ambiente en tu entorno de trabajo. Para poder analizar mejor los errores del modelo, vamos crear un nuevo objeto que contenga, además de nuestras variables de interés (pos_hab y ss), la clave municipal (cvemun), los errores observados y los valores ajustados o predichos:Ejercicio¿Cómo esperarías que luzca un diagrama de dispersión entre los valores observados y ajustados de la variable de respuesta? Elabora dicho diagrama.¿Cómo esperarías que luzca un diagrama de dispersión entre los valores observados y ajustados de la variable de respuesta? Elabora dicho diagrama.Grafica los errores, primero respecto un índice del 1 al 76 y luego respecto los valores ajustados ¿puedes observar algún patrón en ellos?Grafica los errores, primero respecto un índice del 1 al 76 y luego respecto los valores ajustados ¿puedes observar algún patrón en ellos?El supuesto de linealidad de nuestro modelo puede ser verificado partir de una exploración visual de un diagrama de dispersión entre los errores observados y la predictora. Si nuestro modelo es verdaderamente un modelo lineal, en la gráfica debería mostrarse patrón alguno. Veamos:Claramente hay un patrón, por lo que probablemente un modelo lineal como el propuesto sería la mejor opción. ¿Recuerdas cómo luce un diagrama de dispersión entre las dos variables de nuestro modelo?EjercicioElabora un diagrama de dispersión entre pos_hab y ss y añade un ajuste lineal.Entonces, parece que el modelo lineal propuesto capta con precisión la relación entre este par de variables, por lo que un modelo lineal podría ser una mejor opción; sin embargo, esta alternativa de modelación es objeto de estas notas, pero en el capítulo 14 de la obra de (Gujarati y Porter 2011) puedes revisar con detalle una ruta para solventar esta situación.Otro de los supuestos mencionados es que los errores deberían tener una distribución normal, lo que se cumplirá cuando haya observaciones inusuales. Para verificar esto visualmente, es posible recurrir un histograma de los errores, o bien, una gráfica de normalidad:¿Notas los valores extremos en el histograma anterior? Una gráfica cuantil-cuantil o gráfica QQ, contrasta la distribución de una variable con una distribución teórica: si la distribución de la variable fuera idéntica la distribución teórica, los puntos se alinearían sobre la línea de 45°. En este caso, la distribución teórica probar es una distribución normal:EjercicioConsulta la ayuda de las funciones geom_qq() y stat_qq_line(), ¿qué otras distribuciones es posible verificar y con qué argumento se hace?Finalmente, el supuesto de varianza constante de los errores también puede explorarse visualmente través de una gráfica donde se debería lograr ver variabilidad constante de los errores alrededor de la recta de ajuste: este es el denominado supuesto de homoscedasticidad. Exploremos si los errores de nuestro modelo cumplen o con este supuesto través de una gráfica de dispersión:Patrones visuales de “tipo ventilador” o “embudo” deberían ser observados cuando los errores son homoscedásticos, lo que es el caso. Los ejercicios gráficos hechos aquí con los errores para verificar si se cumplen o los supuestos de un modelo de MCO son desde luego la única ruta para verificar los supuestos. El análisis de si una regresión cumple con los supuestos necesarios requiere de práctica. En esta página encontrarás una serie de ejemplos de cuando una regresión cumple o con los supuestos enlistados antes y como lucen las gráficas mencionadas en cada caso. Sin embargo, como recordarás, existen pruebas formales que te permitirán determinar si el modelo cumple o con los supuestos del caso.Ejercicio¿Cuál es el nombre de la prueba más popular para evaluar normalidad y cómo se interpreta?¿Cuál es el nombre de la prueba más popular para evaluar normalidad y cómo se interpreta?Menciona dos pruebas para evaluar homoscedasticidad y cómo son interpretadas.Menciona dos pruebas para evaluar homoscedasticidad y cómo son interpretadas.","code":"\n#Generamos la tabla con la función data.frame(), aquí, los dobles corchetes, [[]], nos utilizados para extraer los elementos solicitados.\ntabla <- data.frame(covid_zmvm$cvemun, covid_zmvm$ss, covid_zmvm$pos_hab, as.data.frame(modelo_simple[[\"fitted.values\"]]),\n                    as.data.frame(modelo_simple[[\"residuals\"]]))\n\n#Creamos una lista que contiene los nombres deseados para cada variable\nnombres <- c(\"cvemun\",\"ss\", \"pos_hab\",\"ajustados\", \"errores\") \n\n#Asignamos los nombres deseados a la tabla creada\ncolnames(tabla)<-nombres\ntabla %>% \n  ggplot()+geom_point(aes(x=pos_hab,y=errores))\ntabla %>%\n  ggplot()+geom_histogram(aes(errores))## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\ntabla %>% \n  ggplot()+geom_qq(aes(sample=errores))+\n  geom_qq_line(aes(sample=errores))\ntabla %>%\n  ggplot()+geom_point(aes(ajustados,errores))"},{"path":"modelos-de-regresión-lineal.html","id":"el-error-estandar-residual-o-error-estándar-de-los-errores","chapter":"4 Modelos de regresión lineal","heading":"4.5.2 El error estandar residual o error estándar de los errores","text":"Además, los errores del modelo nos sirven para calcular la varianza, \\(\\sigma^2\\) del modelo estadístico propuesto. Para ello, recuerda que:\\[\n\\begin{aligned}\ns_e ^2 &=\\frac{1}{n-2}\\sum_{=1}^n(y_i-\\hat y _i)^2 \\\\\n  s_e ^2 &=\\frac{1}{n-2}\\sum_{=1}^n(e_i)^2 \\\\\n  \\end{aligned}\n\\]Este dato por sí mismo ofrece mucha información de la variabilidad de nuestro modelo, pero, si obtenemos su raíz cuadrada. La cifra estará ya en las mismas unidades que las usadas en el modelo y tendremos la desviación estándar, también conocida como error estándar residual.Ejercicio¿cuánto asciende el error estándar residual, residual standard error, de nuestro modelo?¿cuánto asciende el error estándar residual, residual standard error, de nuestro modelo?¿Cómo interpretarías dicha cifra?¿Cómo interpretarías dicha cifra?¿Un modelo con error estándar residual pequeño es preferible uno que tenga un error estándar residual alto?¿Un modelo con error estándar residual pequeño es preferible uno que tenga un error estándar residual alto?","code":""},{"path":"modelos-de-regresión-lineal.html","id":"coeficiente-de-determinación","chapter":"4 Modelos de regresión lineal","heading":"4.5.3 Coeficiente de determinación","text":"El \\(R^2\\) o coeficiente de determinación es una medida de bondad de ajuste, es decir, un número de nos indica qué tanto nuestro modelo explica la variable de interés. El \\(R^2\\) es la proporción de la variabilidad de \\(y\\) que es explicada por \\(x\\). Los valores más cercanos 1 significan que una mayor parte de la variabilidad de \\(y\\) es explicada por \\(x\\). En nuestro caso, hemos obtenido un \\(R^2\\) de 0.22, lo que puede ser interpretado de la siguiente manera: poco más de la quinta parte de la variabilidad de los casos positivos por cada 1 mil habitantes es explicada por el porcentaje de población con acceso servicios de salud.","code":""},{"path":"modelos-de-regresión-lineal.html","id":"una-aproximación-a-la-predicción","chapter":"4 Modelos de regresión lineal","heading":"4.5.4 Una aproximación a la predicción","text":"través de nuestro modelo estimado (también llamado ajustado) es posible hacer predicciones de \\(y\\), es decir, de los casos positivos por cada 1 mil habitantes, siempre que los valores de la variable predictora estén dentro del rango original.Ejercicio¿Cuál es el rango de la variable predictora, es decir de \\(x\\)?Hagamos una predicción de los casos positivos cuando \\(x\\) toma el valor de 60%:\\[\\hat {poshab}=-19.8622+0.3949(60)\\]Con independencia de la consistencia y validez de nuestro modelo, situación que de momento estamos evaluando, ¿cuál es el valor promedio de los casos positivos de COVID19 por cada 100 mil habitantes cuando el porcentaje de población con acceso servicios de salud es de 60%? Bastará resolver la expresión anterior para saber que el valor estimado promedio de los casos activos por cada 1 mil habitantes cuando el porcentaje de población con acceso servicios de salud es de 60%, es decir, 3.83 casos por cada 1 mil habitantes, aproximadamente.Alternativamente, en R contamos con la función predict() para llevar cabo, justamente, predicciones. La función requiere de dos argumentos forzosos: ) el modelo usado para la predicción (argumento object=) y ii) los valores de la regresión (argumento newdata=). Así:En este caso, estamos indicando los nuevos datos de forma manual para la variable de interés, ss.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2022-01-19.book built bookdown R package.","code":"\nstats::predict(object = modelo_simple, newdata=data.frame(ss=60))##        1 \n## 3.831836"},{"path":"modelos-de-regresión-lineal.html","id":"significancia-individual-de-los-coeficientes-obtenidos","chapter":"4 Modelos de regresión lineal","heading":"4.5.5 Significancia individual de los coeficientes obtenidos","text":"Uno de los aspectos más importantes del análisis de regresión es la interpretación de los coeficientes obtenidos. Será de particular interés para nosotros tener la información para cada coeficiente: ) conocer el sentido de su asociación con la variable dependiente, es decir, el signo del coeficiente; ii) identificar la magnitud de la asociación lineal, qué tan grande es dicho coeficiente en el contexto de las unidades de la variable y iii) su significancia estadística, si hay evidencia de que el verdadero valor del estimador es correspondiente con el estimado.Los dos primeros elementos, sentido y magnitud, ya fueron comentados con anterioridad y pueden ser conocidos llamando al objeto que contiene los resultados de nuestro modelo:R despliega en su consola el valor de los dos coeficientes estimados, en este caso \\(\\hat \\beta_0\\) es negativo y \\(\\hat \\beta_1\\) es positivo. El punto iii consiste en verificar si, en términos estadísticos, nuestros coeficientes estimados son iguales o determinado valor, concretamente, buscamos llevar cabo una prueba de hipótesis sobre dichos coeficientes.Sinteticemos el procedimiento de prueba de hipótesis desde una perspectiva netamente práctica y simplificada:Definir el juego de hipótesis verificar, la hipótesis nula (Ho) y la hipótesis alternativa (Ha).Definir un nivel de significancia, conocido como \\(\\alpha\\), valor con base en el cual tomarás una decisión en torno las hipótesis propuestas. Además, recuerda que (1-\\(\\alpha\\)) define el nivel de confianza con el que la decisión sobre el juego de hipótesis verificar es tomada.Comparar el valor p asociado cada coeficiente estimado con el nivel de \\(\\alpha\\) elegido y decidir con arreglo los siguientes criterios:\n. Si el valor-p < \\(\\alpha\\): se rechaza Ho en favor de Ha.\nb. Si el valor-p > \\(\\alpha\\): se rechaza Ho.El juego de hipótesis verificar sobre cada uno de los coeficientes estimados es:\\[H_o:\\hat\\beta_i=0 \\\\\nH_a:\\hat\\beta_i\\=0 \\]Ahora bien, \\(\\alpha\\) usualmente toma valores de 0.1, 0.05 y 0.01, lo que equivale niveles de confianza de 0.9, 0.95 y 0.99, es decir, de 90%, 95% y 99%, respectivamente. ¿Pero de dónde obtendremos el valor-p para tomar la decisión? La salida de la regresión viene acompañada de él y con base en él llevaremos cabo la comparación. Para observar los valores-p de cada uno de los coeficientes estimados solicitamos un resumen del objeto modelo_mco con la función summary():Observa con cuidado los resultados que ahora se presentan como una tabla en su consola. Identifica las columnas estimate y Pr(>|t|): en esta última aparece el valor-p y es el elemento con base en el cual tomaremos la decisión sobre la significancia estadística de los coeficientes obtenidos.Veamos los valores de los coeficientes estimados: el intercepto, \\(\\hat \\beta_0\\) es igual -19.8 y el coeficiente vinculado ss, \\(\\hat \\beta_1\\), tiene un valor de 0.394. Ahora bien, recuerda el juego de hipótesis verificar: \\(H_o:\\hat\\beta_i=0\\) y \\(H_a:\\hat\\beta_i\\=0\\) (paso 1), elijamos un nivel de significancia, \\(\\alpha\\), de 0.05 (paso 2) y comparemos el valor p asociado cada coeficiente con \\(\\alpha\\) (paso 3). El siguiente cuadro sintentiza lo anterior:Cuadro 4.1Así, decimos que para el caso del coeficiente estimado de ss hay evidencia suficiente para afirmar que su verdadero valor es diferente de cero, por lo que decimos que es estadísticamente significativo, o lo que es lo mismo, hay evidencia suficiente con un nivel de confianza de 95% para sostener que entre el número de casos positivos por cada 1 mil habitantes y el porcentaje de población con acceso afiliación servicios de salud presentan una asociación lineal.","code":"\nmodelo_simple## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Coefficients:\n## (Intercept)           ss  \n##    -19.8622       0.3949\nbase::summary(modelo_simple)## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.4391 -2.7744 -0.6709  1.5324 14.9969 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -19.86224    5.80867  -3.419  0.00102 ** \n## ss            0.39490    0.08516   4.637 1.49e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.572 on 74 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2147 \n## F-statistic:  21.5 on 1 and 74 DF,  p-value: 1.488e-05"},{"path":"modelos-de-regresión-lineal.html","id":"algunos-elementos-de-cuidado-con-las-regresiones-lineales","chapter":"4 Modelos de regresión lineal","heading":"4.6 Algunos elementos de cuidado con las regresiones lineales","text":"Antes de concluir esta sección, es importante tener en mente los siguientes puntos cuando usemos regresiones:Los modelos de regresión son útiles para llevar cabo intrapolación, es decir, para obtener valores en el rango de nuestras variables explicativas. obstante, deben ser usados con cuidado cuando lo que se pretende es extrapolar datos, es decir, conocer los valores de \\(y\\) que están en el rango original de \\(X\\).Como el ajuste de mínimos cuadrados depende en buena medida de los valores de \\(x\\) y con este método cada punto \\(x\\) tiene la misma ponderación, la pendiente de la regresión está mayormente influenciada por los valores más extremos de \\(x\\). Cuando se tienen valores inusuales se podría proceder eliminarlos, o bien, recurrir técnicas diferentes los mínimos cuadrados ordinarios que sean menos sensibles estos puntos.Que la regresión tuviera como resultado que dos variables están asociadas, significa que entre ellas haya una relación causal. Causalidad implica asociación, pero lo opuesto es necesariamente cierto.","code":""},{"path":"modelos-de-regresión-lineal.html","id":"regresión-lineal-múltiple-una-propuesta-de-modelo","chapter":"4 Modelos de regresión lineal","heading":"4.7 Regresión lineal múltiple: una propuesta de modelo","text":"La regresión lineal múltiple es, en esencia, una extensión lógica de todos los elementos descritos antes. En palabras de (C., ., y Geoffrey 2012, 81:67), “Un modelo de regresión que involucra más de una variable regresora se llama modelo de regresión múltiple”. Así, por ejemplo, un modelo con dos regresoras, \\(x_1\\) y \\(x_2\\) luciría como:\\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\epsilon\\]Esta expresión que incluye tres variables aún es posible representarse en espacio tridimensional con los ejes \\(y\\), \\(x_1\\) y \\(x_2\\) y correspondería múltiples planos en el espacio. El parámetro \\(\\beta_0\\) es el intercepto del plano de la regresión que, cuando el origen está incluido en el rango de \\(x_1\\) y \\(x_2\\), \\(\\beta_0\\) indica la media de \\(y\\) cuando \\(x_1=x_2=0\\). En tanto, \\(\\beta_1\\) es el cambio esperado \\(y\\) cuando \\(x_1\\) varia en una unidad, siempre que \\(x_2\\) permanezca constante, análogamente para \\(\\beta_2\\). ¿Qué variables de nuestra base incluirías en un modelo que incluya dos regresoras?Ejercicio¿Recuerdas cómo construir una matriz de diagramas de dispersión través del paquete GGally? Explora el conjunto de variables de la base covid_zmvm y construye una matriz de diagramas de dispersión sólo con 4 o 5 variables que presenten el mayor nivel de correlación tanto con los casos positivos como con las defunciones por cada 1 mil habitantes, pos_hab y def_hab.Un modelo con buen nivel de ajuste debería procurar que las variables explicativas propuestas presenten una alta correlación con la variable explicar. Como podrás haberte dado cuenta, las variables que presentan el mayor nivel de asociación lineal negativa son: ppob_basi, pocom, occu, sbasc, ppob_5_o_m. En tanto, las variables que muestran el mayor nivel de asociación lineal positiva son: poss, grad_m, ppob_sup, grad, grad_h, tmss y rmss. obstante, otro principio recomendable la hora de proponer un modelo, amén de su consistencia teórica, es su simpleza: ¿qué relevancia tendrá incluir el grado promedio de escolaridad total, además del de hombres y mujeres por separado? Bastaría incluir sólo una de ellas. Por otro lado, ¿qué tan oportuno sería incluir simultáneamente variables que buscan recoger el mismo aspecto de la realidad? Tanto las variables grad, ppob_basi y sbasc refieren el nivel de educación formal de la población, así, bastaría usar alternativamente alguna de ellas, algo parecido pasa con occu y ppob_5_o_m que expresan las condiciones de vivienda y habitación de las personas. En tanto, parece que de los grandes sectores: industria, comercio y servicios, las variables de éste último son las que muestran un mayor nivel de asociación lineal: poss, tmss y rmss, ¿será relevante incluir todas en el modelo? .Con base en lo dicho antes, propongamos un primer modelo lo más sencillo posible:\\[poshab_i=\\beta_0+\\beta_1grad_i+\\beta_2occu_i+\\beta_3poss_i+\\beta_4pocom_i+\\epsilon_i\\]\nDe nuevo, través de la función lm() estimaremos este modelo y al resultado lo guardaremos en un nuevo objeto llamado modelo_multiEjercicio¿Qué pasa con las variables propuestas en este modelo? ¿Cuáles resultaron estadísticamente significativas y por qué?¿Qué pasa con las variables propuestas en este modelo? ¿Cuáles resultaron estadísticamente significativas y por qué?Aquí hemos expuesto una versión simplificada en extremo de cómo tomar decisiones través de las pruebas de hipótesis, puedes remitirte cualquier libro de econometría y responder, ¿de qué modo la prueba t (t value, penúltima columna del cuadro de resumen) nos permitiría llegar las mismas conclusiones?Aquí hemos expuesto una versión simplificada en extremo de cómo tomar decisiones través de las pruebas de hipótesis, puedes remitirte cualquier libro de econometría y responder, ¿de qué modo la prueba t (t value, penúltima columna del cuadro de resumen) nos permitiría llegar las mismas conclusiones?Seguramente notaste que cuando se llamó el objeto modelo_multi tu consola aparecieron otros tantos elementos ,además de los coeficientes, entre ellos uno llamado F-statistic , el estadístico F. En este contexto es utilizado para evaluar si la particular combinación de variables propuesta contribuye o explicar la variable de interés. En términos concretos, la prueba F y el valor p asociado ella, permiten evaluar el siguiente juego de hipótesis:\\[\n\\begin{aligned}\nHo&: \\hat\\beta_1=\\hat\\beta_2=...=\\hat\\beta_i=0 \\\\\nHa&: al.menos.una. \\hat\\beta_i\\=0 \\\\\n\\end{aligned}\n\\]es decir, al menos uno de los coeficientes estimados, en su particular combinación, es diferente de cero. Así, con un valor p de casi cero (2.112e-09), resulta claro que se rechaza Ho en favor de Ha, es decir, que al menos uno de los coeficientes estimados es diferente de 0 en el modelo propuesto y que esta particular combinación de variables explicativas tiene algo interesante para decirnos.Dejemos hasta aquí este somero repaso de los aspectos más elementales del análisis de regresión. Es primordial que por tu propia cuenta estudies los materiales sugeridos, pues este capítulo en modo alguno suple un curso formal de los tópicos básicos de econometría. En el siguiente capítulo, retomamos el hilo del tratamiento de la información espacial través de su incorporación los modelos econométricos.","code":"\nmodelo_multi <- stats::lm (formula=pos_hab ~ grad+occu+poss+pocom, data = covid_zmvm)"},{"path":"modelos-de-regresión-lineal.html","id":"significancia-conjunta-de-los-coeficientes-calculados","chapter":"4 Modelos de regresión lineal","heading":"4.7.1 Significancia conjunta de los coeficientes calculados","text":"Seguramente notaste que cuando se llamó el objeto modelo_multi tu consola aparecieron otros tantos elementos ,además de los coeficientes, entre ellos uno llamado F-statistic , el estadístico F. En este contexto es utilizado para evaluar si la particular combinación de variables propuesta contribuye o explicar la variable de interés. En términos concretos, la prueba F y el valor p asociado ella, permiten evaluar el siguiente juego de hipótesis:\\[\n\\begin{aligned}\nHo&: \\hat\\beta_1=\\hat\\beta_2=...=\\hat\\beta_i=0 \\\\\nHa&: al.menos.una. \\hat\\beta_i\\=0 \\\\\n\\end{aligned}\n\\]es decir, al menos uno de los coeficientes estimados, en su particular combinación, es diferente de cero. Así, con un valor p de casi cero (2.112e-09), resulta claro que se rechaza Ho en favor de Ha, es decir, que al menos uno de los coeficientes estimados es diferente de 0 en el modelo propuesto y que esta particular combinación de variables explicativas tiene algo interesante para decirnos.Dejemos hasta aquí este somero repaso de los aspectos más elementales del análisis de regresión. Es primordial que por tu propia cuenta estudies los materiales sugeridos, pues este capítulo en modo alguno suple un curso formal de los tópicos básicos de econometría. En el siguiente capítulo, retomamos el hilo del tratamiento de la información espacial través de su incorporación los modelos econométricos.","code":""},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"análisis-espacial-ii-modelos-econométricos-espaciales","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5 Análisis espacial II: modelos econométricos espaciales","text":"En el capítulo 3 nos hemos referido la necesidad de identificar la presencia de autocorrelación espacial en el conjunto de información utilizado. Mencionamos que dos son las razones por las que conviene saber si la información usada presenta este rasgo: una técnica y otra sustantiva. En términos de la segunda, la presencia de autocorrelación espacial significa que el fenómeno de interés de se distribuye de manera aleatoria en el espacio por lo que algo está pasando que debe ser investigado. En términos de la razón de carácter técnico, que está desvinculada necesariamente la razón previa, estimar un modelo clásico de regresión lineal través del método usual de Mínimos Cuadrados Ordinarios implicaría violar uno de sus supuestos, específicamente, aquel que tiene que ver con la independencia de los términos de error. Por tanto, por razones de la técnica utilizada y del interés de investigador, es necesario recurrir una alternativa de modelación que permita tanto resolver el problema de la violación del citado supuesto como poner disposición del interesado información relevante para buscar las causas que explican la distribución aleatoria del fenómeno en el espacio.En este capítulo nos dedicaremos mostrar dos de las alternativas para lograr lo anterior través de los llamados modelos econométricos espaciales. Es crucial que, antes de abordar este capítulo, hayan quedado plenamente asimilados los contenidos del capítulo 3 y 4, en la medida en que lo expuesto aquí se basa en los conocimientos previos.Existen múltiples modelos econométricos que buscan resolver las dos cuestiones mencionadas través de captar las interacciones que se dan en el espacio, más aún, dichas interacciones las hay de diferente naturaleza. El modelo que incorpora todas las posibles interacciones espaciales recibe el nombre de Modelo General Anidado (General Nesting Spatial Model (Elhorst 2006, 7) y puede ser escrito como:\\[\n\\begin{aligned}\nY &= \\rho WY +\\alpha1_N+X\\beta+WX\\theta+u  \\\\\nu &=\\lambda Wu+\\epsilon  \\\\\n\\end{aligned}\n\\]Las interacciones espaciales en la expresión anterior son de tres tipos: ) efectos de interacción endógenos, es decir, cuando la variable dependiente reaparece como independiente pero través de su rezago espacial (\\(WY\\)), aquí el sentido y magnitud de la interacción está dado por \\(\\rho\\); ii) efectos de interacción exógenos, es decir, cuando las variables independientes son incluidas en forma de sus rezagos espaciales (\\(WX\\)) en la que la relación es captada por \\(\\theta\\); iii) efectos de interacción dados por los términos de error de la regresión (\\(Wu\\)) e interesa el sentido y magnitud de \\(\\lambda\\).partir de este modelo general es posible derivar otros, más simples, si se suponen determinados valores para los coeficientes. Toda la constelación de modelos espaciales posibles aparece en la siguiente figura (Elhorst 2006, 7):\nFigura 5.1: Clasificación de modelos espaciales, Elhorst (2006)\nDe toda esta gama de modelos aquí sólo nos ocuparemos de dos de ellos, el modelo de rezago espacial y el modelo de error espacial, siguiendo la ruta metodológica propuesta por Anselin y Rey (Anselin y Rey 2014) que consiste en comenzar con un modelo lineal espacial estimado con mínimos cuadrados ordinarios (MCO) que luego es extendido con la incorporación de interacciones espaciales, ya sea un modelo de rezago espacial o uno de error, con base en pruebas que verifican alternativamente la pertinencia de cada uno:\nFigura 5.2: De un modelo con MCO un modelo espacial\nDespués de haber llevado cabo el análisis exploratorio de datos espaciales (ESDA) y que se ha decidido modelar través de econometría la relación de interésla secuencia general para la estimación de un modelo econométrico espacial consiste en:Estimar un modelo clásico de regresión lineal con mínimos cuadrados ordinarios.Evaluar la presencia de autocorrelación espacial en los errores del modelo.Evaluar, alternativamente, la pertinencia de un modelo de rezago o de error espacial.Estimar el modelo de rezago y de error espacial y seleccionar el mejor modelo.El punto ) ha sido tratado en el capítulo 4 la regresión lineal. Si bien podrías pensar que la evaluación de la autocorrelación ha quedado plenamente cubierta en el capítulo 3 correspondiente al análisis exploratorio de datos espaciales, es necesario distinguir entre autocorrelación en la variable de interés y autocorrelación en los errores del modelo; esto último se mecionó dentro de los supuestos del modelo clásico de regresión lineal y es momento de evaluar formalmente dicho supuesto través de la de Moran.La evaluación sobre las alternativas de los modelos de error y de rezago corren cargo de los estadísticos de prueba de los multiplicadores de Lagrange, también referidos como Prueba de Puntaje de Rao (Rao score test). Las pruebas nos permitirán decidir qué modelo es mejor, evaluándolos de siguiente modo:Modelo MCO vs. Modelo de rezagoModelo MCO vs. Modelo de errorEvaluar entre el modelo de MCO y el modelo de rezago implica recordar la estructura del modelo de rezago:\n\\[\ny=\\rho Wy+X\\beta+u \n\\]\nasí, formalmente el juego de hipótesis evaluar sobre la expresión anterior es:\\[\n\\begin {aligned}\nHo &: \\rho=0 \\\\\nHa &: \\rho\\=0 \\\\\n\\end {aligned}\n\\]El estadístico usado es el multiplicador de Lagrange, que sigue una distribución \\(\\chi^2\\) con un grado de libertad5. Así, dado determinado nivel de significancia, rechazar la hipótesis nula significará que el mejor modelo es un modelo de rezago espacial. De forma semejante, para evaluar la alternativa del modelo de error recuerda que:\\[\n\\begin{aligned}\nY &= \\alpha1_N+X\\beta+u  \\\\\nu &=\\lambda Wu+\\epsilon  \\\\\n\\end{aligned}\n\\]de modo que las hipótesis verificar son:\\[\n\\begin {aligned}\nHo &: \\lambda=0 \\\\\nHa &: \\lambda\\=0 \\\\\n\\end {aligned}\n\\]\nIgualmente, el estadístico usado para evaluar nuestras hipótesis es el multiplicador de Lagrange que sigue una distribución \\(\\chi^2\\) con un grado de libertad, por lo que fijado determinado nivel de significancia, rechazar la hipótesis nula implica que el mejor modelo es un modelo de error espacial.Pero, ¿qué ocurre cuando tanto la prueba sobre la versión de rezago y de error han resultado significativas? En este caso, se recurre las versiones robustas de los multiplicadores de Lagrange. Las versiones robustas de dichos indicadores implican “que el estadístico original es corregido mediante la influencia potencial de ‘otras’ alternativas” (Anseluin y Rey, 2014: 105). En un sentido estrictamente práctico, de nueva cuenta, se evalúa alternativamente el modelo MCO vs. el de rezago, y el MCO vs. el de error, pero esta vez, con las versiones robustas con exactamente los mismos juegos de hipótesis.Lo dicho anteriormente dicho es sintetizado por nuestro autores en el siguiente esquema:\nFigura 5.3: Árbol de decisión de regresión espacial, Anselin y Rey (2014)\nPara simplificar el proceso de selección entre los modelos alternativos, basta recordar lo recomendado por (Anselin 2005, 197):Lo importante recordar es considerar las versiones robustas de loss estadísticos (los multiplicadores de Lagrange) sólo cuando las versiones estándar son significativasLos paquetes utilizados para la estimación de modelos econométricos espaciales son:spatialreg: contiene las funciones para calcular modelos espaciales.spdep: funciones para evaluar dependencia espacial y construir matrices de pesos espaciales.Como es usual, la instalación desde la consola puede hacerse con:Además, echaremos mano de otros tantos paquetes previamente trabajadas, por lo que deberán ser llamadas junto con los recién instalados:La base de datos usar en este ejercicio es la misma que hemos usando antes y que puedes descargar aquí. Cargue su base de datos espacial con:Como recordarás del capítulo relativo al análisis exploratorio de datos espaciales, la definición de una estructura espacial atraviesa por la construcción de una matriz de pesos espaciales, recuerda que través de la función poly2nb() del paquete spdep construimos nuestra lista de vecinos con base en los criterios de contigüidad de tipo reina (queen=TRUE):Ahora, el objeto de tipo nb debe ser trasformado uno que contenga los pesos espaciales, es decir, la matriz ponderada: un objeto de tipo listw:Visualicemos la estructura de dependencia espacial:Sólo con el afán de recordar los elementos previamente revisados en el análisis exploratorio de datos, construyamos el estadístico de correlación espacial global sobre la variable crimen, pos_hab:Y el respectivo diagrama de Moran:EjercicioInterpreta tanto la de Moran como su diagrama, ¿hay evidencia de autocorrelación espacial? Si es así, ¿de qué magnitud y sentido? ¿Cómo interpretarías el estadístico obtenido?De acuerdo lo dicho líneas más arriba, primero debemos estimar un modelo clásico de regresión lineal con mínimos cuadrados ordinarios. Usaremos el mismo modelo del capítulo sobre regresión lineal en su versión simple: un modelo que busca explicar los casos positivos por COVID-19 por cada 1 mil habitantes (pos_hab) mediante la población con acceso servicios de salud (ss):Ejercicio¿Qué puedes decir sobre la significancia individual del coeficiente estimado? ¿Qué sobre la bondad de ajuste del modelo?Una vez que hemos estimado el modelo lineal través del MCO, debemos evaluar la presencia de autocorrelación espacial en los errores del modelo. De nueva cuenta insistimos en la diferencia entre evaluar la autocorrelación sobre la variable de interés (lo que hicimos hace un momento con la de Moran y su diagrama sobre la variable pos_hab) y evaluar la autocorrelación sobre los errores del modelo, lo que haremos en este momento.Verificamos la presencia de autocorrelación espacial de los errores través de la de Moran, lo que nos permitirá responder la pregunta ¿los errores de nuestro modelo están correlacionados? Para ello, nos servimos de la función moran.test() del paquete spdep. Como recordarás, la función requiere dos argumentos: el vector número evaluar (en este caso los errores del modelo MCO) y la estructura espacial:EjercicioCon base en los resultados de la de Moran, ¿dirías que existe autocorrelación espacial en los errores?Lo usual es que cuando la variable de interés presenta autocorrelación, esta característica termine por afectar los resultados de la estimación por mínimos cuadrados ordinarios. Así pues, es necesario, siguiendo la ruta marcada por Anselin y Rey (2014), evaluar, alternativamente, la pertinencia de un modelo de rezago o de error espacial. Como dijimos antes, esto lo haremos con base en las pruebas sobre las dos alternativas propuestas (modelo de rezago o modelo de error), través de los mutiplicadores de Lagrange lo que nos permitirá tener elementos informativos para decantarnos por alguna de estas opciones.Las pruebas son llamadas con la función lm.LMtests() del paquete spdep que requiere de los siguientes argumentos: el modelo lineal evaluar (model=), la estructura espacial (listw=) y los pruebas solicitadas (test=):Revisa la ayuda relacionada con la función lm.LMtests() para familiarizarte con todos sus argumentos. En este caso, solicitamos las 4 pruebas de las que hicimos mención previamente: la prueba del multiplicador de Lagrange sobre la alternativa del modelo de rezago (LMlag), la prueba del multiplicador de Lagrange sobre la alternativa del modelo de error (LMerr), así como sus respectivas versiones robustas (RLMlag,RLMerr).Interpretemos los resultados. Primero, contrastemos las versiones sencillas de los multiplicadores:Tanto el estadístico vinculado al modelo de rezago, como el del error son estadísticamente significativos, es decir, en cada caso se rechaza la hipótesis nula de que \\(\\rho=0\\) y \\(\\lambda=0\\), respectivamente, por lo que tanto un modelo de rezago como de error son plausibles. En este caso, hay que mirar los resultados de las versiones robustas para tomar la decisión final:Sólo el estadistico vinculado al modelo de rezago, RLMlag, resulta significativo, es decir, se rechaza la hipótesis nula de que \\(\\rho=0\\); en tanto, para el caso del estadístico vinculado al modelo de error, RLMerr, podemos rechazar la hipótesis nula de que \\(\\lambda=0\\), así, la mejor alternativa espacial al modelo de MCO es el modelo de rezago. Revisa de nueva cuenta la figura 5.3 para mejor entender nuestra decisión.Finalmente, vamos estimar ambos modelos, aunque sabiendas de que el mejor es el modelo de error, lo usual es presentar los resultados de ambos.El modelo de rezago espacial se invoca con al función lagsarlm() del paquete spatialreg en la que la estimación corre cargo del método de máxima verosimilitud. En la función se especifica el modelo (formula=), la base de datos (data=) y la estructura espacial (listw=).De la salida debemos llamar la atención sobre los coeficientes de interés: \\(\\rho\\), el coeficiente de autocorrelación espacial que en este caso es positivo, significativo y alto, 0.83, esto significa que cuando aumenta en 1 el número de casos positivos en los municipios y alcaldías vecinas, el número de casos positivos por COVID19 por cada 1 mil habitantes en el municipio de interés aumentará en 0.83.De forma semejante, los errores pueden ser visualizados, guardados y evaluados para ver si presentan autocorrelación:EjercicioCon base en los resultados de la de Moran, ¿diría que persite el problema de autocorrelación espacial en los errores en el modelo de rezago?El modelo de error espacial es llamado con la función errorsarlm() y los argumentos son los mismos:Aquí, el parámetro de interés estimado es \\(\\lambda\\). Verifique si se ha resuelto el problema de la autocorrelación espacial en los errores:Con estos resultados hemos completado la ruta más elemental para la estimación de modelos econométricos espaciales. Sin embargo, hay sólo otras tantas alternativas de modelación, tal y como apuntamos en la figura 5.1. Además, hay una serie de tópicos que quedan fuera de estas notas, tales como la modelación de heterogeneidad espacial (violación del supuesto de homoscedasticidad), así como la incorporación de la dimensión temporal nuetro análisis, través de los modelos de panel espacial. Cada uno de estos temas implica un estudio sistemático por sí sólo. Esperamos que estas notas contribuyan alimentar tu curiosidad sobre el tratamiento y análisis de información espacial y que sean un primer paso en tu formación como profesionista especialista en economía urbana y regional.\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2022-01-19.book built bookdown R package.","code":"\n#Instalar dos paquetes\ninstall.packages(c(\"spdep\", \"spatialreg\" ))\n#Paquetes recien instalados\nlibrary(spdep)\nlibrary(spatialreg)## Warning: package 'spatialreg' was built under R version 4.1.2\n#Otros paquetes usados\nlibrary(rgdal)\nlibrary(sp)\ncovid_zmvm <-rgdal::readOGR(\"base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp\")## Warning in OGRSpatialRef(dsn, layer, morphFromESRI = morphFromESRI, dumpSRS =\n## dumpSRS, : Discarded datum D_unknown in Proj4 definition: +proj=lcc +lat_0=12\n## +lon_0=-102 +lat_1=17.5 +lat_2=29.5 +x_0=2500000 +y_0=0 +ellps=GRS80 +units=m\n## +no_defs## OGR data source with driver: ESRI Shapefile \n## Source: \"C:\\Users\\Jarvis\\Desktop\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp\", layer: \"covid_zmvm\"\n## with 76 features\n## It has 55 fields\nmTRUE <- spdep::poly2nb(covid_zmvm)\nmTRUE.pesos<-spdep::nb2listw(mTRUE)\nplot(covid_zmvm)\nplot(mTRUE, coordinates(covid_zmvm), add=TRUE, col=\"blue\")\nspdep::moran.test(covid_zmvm$pos_hab, mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  covid_zmvm$pos_hab  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = 8.8625, p-value < 2.2e-16\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##       0.656334129      -0.013333333       0.005709563\nspdep::moran.plot((covid_zmvm$pos_hab-mean(covid_zmvm$pos_hab))/sd(covid_zmvm$pos_hab), listw = mTRUE.pesos, xlab=\"Casos positivos por cada 100 mil habitab¿ntes\", ylab=\"Promedio de casos positivos en regiones vecinas\", main=\"Diagrama de Moran de los casos positivos en el Valle de México\", col=\"blue\")\nmodelo_mco <- stats::lm (formula=pos_hab ~ ss, data = covid_zmvm)\nsummary(modelo_mco)## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.4391 -2.7744 -0.6709  1.5324 14.9969 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -19.862      5.809  -3.419  0.00102 ** \n## ss            39.490      8.516   4.637 1.49e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.572 on 74 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2147 \n## F-statistic:  21.5 on 1 and 74 DF,  p-value: 1.488e-05\nspdep::moran.test(modelo_mco$residuals, mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  modelo_mco$residuals  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = 6.9825, p-value = 1.45e-12\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##       0.512928287      -0.013333333       0.005680411\nspdep::lm.LMtests(model=modelo_mco, listw=mTRUE.pesos, test=c(\"LMlag\",\"LMerr\", \"RLMlag\",\"RLMerr\"))## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## LMlag = 55.313, df = 1, p-value = 1.028e-13\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## LMerr = 42.733, df = 1, p-value = 6.275e-11\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## RLMlag = 13.358, df = 1, p-value = 0.0002573\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## RLMerr = 0.77786, df = 1, p-value = 0.3778\nlibrary(spatialreg)\nrezago <- spatialreg::lagsarlm(formula=pos_hab ~ ss, data=covid_zmvm, listw=mTRUE.pesos)\nsummary(rezago)## \n## Call:spatialreg::lagsarlm(formula = pos_hab ~ ss, data = covid_zmvm, \n##     listw = mTRUE.pesos)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -5.72501 -1.71081 -0.31629  1.12054 11.64378 \n## \n## Type: lag \n## Coefficients: (asymptotic standard errors) \n##             Estimate Std. Error z value Pr(>|z|)\n## (Intercept)  -8.8906     3.4221 -2.5980 0.009377\n## ss           14.9811     5.1378  2.9159 0.003547\n## \n## Rho: 0.83629, LR test value: 62.655, p-value: 2.4425e-15\n## Asymptotic standard error: 0.056938\n##     z-value: 14.688, p-value: < 2.22e-16\n## Wald statistic: 215.73, p-value: < 2.22e-16\n## \n## Log likelihood: -191.0071 for lag model\n## ML residual variance (sigma squared): 7.1381, (sigma: 2.6717)\n## Number of observations: 76 \n## Number of parameters estimated: 4 \n## AIC: 390.01, (AIC for lm: 450.67)\n## LM test for residual autocorrelation\n## test value: 0.10973, p-value: 0.74045\nspdep::moran.test(rezago[[\"residuals\"]], mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  rezago[[\"residuals\"]]  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = -0.030802, p-value = 0.5123\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##      -0.015615667      -0.013333333       0.005490469\nerror <-  spatialreg::errorsarlm(formula=pos_hab ~ ss, data=covid_zmvm, listw=mTRUE.pesos)\nsummary(error)## \n## Call:spatialreg::errorsarlm(formula = pos_hab ~ ss, data = covid_zmvm, \n##     listw = mTRUE.pesos)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -5.80220 -1.62230 -0.39269  1.11193 11.55180 \n## \n## Type: error \n## Coefficients: (asymptotic standard errors) \n##             Estimate Std. Error z value Pr(>|z|)\n## (Intercept) -0.66237    5.03921 -0.1314  0.89543\n## ss          12.54484    6.30980  1.9882  0.04679\n## \n## Lambda: 0.87101, LR test value: 58.512, p-value: 2.0206e-14\n## Asymptotic standard error: 0.050779\n##     z-value: 17.153, p-value: < 2.22e-16\n## Wald statistic: 294.22, p-value: < 2.22e-16\n## \n## Log likelihood: -193.0788 for error model\n## ML residual variance (sigma squared): 7.3069, (sigma: 2.7031)\n## Number of observations: 76 \n## Number of parameters estimated: 4 \n## AIC: 394.16, (AIC for lm: 450.67)\nspdep::moran.test(error[[\"residuals\"]], mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  error[[\"residuals\"]]  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = 0.18736, p-value = 0.4257\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##      0.0005636417     -0.0133333333      0.0055017348"},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"diversas-alternativas-de-modelos-econométricos","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.1 Diversas alternativas de modelos econométricos","text":"Existen múltiples modelos econométricos que buscan resolver las dos cuestiones mencionadas través de captar las interacciones que se dan en el espacio, más aún, dichas interacciones las hay de diferente naturaleza. El modelo que incorpora todas las posibles interacciones espaciales recibe el nombre de Modelo General Anidado (General Nesting Spatial Model (Elhorst 2006, 7) y puede ser escrito como:\\[\n\\begin{aligned}\nY &= \\rho WY +\\alpha1_N+X\\beta+WX\\theta+u  \\\\\nu &=\\lambda Wu+\\epsilon  \\\\\n\\end{aligned}\n\\]Las interacciones espaciales en la expresión anterior son de tres tipos: ) efectos de interacción endógenos, es decir, cuando la variable dependiente reaparece como independiente pero través de su rezago espacial (\\(WY\\)), aquí el sentido y magnitud de la interacción está dado por \\(\\rho\\); ii) efectos de interacción exógenos, es decir, cuando las variables independientes son incluidas en forma de sus rezagos espaciales (\\(WX\\)) en la que la relación es captada por \\(\\theta\\); iii) efectos de interacción dados por los términos de error de la regresión (\\(Wu\\)) e interesa el sentido y magnitud de \\(\\lambda\\).partir de este modelo general es posible derivar otros, más simples, si se suponen determinados valores para los coeficientes. Toda la constelación de modelos espaciales posibles aparece en la siguiente figura (Elhorst 2006, 7):\nFigura 5.1: Clasificación de modelos espaciales, Elhorst (2006)\nDe toda esta gama de modelos aquí sólo nos ocuparemos de dos de ellos, el modelo de rezago espacial y el modelo de error espacial, siguiendo la ruta metodológica propuesta por Anselin y Rey (Anselin y Rey 2014) que consiste en comenzar con un modelo lineal espacial estimado con mínimos cuadrados ordinarios (MCO) que luego es extendido con la incorporación de interacciones espaciales, ya sea un modelo de rezago espacial o uno de error, con base en pruebas que verifican alternativamente la pertinencia de cada uno:\nFigura 5.2: De un modelo con MCO un modelo espacial\nDespués de haber llevado cabo el análisis exploratorio de datos espaciales (ESDA) y que se ha decidido modelar través de econometría la relación de interésla secuencia general para la estimación de un modelo econométrico espacial consiste en:Estimar un modelo clásico de regresión lineal con mínimos cuadrados ordinarios.Evaluar la presencia de autocorrelación espacial en los errores del modelo.Evaluar, alternativamente, la pertinencia de un modelo de rezago o de error espacial.Estimar el modelo de rezago y de error espacial y seleccionar el mejor modelo.El punto ) ha sido tratado en el capítulo 4 la regresión lineal. Si bien podrías pensar que la evaluación de la autocorrelación ha quedado plenamente cubierta en el capítulo 3 correspondiente al análisis exploratorio de datos espaciales, es necesario distinguir entre autocorrelación en la variable de interés y autocorrelación en los errores del modelo; esto último se mecionó dentro de los supuestos del modelo clásico de regresión lineal y es momento de evaluar formalmente dicho supuesto través de la de Moran.La evaluación sobre las alternativas de los modelos de error y de rezago corren cargo de los estadísticos de prueba de los multiplicadores de Lagrange, también referidos como Prueba de Puntaje de Rao (Rao score test). Las pruebas nos permitirán decidir qué modelo es mejor, evaluándolos de siguiente modo:Modelo MCO vs. Modelo de rezagoModelo MCO vs. Modelo de errorEvaluar entre el modelo de MCO y el modelo de rezago implica recordar la estructura del modelo de rezago:\n\\[\ny=\\rho Wy+X\\beta+u \n\\]\nasí, formalmente el juego de hipótesis evaluar sobre la expresión anterior es:\\[\n\\begin {aligned}\nHo &: \\rho=0 \\\\\nHa &: \\rho\\=0 \\\\\n\\end {aligned}\n\\]El estadístico usado es el multiplicador de Lagrange, que sigue una distribución \\(\\chi^2\\) con un grado de libertad5. Así, dado determinado nivel de significancia, rechazar la hipótesis nula significará que el mejor modelo es un modelo de rezago espacial. De forma semejante, para evaluar la alternativa del modelo de error recuerda que:\\[\n\\begin{aligned}\nY &= \\alpha1_N+X\\beta+u  \\\\\nu &=\\lambda Wu+\\epsilon  \\\\\n\\end{aligned}\n\\]de modo que las hipótesis verificar son:\\[\n\\begin {aligned}\nHo &: \\lambda=0 \\\\\nHa &: \\lambda\\=0 \\\\\n\\end {aligned}\n\\]\nIgualmente, el estadístico usado para evaluar nuestras hipótesis es el multiplicador de Lagrange que sigue una distribución \\(\\chi^2\\) con un grado de libertad, por lo que fijado determinado nivel de significancia, rechazar la hipótesis nula implica que el mejor modelo es un modelo de error espacial.Pero, ¿qué ocurre cuando tanto la prueba sobre la versión de rezago y de error han resultado significativas? En este caso, se recurre las versiones robustas de los multiplicadores de Lagrange. Las versiones robustas de dichos indicadores implican “que el estadístico original es corregido mediante la influencia potencial de ‘otras’ alternativas” (Anseluin y Rey, 2014: 105). En un sentido estrictamente práctico, de nueva cuenta, se evalúa alternativamente el modelo MCO vs. el de rezago, y el MCO vs. el de error, pero esta vez, con las versiones robustas con exactamente los mismos juegos de hipótesis.Lo dicho anteriormente dicho es sintetizado por nuestro autores en el siguiente esquema:\nFigura 5.3: Árbol de decisión de regresión espacial, Anselin y Rey (2014)\nPara simplificar el proceso de selección entre los modelos alternativos, basta recordar lo recomendado por (Anselin 2005, 197):Lo importante recordar es considerar las versiones robustas de loss estadísticos (los multiplicadores de Lagrange) sólo cuando las versiones estándar son significativas\"Análisis de datos espaciales con R\" written Jaime Alberto Prudencio Vázquez. last built 2022-01-19.book built bookdown R package.","code":""},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"modelos-espaciales-en-r-un-ejemplo-para-el-valle-de-méxico","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.2 Modelos espaciales en R: un ejemplo para el Valle de México","text":"Los paquetes utilizados para la estimación de modelos econométricos espaciales son:spatialreg: contiene las funciones para calcular modelos espaciales.spdep: funciones para evaluar dependencia espacial y construir matrices de pesos espaciales.Como es usual, la instalación desde la consola puede hacerse con:Además, echaremos mano de otros tantos paquetes previamente trabajadas, por lo que deberán ser llamadas junto con los recién instalados:La base de datos usar en este ejercicio es la misma que hemos usando antes y que puedes descargar aquí. Cargue su base de datos espacial con:Como recordarás del capítulo relativo al análisis exploratorio de datos espaciales, la definición de una estructura espacial atraviesa por la construcción de una matriz de pesos espaciales, recuerda que través de la función poly2nb() del paquete spdep construimos nuestra lista de vecinos con base en los criterios de contigüidad de tipo reina (queen=TRUE):Ahora, el objeto de tipo nb debe ser trasformado uno que contenga los pesos espaciales, es decir, la matriz ponderada: un objeto de tipo listw:Visualicemos la estructura de dependencia espacial:Sólo con el afán de recordar los elementos previamente revisados en el análisis exploratorio de datos, construyamos el estadístico de correlación espacial global sobre la variable crimen, pos_hab:Y el respectivo diagrama de Moran:EjercicioInterpreta tanto la de Moran como su diagrama, ¿hay evidencia de autocorrelación espacial? Si es así, ¿de qué magnitud y sentido? ¿Cómo interpretarías el estadístico obtenido?De acuerdo lo dicho líneas más arriba, primero debemos estimar un modelo clásico de regresión lineal con mínimos cuadrados ordinarios. Usaremos el mismo modelo del capítulo sobre regresión lineal en su versión simple: un modelo que busca explicar los casos positivos por COVID-19 por cada 1 mil habitantes (pos_hab) mediante la población con acceso servicios de salud (ss):Ejercicio¿Qué puedes decir sobre la significancia individual del coeficiente estimado? ¿Qué sobre la bondad de ajuste del modelo?Una vez que hemos estimado el modelo lineal través del MCO, debemos evaluar la presencia de autocorrelación espacial en los errores del modelo. De nueva cuenta insistimos en la diferencia entre evaluar la autocorrelación sobre la variable de interés (lo que hicimos hace un momento con la de Moran y su diagrama sobre la variable pos_hab) y evaluar la autocorrelación sobre los errores del modelo, lo que haremos en este momento.Verificamos la presencia de autocorrelación espacial de los errores través de la de Moran, lo que nos permitirá responder la pregunta ¿los errores de nuestro modelo están correlacionados? Para ello, nos servimos de la función moran.test() del paquete spdep. Como recordarás, la función requiere dos argumentos: el vector número evaluar (en este caso los errores del modelo MCO) y la estructura espacial:EjercicioCon base en los resultados de la de Moran, ¿dirías que existe autocorrelación espacial en los errores?Lo usual es que cuando la variable de interés presenta autocorrelación, esta característica termine por afectar los resultados de la estimación por mínimos cuadrados ordinarios. Así pues, es necesario, siguiendo la ruta marcada por Anselin y Rey (2014), evaluar, alternativamente, la pertinencia de un modelo de rezago o de error espacial. Como dijimos antes, esto lo haremos con base en las pruebas sobre las dos alternativas propuestas (modelo de rezago o modelo de error), través de los mutiplicadores de Lagrange lo que nos permitirá tener elementos informativos para decantarnos por alguna de estas opciones.Las pruebas son llamadas con la función lm.LMtests() del paquete spdep que requiere de los siguientes argumentos: el modelo lineal evaluar (model=), la estructura espacial (listw=) y los pruebas solicitadas (test=):Revisa la ayuda relacionada con la función lm.LMtests() para familiarizarte con todos sus argumentos. En este caso, solicitamos las 4 pruebas de las que hicimos mención previamente: la prueba del multiplicador de Lagrange sobre la alternativa del modelo de rezago (LMlag), la prueba del multiplicador de Lagrange sobre la alternativa del modelo de error (LMerr), así como sus respectivas versiones robustas (RLMlag,RLMerr).Interpretemos los resultados. Primero, contrastemos las versiones sencillas de los multiplicadores:Tanto el estadístico vinculado al modelo de rezago, como el del error son estadísticamente significativos, es decir, en cada caso se rechaza la hipótesis nula de que \\(\\rho=0\\) y \\(\\lambda=0\\), respectivamente, por lo que tanto un modelo de rezago como de error son plausibles. En este caso, hay que mirar los resultados de las versiones robustas para tomar la decisión final:Sólo el estadistico vinculado al modelo de rezago, RLMlag, resulta significativo, es decir, se rechaza la hipótesis nula de que \\(\\rho=0\\); en tanto, para el caso del estadístico vinculado al modelo de error, RLMerr, podemos rechazar la hipótesis nula de que \\(\\lambda=0\\), así, la mejor alternativa espacial al modelo de MCO es el modelo de rezago. Revisa de nueva cuenta la figura 5.3 para mejor entender nuestra decisión.Finalmente, vamos estimar ambos modelos, aunque sabiendas de que el mejor es el modelo de error, lo usual es presentar los resultados de ambos.","code":"\n#Instalar dos paquetes\ninstall.packages(c(\"spdep\", \"spatialreg\" ))\n#Paquetes recien instalados\nlibrary(spdep)\nlibrary(spatialreg)## Warning: package 'spatialreg' was built under R version 4.1.2\n#Otros paquetes usados\nlibrary(rgdal)\nlibrary(sp)\ncovid_zmvm <-rgdal::readOGR(\"base de datos\\\\covid_zmvm shp\\\\covid_zmvm.shp\")## Warning in OGRSpatialRef(dsn, layer, morphFromESRI = morphFromESRI, dumpSRS =\n## dumpSRS, : Discarded datum D_unknown in Proj4 definition: +proj=lcc +lat_0=12\n## +lon_0=-102 +lat_1=17.5 +lat_2=29.5 +x_0=2500000 +y_0=0 +ellps=GRS80 +units=m\n## +no_defs## OGR data source with driver: ESRI Shapefile \n## Source: \"C:\\Users\\Jarvis\\Desktop\\Analisis-de-datos-espaciales\\base de datos\\covid_zmvm shp\\covid_zmvm.shp\", layer: \"covid_zmvm\"\n## with 76 features\n## It has 55 fields\nmTRUE <- spdep::poly2nb(covid_zmvm)\nmTRUE.pesos<-spdep::nb2listw(mTRUE)\nplot(covid_zmvm)\nplot(mTRUE, coordinates(covid_zmvm), add=TRUE, col=\"blue\")\nspdep::moran.test(covid_zmvm$pos_hab, mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  covid_zmvm$pos_hab  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = 8.8625, p-value < 2.2e-16\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##       0.656334129      -0.013333333       0.005709563\nspdep::moran.plot((covid_zmvm$pos_hab-mean(covid_zmvm$pos_hab))/sd(covid_zmvm$pos_hab), listw = mTRUE.pesos, xlab=\"Casos positivos por cada 100 mil habitab¿ntes\", ylab=\"Promedio de casos positivos en regiones vecinas\", main=\"Diagrama de Moran de los casos positivos en el Valle de México\", col=\"blue\")\nmodelo_mco <- stats::lm (formula=pos_hab ~ ss, data = covid_zmvm)\nsummary(modelo_mco)## \n## Call:\n## stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.4391 -2.7744 -0.6709  1.5324 14.9969 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -19.862      5.809  -3.419  0.00102 ** \n## ss            39.490      8.516   4.637 1.49e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.572 on 74 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2147 \n## F-statistic:  21.5 on 1 and 74 DF,  p-value: 1.488e-05\nspdep::moran.test(modelo_mco$residuals, mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  modelo_mco$residuals  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = 6.9825, p-value = 1.45e-12\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##       0.512928287      -0.013333333       0.005680411\nspdep::lm.LMtests(model=modelo_mco, listw=mTRUE.pesos, test=c(\"LMlag\",\"LMerr\", \"RLMlag\",\"RLMerr\"))## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## LMlag = 55.313, df = 1, p-value = 1.028e-13\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## LMerr = 42.733, df = 1, p-value = 6.275e-11\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## RLMlag = 13.358, df = 1, p-value = 0.0002573\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: stats::lm(formula = pos_hab ~ ss, data = covid_zmvm)\n## weights: mTRUE.pesos\n## \n## RLMerr = 0.77786, df = 1, p-value = 0.3778"},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"modelos-espaciales","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.3 Modelos espaciales","text":"","code":""},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"modelo-de-rezago-espacial","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.3.1 Modelo de rezago espacial","text":"El modelo de rezago espacial se invoca con al función lagsarlm() del paquete spatialreg en la que la estimación corre cargo del método de máxima verosimilitud. En la función se especifica el modelo (formula=), la base de datos (data=) y la estructura espacial (listw=).De la salida debemos llamar la atención sobre los coeficientes de interés: \\(\\rho\\), el coeficiente de autocorrelación espacial que en este caso es positivo, significativo y alto, 0.83, esto significa que cuando aumenta en 1 el número de casos positivos en los municipios y alcaldías vecinas, el número de casos positivos por COVID19 por cada 1 mil habitantes en el municipio de interés aumentará en 0.83.De forma semejante, los errores pueden ser visualizados, guardados y evaluados para ver si presentan autocorrelación:EjercicioCon base en los resultados de la de Moran, ¿diría que persite el problema de autocorrelación espacial en los errores en el modelo de rezago?","code":"\nlibrary(spatialreg)\nrezago <- spatialreg::lagsarlm(formula=pos_hab ~ ss, data=covid_zmvm, listw=mTRUE.pesos)\nsummary(rezago)## \n## Call:spatialreg::lagsarlm(formula = pos_hab ~ ss, data = covid_zmvm, \n##     listw = mTRUE.pesos)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -5.72501 -1.71081 -0.31629  1.12054 11.64378 \n## \n## Type: lag \n## Coefficients: (asymptotic standard errors) \n##             Estimate Std. Error z value Pr(>|z|)\n## (Intercept)  -8.8906     3.4221 -2.5980 0.009377\n## ss           14.9811     5.1378  2.9159 0.003547\n## \n## Rho: 0.83629, LR test value: 62.655, p-value: 2.4425e-15\n## Asymptotic standard error: 0.056938\n##     z-value: 14.688, p-value: < 2.22e-16\n## Wald statistic: 215.73, p-value: < 2.22e-16\n## \n## Log likelihood: -191.0071 for lag model\n## ML residual variance (sigma squared): 7.1381, (sigma: 2.6717)\n## Number of observations: 76 \n## Number of parameters estimated: 4 \n## AIC: 390.01, (AIC for lm: 450.67)\n## LM test for residual autocorrelation\n## test value: 0.10973, p-value: 0.74045\nspdep::moran.test(rezago[[\"residuals\"]], mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  rezago[[\"residuals\"]]  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = -0.030802, p-value = 0.5123\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##      -0.015615667      -0.013333333       0.005490469"},{"path":"análisis-espacial-ii-modelos-econométricos-espaciales.html","id":"modelo-de-error-espacial","chapter":"5 Análisis espacial II: modelos econométricos espaciales","heading":"5.3.2 Modelo de error espacial","text":"El modelo de error espacial es llamado con la función errorsarlm() y los argumentos son los mismos:Aquí, el parámetro de interés estimado es \\(\\lambda\\). Verifique si se ha resuelto el problema de la autocorrelación espacial en los errores:Con estos resultados hemos completado la ruta más elemental para la estimación de modelos econométricos espaciales. Sin embargo, hay sólo otras tantas alternativas de modelación, tal y como apuntamos en la figura 5.1. Además, hay una serie de tópicos que quedan fuera de estas notas, tales como la modelación de heterogeneidad espacial (violación del supuesto de homoscedasticidad), así como la incorporación de la dimensión temporal nuetro análisis, través de los modelos de panel espacial. Cada uno de estos temas implica un estudio sistemático por sí sólo. Esperamos que estas notas contribuyan alimentar tu curiosidad sobre el tratamiento y análisis de información espacial y que sean un primer paso en tu formación como profesionista especialista en economía urbana y regional.","code":"\nerror <-  spatialreg::errorsarlm(formula=pos_hab ~ ss, data=covid_zmvm, listw=mTRUE.pesos)\nsummary(error)## \n## Call:spatialreg::errorsarlm(formula = pos_hab ~ ss, data = covid_zmvm, \n##     listw = mTRUE.pesos)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -5.80220 -1.62230 -0.39269  1.11193 11.55180 \n## \n## Type: error \n## Coefficients: (asymptotic standard errors) \n##             Estimate Std. Error z value Pr(>|z|)\n## (Intercept) -0.66237    5.03921 -0.1314  0.89543\n## ss          12.54484    6.30980  1.9882  0.04679\n## \n## Lambda: 0.87101, LR test value: 58.512, p-value: 2.0206e-14\n## Asymptotic standard error: 0.050779\n##     z-value: 17.153, p-value: < 2.22e-16\n## Wald statistic: 294.22, p-value: < 2.22e-16\n## \n## Log likelihood: -193.0788 for error model\n## ML residual variance (sigma squared): 7.3069, (sigma: 2.7031)\n## Number of observations: 76 \n## Number of parameters estimated: 4 \n## AIC: 394.16, (AIC for lm: 450.67)\nspdep::moran.test(error[[\"residuals\"]], mTRUE.pesos)## \n##  Moran I test under randomisation\n## \n## data:  error[[\"residuals\"]]  \n## weights: mTRUE.pesos    \n## \n## Moran I statistic standard deviate = 0.18736, p-value = 0.4257\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##      0.0005636417     -0.0133333333      0.0055017348"},{"path":"sobre-el-autor.html","id":"sobre-el-autor","chapter":"Sobre el autor","heading":"Sobre el autor","text":"Jaime . Prudencio Vázquez ORCIDGoogle Académico es docente universitario desde 2013 en las Área de Investigación, Métodos cuantitativos, Economía Política y Economía Regional. Cursó sus estudios de maestría y doctorado en la Universidad Nacional Autónoma de México, en el campo de Economía Urbana y Regional.Ha impartido talleres sobre análisis de datos espaciales y econometría espacial, tanto en el ámbito universitario como en el marco del Encuentro Nacional sobre Desarrollo Regional de la Asociación Mexicana de Ciencias para el Desarrollo Regional (AMECIDER).Entre 2012 y 2015 colaboró como investigador asociado en el Centro de Estudios para el Desarrollo Alternativo, . C. Ha participado, primero como becario y luego como académico, en diversos proyectos de investigación gestionados por la Coordinación de Investigación Científica de la UNAM para instituciones como INFONAVIT, BANCOMEXT y el Consejo de Investigación y Evaluación de la Política Social (CIEPS) del Estado de México.Actualmente es profesor visitante en el Departamento de Economía, Área de Relaciones Productivas en México de la Universidad Autónoma Metropolitana Unidad Azcapotzalco con actividad docente en la Licenciatura en Economía en el Área de Concentración de Economía de la Innovación: empresas, redes y territorio.Entre sus intereses de investigación se encuentran: Productividad laboral de la manufactura y desarrollo regional; Ciudades, economías de aglomeración y productividad laboral; Disparidad regiones y su medición, temas en los que cuenta con trabajos publicados.Correo electrónico: japv@azc.uam.mx. Twitter.El autor agradece Montserrat Romero Martínez (vmrm@azc.uam.mx) y Alvaro Martínez Rodríguez (amr@azc.uam.mx), ayudantes en el Área de Relaciones Productivas, quienes se encargaron respectivamente de la revisión de los materiales preliminares de este libro y de su edición para su publicación en línea con Bookdown en GitHub.Ciudad de México, enero de 2022.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
